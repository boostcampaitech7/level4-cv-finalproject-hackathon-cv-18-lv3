| distributed init (rank 0, world 2): env://
| distributed init (rank 1, world 2): env://
[rank0]:[W118 17:44:46.318466805 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W118 17:44:46.319925302 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: kihoon090. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/wandb/run-20250118_174448-dyjuiyaj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama-3.2-1B-1percent-stage1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kihoon090/audio_lm
wandb: üöÄ View run at https://wandb.ai/kihoon090/audio_lm/runs/dyjuiyaj
2025-01-18 17:44:49,075 [INFO] 
=====  Running Parameters    =====
2025-01-18 17:44:49,076 [INFO] {
    "accum_grad_iters": 1,
    "amp": true,
    "batch_size_eval": 8,
    "batch_size_train": 8,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "epoch_based": false,
    "evaluate": false,
    "exp_name": "llama-3.2-1B-1percent-stage1",
    "gpu": 0,
    "iters_per_epoch": 3000,
    "log_freq": 5,
    "num_workers": 8,
    "optims": {
        "beta2": 0.999,
        "init_lr": 3e-05,
        "max_epoch": 30,
        "min_lr": 1e-05,
        "warmup_start_lr": 1e-06,
        "warmup_steps": 3000,
        "weight_decay": 0.05
    },
    "output_dir": "outputs_stage1_only",
    "rank": 0,
    "seed": 42,
    "use_distributed": true,
    "world_size": 2
}
2025-01-18 17:44:49,076 [INFO] 
======  Dataset Attributes  ======
2025-01-18 17:44:49,077 [INFO] {
    "prefix": "/data/data",
    "test_ann_path": "/data/data/stage1_test.json",
    "train_ann_path": "/data/data/stage1_train.json",
    "valid_ann_path": "/data/data/stage1_valid.json",
    "whisper_path": "/data/ckp/whisper"
}
2025-01-18 17:44:49,077 [INFO] 
======  Model Attributes  ======
2025-01-18 17:44:49,077 [INFO] {
    "beats_path": "/data/ckp/beats/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt",
    "ckpt": "/data/ckp/salmonn/salmonn_3b_nota.pth",
    "end_sym": "</s>",
    "freeze_beats": true,
    "freeze_speech_QFormer": false,
    "freeze_speech_llama_proj": false,
    "freeze_whisper": true,
    "llama_path": "/data/ckp/llama",
    "lora": true,
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "lora_rank": 8,
    "max_txt_len": 300,
    "multi_prompt": true,
    "num_speech_query_token": 1,
    "prompt_path": "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/data/prompts/train_prompt.json",
    "prompt_template": "USER: {}\nASSISTANT:",
    "second_per_window": 0.333333,
    "second_stride": 0.333333,
    "speech_llama_proj_model": "",
    "test_prompt_path": "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/data/prompts/test_prompt.json",
    "token": "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",
    "use_speech_Qformer": true,
    "whisper_path": "/data/ckp/whisper",
    "window_level_Qformer": true
}
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  3.52it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.64it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.62it/s]
2025-01-18 17:44:51,332 [INFO] Loading LLaMA Tokenizer
2025-01-18 17:44:51,940 [INFO] Loading LLaMA Model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.73s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.05s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.45s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading training prompts done!
2025-01-18 17:46:30,701 [INFO] Loading LLaMA Done
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
2025-01-18 17:46:36,252 [INFO] LoRA Training
2025-01-18 17:46:36,252 [INFO] Loading Whisper Model
2025-01-18 17:46:36,999 [INFO] freeze Whisper
2025-01-18 17:46:36,999 [INFO] Loading BEATs Model
2025-01-18 17:46:37,295 [INFO] BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 0.6, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': True, 'predictor_dropout': 0.0, 'predictor_class': 527}
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
2025-01-18 17:46:40,052 [INFO] freeze BEATs
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
2025-01-18 17:46:41,680 [INFO] Loading speech LLAMA proj
Loading training prompts done!
2025-01-18 17:46:41,698 [INFO] Load SALMONN ckpt from: /data/ckp/salmonn/salmonn_3b_nota.pth
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
module.speech_query_tokens
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
module.ln_speech.weight
module.ln_speech.bias
module.ln_audio.weight
module.ln_audio.bias
module.speech_Qformer.bert.embeddings.LayerNorm.weight
module.speech_Qformer.bert.embeddings.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias
module.speech_llama_proj.weight
module.speech_llama_proj.bias
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
module.speech_query_tokens
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
module.ln_speech.weight
module.ln_speech.bias
module.ln_audio.weight
module.ln_audio.bias
module.speech_Qformer.bert.embeddings.LayerNorm.weight
module.speech_Qformer.bert.embeddings.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias
module.speech_llama_proj.weight
module.speech_llama_proj.bias
2025-01-18 17:46:45,889 [INFO] number of trainable parameters: 27498240
2025-01-18 17:46:45,898 [INFO] Training Phase
2025-01-18 17:46:45,908 [INFO] Start training epoch 0, 3000 iters per inner epoch.
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.use_amp):
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.use_amp):
Train: data epoch: [0]  [   0/3000]  eta: 2:38:10  lr: 0.000001  loss: 1.3924  time: 3.1636  data: 0.0001  max mem: 16781
Train: data epoch: [0]  [   0/3000]  eta: 2:36:57  lr: 0.000001  loss: 3.3616  time: 3.1393  data: 0.0001  max mem: 15808
Train: data epoch: [0]  [   5/3000]  eta: 1:31:57  lr: 0.000001  loss: 2.8340  time: 1.8421  data: 0.0000  max mem: 16781
Train: data epoch: [0]  [   5/3000]  eta: 1:31:41  lr: 0.000001  loss: 2.9817  time: 1.8368  data: 0.0000  max mem: 16613
Train: data epoch: [0]  [  10/3000]  eta: 1:25:38  lr: 0.000001  loss: 3.0062  time: 1.7186  data: 0.0000  max mem: 16790
Train: data epoch: [0]  [  10/3000]  eta: 1:25:29  lr: 0.000001  loss: 3.3956  time: 1.7155  data: 0.0000  max mem: 16613
Train: data epoch: [0]  [  15/3000]  eta: 1:21:50  lr: 0.000001  loss: 3.1219  time: 1.6451  data: 0.0000  max mem: 16790
Train: data epoch: [0]  [  15/3000]  eta: 1:21:44  lr: 0.000001  loss: 2.0449  time: 1.6429  data: 0.0000  max mem: 16791
Train: data epoch: [0]  [  20/3000]  eta: 1:20:39  lr: 0.000001  loss: 2.3510  time: 1.5469  data: 0.0000  max mem: 16790
Train: data epoch: [0]  [  20/3000]  eta: 1:20:33  lr: 0.000001  loss: 4.2005  time: 1.5463  data: 0.0000  max mem: 16791
Train: data epoch: [0]  [  25/3000]  eta: 1:19:33  lr: 0.000001  loss: 2.5994  time: 1.5334  data: 0.0000  max mem: 16790
Train: data epoch: [0]  [  25/3000]  eta: 1:19:29  lr: 0.000001  loss: 2.6028  time: 1.5330  data: 0.0000  max mem: 17000
Train: data epoch: [0]  [  30/3000]  eta: 1:18:39  lr: 0.000001  loss: 1.5971  time: 1.5178  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  30/3000]  eta: 1:18:35  lr: 0.000001  loss: 3.4828  time: 1.5175  data: 0.0000  max mem: 17000
Train: data epoch: [0]  [  35/3000]  eta: 1:17:48  lr: 0.000001  loss: 3.4269  time: 1.5183  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  35/3000]  eta: 1:17:45  lr: 0.000001  loss: 1.9685  time: 1.5180  data: 0.0000  max mem: 17000
Train: data epoch: [0]  [  40/3000]  eta: 1:16:58  lr: 0.000001  loss: 1.7886  time: 1.4935  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  40/3000]  eta: 1:16:55  lr: 0.000001  loss: 2.0398  time: 1.4932  data: 0.0000  max mem: 17000
Train: data epoch: [0]  [  45/3000]  eta: 1:16:37  lr: 0.000001  loss: 2.6366  time: 1.4921  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  45/3000]  eta: 1:16:34  lr: 0.000001  loss: 1.8990  time: 1.4919  data: 0.0000  max mem: 17034
Train: data epoch: [0]  [  50/3000]  eta: 1:16:11  lr: 0.000001  loss: 2.9422  time: 1.4884  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  50/3000]  eta: 1:16:08  lr: 0.000001  loss: 1.6355  time: 1.4881  data: 0.0000  max mem: 17034
Train: data epoch: [0]  [  55/3000]  eta: 1:15:57  lr: 0.000002  loss: 1.8904  time: 1.4984  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  55/3000]  eta: 1:15:54  lr: 0.000002  loss: 1.3336  time: 1.4982  data: 0.0000  max mem: 17034
Train: data epoch: [0]  [  60/3000]  eta: 1:15:53  lr: 0.000002  loss: 1.3901  time: 1.5257  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  60/3000]  eta: 1:15:51  lr: 0.000002  loss: 1.8161  time: 1.5254  data: 0.0000  max mem: 17034
Train: data epoch: [0]  [  65/3000]  eta: 1:15:29  lr: 0.000002  loss: 2.5540  time: 1.5144  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  65/3000]  eta: 1:15:27  lr: 0.000002  loss: 1.5380  time: 1.5142  data: 0.0000  max mem: 17034
Train: data epoch: [0]  [  70/3000]  eta: 1:15:13  lr: 0.000002  loss: 1.6548  time: 1.5172  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  70/3000]  eta: 1:15:11  lr: 0.000002  loss: 1.1932  time: 1.5169  data: 0.0000  max mem: 17034
Train: data epoch: [0]  [  75/3000]  eta: 1:14:56  lr: 0.000002  loss: 2.0116  time: 1.5092  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  75/3000]  eta: 1:14:54  lr: 0.000002  loss: 1.8431  time: 1.5089  data: 0.0000  max mem: 17034
Train: data epoch: [0]  [  80/3000]  eta: 1:14:37  lr: 0.000002  loss: 1.2045  time: 1.4862  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  80/3000]  eta: 1:14:35  lr: 0.000002  loss: 1.5062  time: 1.4859  data: 0.0000  max mem: 17034
Train: data epoch: [0]  [  85/3000]  eta: 1:14:29  lr: 0.000002  loss: 1.2050  time: 1.4998  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  85/3000]  eta: 1:14:27  lr: 0.000002  loss: 0.9669  time: 1.4996  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [  90/3000]  eta: 1:14:20  lr: 0.000002  loss: 1.9763  time: 1.5054  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  90/3000]  eta: 1:14:18  lr: 0.000002  loss: 1.0132  time: 1.5052  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [  95/3000]  eta: 1:14:12  lr: 0.000002  loss: 0.9434  time: 1.5154  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  95/3000]  eta: 1:14:11  lr: 0.000002  loss: 1.2544  time: 1.5152  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 100/3000]  eta: 1:13:53  lr: 0.000002  loss: 2.5408  time: 1.5107  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 100/3000]  eta: 1:13:52  lr: 0.000002  loss: 0.8759  time: 1.5105  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 105/3000]  eta: 1:13:46  lr: 0.000002  loss: 1.5355  time: 1.5119  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 105/3000]  eta: 1:13:45  lr: 0.000002  loss: 1.4540  time: 1.5116  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 110/3000]  eta: 1:13:39  lr: 0.000002  loss: 1.3945  time: 1.5133  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 110/3000]  eta: 1:13:37  lr: 0.000002  loss: 0.9548  time: 1.5130  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 115/3000]  eta: 1:13:32  lr: 0.000002  loss: 0.5629  time: 1.5135  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 115/3000]  eta: 1:13:30  lr: 0.000002  loss: 1.1217  time: 1.5131  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 120/3000]  eta: 1:13:16  lr: 0.000002  loss: 0.4977  time: 1.5145  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 120/3000]  eta: 1:13:14  lr: 0.000002  loss: 1.5109  time: 1.5142  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 125/3000]  eta: 1:13:10  lr: 0.000002  loss: 1.1336  time: 1.5162  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 125/3000]  eta: 1:13:08  lr: 0.000002  loss: 0.6050  time: 1.5160  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 130/3000]  eta: 1:12:59  lr: 0.000002  loss: 0.2990  time: 1.5076  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 130/3000]  eta: 1:12:57  lr: 0.000002  loss: 0.5359  time: 1.5073  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 135/3000]  eta: 1:12:41  lr: 0.000002  loss: 0.4987  time: 1.4817  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 135/3000]  eta: 1:12:40  lr: 0.000002  loss: 0.9978  time: 1.4815  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 140/3000]  eta: 1:12:31  lr: 0.000002  loss: 0.8981  time: 1.4919  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 140/3000]  eta: 1:12:30  lr: 0.000002  loss: 0.5806  time: 1.4917  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 145/3000]  eta: 1:12:26  lr: 0.000002  loss: 0.4122  time: 1.4918  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 145/3000]  eta: 1:12:24  lr: 0.000002  loss: 0.3906  time: 1.4915  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 150/3000]  eta: 1:12:20  lr: 0.000002  loss: 0.6823  time: 1.5025  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 150/3000]  eta: 1:12:18  lr: 0.000002  loss: 0.1260  time: 1.5022  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 155/3000]  eta: 1:12:09  lr: 0.000002  loss: 0.7749  time: 1.5176  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 155/3000]  eta: 1:12:08  lr: 0.000002  loss: 1.0153  time: 1.5175  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 160/3000]  eta: 1:12:01  lr: 0.000003  loss: 0.4784  time: 1.5230  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 160/3000]  eta: 1:12:00  lr: 0.000003  loss: 0.4181  time: 1.5227  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 165/3000]  eta: 1:11:54  lr: 0.000003  loss: 0.8879  time: 1.5184  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 165/3000]  eta: 1:11:52  lr: 0.000003  loss: 1.0009  time: 1.5182  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 170/3000]  eta: 1:11:47  lr: 0.000003  loss: 0.6154  time: 1.5177  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 170/3000]  eta: 1:11:46  lr: 0.000003  loss: 0.3020  time: 1.5174  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 175/3000]  eta: 1:11:37  lr: 0.000003  loss: 0.3481  time: 1.5164  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 175/3000]  eta: 1:11:36  lr: 0.000003  loss: 0.2901  time: 1.5159  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 180/3000]  eta: 1:11:34  lr: 0.000003  loss: 0.1945  time: 1.5316  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 180/3000]  eta: 1:11:33  lr: 0.000003  loss: 0.3138  time: 1.5314  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 185/3000]  eta: 1:11:28  lr: 0.000003  loss: 1.1727  time: 1.5374  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 185/3000]  eta: 1:11:27  lr: 0.000003  loss: 0.5862  time: 1.5371  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 190/3000]  eta: 1:11:18  lr: 0.000003  loss: 0.4680  time: 1.5245  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 190/3000]  eta: 1:11:16  lr: 0.000003  loss: 0.4536  time: 1.5242  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 195/3000]  eta: 1:11:07  lr: 0.000003  loss: 0.2797  time: 1.5233  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 195/3000]  eta: 1:11:06  lr: 0.000003  loss: 0.3063  time: 1.5231  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 200/3000]  eta: 1:11:05  lr: 0.000003  loss: 0.4318  time: 1.5275  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 200/3000]  eta: 1:11:04  lr: 0.000003  loss: 1.1321  time: 1.5272  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 205/3000]  eta: 1:10:59  lr: 0.000003  loss: 0.6487  time: 1.5295  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 205/3000]  eta: 1:10:58  lr: 0.000003  loss: 0.8644  time: 1.5292  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 210/3000]  eta: 1:10:54  lr: 0.000003  loss: 1.0248  time: 1.5474  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 210/3000]  eta: 1:10:53  lr: 0.000003  loss: 0.3899  time: 1.5471  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 215/3000]  eta: 1:10:46  lr: 0.000003  loss: 1.1371  time: 1.5571  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 215/3000]  eta: 1:10:45  lr: 0.000003  loss: 0.3632  time: 1.5567  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 220/3000]  eta: 1:10:40  lr: 0.000003  loss: 0.2890  time: 1.5458  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 220/3000]  eta: 1:10:39  lr: 0.000003  loss: 1.2111  time: 1.5455  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 225/3000]  eta: 1:10:33  lr: 0.000003  loss: 0.7088  time: 1.5397  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 225/3000]  eta: 1:10:31  lr: 0.000003  loss: 0.2837  time: 1.5394  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 230/3000]  eta: 1:10:28  lr: 0.000003  loss: 0.8568  time: 1.5462  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 230/3000]  eta: 1:10:27  lr: 0.000003  loss: 0.4288  time: 1.5459  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 235/3000]  eta: 1:10:21  lr: 0.000003  loss: 0.2632  time: 1.5469  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 235/3000]  eta: 1:10:19  lr: 0.000003  loss: 1.1646  time: 1.5467  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 240/3000]  eta: 1:10:14  lr: 0.000003  loss: 0.4159  time: 1.5430  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 240/3000]  eta: 1:10:12  lr: 0.000003  loss: 0.7663  time: 1.5427  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 245/3000]  eta: 1:10:03  lr: 0.000003  loss: 0.9590  time: 1.5302  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 245/3000]  eta: 1:10:02  lr: 0.000003  loss: 0.1704  time: 1.5300  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 250/3000]  eta: 1:09:56  lr: 0.000003  loss: 0.9075  time: 1.5170  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 250/3000]  eta: 1:09:55  lr: 0.000003  loss: 0.9427  time: 1.5167  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 255/3000]  eta: 1:09:48  lr: 0.000003  loss: 0.5562  time: 1.5170  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 255/3000]  eta: 1:09:47  lr: 0.000003  loss: 0.6769  time: 1.5167  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 260/3000]  eta: 1:09:40  lr: 0.000004  loss: 0.4278  time: 1.5146  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 260/3000]  eta: 1:09:39  lr: 0.000004  loss: 0.4881  time: 1.5144  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 265/3000]  eta: 1:09:28  lr: 0.000004  loss: 0.0710  time: 1.5017  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 265/3000]  eta: 1:09:27  lr: 0.000004  loss: 0.4269  time: 1.5014  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 270/3000]  eta: 1:09:22  lr: 0.000004  loss: 0.3308  time: 1.5106  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 270/3000]  eta: 1:09:21  lr: 0.000004  loss: 0.5749  time: 1.5104  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 275/3000]  eta: 1:09:13  lr: 0.000004  loss: 0.3336  time: 1.5058  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 275/3000]  eta: 1:09:12  lr: 0.000004  loss: 0.4633  time: 1.5056  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 280/3000]  eta: 1:09:05  lr: 0.000004  loss: 0.1878  time: 1.4987  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 280/3000]  eta: 1:09:04  lr: 0.000004  loss: 0.4157  time: 1.4984  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 285/3000]  eta: 1:08:58  lr: 0.000004  loss: 0.4514  time: 1.5257  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 285/3000]  eta: 1:08:57  lr: 0.000004  loss: 0.5617  time: 1.5256  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 290/3000]  eta: 1:08:49  lr: 0.000004  loss: 0.2237  time: 1.5133  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 290/3000]  eta: 1:08:48  lr: 0.000004  loss: 0.5018  time: 1.5131  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 295/3000]  eta: 1:08:38  lr: 0.000004  loss: 0.4988  time: 1.4998  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 295/3000]  eta: 1:08:37  lr: 0.000004  loss: 0.3120  time: 1.4995  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 300/3000]  eta: 1:08:28  lr: 0.000004  loss: 0.3701  time: 1.4882  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 300/3000]  eta: 1:08:27  lr: 0.000004  loss: 0.3187  time: 1.4879  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 305/3000]  eta: 1:08:19  lr: 0.000004  loss: 0.2379  time: 1.4779  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 305/3000]  eta: 1:08:18  lr: 0.000004  loss: 0.2395  time: 1.4776  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 310/3000]  eta: 1:08:11  lr: 0.000004  loss: 0.1589  time: 1.4760  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 310/3000]  eta: 1:08:10  lr: 0.000004  loss: 0.2735  time: 1.4757  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 315/3000]  eta: 1:08:04  lr: 0.000004  loss: 0.4633  time: 1.4966  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 315/3000]  eta: 1:08:03  lr: 0.000004  loss: 0.2629  time: 1.4963  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 320/3000]  eta: 1:07:55  lr: 0.000004  loss: 0.5843  time: 1.5045  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 320/3000]  eta: 1:07:54  lr: 0.000004  loss: 0.1761  time: 1.5042  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 325/3000]  eta: 1:07:50  lr: 0.000004  loss: 0.8992  time: 1.5301  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 325/3000]  eta: 1:07:49  lr: 0.000004  loss: 0.7228  time: 1.5298  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 330/3000]  eta: 1:07:40  lr: 0.000004  loss: 0.4853  time: 1.5209  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 330/3000]  eta: 1:07:39  lr: 0.000004  loss: 0.5101  time: 1.5206  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 335/3000]  eta: 1:07:33  lr: 0.000004  loss: 0.3536  time: 1.5215  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 335/3000]  eta: 1:07:32  lr: 0.000004  loss: 1.0760  time: 1.5212  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 340/3000]  eta: 1:07:27  lr: 0.000004  loss: 0.5300  time: 1.5417  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 340/3000]  eta: 1:07:26  lr: 0.000004  loss: 0.6217  time: 1.5414  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 345/3000]  eta: 1:07:21  lr: 0.000004  loss: 0.7742  time: 1.5333  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 345/3000]  eta: 1:07:20  lr: 0.000004  loss: 0.5700  time: 1.5328  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 350/3000]  eta: 1:07:12  lr: 0.000004  loss: 0.7007  time: 1.5332  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 350/3000]  eta: 1:07:11  lr: 0.000004  loss: 0.5475  time: 1.5328  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 355/3000]  eta: 1:07:06  lr: 0.000004  loss: 0.3894  time: 1.5445  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 355/3000]  eta: 1:07:05  lr: 0.000004  loss: 1.0516  time: 1.5441  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 360/3000]  eta: 1:06:58  lr: 0.000004  loss: 0.2254  time: 1.5298  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 360/3000]  eta: 1:06:57  lr: 0.000004  loss: 1.0816  time: 1.5294  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 365/3000]  eta: 1:06:53  lr: 0.000005  loss: 0.4129  time: 1.5340  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 365/3000]  eta: 1:06:52  lr: 0.000005  loss: 0.2944  time: 1.5337  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 370/3000]  eta: 1:06:44  lr: 0.000005  loss: 0.9662  time: 1.5374  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 370/3000]  eta: 1:06:43  lr: 0.000005  loss: 0.4562  time: 1.5371  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 375/3000]  eta: 1:06:36  lr: 0.000005  loss: 0.5763  time: 1.5268  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 375/3000]  eta: 1:06:36  lr: 0.000005  loss: 0.5468  time: 1.5265  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 380/3000]  eta: 1:06:28  lr: 0.000005  loss: 0.0963  time: 1.5261  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 380/3000]  eta: 1:06:27  lr: 0.000005  loss: 0.9968  time: 1.5259  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 385/3000]  eta: 1:06:21  lr: 0.000005  loss: 0.2431  time: 1.5147  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 385/3000]  eta: 1:06:20  lr: 0.000005  loss: 0.5485  time: 1.5145  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 390/3000]  eta: 1:06:14  lr: 0.000005  loss: 0.4972  time: 1.5283  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 390/3000]  eta: 1:06:13  lr: 0.000005  loss: 1.0653  time: 1.5280  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 395/3000]  eta: 1:06:04  lr: 0.000005  loss: 0.3124  time: 1.5051  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 395/3000]  eta: 1:06:03  lr: 0.000005  loss: 0.3920  time: 1.5048  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 400/3000]  eta: 1:05:56  lr: 0.000005  loss: 0.0863  time: 1.5054  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 400/3000]  eta: 1:05:55  lr: 0.000005  loss: 0.4291  time: 1.5051  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 405/3000]  eta: 1:05:48  lr: 0.000005  loss: 0.1046  time: 1.4997  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 405/3000]  eta: 1:05:47  lr: 0.000005  loss: 0.4533  time: 1.4994  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 410/3000]  eta: 1:05:41  lr: 0.000005  loss: 0.6142  time: 1.4998  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 410/3000]  eta: 1:05:40  lr: 0.000005  loss: 0.7654  time: 1.4995  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 415/3000]  eta: 1:05:33  lr: 0.000005  loss: 0.7850  time: 1.5157  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 415/3000]  eta: 1:05:32  lr: 0.000005  loss: 0.4562  time: 1.5154  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 420/3000]  eta: 1:05:26  lr: 0.000005  loss: 0.2219  time: 1.5257  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 420/3000]  eta: 1:05:25  lr: 0.000005  loss: 1.2791  time: 1.5255  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 425/3000]  eta: 1:05:16  lr: 0.000005  loss: 0.2265  time: 1.5159  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 425/3000]  eta: 1:05:16  lr: 0.000005  loss: 0.1091  time: 1.5156  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 430/3000]  eta: 1:05:09  lr: 0.000005  loss: 0.8980  time: 1.5158  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 430/3000]  eta: 1:05:09  lr: 0.000005  loss: 0.4927  time: 1.5156  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 435/3000]  eta: 1:05:02  lr: 0.000005  loss: 0.2511  time: 1.5191  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 435/3000]  eta: 1:05:01  lr: 0.000005  loss: 0.2056  time: 1.5190  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 440/3000]  eta: 1:04:54  lr: 0.000005  loss: 0.2560  time: 1.5108  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 440/3000]  eta: 1:04:53  lr: 0.000005  loss: 0.1101  time: 1.5105  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 445/3000]  eta: 1:04:47  lr: 0.000005  loss: 0.3205  time: 1.5262  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 445/3000]  eta: 1:04:46  lr: 0.000005  loss: 0.2148  time: 1.5259  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 450/3000]  eta: 1:04:39  lr: 0.000005  loss: 0.8332  time: 1.5227  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 450/3000]  eta: 1:04:38  lr: 0.000005  loss: 0.7151  time: 1.5224  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 455/3000]  eta: 1:04:31  lr: 0.000005  loss: 0.5383  time: 1.5179  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 455/3000]  eta: 1:04:30  lr: 0.000005  loss: 0.9633  time: 1.5177  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 460/3000]  eta: 1:04:26  lr: 0.000005  loss: 0.3447  time: 1.5408  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 460/3000]  eta: 1:04:25  lr: 0.000005  loss: 0.1456  time: 1.5405  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 465/3000]  eta: 1:04:19  lr: 0.000005  loss: 0.4385  time: 1.5471  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 465/3000]  eta: 1:04:18  lr: 0.000005  loss: 0.1412  time: 1.5468  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 470/3000]  eta: 1:04:10  lr: 0.000006  loss: 0.3499  time: 1.5349  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 470/3000]  eta: 1:04:09  lr: 0.000006  loss: 0.2347  time: 1.5347  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 475/3000]  eta: 1:04:00  lr: 0.000006  loss: 0.2606  time: 1.5169  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 475/3000]  eta: 1:03:59  lr: 0.000006  loss: 0.1294  time: 1.5167  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 480/3000]  eta: 1:03:52  lr: 0.000006  loss: 0.3652  time: 1.4910  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 480/3000]  eta: 1:03:51  lr: 0.000006  loss: 0.2968  time: 1.4907  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 485/3000]  eta: 1:03:45  lr: 0.000006  loss: 0.4195  time: 1.4856  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 485/3000]  eta: 1:03:44  lr: 0.000006  loss: 0.1703  time: 1.4854  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 490/3000]  eta: 1:03:38  lr: 0.000006  loss: 0.7489  time: 1.5086  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 490/3000]  eta: 1:03:37  lr: 0.000006  loss: 0.1736  time: 1.5083  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 495/3000]  eta: 1:03:32  lr: 0.000006  loss: 0.9394  time: 1.5438  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 495/3000]  eta: 1:03:31  lr: 0.000006  loss: 0.4232  time: 1.5432  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 500/3000]  eta: 1:03:26  lr: 0.000006  loss: 0.4894  time: 1.5627  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 500/3000]  eta: 1:03:25  lr: 0.000006  loss: 0.9279  time: 1.5622  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 505/3000]  eta: 1:03:17  lr: 0.000006  loss: 0.0821  time: 1.5470  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 505/3000]  eta: 1:03:16  lr: 0.000006  loss: 0.1928  time: 1.5466  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 510/3000]  eta: 1:03:08  lr: 0.000006  loss: 0.4321  time: 1.5192  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 510/3000]  eta: 1:03:07  lr: 0.000006  loss: 0.2768  time: 1.5188  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 515/3000]  eta: 1:03:00  lr: 0.000006  loss: 0.4972  time: 1.5017  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 515/3000]  eta: 1:02:59  lr: 0.000006  loss: 0.2853  time: 1.5015  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 520/3000]  eta: 1:02:51  lr: 0.000006  loss: 1.0563  time: 1.4815  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 520/3000]  eta: 1:02:51  lr: 0.000006  loss: 0.1981  time: 1.4813  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 525/3000]  eta: 1:02:44  lr: 0.000006  loss: 0.4758  time: 1.4929  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 525/3000]  eta: 1:02:43  lr: 0.000006  loss: 0.8181  time: 1.4926  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 530/3000]  eta: 1:02:36  lr: 0.000006  loss: 0.8947  time: 1.5127  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 530/3000]  eta: 1:02:36  lr: 0.000006  loss: 0.1187  time: 1.5124  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 535/3000]  eta: 1:02:28  lr: 0.000006  loss: 0.2593  time: 1.5096  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 535/3000]  eta: 1:02:27  lr: 0.000006  loss: 1.1586  time: 1.5091  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 540/3000]  eta: 1:02:21  lr: 0.000006  loss: 0.2555  time: 1.5178  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 540/3000]  eta: 1:02:20  lr: 0.000006  loss: 0.5281  time: 1.5173  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 545/3000]  eta: 1:02:14  lr: 0.000006  loss: 0.4084  time: 1.5285  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 545/3000]  eta: 1:02:13  lr: 0.000006  loss: 0.7654  time: 1.5281  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 550/3000]  eta: 1:02:07  lr: 0.000006  loss: 0.3192  time: 1.5264  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 550/3000]  eta: 1:02:06  lr: 0.000006  loss: 0.4126  time: 1.5260  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 555/3000]  eta: 1:01:56  lr: 0.000006  loss: 0.3223  time: 1.4989  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 555/3000]  eta: 1:01:55  lr: 0.000006  loss: 0.3957  time: 1.4987  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 560/3000]  eta: 1:01:48  lr: 0.000006  loss: 0.6688  time: 1.4922  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 560/3000]  eta: 1:01:47  lr: 0.000006  loss: 0.2963  time: 1.4920  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 565/3000]  eta: 1:01:42  lr: 0.000006  loss: 0.5198  time: 1.4979  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 565/3000]  eta: 1:01:41  lr: 0.000006  loss: 1.0946  time: 1.4976  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 570/3000]  eta: 1:01:35  lr: 0.000007  loss: 1.2428  time: 1.5092  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 570/3000]  eta: 1:01:34  lr: 0.000007  loss: 0.7245  time: 1.5089  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 575/3000]  eta: 1:01:28  lr: 0.000007  loss: 0.1104  time: 1.5466  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 575/3000]  eta: 1:01:27  lr: 0.000007  loss: 0.6541  time: 1.5462  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 580/3000]  eta: 1:01:20  lr: 0.000007  loss: 0.4771  time: 1.5513  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 580/3000]  eta: 1:01:19  lr: 0.000007  loss: 0.3830  time: 1.5510  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 585/3000]  eta: 1:01:11  lr: 0.000007  loss: 0.2185  time: 1.5145  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 585/3000]  eta: 1:01:10  lr: 0.000007  loss: 0.7556  time: 1.5142  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 590/3000]  eta: 1:01:02  lr: 0.000007  loss: 0.2062  time: 1.4843  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 590/3000]  eta: 1:01:01  lr: 0.000007  loss: 0.3226  time: 1.4839  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 595/3000]  eta: 1:00:55  lr: 0.000007  loss: 0.7409  time: 1.4911  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 595/3000]  eta: 1:00:54  lr: 0.000007  loss: 0.5146  time: 1.4907  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 600/3000]  eta: 1:00:48  lr: 0.000007  loss: 0.3366  time: 1.5007  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 600/3000]  eta: 1:00:47  lr: 0.000007  loss: 0.1271  time: 1.5004  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 605/3000]  eta: 1:00:41  lr: 0.000007  loss: 0.3843  time: 1.5235  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 605/3000]  eta: 1:00:40  lr: 0.000007  loss: 0.3670  time: 1.5233  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 610/3000]  eta: 1:00:33  lr: 0.000007  loss: 0.4322  time: 1.5436  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 610/3000]  eta: 1:00:32  lr: 0.000007  loss: 0.2707  time: 1.5434  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 615/3000]  eta: 1:00:26  lr: 0.000007  loss: 0.2266  time: 1.5389  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 615/3000]  eta: 1:00:25  lr: 0.000007  loss: 0.4215  time: 1.5387  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 620/3000]  eta: 1:00:19  lr: 0.000007  loss: 0.5590  time: 1.5362  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 620/3000]  eta: 1:00:18  lr: 0.000007  loss: 0.1711  time: 1.5359  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 625/3000]  eta: 1:00:11  lr: 0.000007  loss: 0.1005  time: 1.5284  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 625/3000]  eta: 1:00:10  lr: 0.000007  loss: 0.2132  time: 1.5281  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 630/3000]  eta: 1:00:03  lr: 0.000007  loss: 0.4654  time: 1.5289  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 630/3000]  eta: 1:00:03  lr: 0.000007  loss: 0.9877  time: 1.5287  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 635/3000]  eta: 0:59:56  lr: 0.000007  loss: 0.2107  time: 1.5299  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 635/3000]  eta: 0:59:55  lr: 0.000007  loss: 0.1131  time: 1.5296  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 640/3000]  eta: 0:59:47  lr: 0.000007  loss: 0.2753  time: 1.5009  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 640/3000]  eta: 0:59:46  lr: 0.000007  loss: 0.2545  time: 1.5007  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 645/3000]  eta: 0:59:40  lr: 0.000007  loss: 0.5264  time: 1.5204  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 645/3000]  eta: 0:59:40  lr: 0.000007  loss: 0.1258  time: 1.5201  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 650/3000]  eta: 0:59:33  lr: 0.000007  loss: 0.7160  time: 1.5204  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 650/3000]  eta: 0:59:32  lr: 0.000007  loss: 0.3667  time: 1.5201  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 655/3000]  eta: 0:59:24  lr: 0.000007  loss: 0.4606  time: 1.5015  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 655/3000]  eta: 0:59:24  lr: 0.000007  loss: 0.4710  time: 1.5013  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 660/3000]  eta: 0:59:17  lr: 0.000007  loss: 1.0145  time: 1.5262  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 660/3000]  eta: 0:59:16  lr: 0.000007  loss: 0.1626  time: 1.5259  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 665/3000]  eta: 0:59:09  lr: 0.000007  loss: 0.2926  time: 1.5020  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 665/3000]  eta: 0:59:08  lr: 0.000007  loss: 0.3419  time: 1.5016  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 670/3000]  eta: 0:59:01  lr: 0.000007  loss: 0.2155  time: 1.4962  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 670/3000]  eta: 0:59:00  lr: 0.000007  loss: 0.2818  time: 1.4957  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 675/3000]  eta: 0:58:53  lr: 0.000008  loss: 0.2515  time: 1.5062  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 675/3000]  eta: 0:58:52  lr: 0.000008  loss: 0.2746  time: 1.5058  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 680/3000]  eta: 0:58:46  lr: 0.000008  loss: 0.2759  time: 1.5086  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 680/3000]  eta: 0:58:45  lr: 0.000008  loss: 0.4611  time: 1.5083  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 685/3000]  eta: 0:58:38  lr: 0.000008  loss: 0.2856  time: 1.5153  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 685/3000]  eta: 0:58:37  lr: 0.000008  loss: 0.6103  time: 1.5150  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 690/3000]  eta: 0:58:30  lr: 0.000008  loss: 0.3057  time: 1.5173  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 690/3000]  eta: 0:58:30  lr: 0.000008  loss: 0.2785  time: 1.5170  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 695/3000]  eta: 0:58:22  lr: 0.000008  loss: 0.3472  time: 1.5044  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 695/3000]  eta: 0:58:21  lr: 0.000008  loss: 0.2689  time: 1.5042  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 700/3000]  eta: 0:58:13  lr: 0.000008  loss: 0.4179  time: 1.4864  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 700/3000]  eta: 0:58:12  lr: 0.000008  loss: 0.1676  time: 1.4862  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 705/3000]  eta: 0:58:05  lr: 0.000008  loss: 0.2611  time: 1.4821  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 705/3000]  eta: 0:58:04  lr: 0.000008  loss: 0.4763  time: 1.4818  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 710/3000]  eta: 0:57:58  lr: 0.000008  loss: 0.4398  time: 1.4839  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 710/3000]  eta: 0:57:57  lr: 0.000008  loss: 0.2094  time: 1.4837  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 715/3000]  eta: 0:57:49  lr: 0.000008  loss: 0.1142  time: 1.4815  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 715/3000]  eta: 0:57:48  lr: 0.000008  loss: 0.4363  time: 1.4812  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 720/3000]  eta: 0:57:41  lr: 0.000008  loss: 1.1168  time: 1.4878  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 720/3000]  eta: 0:57:40  lr: 0.000008  loss: 0.4019  time: 1.4876  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 725/3000]  eta: 0:57:32  lr: 0.000008  loss: 0.1445  time: 1.4746  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 725/3000]  eta: 0:57:31  lr: 0.000008  loss: 0.4047  time: 1.4744  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 730/3000]  eta: 0:57:24  lr: 0.000008  loss: 0.9027  time: 1.4616  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 730/3000]  eta: 0:57:23  lr: 0.000008  loss: 0.3345  time: 1.4613  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 735/3000]  eta: 0:57:17  lr: 0.000008  loss: 0.4242  time: 1.4944  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 735/3000]  eta: 0:57:16  lr: 0.000008  loss: 0.2579  time: 1.4940  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 740/3000]  eta: 0:57:08  lr: 0.000008  loss: 0.1097  time: 1.4798  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 740/3000]  eta: 0:57:07  lr: 0.000008  loss: 0.6654  time: 1.4789  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 745/3000]  eta: 0:57:02  lr: 0.000008  loss: 0.7808  time: 1.5183  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 745/3000]  eta: 0:57:01  lr: 0.000008  loss: 0.1397  time: 1.5174  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 750/3000]  eta: 0:56:55  lr: 0.000008  loss: 0.1781  time: 1.5420  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 750/3000]  eta: 0:56:54  lr: 0.000008  loss: 0.5473  time: 1.5413  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 755/3000]  eta: 0:56:48  lr: 0.000008  loss: 1.3440  time: 1.5368  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 755/3000]  eta: 0:56:47  lr: 0.000008  loss: 0.4811  time: 1.5361  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 760/3000]  eta: 0:56:40  lr: 0.000008  loss: 0.7387  time: 1.5629  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 760/3000]  eta: 0:56:40  lr: 0.000008  loss: 0.4870  time: 1.5627  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 765/3000]  eta: 0:56:34  lr: 0.000008  loss: 0.5197  time: 1.5606  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 765/3000]  eta: 0:56:33  lr: 0.000008  loss: 0.7205  time: 1.5604  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 770/3000]  eta: 0:56:25  lr: 0.000008  loss: 0.8519  time: 1.5303  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 770/3000]  eta: 0:56:24  lr: 0.000008  loss: 0.7359  time: 1.5300  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 775/3000]  eta: 0:56:17  lr: 0.000008  loss: 0.3081  time: 1.5183  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 775/3000]  eta: 0:56:17  lr: 0.000008  loss: 0.3772  time: 1.5180  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 780/3000]  eta: 0:56:11  lr: 0.000009  loss: 0.4648  time: 1.5297  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 780/3000]  eta: 0:56:10  lr: 0.000009  loss: 1.0756  time: 1.5294  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 785/3000]  eta: 0:56:04  lr: 0.000009  loss: 0.3866  time: 1.5282  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 785/3000]  eta: 0:56:03  lr: 0.000009  loss: 0.2445  time: 1.5279  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 790/3000]  eta: 0:55:56  lr: 0.000009  loss: 0.3632  time: 1.5398  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 790/3000]  eta: 0:55:55  lr: 0.000009  loss: 0.2820  time: 1.5395  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 795/3000]  eta: 0:55:49  lr: 0.000009  loss: 0.3670  time: 1.5606  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 795/3000]  eta: 0:55:49  lr: 0.000009  loss: 0.7615  time: 1.5603  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 800/3000]  eta: 0:55:42  lr: 0.000009  loss: 0.2929  time: 1.5417  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 800/3000]  eta: 0:55:41  lr: 0.000009  loss: 0.2534  time: 1.5415  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 805/3000]  eta: 0:55:34  lr: 0.000009  loss: 0.3227  time: 1.5316  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 805/3000]  eta: 0:55:33  lr: 0.000009  loss: 0.3282  time: 1.5314  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 810/3000]  eta: 0:55:27  lr: 0.000009  loss: 0.5513  time: 1.5537  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 810/3000]  eta: 0:55:27  lr: 0.000009  loss: 0.5123  time: 1.5534  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 815/3000]  eta: 0:55:20  lr: 0.000009  loss: 0.3212  time: 1.5388  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 815/3000]  eta: 0:55:19  lr: 0.000009  loss: 0.4610  time: 1.5385  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 820/3000]  eta: 0:55:12  lr: 0.000009  loss: 0.6776  time: 1.5401  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 820/3000]  eta: 0:55:12  lr: 0.000009  loss: 0.2068  time: 1.5398  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 825/3000]  eta: 0:55:05  lr: 0.000009  loss: 0.4793  time: 1.5444  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 825/3000]  eta: 0:55:04  lr: 0.000009  loss: 0.1779  time: 1.5441  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 830/3000]  eta: 0:54:57  lr: 0.000009  loss: 0.6057  time: 1.5177  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 830/3000]  eta: 0:54:56  lr: 0.000009  loss: 0.8985  time: 1.5174  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 835/3000]  eta: 0:54:50  lr: 0.000009  loss: 0.3463  time: 1.5218  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 835/3000]  eta: 0:54:49  lr: 0.000009  loss: 0.4714  time: 1.5216  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 840/3000]  eta: 0:54:42  lr: 0.000009  loss: 0.2894  time: 1.5192  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 840/3000]  eta: 0:54:41  lr: 0.000009  loss: 0.1330  time: 1.5189  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 845/3000]  eta: 0:54:34  lr: 0.000009  loss: 0.3761  time: 1.5097  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 845/3000]  eta: 0:54:34  lr: 0.000009  loss: 0.5506  time: 1.5094  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 850/3000]  eta: 0:54:26  lr: 0.000009  loss: 0.0891  time: 1.5145  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 850/3000]  eta: 0:54:26  lr: 0.000009  loss: 0.3091  time: 1.5142  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 855/3000]  eta: 0:54:19  lr: 0.000009  loss: 0.4091  time: 1.5055  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 855/3000]  eta: 0:54:18  lr: 0.000009  loss: 0.2942  time: 1.5053  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 860/3000]  eta: 0:54:11  lr: 0.000009  loss: 0.2926  time: 1.5039  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 860/3000]  eta: 0:54:10  lr: 0.000009  loss: 0.3384  time: 1.5037  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 865/3000]  eta: 0:54:03  lr: 0.000009  loss: 0.1952  time: 1.4983  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 865/3000]  eta: 0:54:02  lr: 0.000009  loss: 0.7203  time: 1.4980  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 870/3000]  eta: 0:53:55  lr: 0.000009  loss: 0.4548  time: 1.4899  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 870/3000]  eta: 0:53:54  lr: 0.000009  loss: 0.2008  time: 1.4897  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 875/3000]  eta: 0:53:46  lr: 0.000009  loss: 0.9553  time: 1.4820  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 875/3000]  eta: 0:53:46  lr: 0.000009  loss: 1.0235  time: 1.4817  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 880/3000]  eta: 0:53:40  lr: 0.000010  loss: 0.6388  time: 1.5044  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 880/3000]  eta: 0:53:39  lr: 0.000010  loss: 0.1595  time: 1.5042  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 885/3000]  eta: 0:53:32  lr: 0.000010  loss: 0.2005  time: 1.5171  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 885/3000]  eta: 0:53:32  lr: 0.000010  loss: 0.0980  time: 1.5168  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 890/3000]  eta: 0:53:24  lr: 0.000010  loss: 0.3410  time: 1.5243  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 890/3000]  eta: 0:53:24  lr: 0.000010  loss: 0.1011  time: 1.5240  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 895/3000]  eta: 0:53:18  lr: 0.000010  loss: 0.5195  time: 1.5490  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 895/3000]  eta: 0:53:17  lr: 0.000010  loss: 0.3627  time: 1.5488  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 900/3000]  eta: 0:53:10  lr: 0.000010  loss: 0.4414  time: 1.5441  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 900/3000]  eta: 0:53:10  lr: 0.000010  loss: 0.2709  time: 1.5438  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 905/3000]  eta: 0:53:02  lr: 0.000010  loss: 0.3534  time: 1.5228  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 905/3000]  eta: 0:53:01  lr: 0.000010  loss: 0.5923  time: 1.5226  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 910/3000]  eta: 0:52:55  lr: 0.000010  loss: 0.2484  time: 1.5383  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 910/3000]  eta: 0:52:54  lr: 0.000010  loss: 0.5683  time: 1.5380  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 915/3000]  eta: 0:52:47  lr: 0.000010  loss: 0.6276  time: 1.5182  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 915/3000]  eta: 0:52:46  lr: 0.000010  loss: 0.4320  time: 1.5180  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 920/3000]  eta: 0:52:39  lr: 0.000010  loss: 0.1853  time: 1.5032  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 920/3000]  eta: 0:52:39  lr: 0.000010  loss: 0.0891  time: 1.5029  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 925/3000]  eta: 0:52:32  lr: 0.000010  loss: 0.5468  time: 1.5261  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 925/3000]  eta: 0:52:31  lr: 0.000010  loss: 0.6778  time: 1.5259  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 930/3000]  eta: 0:52:25  lr: 0.000010  loss: 0.2833  time: 1.5248  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 930/3000]  eta: 0:52:24  lr: 0.000010  loss: 0.4266  time: 1.5245  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 935/3000]  eta: 0:52:18  lr: 0.000010  loss: 0.4606  time: 1.5378  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 935/3000]  eta: 0:52:17  lr: 0.000010  loss: 0.4057  time: 1.5376  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 940/3000]  eta: 0:52:10  lr: 0.000010  loss: 0.3690  time: 1.5367  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 940/3000]  eta: 0:52:09  lr: 0.000010  loss: 0.4242  time: 1.5364  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 945/3000]  eta: 0:52:02  lr: 0.000010  loss: 0.2806  time: 1.5287  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 945/3000]  eta: 0:52:01  lr: 0.000010  loss: 0.2442  time: 1.5284  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 950/3000]  eta: 0:51:55  lr: 0.000010  loss: 0.3818  time: 1.5369  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 950/3000]  eta: 0:51:54  lr: 0.000010  loss: 0.5430  time: 1.5366  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 955/3000]  eta: 0:51:47  lr: 0.000010  loss: 0.4322  time: 1.5195  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 955/3000]  eta: 0:51:46  lr: 0.000010  loss: 0.4239  time: 1.5192  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 960/3000]  eta: 0:51:39  lr: 0.000010  loss: 0.3715  time: 1.5197  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 960/3000]  eta: 0:51:39  lr: 0.000010  loss: 0.4224  time: 1.5194  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 965/3000]  eta: 0:51:31  lr: 0.000010  loss: 0.3521  time: 1.5060  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 965/3000]  eta: 0:51:30  lr: 0.000010  loss: 0.1962  time: 1.5057  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 970/3000]  eta: 0:51:24  lr: 0.000010  loss: 0.4234  time: 1.5084  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 970/3000]  eta: 0:51:24  lr: 0.000010  loss: 0.4977  time: 1.5082  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 975/3000]  eta: 0:51:17  lr: 0.000010  loss: 0.5285  time: 1.5201  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 975/3000]  eta: 0:51:16  lr: 0.000010  loss: 0.2108  time: 1.5198  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 980/3000]  eta: 0:51:09  lr: 0.000010  loss: 0.1682  time: 1.5207  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 980/3000]  eta: 0:51:08  lr: 0.000010  loss: 0.1602  time: 1.5205  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 985/3000]  eta: 0:51:02  lr: 0.000011  loss: 0.3831  time: 1.5394  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 985/3000]  eta: 0:51:01  lr: 0.000011  loss: 0.9316  time: 1.5392  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 990/3000]  eta: 0:50:54  lr: 0.000011  loss: 0.3430  time: 1.5264  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 990/3000]  eta: 0:50:53  lr: 0.000011  loss: 0.3044  time: 1.5260  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 995/3000]  eta: 0:50:46  lr: 0.000011  loss: 0.3862  time: 1.5127  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 995/3000]  eta: 0:50:45  lr: 0.000011  loss: 0.2649  time: 1.5123  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1000/3000]  eta: 0:50:38  lr: 0.000011  loss: 0.1545  time: 1.5098  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1000/3000]  eta: 0:50:38  lr: 0.000011  loss: 0.3036  time: 1.5094  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1005/3000]  eta: 0:50:31  lr: 0.000011  loss: 0.8424  time: 1.5122  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1005/3000]  eta: 0:50:30  lr: 0.000011  loss: 0.5958  time: 1.5118  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1010/3000]  eta: 0:50:23  lr: 0.000011  loss: 0.6858  time: 1.5107  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1010/3000]  eta: 0:50:23  lr: 0.000011  loss: 0.1765  time: 1.5105  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1015/3000]  eta: 0:50:15  lr: 0.000011  loss: 0.1748  time: 1.5090  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1015/3000]  eta: 0:50:15  lr: 0.000011  loss: 0.1809  time: 1.5088  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1020/3000]  eta: 0:50:08  lr: 0.000011  loss: 0.4370  time: 1.5338  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1020/3000]  eta: 0:50:08  lr: 0.000011  loss: 0.1752  time: 1.5335  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1025/3000]  eta: 0:50:01  lr: 0.000011  loss: 0.1809  time: 1.5383  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1025/3000]  eta: 0:50:01  lr: 0.000011  loss: 0.4706  time: 1.5380  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1030/3000]  eta: 0:49:54  lr: 0.000011  loss: 0.3047  time: 1.5346  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1030/3000]  eta: 0:49:53  lr: 0.000011  loss: 1.3890  time: 1.5343  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1035/3000]  eta: 0:49:45  lr: 0.000011  loss: 0.1314  time: 1.5288  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1035/3000]  eta: 0:49:45  lr: 0.000011  loss: 0.5040  time: 1.5296  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1040/3000]  eta: 0:49:37  lr: 0.000011  loss: 0.1163  time: 1.5052  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1040/3000]  eta: 0:49:37  lr: 0.000011  loss: 0.5827  time: 1.5049  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1045/3000]  eta: 0:49:30  lr: 0.000011  loss: 0.6556  time: 1.5081  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1045/3000]  eta: 0:49:30  lr: 0.000011  loss: 0.2914  time: 1.5077  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1050/3000]  eta: 0:49:23  lr: 0.000011  loss: 0.0758  time: 1.5114  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1050/3000]  eta: 0:49:22  lr: 0.000011  loss: 0.2239  time: 1.5112  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1055/3000]  eta: 0:49:14  lr: 0.000011  loss: 0.5548  time: 1.5147  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1055/3000]  eta: 0:49:15  lr: 0.000011  loss: 0.4813  time: 1.5161  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1060/3000]  eta: 0:49:07  lr: 0.000011  loss: 0.5128  time: 1.5072  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1060/3000]  eta: 0:49:06  lr: 0.000011  loss: 0.3162  time: 1.5070  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1065/3000]  eta: 0:48:59  lr: 0.000011  loss: 0.3610  time: 1.4938  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1065/3000]  eta: 0:48:58  lr: 0.000011  loss: 0.4314  time: 1.4937  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1070/3000]  eta: 0:48:52  lr: 0.000011  loss: 0.4515  time: 1.5071  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1070/3000]  eta: 0:48:51  lr: 0.000011  loss: 0.3615  time: 1.5068  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1075/3000]  eta: 0:48:45  lr: 0.000011  loss: 0.3601  time: 1.5267  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1075/3000]  eta: 0:48:44  lr: 0.000011  loss: 0.3708  time: 1.5264  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1080/3000]  eta: 0:48:37  lr: 0.000011  loss: 0.1821  time: 1.5305  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1080/3000]  eta: 0:48:36  lr: 0.000011  loss: 0.4108  time: 1.5302  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1085/3000]  eta: 0:48:28  lr: 0.000011  loss: 0.5236  time: 1.5123  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1085/3000]  eta: 0:48:28  lr: 0.000011  loss: 0.1977  time: 1.5119  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1090/3000]  eta: 0:48:21  lr: 0.000012  loss: 0.2731  time: 1.5006  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1090/3000]  eta: 0:48:20  lr: 0.000012  loss: 0.6137  time: 1.5003  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1095/3000]  eta: 0:48:14  lr: 0.000012  loss: 0.7515  time: 1.4991  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1095/3000]  eta: 0:48:13  lr: 0.000012  loss: 1.0924  time: 1.4988  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1100/3000]  eta: 0:48:06  lr: 0.000012  loss: 0.5114  time: 1.5105  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1100/3000]  eta: 0:48:05  lr: 0.000012  loss: 0.4595  time: 1.5102  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1105/3000]  eta: 0:47:58  lr: 0.000012  loss: 0.6021  time: 1.5168  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1105/3000]  eta: 0:47:57  lr: 0.000012  loss: 0.3820  time: 1.5165  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1110/3000]  eta: 0:47:50  lr: 0.000012  loss: 0.6908  time: 1.4986  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1110/3000]  eta: 0:47:49  lr: 0.000012  loss: 0.2684  time: 1.4983  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1115/3000]  eta: 0:47:42  lr: 0.000012  loss: 0.0714  time: 1.4802  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1115/3000]  eta: 0:47:41  lr: 0.000012  loss: 0.4182  time: 1.4799  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1120/3000]  eta: 0:47:34  lr: 0.000012  loss: 0.2589  time: 1.4658  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1120/3000]  eta: 0:47:33  lr: 0.000012  loss: 0.2373  time: 1.4656  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1125/3000]  eta: 0:47:26  lr: 0.000012  loss: 1.0797  time: 1.4858  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1125/3000]  eta: 0:47:26  lr: 0.000012  loss: 0.4677  time: 1.4855  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1130/3000]  eta: 0:47:19  lr: 0.000012  loss: 0.2905  time: 1.5083  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1130/3000]  eta: 0:47:19  lr: 0.000012  loss: 0.2170  time: 1.5080  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1135/3000]  eta: 0:47:11  lr: 0.000012  loss: 0.6695  time: 1.5030  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1135/3000]  eta: 0:47:10  lr: 0.000012  loss: 0.8408  time: 1.5028  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1140/3000]  eta: 0:47:03  lr: 0.000012  loss: 0.5647  time: 1.5066  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1140/3000]  eta: 0:47:02  lr: 0.000012  loss: 0.9046  time: 1.5064  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1145/3000]  eta: 0:46:55  lr: 0.000012  loss: 0.2826  time: 1.4967  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1145/3000]  eta: 0:46:55  lr: 0.000012  loss: 0.3443  time: 1.4965  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1150/3000]  eta: 0:46:48  lr: 0.000012  loss: 0.6821  time: 1.4839  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1150/3000]  eta: 0:46:47  lr: 0.000012  loss: 0.2353  time: 1.4836  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1155/3000]  eta: 0:46:39  lr: 0.000012  loss: 0.2056  time: 1.4833  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1155/3000]  eta: 0:46:39  lr: 0.000012  loss: 0.1970  time: 1.4830  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1160/3000]  eta: 0:46:32  lr: 0.000012  loss: 0.3644  time: 1.4901  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1160/3000]  eta: 0:46:31  lr: 0.000012  loss: 0.7075  time: 1.4898  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1165/3000]  eta: 0:46:24  lr: 0.000012  loss: 0.2964  time: 1.4955  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1165/3000]  eta: 0:46:24  lr: 0.000012  loss: 0.2247  time: 1.4954  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1170/3000]  eta: 0:46:17  lr: 0.000012  loss: 0.3064  time: 1.5098  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1170/3000]  eta: 0:46:16  lr: 0.000012  loss: 0.4614  time: 1.5096  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1175/3000]  eta: 0:46:09  lr: 0.000012  loss: 0.2223  time: 1.5237  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1175/3000]  eta: 0:46:09  lr: 0.000012  loss: 0.2260  time: 1.5235  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1180/3000]  eta: 0:46:02  lr: 0.000012  loss: 0.8712  time: 1.5370  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1180/3000]  eta: 0:46:01  lr: 0.000012  loss: 0.1492  time: 1.5368  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1185/3000]  eta: 0:45:55  lr: 0.000012  loss: 0.2817  time: 1.5355  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1185/3000]  eta: 0:45:54  lr: 0.000012  loss: 0.5412  time: 1.5352  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1190/3000]  eta: 0:45:47  lr: 0.000013  loss: 0.4069  time: 1.5201  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1190/3000]  eta: 0:45:46  lr: 0.000013  loss: 0.3367  time: 1.5198  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1195/3000]  eta: 0:45:39  lr: 0.000013  loss: 0.5245  time: 1.5103  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1195/3000]  eta: 0:45:38  lr: 0.000013  loss: 0.5146  time: 1.5100  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1200/3000]  eta: 0:45:31  lr: 0.000013  loss: 0.1454  time: 1.4895  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1200/3000]  eta: 0:45:30  lr: 0.000013  loss: 0.3431  time: 1.4893  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1205/3000]  eta: 0:45:23  lr: 0.000013  loss: 1.2387  time: 1.4791  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1205/3000]  eta: 0:45:22  lr: 0.000013  loss: 0.1242  time: 1.4789  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1210/3000]  eta: 0:45:15  lr: 0.000013  loss: 0.1164  time: 1.4803  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1210/3000]  eta: 0:45:15  lr: 0.000013  loss: 0.3600  time: 1.4801  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1215/3000]  eta: 0:45:07  lr: 0.000013  loss: 0.5109  time: 1.4863  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1215/3000]  eta: 0:45:07  lr: 0.000013  loss: 0.3333  time: 1.4861  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1220/3000]  eta: 0:45:00  lr: 0.000013  loss: 0.7336  time: 1.4922  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1220/3000]  eta: 0:44:59  lr: 0.000013  loss: 0.4909  time: 1.4919  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1225/3000]  eta: 0:44:52  lr: 0.000013  loss: 0.4469  time: 1.4808  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1225/3000]  eta: 0:44:51  lr: 0.000013  loss: 0.2299  time: 1.4805  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1230/3000]  eta: 0:44:44  lr: 0.000013  loss: 0.7886  time: 1.4956  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1230/3000]  eta: 0:44:44  lr: 0.000013  loss: 0.6876  time: 1.4954  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1235/3000]  eta: 0:44:37  lr: 0.000013  loss: 0.4503  time: 1.5059  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1235/3000]  eta: 0:44:36  lr: 0.000013  loss: 0.2471  time: 1.5056  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1240/3000]  eta: 0:44:29  lr: 0.000013  loss: 0.2562  time: 1.5200  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1240/3000]  eta: 0:44:29  lr: 0.000013  loss: 0.3638  time: 1.5197  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1245/3000]  eta: 0:44:22  lr: 0.000013  loss: 0.7980  time: 1.5513  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1245/3000]  eta: 0:44:22  lr: 0.000013  loss: 0.2506  time: 1.5510  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1250/3000]  eta: 0:44:15  lr: 0.000013  loss: 0.6363  time: 1.5451  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1250/3000]  eta: 0:44:14  lr: 0.000013  loss: 0.3047  time: 1.5448  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1255/3000]  eta: 0:44:07  lr: 0.000013  loss: 0.6356  time: 1.5504  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1255/3000]  eta: 0:44:07  lr: 0.000013  loss: 0.1228  time: 1.5501  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1260/3000]  eta: 0:44:00  lr: 0.000013  loss: 0.4437  time: 1.5363  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1260/3000]  eta: 0:43:59  lr: 0.000013  loss: 0.2452  time: 1.5360  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1265/3000]  eta: 0:43:52  lr: 0.000013  loss: 0.3078  time: 1.5357  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1265/3000]  eta: 0:43:52  lr: 0.000013  loss: 0.3988  time: 1.5355  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1270/3000]  eta: 0:43:45  lr: 0.000013  loss: 0.8551  time: 1.5417  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1270/3000]  eta: 0:43:45  lr: 0.000013  loss: 0.5489  time: 1.5415  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1275/3000]  eta: 0:43:38  lr: 0.000013  loss: 0.4673  time: 1.5503  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1275/3000]  eta: 0:43:37  lr: 0.000013  loss: 0.3371  time: 1.5500  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1280/3000]  eta: 0:43:30  lr: 0.000013  loss: 0.2346  time: 1.5532  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1280/3000]  eta: 0:43:30  lr: 0.000013  loss: 0.8497  time: 1.5530  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1285/3000]  eta: 0:43:23  lr: 0.000013  loss: 0.1923  time: 1.5382  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1285/3000]  eta: 0:43:22  lr: 0.000013  loss: 0.5424  time: 1.5380  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1290/3000]  eta: 0:43:15  lr: 0.000013  loss: 0.4408  time: 1.5325  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1290/3000]  eta: 0:43:15  lr: 0.000013  loss: 0.1748  time: 1.5323  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1295/3000]  eta: 0:43:08  lr: 0.000014  loss: 0.2627  time: 1.5177  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1295/3000]  eta: 0:43:07  lr: 0.000014  loss: 0.6288  time: 1.5175  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1300/3000]  eta: 0:43:00  lr: 0.000014  loss: 0.6587  time: 1.5102  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1300/3000]  eta: 0:42:59  lr: 0.000014  loss: 0.3976  time: 1.5098  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1305/3000]  eta: 0:42:52  lr: 0.000014  loss: 0.2975  time: 1.4920  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1305/3000]  eta: 0:42:51  lr: 0.000014  loss: 0.6709  time: 1.4917  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1310/3000]  eta: 0:42:44  lr: 0.000014  loss: 0.2752  time: 1.4970  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1310/3000]  eta: 0:42:44  lr: 0.000014  loss: 0.4497  time: 1.4966  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1315/3000]  eta: 0:42:37  lr: 0.000014  loss: 0.3182  time: 1.5120  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1315/3000]  eta: 0:42:37  lr: 0.000014  loss: 0.1257  time: 1.5116  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1320/3000]  eta: 0:42:29  lr: 0.000014  loss: 0.3527  time: 1.5181  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1320/3000]  eta: 0:42:29  lr: 0.000014  loss: 0.3142  time: 1.5179  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1325/3000]  eta: 0:42:22  lr: 0.000014  loss: 0.2114  time: 1.5328  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1325/3000]  eta: 0:42:21  lr: 0.000014  loss: 0.0812  time: 1.5326  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1330/3000]  eta: 0:42:14  lr: 0.000014  loss: 0.1064  time: 1.5201  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1330/3000]  eta: 0:42:13  lr: 0.000014  loss: 0.4208  time: 1.5198  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1335/3000]  eta: 0:42:06  lr: 0.000014  loss: 0.5287  time: 1.5085  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1335/3000]  eta: 0:42:06  lr: 0.000014  loss: 0.4120  time: 1.5082  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1340/3000]  eta: 0:41:59  lr: 0.000014  loss: 0.4557  time: 1.5162  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1340/3000]  eta: 0:41:58  lr: 0.000014  loss: 0.2671  time: 1.5159  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1345/3000]  eta: 0:41:51  lr: 0.000014  loss: 0.5240  time: 1.4986  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1345/3000]  eta: 0:41:50  lr: 0.000014  loss: 0.6454  time: 1.4984  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1350/3000]  eta: 0:41:42  lr: 0.000014  loss: 0.4718  time: 1.4672  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1350/3000]  eta: 0:41:42  lr: 0.000014  loss: 0.9199  time: 1.4670  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1355/3000]  eta: 0:41:35  lr: 0.000014  loss: 0.6253  time: 1.4592  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1355/3000]  eta: 0:41:34  lr: 0.000014  loss: 0.2180  time: 1.4589  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1360/3000]  eta: 0:41:27  lr: 0.000014  loss: 0.5425  time: 1.4421  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1360/3000]  eta: 0:41:26  lr: 0.000014  loss: 0.0522  time: 1.4418  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1365/3000]  eta: 0:41:19  lr: 0.000014  loss: 0.3894  time: 1.4606  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1365/3000]  eta: 0:41:19  lr: 0.000014  loss: 0.1351  time: 1.4601  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1370/3000]  eta: 0:41:12  lr: 0.000014  loss: 0.2622  time: 1.5025  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1370/3000]  eta: 0:41:11  lr: 0.000014  loss: 0.1563  time: 1.5020  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1375/3000]  eta: 0:41:04  lr: 0.000014  loss: 1.1724  time: 1.4861  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1375/3000]  eta: 0:41:03  lr: 0.000014  loss: 0.1783  time: 1.4856  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1380/3000]  eta: 0:40:56  lr: 0.000014  loss: 0.0931  time: 1.4839  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1380/3000]  eta: 0:40:55  lr: 0.000014  loss: 0.3467  time: 1.4835  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1385/3000]  eta: 0:40:48  lr: 0.000014  loss: 0.3525  time: 1.4850  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1385/3000]  eta: 0:40:48  lr: 0.000014  loss: 0.2979  time: 1.4848  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1390/3000]  eta: 0:40:41  lr: 0.000014  loss: 0.4976  time: 1.4882  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1390/3000]  eta: 0:40:40  lr: 0.000014  loss: 0.2456  time: 1.4879  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1395/3000]  eta: 0:40:33  lr: 0.000014  loss: 0.2160  time: 1.5076  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1395/3000]  eta: 0:40:33  lr: 0.000014  loss: 0.9240  time: 1.5074  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1400/3000]  eta: 0:40:25  lr: 0.000015  loss: 0.3884  time: 1.5176  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1400/3000]  eta: 0:40:25  lr: 0.000015  loss: 0.4153  time: 1.5174  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1405/3000]  eta: 0:40:17  lr: 0.000015  loss: 0.3133  time: 1.5066  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1405/3000]  eta: 0:40:17  lr: 0.000015  loss: 0.2100  time: 1.5064  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1410/3000]  eta: 0:40:10  lr: 0.000015  loss: 0.0936  time: 1.4949  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1410/3000]  eta: 0:40:09  lr: 0.000015  loss: 0.3842  time: 1.4947  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1415/3000]  eta: 0:40:02  lr: 0.000015  loss: 0.8059  time: 1.4796  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1415/3000]  eta: 0:40:01  lr: 0.000015  loss: 0.4773  time: 1.4793  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1420/3000]  eta: 0:39:54  lr: 0.000015  loss: 0.6853  time: 1.4788  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1420/3000]  eta: 0:39:54  lr: 0.000015  loss: 0.6999  time: 1.4785  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1425/3000]  eta: 0:39:47  lr: 0.000015  loss: 0.2554  time: 1.4992  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1425/3000]  eta: 0:39:46  lr: 0.000015  loss: 0.7707  time: 1.4990  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1430/3000]  eta: 0:39:39  lr: 0.000015  loss: 0.5548  time: 1.4878  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1430/3000]  eta: 0:39:38  lr: 0.000015  loss: 0.2668  time: 1.4875  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1435/3000]  eta: 0:39:31  lr: 0.000015  loss: 0.4400  time: 1.4920  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1435/3000]  eta: 0:39:31  lr: 0.000015  loss: 0.6225  time: 1.4918  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1440/3000]  eta: 0:39:23  lr: 0.000015  loss: 0.1877  time: 1.4855  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1440/3000]  eta: 0:39:23  lr: 0.000015  loss: 0.3875  time: 1.4853  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1445/3000]  eta: 0:39:16  lr: 0.000015  loss: 0.3053  time: 1.4776  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1445/3000]  eta: 0:39:15  lr: 0.000015  loss: 0.9015  time: 1.4773  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1450/3000]  eta: 0:39:08  lr: 0.000015  loss: 0.3817  time: 1.4908  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1450/3000]  eta: 0:39:08  lr: 0.000015  loss: 0.0702  time: 1.4905  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1455/3000]  eta: 0:39:00  lr: 0.000015  loss: 0.2914  time: 1.5011  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1455/3000]  eta: 0:39:00  lr: 0.000015  loss: 0.3960  time: 1.5007  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1460/3000]  eta: 0:38:53  lr: 0.000015  loss: 0.3054  time: 1.5182  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1460/3000]  eta: 0:38:53  lr: 0.000015  loss: 0.3721  time: 1.5178  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1465/3000]  eta: 0:38:45  lr: 0.000015  loss: 0.4550  time: 1.5040  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1465/3000]  eta: 0:38:45  lr: 0.000015  loss: 0.3437  time: 1.5038  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1470/3000]  eta: 0:38:38  lr: 0.000015  loss: 0.0751  time: 1.5198  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1470/3000]  eta: 0:38:37  lr: 0.000015  loss: 0.2029  time: 1.5196  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1475/3000]  eta: 0:38:30  lr: 0.000015  loss: 1.2188  time: 1.5166  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1475/3000]  eta: 0:38:30  lr: 0.000015  loss: 0.2162  time: 1.5164  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1480/3000]  eta: 0:38:23  lr: 0.000015  loss: 0.3956  time: 1.5115  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1480/3000]  eta: 0:38:22  lr: 0.000015  loss: 0.1635  time: 1.5113  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1485/3000]  eta: 0:38:15  lr: 0.000015  loss: 0.3090  time: 1.5245  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1485/3000]  eta: 0:38:15  lr: 0.000015  loss: 0.5827  time: 1.5243  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1490/3000]  eta: 0:38:07  lr: 0.000015  loss: 0.5244  time: 1.5051  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1490/3000]  eta: 0:38:07  lr: 0.000015  loss: 0.0645  time: 1.5047  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1495/3000]  eta: 0:38:00  lr: 0.000015  loss: 0.8614  time: 1.5140  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1495/3000]  eta: 0:37:59  lr: 0.000015  loss: 0.1573  time: 1.5137  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1500/3000]  eta: 0:37:52  lr: 0.000015  loss: 0.2380  time: 1.5077  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1500/3000]  eta: 0:37:52  lr: 0.000015  loss: 0.3815  time: 1.5075  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1505/3000]  eta: 0:37:45  lr: 0.000016  loss: 0.6114  time: 1.5178  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1505/3000]  eta: 0:37:44  lr: 0.000016  loss: 0.7625  time: 1.5175  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1510/3000]  eta: 0:37:37  lr: 0.000016  loss: 0.8694  time: 1.5213  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1510/3000]  eta: 0:37:37  lr: 0.000016  loss: 0.1928  time: 1.5210  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1515/3000]  eta: 0:37:30  lr: 0.000016  loss: 0.7335  time: 1.5227  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1515/3000]  eta: 0:37:29  lr: 0.000016  loss: 0.3205  time: 1.5225  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1520/3000]  eta: 0:37:22  lr: 0.000016  loss: 0.4722  time: 1.5418  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1520/3000]  eta: 0:37:22  lr: 0.000016  loss: 1.0469  time: 1.5416  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1525/3000]  eta: 0:37:15  lr: 0.000016  loss: 0.2238  time: 1.5340  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1525/3000]  eta: 0:37:14  lr: 0.000016  loss: 0.3137  time: 1.5337  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1530/3000]  eta: 0:37:07  lr: 0.000016  loss: 0.2224  time: 1.5360  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1530/3000]  eta: 0:37:07  lr: 0.000016  loss: 0.4464  time: 1.5358  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1535/3000]  eta: 0:37:00  lr: 0.000016  loss: 0.7045  time: 1.5276  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1535/3000]  eta: 0:36:59  lr: 0.000016  loss: 0.8790  time: 1.5273  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1540/3000]  eta: 0:36:52  lr: 0.000016  loss: 0.4827  time: 1.5299  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1540/3000]  eta: 0:36:52  lr: 0.000016  loss: 0.5088  time: 1.5296  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1545/3000]  eta: 0:36:45  lr: 0.000016  loss: 0.5004  time: 1.5361  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1545/3000]  eta: 0:36:44  lr: 0.000016  loss: 0.9582  time: 1.5359  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1550/3000]  eta: 0:36:37  lr: 0.000016  loss: 0.3790  time: 1.5348  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1550/3000]  eta: 0:36:37  lr: 0.000016  loss: 0.2789  time: 1.5345  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1555/3000]  eta: 0:36:30  lr: 0.000016  loss: 0.1231  time: 1.5264  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1555/3000]  eta: 0:36:29  lr: 0.000016  loss: 0.3320  time: 1.5262  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1560/3000]  eta: 0:36:22  lr: 0.000016  loss: 0.4895  time: 1.5129  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1560/3000]  eta: 0:36:22  lr: 0.000016  loss: 0.7904  time: 1.5126  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1565/3000]  eta: 0:36:14  lr: 0.000016  loss: 0.4697  time: 1.5070  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1565/3000]  eta: 0:36:14  lr: 0.000016  loss: 0.2122  time: 1.5069  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1570/3000]  eta: 0:36:07  lr: 0.000016  loss: 1.5465  time: 1.4904  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1570/3000]  eta: 0:36:06  lr: 0.000016  loss: 0.0551  time: 1.4902  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1575/3000]  eta: 0:35:59  lr: 0.000016  loss: 0.3088  time: 1.5047  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1575/3000]  eta: 0:35:59  lr: 0.000016  loss: 0.3514  time: 1.5045  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1580/3000]  eta: 0:35:51  lr: 0.000016  loss: 0.3471  time: 1.4983  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1580/3000]  eta: 0:35:51  lr: 0.000016  loss: 0.3459  time: 1.4982  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1585/3000]  eta: 0:35:44  lr: 0.000016  loss: 0.1606  time: 1.4913  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1585/3000]  eta: 0:35:43  lr: 0.000016  loss: 0.3403  time: 1.4910  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1590/3000]  eta: 0:35:36  lr: 0.000016  loss: 0.5345  time: 1.5001  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1590/3000]  eta: 0:35:36  lr: 0.000016  loss: 0.4744  time: 1.4999  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1595/3000]  eta: 0:35:29  lr: 0.000016  loss: 0.3666  time: 1.5077  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1595/3000]  eta: 0:35:28  lr: 0.000016  loss: 0.7671  time: 1.5075  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1600/3000]  eta: 0:35:21  lr: 0.000016  loss: 0.4258  time: 1.5104  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1600/3000]  eta: 0:35:21  lr: 0.000016  loss: 0.1901  time: 1.5101  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1605/3000]  eta: 0:35:14  lr: 0.000017  loss: 0.2222  time: 1.5299  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1605/3000]  eta: 0:35:13  lr: 0.000017  loss: 0.7142  time: 1.5297  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1610/3000]  eta: 0:35:06  lr: 0.000017  loss: 0.4010  time: 1.5418  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1610/3000]  eta: 0:35:06  lr: 0.000017  loss: 0.7074  time: 1.5414  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1615/3000]  eta: 0:34:58  lr: 0.000017  loss: 0.4981  time: 1.5129  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1615/3000]  eta: 0:34:58  lr: 0.000017  loss: 0.4095  time: 1.5127  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1620/3000]  eta: 0:34:51  lr: 0.000017  loss: 0.2477  time: 1.5054  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1620/3000]  eta: 0:34:50  lr: 0.000017  loss: 0.3582  time: 1.5051  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1625/3000]  eta: 0:34:43  lr: 0.000017  loss: 0.4243  time: 1.5038  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1625/3000]  eta: 0:34:43  lr: 0.000017  loss: 0.3881  time: 1.5035  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1630/3000]  eta: 0:34:36  lr: 0.000017  loss: 0.0707  time: 1.5019  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1630/3000]  eta: 0:34:35  lr: 0.000017  loss: 0.0896  time: 1.5017  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1635/3000]  eta: 0:34:28  lr: 0.000017  loss: 0.3347  time: 1.5063  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1635/3000]  eta: 0:34:27  lr: 0.000017  loss: 0.5945  time: 1.5061  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1640/3000]  eta: 0:34:20  lr: 0.000017  loss: 0.3236  time: 1.5091  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1640/3000]  eta: 0:34:20  lr: 0.000017  loss: 0.7328  time: 1.5089  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1645/3000]  eta: 0:34:12  lr: 0.000017  loss: 0.9971  time: 1.4917  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1645/3000]  eta: 0:34:12  lr: 0.000017  loss: 0.4816  time: 1.4915  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1650/3000]  eta: 0:34:05  lr: 0.000017  loss: 0.1356  time: 1.4795  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1650/3000]  eta: 0:34:04  lr: 0.000017  loss: 0.1796  time: 1.4792  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1655/3000]  eta: 0:33:57  lr: 0.000017  loss: 0.6226  time: 1.4838  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1655/3000]  eta: 0:33:57  lr: 0.000017  loss: 0.6995  time: 1.4837  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1660/3000]  eta: 0:33:50  lr: 0.000017  loss: 0.3887  time: 1.4989  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1660/3000]  eta: 0:33:49  lr: 0.000017  loss: 0.5463  time: 1.4986  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1665/3000]  eta: 0:33:42  lr: 0.000017  loss: 0.3045  time: 1.5133  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1665/3000]  eta: 0:33:42  lr: 0.000017  loss: 0.3328  time: 1.5130  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1670/3000]  eta: 0:33:34  lr: 0.000017  loss: 0.6865  time: 1.5047  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1670/3000]  eta: 0:33:34  lr: 0.000017  loss: 0.5850  time: 1.5044  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1675/3000]  eta: 0:33:27  lr: 0.000017  loss: 0.2919  time: 1.5248  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1675/3000]  eta: 0:33:26  lr: 0.000017  loss: 0.5063  time: 1.5245  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1680/3000]  eta: 0:33:19  lr: 0.000017  loss: 0.5370  time: 1.5275  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1680/3000]  eta: 0:33:19  lr: 0.000017  loss: 0.1188  time: 1.5272  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1685/3000]  eta: 0:33:12  lr: 0.000017  loss: 0.1832  time: 1.5374  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1685/3000]  eta: 0:33:12  lr: 0.000017  loss: 0.1137  time: 1.5372  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1690/3000]  eta: 0:33:05  lr: 0.000017  loss: 0.3002  time: 1.5577  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1690/3000]  eta: 0:33:04  lr: 0.000017  loss: 0.6199  time: 1.5574  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1695/3000]  eta: 0:32:57  lr: 0.000017  loss: 0.2281  time: 1.5517  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1695/3000]  eta: 0:32:57  lr: 0.000017  loss: 0.4091  time: 1.5515  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1700/3000]  eta: 0:32:50  lr: 0.000017  loss: 0.4559  time: 1.5381  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1700/3000]  eta: 0:32:49  lr: 0.000017  loss: 0.7199  time: 1.5379  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1705/3000]  eta: 0:32:42  lr: 0.000017  loss: 0.2182  time: 1.5118  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1705/3000]  eta: 0:32:41  lr: 0.000017  loss: 0.1261  time: 1.5116  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1710/3000]  eta: 0:32:34  lr: 0.000018  loss: 0.2895  time: 1.4928  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1710/3000]  eta: 0:32:34  lr: 0.000018  loss: 0.8716  time: 1.4924  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1715/3000]  eta: 0:32:26  lr: 0.000018  loss: 0.4391  time: 1.4846  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1715/3000]  eta: 0:32:26  lr: 0.000018  loss: 0.1032  time: 1.4849  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1720/3000]  eta: 0:32:19  lr: 0.000018  loss: 0.5026  time: 1.4895  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1720/3000]  eta: 0:32:18  lr: 0.000018  loss: 0.7856  time: 1.4893  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1725/3000]  eta: 0:32:11  lr: 0.000018  loss: 0.2145  time: 1.4942  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1725/3000]  eta: 0:32:11  lr: 0.000018  loss: 0.2158  time: 1.4938  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1730/3000]  eta: 0:32:04  lr: 0.000018  loss: 0.3315  time: 1.5072  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1730/3000]  eta: 0:32:03  lr: 0.000018  loss: 0.4938  time: 1.5070  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1735/3000]  eta: 0:31:56  lr: 0.000018  loss: 0.1116  time: 1.4994  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1735/3000]  eta: 0:31:55  lr: 0.000018  loss: 0.9369  time: 1.4984  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1740/3000]  eta: 0:31:48  lr: 0.000018  loss: 0.4159  time: 1.5012  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1740/3000]  eta: 0:31:48  lr: 0.000018  loss: 0.2289  time: 1.5008  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1745/3000]  eta: 0:31:41  lr: 0.000018  loss: 0.2701  time: 1.5203  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1745/3000]  eta: 0:31:41  lr: 0.000018  loss: 0.2621  time: 1.5201  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1750/3000]  eta: 0:31:33  lr: 0.000018  loss: 0.5365  time: 1.5261  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1750/3000]  eta: 0:31:33  lr: 0.000018  loss: 0.6084  time: 1.5259  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1755/3000]  eta: 0:31:26  lr: 0.000018  loss: 0.2984  time: 1.5390  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1755/3000]  eta: 0:31:25  lr: 0.000018  loss: 1.3529  time: 1.5388  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1760/3000]  eta: 0:31:18  lr: 0.000018  loss: 0.5291  time: 1.5450  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1760/3000]  eta: 0:31:18  lr: 0.000018  loss: 1.0383  time: 1.5447  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1765/3000]  eta: 0:31:11  lr: 0.000018  loss: 0.2804  time: 1.5337  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1765/3000]  eta: 0:31:11  lr: 0.000018  loss: 0.4255  time: 1.5335  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1770/3000]  eta: 0:31:03  lr: 0.000018  loss: 0.2239  time: 1.5148  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1770/3000]  eta: 0:31:03  lr: 0.000018  loss: 0.5758  time: 1.5145  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1775/3000]  eta: 0:30:55  lr: 0.000018  loss: 0.4135  time: 1.5100  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1775/3000]  eta: 0:30:55  lr: 0.000018  loss: 0.1869  time: 1.5097  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1780/3000]  eta: 0:30:48  lr: 0.000018  loss: 0.2720  time: 1.5040  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1780/3000]  eta: 0:30:48  lr: 0.000018  loss: 0.3802  time: 1.5038  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1785/3000]  eta: 0:30:40  lr: 0.000018  loss: 0.8002  time: 1.4936  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1785/3000]  eta: 0:30:40  lr: 0.000018  loss: 0.7456  time: 1.4933  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1790/3000]  eta: 0:30:33  lr: 0.000018  loss: 0.6265  time: 1.5121  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1790/3000]  eta: 0:30:32  lr: 0.000018  loss: 0.2129  time: 1.5118  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1795/3000]  eta: 0:30:25  lr: 0.000018  loss: 0.6529  time: 1.5017  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1795/3000]  eta: 0:30:25  lr: 0.000018  loss: 0.3184  time: 1.5014  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1800/3000]  eta: 0:30:17  lr: 0.000018  loss: 0.0802  time: 1.4909  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1800/3000]  eta: 0:30:17  lr: 0.000018  loss: 0.7188  time: 1.4907  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1805/3000]  eta: 0:30:10  lr: 0.000018  loss: 0.4138  time: 1.5021  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1805/3000]  eta: 0:30:09  lr: 0.000018  loss: 0.4090  time: 1.5019  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1810/3000]  eta: 0:30:02  lr: 0.000018  loss: 0.2373  time: 1.5000  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1810/3000]  eta: 0:30:02  lr: 0.000018  loss: 0.6579  time: 1.4998  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1815/3000]  eta: 0:29:55  lr: 0.000019  loss: 0.3839  time: 1.5100  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1815/3000]  eta: 0:29:54  lr: 0.000019  loss: 0.3881  time: 1.5098  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1820/3000]  eta: 0:29:47  lr: 0.000019  loss: 0.1715  time: 1.5158  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1820/3000]  eta: 0:29:47  lr: 0.000019  loss: 0.3403  time: 1.5155  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1825/3000]  eta: 0:29:40  lr: 0.000019  loss: 0.0759  time: 1.5240  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1825/3000]  eta: 0:29:39  lr: 0.000019  loss: 0.4913  time: 1.5238  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1830/3000]  eta: 0:29:32  lr: 0.000019  loss: 0.7555  time: 1.5227  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1830/3000]  eta: 0:29:32  lr: 0.000019  loss: 0.5410  time: 1.5225  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1835/3000]  eta: 0:29:25  lr: 0.000019  loss: 0.1157  time: 1.5392  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1835/3000]  eta: 0:29:24  lr: 0.000019  loss: 0.4462  time: 1.5390  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1840/3000]  eta: 0:29:17  lr: 0.000019  loss: 1.1343  time: 1.5310  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1840/3000]  eta: 0:29:17  lr: 0.000019  loss: 0.4467  time: 1.5308  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1845/3000]  eta: 0:29:09  lr: 0.000019  loss: 0.4811  time: 1.5185  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1845/3000]  eta: 0:29:09  lr: 0.000019  loss: 0.4756  time: 1.5182  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1850/3000]  eta: 0:29:02  lr: 0.000019  loss: 0.9577  time: 1.5335  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1850/3000]  eta: 0:29:02  lr: 0.000019  loss: 0.3261  time: 1.5332  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1855/3000]  eta: 0:28:54  lr: 0.000019  loss: 0.1615  time: 1.5167  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1855/3000]  eta: 0:28:54  lr: 0.000019  loss: 0.3774  time: 1.5163  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1860/3000]  eta: 0:28:47  lr: 0.000019  loss: 0.3618  time: 1.5292  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1860/3000]  eta: 0:28:47  lr: 0.000019  loss: 0.4284  time: 1.5289  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1865/3000]  eta: 0:28:39  lr: 0.000019  loss: 0.3229  time: 1.5445  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1865/3000]  eta: 0:28:39  lr: 0.000019  loss: 0.5021  time: 1.5441  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1870/3000]  eta: 0:28:32  lr: 0.000019  loss: 0.6375  time: 1.5453  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1870/3000]  eta: 0:28:32  lr: 0.000019  loss: 0.8131  time: 1.5450  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1875/3000]  eta: 0:28:25  lr: 0.000019  loss: 0.7702  time: 1.5551  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1875/3000]  eta: 0:28:24  lr: 0.000019  loss: 0.1090  time: 1.5548  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1880/3000]  eta: 0:28:17  lr: 0.000019  loss: 0.1112  time: 1.5432  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1880/3000]  eta: 0:28:17  lr: 0.000019  loss: 0.3300  time: 1.5428  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1885/3000]  eta: 0:28:09  lr: 0.000019  loss: 0.4587  time: 1.5320  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1885/3000]  eta: 0:28:09  lr: 0.000019  loss: 0.3099  time: 1.5316  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1890/3000]  eta: 0:28:02  lr: 0.000019  loss: 0.7619  time: 1.5092  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1890/3000]  eta: 0:28:01  lr: 0.000019  loss: 0.5091  time: 1.5089  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1895/3000]  eta: 0:27:54  lr: 0.000019  loss: 0.2091  time: 1.5146  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1895/3000]  eta: 0:27:54  lr: 0.000019  loss: 0.2096  time: 1.5144  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1900/3000]  eta: 0:27:47  lr: 0.000019  loss: 0.2904  time: 1.5263  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1900/3000]  eta: 0:27:46  lr: 0.000019  loss: 0.2164  time: 1.5262  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1905/3000]  eta: 0:27:39  lr: 0.000019  loss: 0.3868  time: 1.5234  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1905/3000]  eta: 0:27:39  lr: 0.000019  loss: 0.2403  time: 1.5232  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1910/3000]  eta: 0:27:32  lr: 0.000019  loss: 0.2795  time: 1.5372  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1910/3000]  eta: 0:27:31  lr: 0.000019  loss: 0.5357  time: 1.5370  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1915/3000]  eta: 0:27:24  lr: 0.000020  loss: 1.6886  time: 1.5364  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1915/3000]  eta: 0:27:24  lr: 0.000020  loss: 0.2903  time: 1.5362  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1920/3000]  eta: 0:27:16  lr: 0.000020  loss: 0.1013  time: 1.5155  data: 0.0000  max mem: 17768Train: data epoch: [0]  [1920/3000]  eta: 0:27:16  lr: 0.000020  loss: 0.3128  time: 1.5158  data: 0.0000  max mem: 17924

Train: data epoch: [0]  [1925/3000]  eta: 0:27:09  lr: 0.000020  loss: 0.3594  time: 1.4966  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1925/3000]  eta: 0:27:08  lr: 0.000020  loss: 0.2983  time: 1.4963  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1930/3000]  eta: 0:27:01  lr: 0.000020  loss: 0.1347  time: 1.4817  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1930/3000]  eta: 0:27:01  lr: 0.000020  loss: 0.2497  time: 1.4814  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1935/3000]  eta: 0:26:53  lr: 0.000020  loss: 0.3575  time: 1.4769  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1935/3000]  eta: 0:26:53  lr: 0.000020  loss: 0.8861  time: 1.4772  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1940/3000]  eta: 0:26:46  lr: 0.000020  loss: 0.5432  time: 1.4918  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1940/3000]  eta: 0:26:45  lr: 0.000020  loss: 0.1515  time: 1.4916  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1945/3000]  eta: 0:26:38  lr: 0.000020  loss: 1.3121  time: 1.5225  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1945/3000]  eta: 0:26:38  lr: 0.000020  loss: 0.2242  time: 1.5223  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1950/3000]  eta: 0:26:31  lr: 0.000020  loss: 0.1387  time: 1.5406  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1950/3000]  eta: 0:26:31  lr: 0.000020  loss: 0.6260  time: 1.5404  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1955/3000]  eta: 0:26:23  lr: 0.000020  loss: 0.2594  time: 1.5467  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1955/3000]  eta: 0:26:23  lr: 0.000020  loss: 0.6555  time: 1.5466  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1960/3000]  eta: 0:26:16  lr: 0.000020  loss: 0.3966  time: 1.5327  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1960/3000]  eta: 0:26:15  lr: 0.000020  loss: 0.6118  time: 1.5324  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1965/3000]  eta: 0:26:08  lr: 0.000020  loss: 0.6340  time: 1.5205  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1965/3000]  eta: 0:26:08  lr: 0.000020  loss: 0.3700  time: 1.5204  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1970/3000]  eta: 0:26:01  lr: 0.000020  loss: 0.6117  time: 1.5212  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1970/3000]  eta: 0:26:00  lr: 0.000020  loss: 1.0036  time: 1.5210  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1975/3000]  eta: 0:25:53  lr: 0.000020  loss: 0.3041  time: 1.4987  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1975/3000]  eta: 0:25:53  lr: 0.000020  loss: 0.5990  time: 1.4984  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1980/3000]  eta: 0:25:45  lr: 0.000020  loss: 0.3048  time: 1.5170  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1980/3000]  eta: 0:25:45  lr: 0.000020  loss: 0.4183  time: 1.5168  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1985/3000]  eta: 0:25:38  lr: 0.000020  loss: 0.2234  time: 1.5313  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1985/3000]  eta: 0:25:38  lr: 0.000020  loss: 0.8229  time: 1.5311  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1990/3000]  eta: 0:25:30  lr: 0.000020  loss: 1.0117  time: 1.5161  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1990/3000]  eta: 0:25:30  lr: 0.000020  loss: 0.2855  time: 1.5158  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1995/3000]  eta: 0:25:23  lr: 0.000020  loss: 0.5434  time: 1.5331  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1995/3000]  eta: 0:25:23  lr: 0.000020  loss: 0.3109  time: 1.5329  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2000/3000]  eta: 0:25:15  lr: 0.000020  loss: 0.2665  time: 1.5377  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2000/3000]  eta: 0:25:15  lr: 0.000020  loss: 0.1033  time: 1.5374  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2005/3000]  eta: 0:25:08  lr: 0.000020  loss: 0.6366  time: 1.5152  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2005/3000]  eta: 0:25:07  lr: 0.000020  loss: 0.3868  time: 1.5150  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2010/3000]  eta: 0:25:00  lr: 0.000020  loss: 0.3838  time: 1.5135  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2010/3000]  eta: 0:25:00  lr: 0.000020  loss: 0.4926  time: 1.5134  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2015/3000]  eta: 0:24:52  lr: 0.000020  loss: 0.5073  time: 1.5068  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2015/3000]  eta: 0:24:52  lr: 0.000020  loss: 0.1968  time: 1.5065  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2020/3000]  eta: 0:24:45  lr: 0.000021  loss: 0.4773  time: 1.4969  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2020/3000]  eta: 0:24:45  lr: 0.000021  loss: 0.4637  time: 1.4966  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2025/3000]  eta: 0:24:37  lr: 0.000021  loss: 0.2653  time: 1.5128  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2025/3000]  eta: 0:24:37  lr: 0.000021  loss: 0.4364  time: 1.5125  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2030/3000]  eta: 0:24:30  lr: 0.000021  loss: 0.6825  time: 1.5274  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2030/3000]  eta: 0:24:30  lr: 0.000021  loss: 0.5219  time: 1.5271  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2035/3000]  eta: 0:24:22  lr: 0.000021  loss: 0.2613  time: 1.5394  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2035/3000]  eta: 0:24:22  lr: 0.000021  loss: 0.4301  time: 1.5392  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2040/3000]  eta: 0:24:14  lr: 0.000021  loss: 0.3441  time: 1.5154  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2040/3000]  eta: 0:24:14  lr: 0.000021  loss: 0.1112  time: 1.5152  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2045/3000]  eta: 0:24:07  lr: 0.000021  loss: 0.1305  time: 1.5062  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2045/3000]  eta: 0:24:07  lr: 0.000021  loss: 0.1351  time: 1.5060  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2050/3000]  eta: 0:23:59  lr: 0.000021  loss: 0.4441  time: 1.4956  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2050/3000]  eta: 0:23:59  lr: 0.000021  loss: 0.2882  time: 1.4955  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2055/3000]  eta: 0:23:52  lr: 0.000021  loss: 0.3390  time: 1.4731  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2055/3000]  eta: 0:23:51  lr: 0.000021  loss: 0.3811  time: 1.4729  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2060/3000]  eta: 0:23:44  lr: 0.000021  loss: 0.3501  time: 1.5162  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2060/3000]  eta: 0:23:44  lr: 0.000021  loss: 0.9980  time: 1.5159  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2065/3000]  eta: 0:23:37  lr: 0.000021  loss: 0.3687  time: 1.5335  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2065/3000]  eta: 0:23:36  lr: 0.000021  loss: 0.5613  time: 1.5333  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2070/3000]  eta: 0:23:29  lr: 0.000021  loss: 0.7614  time: 1.5303  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2070/3000]  eta: 0:23:29  lr: 0.000021  loss: 0.2783  time: 1.5300  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2075/3000]  eta: 0:23:22  lr: 0.000021  loss: 0.5749  time: 1.5591  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2075/3000]  eta: 0:23:21  lr: 0.000021  loss: 0.4349  time: 1.5587  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2080/3000]  eta: 0:23:14  lr: 0.000021  loss: 0.8984  time: 1.5528  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2080/3000]  eta: 0:23:14  lr: 0.000021  loss: 0.3165  time: 1.5525  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2085/3000]  eta: 0:23:07  lr: 0.000021  loss: 0.1841  time: 1.5417  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2085/3000]  eta: 0:23:06  lr: 0.000021  loss: 0.9353  time: 1.5414  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2090/3000]  eta: 0:22:59  lr: 0.000021  loss: 0.0604  time: 1.5532  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2090/3000]  eta: 0:22:59  lr: 0.000021  loss: 0.1555  time: 1.5528  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2095/3000]  eta: 0:22:52  lr: 0.000021  loss: 0.2206  time: 1.5453  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2095/3000]  eta: 0:22:51  lr: 0.000021  loss: 0.7003  time: 1.5450  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2100/3000]  eta: 0:22:44  lr: 0.000021  loss: 0.4123  time: 1.5435  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2100/3000]  eta: 0:22:44  lr: 0.000021  loss: 0.3630  time: 1.5430  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2105/3000]  eta: 0:22:37  lr: 0.000021  loss: 0.4856  time: 1.5505  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2105/3000]  eta: 0:22:36  lr: 0.000021  loss: 0.1757  time: 1.5500  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2110/3000]  eta: 0:22:29  lr: 0.000021  loss: 0.2028  time: 1.5560  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2110/3000]  eta: 0:22:29  lr: 0.000021  loss: 1.0852  time: 1.5556  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2115/3000]  eta: 0:22:22  lr: 0.000021  loss: 0.7813  time: 1.5662  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2115/3000]  eta: 0:22:21  lr: 0.000021  loss: 0.6070  time: 1.5658  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2120/3000]  eta: 0:22:14  lr: 0.000021  loss: 0.4191  time: 1.5610  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2120/3000]  eta: 0:22:14  lr: 0.000021  loss: 0.2964  time: 1.5608  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2125/3000]  eta: 0:22:07  lr: 0.000022  loss: 0.4125  time: 1.5498  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2125/3000]  eta: 0:22:06  lr: 0.000022  loss: 0.3454  time: 1.5495  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2130/3000]  eta: 0:21:59  lr: 0.000022  loss: 0.4227  time: 1.5451  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2130/3000]  eta: 0:21:59  lr: 0.000022  loss: 0.6630  time: 1.5447  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2135/3000]  eta: 0:21:52  lr: 0.000022  loss: 0.4656  time: 1.5399  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2135/3000]  eta: 0:21:51  lr: 0.000022  loss: 0.2274  time: 1.5395  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2140/3000]  eta: 0:21:44  lr: 0.000022  loss: 0.5777  time: 1.5302  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2140/3000]  eta: 0:21:44  lr: 0.000022  loss: 0.6182  time: 1.5298  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2145/3000]  eta: 0:21:36  lr: 0.000022  loss: 0.3498  time: 1.5120  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2145/3000]  eta: 0:21:36  lr: 0.000022  loss: 0.1921  time: 1.5118  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2150/3000]  eta: 0:21:29  lr: 0.000022  loss: 0.5083  time: 1.5157  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2150/3000]  eta: 0:21:28  lr: 0.000022  loss: 0.4131  time: 1.5153  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2155/3000]  eta: 0:21:21  lr: 0.000022  loss: 0.5061  time: 1.4737  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2155/3000]  eta: 0:21:21  lr: 0.000022  loss: 0.3944  time: 1.4734  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2160/3000]  eta: 0:21:13  lr: 0.000022  loss: 0.4839  time: 1.4772  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2160/3000]  eta: 0:21:13  lr: 0.000022  loss: 0.5242  time: 1.4767  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2165/3000]  eta: 0:21:06  lr: 0.000022  loss: 0.1538  time: 1.4798  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2165/3000]  eta: 0:21:05  lr: 0.000022  loss: 0.7396  time: 1.4793  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2170/3000]  eta: 0:20:58  lr: 0.000022  loss: 0.3889  time: 1.4758  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2170/3000]  eta: 0:20:58  lr: 0.000022  loss: 0.2127  time: 1.4755  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2175/3000]  eta: 0:20:51  lr: 0.000022  loss: 0.6110  time: 1.5168  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2175/3000]  eta: 0:20:50  lr: 0.000022  loss: 0.6305  time: 1.5165  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2180/3000]  eta: 0:20:43  lr: 0.000022  loss: 0.3706  time: 1.5216  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2180/3000]  eta: 0:20:43  lr: 0.000022  loss: 0.2341  time: 1.5214  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2185/3000]  eta: 0:20:35  lr: 0.000022  loss: 0.2176  time: 1.5426  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2185/3000]  eta: 0:20:35  lr: 0.000022  loss: 0.2961  time: 1.5423  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2190/3000]  eta: 0:20:28  lr: 0.000022  loss: 0.3371  time: 1.5169  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2190/3000]  eta: 0:20:27  lr: 0.000022  loss: 0.5212  time: 1.5166  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2195/3000]  eta: 0:20:20  lr: 0.000022  loss: 0.3089  time: 1.5111  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2195/3000]  eta: 0:20:20  lr: 0.000022  loss: 0.1734  time: 1.5109  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2200/3000]  eta: 0:20:13  lr: 0.000022  loss: 0.2063  time: 1.5097  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2200/3000]  eta: 0:20:12  lr: 0.000022  loss: 0.3374  time: 1.5094  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2205/3000]  eta: 0:20:05  lr: 0.000022  loss: 0.5359  time: 1.5137  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2205/3000]  eta: 0:20:05  lr: 0.000022  loss: 0.3193  time: 1.5136  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2210/3000]  eta: 0:19:58  lr: 0.000022  loss: 0.6589  time: 1.5370  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2210/3000]  eta: 0:19:57  lr: 0.000022  loss: 0.4812  time: 1.5368  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2215/3000]  eta: 0:19:50  lr: 0.000022  loss: 0.3180  time: 1.5145  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2215/3000]  eta: 0:19:50  lr: 0.000022  loss: 0.2467  time: 1.5143  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2220/3000]  eta: 0:19:42  lr: 0.000022  loss: 0.3573  time: 1.5159  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2220/3000]  eta: 0:19:42  lr: 0.000022  loss: 0.7144  time: 1.5157  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2225/3000]  eta: 0:19:35  lr: 0.000023  loss: 0.5322  time: 1.4936  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2225/3000]  eta: 0:19:34  lr: 0.000023  loss: 0.5807  time: 1.4935  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2230/3000]  eta: 0:19:27  lr: 0.000023  loss: 0.6971  time: 1.4868  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2230/3000]  eta: 0:19:27  lr: 0.000023  loss: 0.3094  time: 1.4866  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2235/3000]  eta: 0:19:19  lr: 0.000023  loss: 0.0875  time: 1.5004  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2235/3000]  eta: 0:19:19  lr: 0.000023  loss: 0.2660  time: 1.5003  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2240/3000]  eta: 0:19:12  lr: 0.000023  loss: 0.4991  time: 1.5140  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2240/3000]  eta: 0:19:12  lr: 0.000023  loss: 0.4258  time: 1.5137  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2245/3000]  eta: 0:19:04  lr: 0.000023  loss: 0.1777  time: 1.5145  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2245/3000]  eta: 0:19:04  lr: 0.000023  loss: 0.3045  time: 1.5142  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2250/3000]  eta: 0:18:57  lr: 0.000023  loss: 0.3049  time: 1.4999  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2250/3000]  eta: 0:18:56  lr: 0.000023  loss: 0.3967  time: 1.4996  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2255/3000]  eta: 0:18:49  lr: 0.000023  loss: 0.6435  time: 1.4998  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2255/3000]  eta: 0:18:49  lr: 0.000023  loss: 0.1958  time: 1.4995  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2260/3000]  eta: 0:18:41  lr: 0.000023  loss: 0.3335  time: 1.4872  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2260/3000]  eta: 0:18:41  lr: 0.000023  loss: 0.1685  time: 1.4870  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2265/3000]  eta: 0:18:34  lr: 0.000023  loss: 0.5148  time: 1.5167  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2265/3000]  eta: 0:18:34  lr: 0.000023  loss: 0.5048  time: 1.5164  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2270/3000]  eta: 0:18:26  lr: 0.000023  loss: 0.1167  time: 1.5305  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2270/3000]  eta: 0:18:26  lr: 0.000023  loss: 0.2059  time: 1.5303  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2275/3000]  eta: 0:18:19  lr: 0.000023  loss: 0.5222  time: 1.5337  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2275/3000]  eta: 0:18:19  lr: 0.000023  loss: 0.6495  time: 1.5335  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2280/3000]  eta: 0:18:11  lr: 0.000023  loss: 0.4195  time: 1.4957  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2280/3000]  eta: 0:18:11  lr: 0.000023  loss: 0.6102  time: 1.4955  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2285/3000]  eta: 0:18:03  lr: 0.000023  loss: 1.4121  time: 1.4719  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2285/3000]  eta: 0:18:03  lr: 0.000023  loss: 0.1753  time: 1.4716  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2290/3000]  eta: 0:17:56  lr: 0.000023  loss: 0.2583  time: 1.4885  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2290/3000]  eta: 0:17:56  lr: 0.000023  loss: 0.2450  time: 1.4882  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2295/3000]  eta: 0:17:48  lr: 0.000023  loss: 0.4479  time: 1.4805  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2295/3000]  eta: 0:17:48  lr: 0.000023  loss: 0.5869  time: 1.4802  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2300/3000]  eta: 0:17:41  lr: 0.000023  loss: 0.5936  time: 1.5157  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2300/3000]  eta: 0:17:40  lr: 0.000023  loss: 0.1772  time: 1.5154  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2305/3000]  eta: 0:17:33  lr: 0.000023  loss: 1.4499  time: 1.5392  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2305/3000]  eta: 0:17:33  lr: 0.000023  loss: 0.2038  time: 1.5389  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2310/3000]  eta: 0:17:26  lr: 0.000023  loss: 0.7246  time: 1.5376  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2310/3000]  eta: 0:17:25  lr: 0.000023  loss: 0.3629  time: 1.5373  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2315/3000]  eta: 0:17:18  lr: 0.000023  loss: 0.3274  time: 1.5507  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2315/3000]  eta: 0:17:18  lr: 0.000023  loss: 0.3215  time: 1.5504  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2320/3000]  eta: 0:17:11  lr: 0.000023  loss: 0.2097  time: 1.5516  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2320/3000]  eta: 0:17:10  lr: 0.000023  loss: 1.2172  time: 1.5514  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2325/3000]  eta: 0:17:03  lr: 0.000023  loss: 0.5112  time: 1.5175  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2325/3000]  eta: 0:17:03  lr: 0.000023  loss: 0.6097  time: 1.5173  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2330/3000]  eta: 0:16:55  lr: 0.000024  loss: 0.1148  time: 1.4917  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2330/3000]  eta: 0:16:55  lr: 0.000024  loss: 0.4092  time: 1.4915  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2335/3000]  eta: 0:16:48  lr: 0.000024  loss: 0.1509  time: 1.4709  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2335/3000]  eta: 0:16:47  lr: 0.000024  loss: 0.2892  time: 1.4707  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2340/3000]  eta: 0:16:40  lr: 0.000024  loss: 0.3134  time: 1.4653  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2340/3000]  eta: 0:16:40  lr: 0.000024  loss: 0.5974  time: 1.4650  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2345/3000]  eta: 0:16:32  lr: 0.000024  loss: 0.3458  time: 1.4891  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2345/3000]  eta: 0:16:32  lr: 0.000024  loss: 0.5285  time: 1.4888  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2350/3000]  eta: 0:16:25  lr: 0.000024  loss: 0.3863  time: 1.4842  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2350/3000]  eta: 0:16:25  lr: 0.000024  loss: 0.4622  time: 1.4840  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2355/3000]  eta: 0:16:17  lr: 0.000024  loss: 0.8523  time: 1.4866  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2355/3000]  eta: 0:16:17  lr: 0.000024  loss: 0.3527  time: 1.4865  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2360/3000]  eta: 0:16:10  lr: 0.000024  loss: 0.2889  time: 1.4952  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2360/3000]  eta: 0:16:09  lr: 0.000024  loss: 0.4343  time: 1.4950  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2365/3000]  eta: 0:16:02  lr: 0.000024  loss: 0.2778  time: 1.5004  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2365/3000]  eta: 0:16:02  lr: 0.000024  loss: 1.5819  time: 1.5002  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2370/3000]  eta: 0:15:54  lr: 0.000024  loss: 1.0080  time: 1.5304  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2370/3000]  eta: 0:15:54  lr: 0.000024  loss: 0.2293  time: 1.5301  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2375/3000]  eta: 0:15:47  lr: 0.000024  loss: 0.6793  time: 1.5371  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2375/3000]  eta: 0:15:47  lr: 0.000024  loss: 0.1875  time: 1.5368  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2380/3000]  eta: 0:15:39  lr: 0.000024  loss: 0.3278  time: 1.5128  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2380/3000]  eta: 0:15:39  lr: 0.000024  loss: 0.7109  time: 1.5126  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2385/3000]  eta: 0:15:32  lr: 0.000024  loss: 0.3392  time: 1.4995  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2385/3000]  eta: 0:15:31  lr: 0.000024  loss: 0.1915  time: 1.4992  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2390/3000]  eta: 0:15:24  lr: 0.000024  loss: 1.3317  time: 1.4966  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2390/3000]  eta: 0:15:24  lr: 0.000024  loss: 0.4341  time: 1.4964  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2395/3000]  eta: 0:15:16  lr: 0.000024  loss: 0.0916  time: 1.5030  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2395/3000]  eta: 0:15:16  lr: 0.000024  loss: 0.6370  time: 1.5028  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2400/3000]  eta: 0:15:09  lr: 0.000024  loss: 0.3223  time: 1.5382  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2400/3000]  eta: 0:15:09  lr: 0.000024  loss: 0.2284  time: 1.5379  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2405/3000]  eta: 0:15:01  lr: 0.000024  loss: 0.4910  time: 1.5378  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2405/3000]  eta: 0:15:01  lr: 0.000024  loss: 0.3901  time: 1.5375  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2410/3000]  eta: 0:14:54  lr: 0.000024  loss: 0.3574  time: 1.5326  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2410/3000]  eta: 0:14:54  lr: 0.000024  loss: 0.1052  time: 1.5322  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2415/3000]  eta: 0:14:46  lr: 0.000024  loss: 0.3185  time: 1.5346  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2415/3000]  eta: 0:14:46  lr: 0.000024  loss: 0.7168  time: 1.5343  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2420/3000]  eta: 0:14:39  lr: 0.000024  loss: 0.6043  time: 1.5370  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2420/3000]  eta: 0:14:39  lr: 0.000024  loss: 0.4336  time: 1.5367  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2425/3000]  eta: 0:14:31  lr: 0.000024  loss: 0.2790  time: 1.5457  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2425/3000]  eta: 0:14:31  lr: 0.000024  loss: 0.6768  time: 1.5454  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2430/3000]  eta: 0:14:24  lr: 0.000024  loss: 0.6194  time: 1.5423  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2430/3000]  eta: 0:14:23  lr: 0.000024  loss: 0.2583  time: 1.5421  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2435/3000]  eta: 0:14:16  lr: 0.000025  loss: 0.2524  time: 1.5428  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2435/3000]  eta: 0:14:16  lr: 0.000025  loss: 0.1258  time: 1.5425  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2440/3000]  eta: 0:14:08  lr: 0.000025  loss: 0.5045  time: 1.5254  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2440/3000]  eta: 0:14:08  lr: 0.000025  loss: 0.6617  time: 1.5251  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2445/3000]  eta: 0:14:01  lr: 0.000025  loss: 0.7743  time: 1.4985  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2445/3000]  eta: 0:14:01  lr: 0.000025  loss: 0.6510  time: 1.4983  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2450/3000]  eta: 0:13:53  lr: 0.000025  loss: 0.1843  time: 1.4950  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2450/3000]  eta: 0:13:53  lr: 0.000025  loss: 0.4982  time: 1.4948  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2455/3000]  eta: 0:13:46  lr: 0.000025  loss: 0.2879  time: 1.4904  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2455/3000]  eta: 0:13:45  lr: 0.000025  loss: 0.4069  time: 1.4904  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2460/3000]  eta: 0:13:38  lr: 0.000025  loss: 0.3788  time: 1.4869  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2460/3000]  eta: 0:13:38  lr: 0.000025  loss: 0.2830  time: 1.4867  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2465/3000]  eta: 0:13:30  lr: 0.000025  loss: 0.3260  time: 1.4782  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2465/3000]  eta: 0:13:30  lr: 0.000025  loss: 0.3247  time: 1.4780  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2470/3000]  eta: 0:13:23  lr: 0.000025  loss: 0.4252  time: 1.4809  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2470/3000]  eta: 0:13:23  lr: 0.000025  loss: 0.2692  time: 1.4806  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2475/3000]  eta: 0:13:15  lr: 0.000025  loss: 0.2485  time: 1.4875  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2475/3000]  eta: 0:13:15  lr: 0.000025  loss: 0.2703  time: 1.4872  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2480/3000]  eta: 0:13:08  lr: 0.000025  loss: 0.2253  time: 1.4987  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2480/3000]  eta: 0:13:08  lr: 0.000025  loss: 0.2732  time: 1.4985  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2485/3000]  eta: 0:13:00  lr: 0.000025  loss: 0.1506  time: 1.5255  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2485/3000]  eta: 0:13:00  lr: 0.000025  loss: 0.6083  time: 1.5252  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2490/3000]  eta: 0:12:52  lr: 0.000025  loss: 0.2494  time: 1.5268  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2490/3000]  eta: 0:12:52  lr: 0.000025  loss: 0.4925  time: 1.5265  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2495/3000]  eta: 0:12:45  lr: 0.000025  loss: 0.3813  time: 1.5289  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2495/3000]  eta: 0:12:45  lr: 0.000025  loss: 0.5751  time: 1.5286  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2500/3000]  eta: 0:12:37  lr: 0.000025  loss: 0.1217  time: 1.5295  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2500/3000]  eta: 0:12:37  lr: 0.000025  loss: 0.5107  time: 1.5292  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2505/3000]  eta: 0:12:30  lr: 0.000025  loss: 0.0869  time: 1.5036  data: 0.0000  max mem: 17871Train: data epoch: [0]  [2505/3000]  eta: 0:12:30  lr: 0.000025  loss: 0.1956  time: 1.5039  data: 0.0000  max mem: 17924

Train: data epoch: [0]  [2510/3000]  eta: 0:12:22  lr: 0.000025  loss: 0.7027  time: 1.5095  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2510/3000]  eta: 0:12:22  lr: 0.000025  loss: 0.9121  time: 1.5093  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2515/3000]  eta: 0:12:15  lr: 0.000025  loss: 0.4963  time: 1.4935  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2515/3000]  eta: 0:12:14  lr: 0.000025  loss: 0.4882  time: 1.4932  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2520/3000]  eta: 0:12:07  lr: 0.000025  loss: 0.2098  time: 1.4863  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2520/3000]  eta: 0:12:07  lr: 0.000025  loss: 1.1056  time: 1.4861  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2525/3000]  eta: 0:11:59  lr: 0.000025  loss: 0.1940  time: 1.5073  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2525/3000]  eta: 0:11:59  lr: 0.000025  loss: 0.8401  time: 1.5072  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2530/3000]  eta: 0:11:52  lr: 0.000025  loss: 0.3091  time: 1.4959  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2530/3000]  eta: 0:11:52  lr: 0.000025  loss: 0.7938  time: 1.4956  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2535/3000]  eta: 0:11:44  lr: 0.000026  loss: 0.1291  time: 1.5103  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2535/3000]  eta: 0:11:44  lr: 0.000026  loss: 0.4503  time: 1.5101  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2540/3000]  eta: 0:11:37  lr: 0.000026  loss: 0.3563  time: 1.5131  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2540/3000]  eta: 0:11:37  lr: 0.000026  loss: 0.2343  time: 1.5129  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2545/3000]  eta: 0:11:29  lr: 0.000026  loss: 0.2563  time: 1.5091  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2545/3000]  eta: 0:11:29  lr: 0.000026  loss: 0.3286  time: 1.5095  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2550/3000]  eta: 0:11:21  lr: 0.000026  loss: 0.2389  time: 1.5174  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2550/3000]  eta: 0:11:21  lr: 0.000026  loss: 0.3273  time: 1.5171  data: 0.0000  max mem: 17871
W0118 18:51:18.737974 948448 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 948501 closing signal SIGTERM
E0118 18:51:19.408338 948448 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -9) local_rank: 1 (pid: 948502) of binary: /data/anaconda3/envs/salmonn_env/bin/python
Traceback (most recent call last):
  File "/data/anaconda3/envs/salmonn_env/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
train.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-01-18_18:51:18
  host      : nota-gpu-svr004
  rank      : 1 (local_rank: 1)
  exitcode  : -9 (pid: 948502)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 948502
=======================================================
| distributed init (rank 0, world 2): env://
| distributed init (rank 1, world 2): env://
[rank0]:[W118 18:52:38.183426139 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W118 18:52:38.185341242 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: kihoon090. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/wandb/run-20250118_185239-d5tiqnc4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama-3.2-1B-1percent-stage1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kihoon090/audio_lm
wandb: üöÄ View run at https://wandb.ai/kihoon090/audio_lm/runs/d5tiqnc4
2025-01-18 18:52:40,501 [INFO] 
=====  Running Parameters    =====
2025-01-18 18:52:40,502 [INFO] {
    "accum_grad_iters": 1,
    "amp": true,
    "batch_size_eval": 8,
    "batch_size_train": 8,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "epoch_based": false,
    "evaluate": false,
    "exp_name": "llama-3.2-1B-1percent-stage1",
    "gpu": 0,
    "iters_per_epoch": 20,
    "log_freq": 5,
    "num_workers": 8,
    "optims": {
        "beta2": 0.999,
        "init_lr": 3e-05,
        "max_epoch": 30,
        "min_lr": 1e-05,
        "warmup_start_lr": 1e-06,
        "warmup_steps": 3000,
        "weight_decay": 0.05
    },
    "output_dir": "outputs_stage1_only",
    "rank": 0,
    "seed": 42,
    "use_distributed": true,
    "world_size": 2
}
2025-01-18 18:52:40,503 [INFO] 
======  Dataset Attributes  ======
2025-01-18 18:52:40,503 [INFO] {
    "prefix": "/data/data",
    "test_ann_path": "/data/data/stage1_test.json",
    "train_ann_path": "/data/data/stage1_train.json",
    "valid_ann_path": "/data/data/stage1_valid.json",
    "whisper_path": "/data/ckp/whisper"
}
2025-01-18 18:52:40,503 [INFO] 
======  Model Attributes  ======
2025-01-18 18:52:40,503 [INFO] {
    "beats_path": "/data/ckp/beats/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt",
    "ckpt": "/data/ckp/salmonn/salmonn_3b_nota.pth",
    "end_sym": "</s>",
    "freeze_beats": true,
    "freeze_speech_QFormer": false,
    "freeze_speech_llama_proj": false,
    "freeze_whisper": true,
    "llama_path": "/data/ckp/llama",
    "lora": true,
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "lora_rank": 8,
    "max_txt_len": 300,
    "multi_prompt": true,
    "num_speech_query_token": 1,
    "prompt_path": "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/data/prompts/train_prompt.json",
    "prompt_template": "USER: {}\nASSISTANT:",
    "second_per_window": 0.333333,
    "second_stride": 0.333333,
    "speech_llama_proj_model": "",
    "test_prompt_path": "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/data/prompts/test_prompt.json",
    "token": "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",
    "use_speech_Qformer": true,
    "whisper_path": "/data/ckp/whisper",
    "window_level_Qformer": true
}
2025-01-18 18:52:42,788 [INFO] Loading LLaMA Tokenizer
2025-01-18 18:52:43,361 [INFO] Loading LLaMA Model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  3.54it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.68it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.65it/s]
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.87s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.12s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.53s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading training prompts done!
2025-01-18 18:54:22,309 [INFO] Loading LLaMA Done
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
2025-01-18 18:54:27,882 [INFO] LoRA Training
2025-01-18 18:54:27,882 [INFO] Loading Whisper Model
2025-01-18 18:54:28,629 [INFO] freeze Whisper
2025-01-18 18:54:28,629 [INFO] Loading BEATs Model
2025-01-18 18:54:28,910 [INFO] BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 0.6, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': True, 'predictor_dropout': 0.0, 'predictor_class': 527}
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
2025-01-18 18:54:31,678 [INFO] freeze BEATs
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
2025-01-18 18:54:33,306 [INFO] Loading speech LLAMA proj
Loading training prompts done!
2025-01-18 18:54:33,324 [INFO] Load SALMONN ckpt from: /data/ckp/salmonn/salmonn_3b_nota.pth
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
module.speech_query_tokens
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
module.ln_speech.weight
module.ln_speech.bias
module.ln_audio.weight
module.ln_audio.bias
module.speech_Qformer.bert.embeddings.LayerNorm.weight
module.speech_Qformer.bert.embeddings.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias
module.speech_llama_proj.weight
module.speech_llama_proj.bias
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
module.speech_query_tokens
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
module.ln_speech.weight
module.ln_speech.bias
module.ln_audio.weight
module.ln_audio.bias
module.speech_Qformer.bert.embeddings.LayerNorm.weight
module.speech_Qformer.bert.embeddings.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias
module.speech_llama_proj.weight
module.speech_llama_proj.bias
2025-01-18 18:54:37,589 [INFO] number of trainable parameters: 27498240
2025-01-18 18:54:37,593 [INFO] Training Phase
2025-01-18 18:54:37,602 [INFO] Start training epoch 0, 20 iters per inner epoch.
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.use_amp):
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.use_amp):
Train: data epoch: [0]  [ 0/20]  eta: 0:01:03  lr: 0.000001  loss: 1.3924  time: 3.1508  data: 0.0001  max mem: 16781
Train: data epoch: [0]  [ 0/20]  eta: 0:01:01  lr: 0.000001  loss: 3.3616  time: 3.0588  data: 0.0001  max mem: 15808
Train: data epoch: [0]  [ 5/20]  eta: 0:00:27  lr: 0.000001  loss: 2.8340  time: 1.8351  data: 0.0000  max mem: 16781
Train: data epoch: [0]  [ 5/20]  eta: 0:00:27  lr: 0.000001  loss: 2.9821  time: 1.8183  data: 0.0000  max mem: 16613
Train: data epoch: [0]  [10/20]  eta: 0:00:17  lr: 0.000001  loss: 3.3957  time: 1.7111  data: 0.0000  max mem: 16613
Train: data epoch: [0]  [10/20]  eta: 0:00:17  lr: 0.000001  loss: 3.0065  time: 1.7206  data: 0.0000  max mem: 16790
Train: data epoch: [0]  [15/20]  eta: 0:00:08  lr: 0.000001  loss: 2.0444  time: 1.6427  data: 0.0000  max mem: 16791
Train: data epoch: [0]  [15/20]  eta: 0:00:08  lr: 0.000001  loss: 3.1226  time: 1.6494  data: 0.0000  max mem: 16790
Train: data epoch: [0]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 1.7590  time: 1.6343  data: 0.0000  max mem: 16790
Train: data epoch: [0] Total time: 0:00:32 (1.6345 s / it)
Train: data epoch: [0]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 1.8912  time: 1.6291  data: 0.0000  max mem: 16791
Train: data epoch: [0] Total time: 0:00:32 (1.6296 s / it)
2025-01-18 18:55:10,196 [INFO] Averaged stats: lr: 0.0000  loss: 2.5659
2025-01-18 18:55:10,200 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:179: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.use_amp):
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:179: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.use_amp):
Eval: data epoch: [0]  [0/1]  eta: 0:00:00    time: 0.9941  data: 0.6411  max mem: 16791
Eval: data epoch: [0]  [0/1]  eta: 0:00:01    time: 1.0365  data: 0.6514  max mem: 16790
Eval: data epoch: [0] Total time: 0:00:01 (1.1382 s / it)
Eval: data epoch: [0] Total time: 0:00:01 (1.1753 s / it)
2025-01-18 18:55:11,405 [INFO] Saving checkpoint at epoch 0 to outputs_stage1_only/202501181852/checkpoint_best.pth.
2025-01-18 18:55:13,825 [INFO] Saving checkpoint at epoch 0 to outputs_stage1_only/202501181852/checkpoint_0.pth.
2025-01-18 18:55:16,233 [INFO] Training Phase
2025-01-18 18:55:16,241 [INFO] Start training epoch 1, 20 iters per inner epoch.
Train: data epoch: [1]  [ 0/20]  eta: 0:00:33  lr: 0.000001  loss: 4.1522  time: 1.6661  data: 0.0000  max mem: 16791
Train: data epoch: [1]  [ 0/20]  eta: 0:00:33  lr: 0.000001  loss: 2.3856  time: 1.6663  data: 0.0001  max mem: 16790
Train: data epoch: [1]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 2.6205  time: 1.5462  data: 0.0000  max mem: 16790
Train: data epoch: [1]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 2.5647  time: 1.5464  data: 0.0000  max mem: 16998
Train: data epoch: [1]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 3.4311  time: 1.5388  data: 0.0000  max mem: 16998Train: data epoch: [1]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 1.6210  time: 1.5391  data: 0.0000  max mem: 17399

Train: data epoch: [1]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 3.4988  time: 1.5263  data: 0.0000  max mem: 17399
Train: data epoch: [1]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 1.9881  time: 1.5261  data: 0.0000  max mem: 16998
Train: data epoch: [1]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 2.8630  time: 1.5195  data: 0.0000  max mem: 17399
Train: data epoch: [1] Total time: 0:00:30 (1.5197 s / it)
Train: data epoch: [1]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 2.1125  time: 1.5193  data: 0.0000  max mem: 16998
Train: data epoch: [1] Total time: 0:00:30 (1.5199 s / it)
2025-01-18 18:55:46,640 [INFO] Averaged stats: lr: 0.0000  loss: 2.4021
2025-01-18 18:55:46,644 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [1]  [0/1]  eta: 0:00:00    time: 0.8527  data: 0.5732  max mem: 16998
Eval: data epoch: [1]  [0/1]  eta: 0:00:00    time: 0.8808  data: 0.5930  max mem: 17399
Eval: data epoch: [1] Total time: 0:00:01 (1.0077 s / it)
Eval: data epoch: [1] Total time: 0:00:01 (1.0315 s / it)
2025-01-18 18:55:47,702 [INFO] Saving checkpoint at epoch 1 to outputs_stage1_only/202501181852/checkpoint_1.pth.
2025-01-18 18:55:50,151 [INFO] Training Phase
2025-01-18 18:55:50,158 [INFO] Start training epoch 2, 20 iters per inner epoch.
Train: data epoch: [2]  [ 0/20]  eta: 0:00:27  lr: 0.000001  loss: 1.8613  time: 1.3765  data: 0.0000  max mem: 17399
Train: data epoch: [2]  [ 0/20]  eta: 0:00:27  lr: 0.000001  loss: 2.1747  time: 1.3757  data: 0.0000  max mem: 16998
Train: data epoch: [2]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 2.7988  time: 1.5202  data: 0.0000  max mem: 17399
Train: data epoch: [2]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 2.0201  time: 1.5200  data: 0.0000  max mem: 17032
Train: data epoch: [2]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 3.1085  time: 1.5070  data: 0.0000  max mem: 17399
Train: data epoch: [2]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 1.7319  time: 1.5069  data: 0.0000  max mem: 17032
Train: data epoch: [2]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 1.9803  time: 1.5149  data: 0.0000  max mem: 17399
Train: data epoch: [2]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 1.4522  time: 1.5146  data: 0.0000  max mem: 17032
Train: data epoch: [2]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 1.1542  time: 1.5243  data: 0.0000  max mem: 17399
Train: data epoch: [2] Total time: 0:00:30 (1.5245 s / it)
Train: data epoch: [2]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 2.6611  time: 1.5240  data: 0.0000  max mem: 17032
Train: data epoch: [2] Total time: 0:00:30 (1.5246 s / it)
2025-01-18 18:56:20,651 [INFO] Averaged stats: lr: 0.0000  loss: 2.0084
2025-01-18 18:56:20,654 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [2]  [0/1]  eta: 0:00:00    time: 0.8537  data: 0.5681  max mem: 17032
Eval: data epoch: [2]  [0/1]  eta: 0:00:00    time: 0.8547  data: 0.5779  max mem: 17399
Eval: data epoch: [2] Total time: 0:00:01 (1.0004 s / it)
Eval: data epoch: [2] Total time: 0:00:01 (1.0142 s / it)
2025-01-18 18:56:21,693 [INFO] Saving checkpoint at epoch 2 to outputs_stage1_only/202501181852/checkpoint_2.pth.
2025-01-18 18:56:24,057 [INFO] Training Phase
2025-01-18 18:56:24,065 [INFO] Start training epoch 3, 20 iters per inner epoch.
Train: data epoch: [3]  [ 0/20]  eta: 0:00:31  lr: 0.000001  loss: 1.5520  time: 1.5951  data: 0.0000  max mem: 17399
Train: data epoch: [3]  [ 0/20]  eta: 0:00:31  lr: 0.000001  loss: 1.9801  time: 1.5945  data: 0.0000  max mem: 17032
Train: data epoch: [3]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 2.6631  time: 1.4943  data: 0.0000  max mem: 17399
Train: data epoch: [3]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 1.6398  time: 1.4940  data: 0.0000  max mem: 17032
Train: data epoch: [3]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 1.9284  time: 1.5015  data: 0.0000  max mem: 17399
Train: data epoch: [3]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 1.3664  time: 1.5012  data: 0.0000  max mem: 17032
Train: data epoch: [3]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 2.1754  time: 1.5009  data: 0.0000  max mem: 17399
Train: data epoch: [3]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 2.0709  time: 1.5005  data: 0.0000  max mem: 17032
Train: data epoch: [3]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 1.7145  time: 1.4913  data: 0.0000  max mem: 17399
Train: data epoch: [3] Total time: 0:00:29 (1.4916 s / it)
Train: data epoch: [3]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 3.4081  time: 1.4911  data: 0.0000  max mem: 17032
Train: data epoch: [3] Total time: 0:00:29 (1.4917 s / it)
2025-01-18 18:56:53,899 [INFO] Averaged stats: lr: 0.0000  loss: 1.8455
2025-01-18 18:56:53,903 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [3]  [0/1]  eta: 0:00:00    time: 0.8424  data: 0.5695  max mem: 17399
Eval: data epoch: [3]  [0/1]  eta: 0:00:00    time: 0.8638  data: 0.5859  max mem: 17032
Eval: data epoch: [3] Total time: 0:00:00 (0.9854 s / it)
Eval: data epoch: [3] Total time: 0:00:01 (1.0128 s / it)
2025-01-18 18:56:54,941 [INFO] Saving checkpoint at epoch 3 to outputs_stage1_only/202501181852/checkpoint_3.pth.
2025-01-18 18:56:57,362 [INFO] Training Phase
2025-01-18 18:56:57,370 [INFO] Start training epoch 4, 20 iters per inner epoch.
Train: data epoch: [4]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 1.3605  time: 1.6235  data: 0.0001  max mem: 17399
Train: data epoch: [4]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 1.7115  time: 1.6221  data: 0.0000  max mem: 17032
Train: data epoch: [4]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 1.3494  time: 1.5419  data: 0.0000  max mem: 17399
Train: data epoch: [4]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 1.1005  time: 1.5416  data: 0.0000  max mem: 17520
Train: data epoch: [4]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 2.3170  time: 1.5411  data: 0.0000  max mem: 17399
Train: data epoch: [4]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 1.1624  time: 1.5407  data: 0.0000  max mem: 17520
Train: data epoch: [4]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 1.2095  time: 1.5354  data: 0.0000  max mem: 17399
Train: data epoch: [4]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 1.4504  time: 1.5351  data: 0.0000  max mem: 17520
Train: data epoch: [4]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 1.0370  time: 1.5157  data: 0.0000  max mem: 17399
Train: data epoch: [4] Total time: 0:00:30 (1.5159 s / it)
Train: data epoch: [4]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.8341  time: 1.5154  data: 0.0000  max mem: 17520
Train: data epoch: [4] Total time: 0:00:30 (1.5159 s / it)
2025-01-18 18:57:27,690 [INFO] Averaged stats: lr: 0.0000  loss: 1.4539
2025-01-18 18:57:27,693 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [4]  [0/1]  eta: 0:00:00    time: 0.8292  data: 0.5579  max mem: 17520
Eval: data epoch: [4]  [0/1]  eta: 0:00:00    time: 0.8626  data: 0.5833  max mem: 17399
Eval: data epoch: [4] Total time: 0:00:00 (0.9633 s / it)
Eval: data epoch: [4] Total time: 0:00:00 (0.9906 s / it)
2025-01-18 18:57:28,709 [INFO] Saving checkpoint at epoch 4 to outputs_stage1_only/202501181852/checkpoint_4.pth.
2025-01-18 18:57:31,056 [INFO] Training Phase
2025-01-18 18:57:31,063 [INFO] Start training epoch 5, 20 iters per inner epoch.
Train: data epoch: [5]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 2.9034  time: 1.6015  data: 0.0000  max mem: 17399
Train: data epoch: [5]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 1.0672  time: 1.6012  data: 0.0000  max mem: 17520
Train: data epoch: [5]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 1.9652  time: 1.5501  data: 0.0000  max mem: 17399
Train: data epoch: [5]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 1.7916  time: 1.5501  data: 0.0000  max mem: 17520
Train: data epoch: [5]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 1.9223  time: 1.5444  data: 0.0000  max mem: 17399
Train: data epoch: [5]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 1.2779  time: 1.5443  data: 0.0000  max mem: 17520
Train: data epoch: [5]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.8357  time: 1.5396  data: 0.0000  max mem: 17399
Train: data epoch: [5]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 1.6365  time: 1.5395  data: 0.0000  max mem: 17520
Train: data epoch: [5]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 1.2628  time: 1.5218  data: 0.0000  max mem: 17399
Train: data epoch: [5] Total time: 0:00:30 (1.5220 s / it)
Train: data epoch: [5]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 2.3236  time: 1.5214  data: 0.0000  max mem: 17520
Train: data epoch: [5] Total time: 0:00:30 (1.5221 s / it)
2025-01-18 18:58:01,507 [INFO] Averaged stats: lr: 0.0000  loss: 1.3272
2025-01-18 18:58:01,511 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [5]  [0/1]  eta: 0:00:00    time: 0.8391  data: 0.5657  max mem: 17399
Eval: data epoch: [5]  [0/1]  eta: 0:00:00    time: 0.8652  data: 0.5786  max mem: 17520
Eval: data epoch: [5] Total time: 0:00:00 (0.9619 s / it)
Eval: data epoch: [5] Total time: 0:00:01 (1.0113 s / it)
2025-01-18 18:58:02,547 [INFO] Saving checkpoint at epoch 5 to outputs_stage1_only/202501181852/checkpoint_best.pth.
2025-01-18 18:58:10,664 [INFO] Saving checkpoint at epoch 5 to outputs_stage1_only/202501181852/checkpoint_5.pth.
2025-01-18 18:58:13,010 [INFO] Training Phase
2025-01-18 18:58:13,018 [INFO] Start training epoch 6, 20 iters per inner epoch.
Train: data epoch: [6]  [ 0/20]  eta: 0:00:28  lr: 0.000001  loss: 0.8227  time: 1.4292  data: 0.0000  max mem: 17399
Train: data epoch: [6]  [ 0/20]  eta: 0:00:28  lr: 0.000001  loss: 1.9875  time: 1.4293  data: 0.0000  max mem: 17520
Train: data epoch: [6]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 1.5109  time: 1.5223  data: 0.0000  max mem: 17399
Train: data epoch: [6]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.9939  time: 1.5221  data: 0.0000  max mem: 17520
Train: data epoch: [6]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.6963  time: 1.5176  data: 0.0000  max mem: 17399
Train: data epoch: [6]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.9115  time: 1.5175  data: 0.0000  max mem: 17520
Train: data epoch: [6]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.8844  time: 1.4882  data: 0.0000  max mem: 17399
Train: data epoch: [6]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 1.3867  time: 1.4880  data: 0.0000  max mem: 17520
Train: data epoch: [6]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 1.2250  time: 1.4973  data: 0.0000  max mem: 17399
Train: data epoch: [6] Total time: 0:00:29 (1.4975 s / it)
Train: data epoch: [6]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 1.2828  time: 1.4971  data: 0.0000  max mem: 17520
Train: data epoch: [6] Total time: 0:00:29 (1.4977 s / it)
2025-01-18 18:58:42,972 [INFO] Averaged stats: lr: 0.0000  loss: 1.2159
2025-01-18 18:58:42,977 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [6]  [0/1]  eta: 0:00:00    time: 0.8471  data: 0.5706  max mem: 17399
Eval: data epoch: [6]  [0/1]  eta: 0:00:00    time: 0.8673  data: 0.5680  max mem: 17520
Eval: data epoch: [6] Total time: 0:00:00 (0.9601 s / it)
Eval: data epoch: [6] Total time: 0:00:01 (1.0272 s / it)
2025-01-18 18:58:44,029 [INFO] Saving checkpoint at epoch 6 to outputs_stage1_only/202501181852/checkpoint_best.pth.
2025-01-18 18:58:51,397 [INFO] Saving checkpoint at epoch 6 to outputs_stage1_only/202501181852/checkpoint_6.pth.
2025-01-18 18:58:53,731 [INFO] Training Phase
2025-01-18 18:58:53,739 [INFO] Start training epoch 7, 20 iters per inner epoch.
Train: data epoch: [7]  [ 0/20]  eta: 0:00:27  lr: 0.000001  loss: 1.5703  time: 1.3774  data: 0.0000  max mem: 17399
Train: data epoch: [7]  [ 0/20]  eta: 0:00:27  lr: 0.000001  loss: 0.9823  time: 1.3776  data: 0.0000  max mem: 17520
Train: data epoch: [7]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.7625  time: 1.5297  data: 0.0000  max mem: 17399
Train: data epoch: [7]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.8162  time: 1.5297  data: 0.0000  max mem: 17520
Train: data epoch: [7]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.9745  time: 1.5284  data: 0.0000  max mem: 17399
Train: data epoch: [7]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.3564  time: 1.5281  data: 0.0000  max mem: 17520
Train: data epoch: [7]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 1.1819  time: 1.5174  data: 0.0000  max mem: 17399
Train: data epoch: [7]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 1.4391  time: 1.5172  data: 0.0000  max mem: 17520
Train: data epoch: [7]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.7328  time: 1.5169  data: 0.0000  max mem: 17399
Train: data epoch: [7] Total time: 0:00:30 (1.5171 s / it)
Train: data epoch: [7]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.7661  time: 1.5167  data: 0.0000  max mem: 17520
Train: data epoch: [7] Total time: 0:00:30 (1.5172 s / it)
2025-01-18 18:59:24,084 [INFO] Averaged stats: lr: 0.0000  loss: 1.0399
2025-01-18 18:59:24,088 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [7]  [0/1]  eta: 0:00:00    time: 0.8349  data: 0.5631  max mem: 17520
Eval: data epoch: [7]  [0/1]  eta: 0:00:00    time: 0.8398  data: 0.5696  max mem: 17399
Eval: data epoch: [7] Total time: 0:00:00 (0.9684 s / it)
Eval: data epoch: [7] Total time: 0:00:00 (0.9686 s / it)
2025-01-18 18:59:25,082 [INFO] Saving checkpoint at epoch 7 to outputs_stage1_only/202501181852/checkpoint_7.pth.
2025-01-18 18:59:27,427 [INFO] Training Phase
2025-01-18 18:59:27,435 [INFO] Start training epoch 8, 20 iters per inner epoch.
Train: data epoch: [8]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 0.8369  time: 1.6335  data: 0.0000  max mem: 17399
Train: data epoch: [8]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 0.6994  time: 1.6332  data: 0.0000  max mem: 17520
Train: data epoch: [8]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 1.2551  time: 1.5476  data: 0.0000  max mem: 17399
Train: data epoch: [8]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 1.2902  time: 1.5552  data: 0.0000  max mem: 17520
Train: data epoch: [8]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.8596  time: 1.5596  data: 0.0000  max mem: 17399
Train: data epoch: [8]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.4926  time: 1.5594  data: 0.0000  max mem: 17520
Train: data epoch: [8]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.6100  time: 1.5355  data: 0.0000  max mem: 17399
Train: data epoch: [8]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.4931  time: 1.5352  data: 0.0000  max mem: 17520
Train: data epoch: [8]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.7784  time: 1.5425  data: 0.0000  max mem: 17399
Train: data epoch: [8] Total time: 0:00:30 (1.5426 s / it)
Train: data epoch: [8]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.5570  time: 1.5422  data: 0.0000  max mem: 17520
Train: data epoch: [8] Total time: 0:00:30 (1.5427 s / it)
2025-01-18 18:59:58,290 [INFO] Averaged stats: lr: 0.0000  loss: 0.9244
2025-01-18 18:59:58,294 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [8]  [0/1]  eta: 0:00:00    time: 0.7109  data: 0.4354  max mem: 17520
Eval: data epoch: [8] Total time: 0:00:00 (0.8205 s / it)
Eval: data epoch: [8]  [0/1]  eta: 0:00:00    time: 0.8446  data: 0.5675  max mem: 17399
Eval: data epoch: [8] Total time: 0:00:00 (0.9557 s / it)
2025-01-18 18:59:59,274 [INFO] Saving checkpoint at epoch 8 to outputs_stage1_only/202501181852/checkpoint_8.pth.
2025-01-18 19:00:01,631 [INFO] Training Phase
2025-01-18 19:00:01,639 [INFO] Start training epoch 9, 20 iters per inner epoch.
Train: data epoch: [9]  [ 0/20]  eta: 0:00:31  lr: 0.000001  loss: 0.4375  time: 1.5703  data: 0.0000  max mem: 17399
Train: data epoch: [9]  [ 0/20]  eta: 0:00:31  lr: 0.000001  loss: 0.4830  time: 1.5706  data: 0.0000  max mem: 17520
Train: data epoch: [9]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 1.5100  time: 1.5436  data: 0.0000  max mem: 17399
Train: data epoch: [9]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 0.7649  time: 1.5433  data: 0.0000  max mem: 17520
Train: data epoch: [9]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.6582  time: 1.5142  data: 0.0000  max mem: 17399
Train: data epoch: [9]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.6210  time: 1.5139  data: 0.0000  max mem: 17520
Train: data epoch: [9]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.4125  time: 1.5064  data: 0.0000  max mem: 17399
Train: data epoch: [9]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.4442  time: 1.5062  data: 0.0000  max mem: 17520
Train: data epoch: [9]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.4356  time: 1.5275  data: 0.0000  max mem: 17399
Train: data epoch: [9] Total time: 0:00:30 (1.5276 s / it)
Train: data epoch: [9]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.3814  time: 1.5272  data: 0.0000  max mem: 17520
Train: data epoch: [9] Total time: 0:00:30 (1.5277 s / it)
2025-01-18 19:00:32,194 [INFO] Averaged stats: lr: 0.0000  loss: 0.6566
2025-01-18 19:00:32,199 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [9]  [0/1]  eta: 0:00:00    time: 0.6879  data: 0.4167  max mem: 17520
Eval: data epoch: [9] Total time: 0:00:00 (0.7911 s / it)
Eval: data epoch: [9]  [0/1]  eta: 0:00:00    time: 0.8253  data: 0.5513  max mem: 17399
Eval: data epoch: [9] Total time: 0:00:00 (0.9424 s / it)
2025-01-18 19:00:33,166 [INFO] Saving checkpoint at epoch 9 to outputs_stage1_only/202501181852/checkpoint_9.pth.
2025-01-18 19:00:35,479 [INFO] Training Phase
2025-01-18 19:00:35,486 [INFO] Start training epoch 10, 20 iters per inner epoch.
Train: data epoch: [10]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 0.6249  time: 1.6183  data: 0.0000  max mem: 17399
Train: data epoch: [10]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 1.4651  time: 1.6173  data: 0.0000  max mem: 17520
Train: data epoch: [10]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 0.8782  time: 1.5781  data: 0.0000  max mem: 17399
Train: data epoch: [10]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 1.2102  time: 1.5778  data: 0.0000  max mem: 17520
Train: data epoch: [10]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 1.1891  time: 1.5560  data: 0.0000  max mem: 17399
Train: data epoch: [10]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.5099  time: 1.5557  data: 0.0000  max mem: 17520
Train: data epoch: [10]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 1.5746  time: 1.5413  data: 0.0000  max mem: 17399
Train: data epoch: [10]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.4829  time: 1.5409  data: 0.0000  max mem: 17520
Train: data epoch: [10]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 1.6799  time: 1.5449  data: 0.0000  max mem: 17399
Train: data epoch: [10] Total time: 0:00:30 (1.5451 s / it)
Train: data epoch: [10]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.5090  time: 1.5446  data: 0.0000  max mem: 17520
Train: data epoch: [10] Total time: 0:00:30 (1.5451 s / it)
2025-01-18 19:01:06,390 [INFO] Averaged stats: lr: 0.0000  loss: 0.8033
2025-01-18 19:01:06,394 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [10]  [0/1]  eta: 0:00:00    time: 0.6953  data: 0.4187  max mem: 17520
Eval: data epoch: [10] Total time: 0:00:00 (0.7879 s / it)
Eval: data epoch: [10]  [0/1]  eta: 0:00:00    time: 0.8643  data: 0.5515  max mem: 17399
Eval: data epoch: [10] Total time: 0:00:00 (0.9834 s / it)
2025-01-18 19:01:07,402 [INFO] Saving checkpoint at epoch 10 to outputs_stage1_only/202501181852/checkpoint_10.pth.
2025-01-18 19:01:09,730 [INFO] Training Phase
2025-01-18 19:01:09,738 [INFO] Start training epoch 11, 20 iters per inner epoch.
Train: data epoch: [11]  [ 0/20]  eta: 0:00:30  lr: 0.000001  loss: 0.3835  time: 1.5470  data: 0.0000  max mem: 17399
Train: data epoch: [11]  [ 0/20]  eta: 0:00:30  lr: 0.000001  loss: 1.6815  time: 1.5464  data: 0.0000  max mem: 17520
Train: data epoch: [11]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.8890  time: 1.5050  data: 0.0000  max mem: 17399
Train: data epoch: [11]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.4249  time: 1.5047  data: 0.0000  max mem: 17520
Train: data epoch: [11]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 1.0858  time: 1.5404  data: 0.0000  max mem: 17399
Train: data epoch: [11]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.6167  time: 1.5401  data: 0.0000  max mem: 17520
Train: data epoch: [11]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.3798  time: 1.5353  data: 0.0000  max mem: 17399
Train: data epoch: [11]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 1.5971  time: 1.5351  data: 0.0000  max mem: 17520
Train: data epoch: [11]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.9346  time: 1.5318  data: 0.0000  max mem: 17714
Train: data epoch: [11] Total time: 0:00:30 (1.5320 s / it)
Train: data epoch: [11]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.4253  time: 1.5316  data: 0.0000  max mem: 17520
Train: data epoch: [11] Total time: 0:00:30 (1.5321 s / it)
2025-01-18 19:01:40,381 [INFO] Averaged stats: lr: 0.0000  loss: 0.7544
2025-01-18 19:01:40,385 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [11]  [0/1]  eta: 0:00:00    time: 0.6979  data: 0.4247  max mem: 17520
Eval: data epoch: [11] Total time: 0:00:00 (0.7854 s / it)
Eval: data epoch: [11]  [0/1]  eta: 0:00:00    time: 0.8634  data: 0.5677  max mem: 17714
Eval: data epoch: [11] Total time: 0:00:00 (0.9852 s / it)
2025-01-18 19:01:41,396 [INFO] Saving checkpoint at epoch 11 to outputs_stage1_only/202501181852/checkpoint_11.pth.
2025-01-18 19:01:43,694 [INFO] Training Phase
2025-01-18 19:01:43,702 [INFO] Start training epoch 12, 20 iters per inner epoch.
Train: data epoch: [12]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 0.9884  time: 1.6440  data: 0.0000  max mem: 17520Train: data epoch: [12]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 0.5473  time: 1.6456  data: 0.0001  max mem: 17714

Train: data epoch: [12]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 1.2756  time: 1.5038  data: 0.0000  max mem: 17714
Train: data epoch: [12]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.2639  time: 1.5035  data: 0.0000  max mem: 17520
Train: data epoch: [12]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 1.1394  time: 1.5107  data: 0.0000  max mem: 17714
Train: data epoch: [12]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 1.1064  time: 1.5104  data: 0.0000  max mem: 17520
Train: data epoch: [12]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.7866  time: 1.5185  data: 0.0000  max mem: 17714
Train: data epoch: [12]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.8147  time: 1.5183  data: 0.0000  max mem: 17520
Train: data epoch: [12]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.7140  time: 1.5216  data: 0.0000  max mem: 17714
Train: data epoch: [12] Total time: 0:00:30 (1.5218 s / it)
Train: data epoch: [12]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.8493  time: 1.5213  data: 0.0000  max mem: 17520
Train: data epoch: [12] Total time: 0:00:30 (1.5218 s / it)
2025-01-18 19:02:14,138 [INFO] Averaged stats: lr: 0.0000  loss: 0.6757
2025-01-18 19:02:14,142 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [12]  [0/1]  eta: 0:00:00    time: 0.6906  data: 0.4174  max mem: 17520
Eval: data epoch: [12] Total time: 0:00:00 (0.7901 s / it)
Eval: data epoch: [12]  [0/1]  eta: 0:00:00    time: 0.8561  data: 0.5573  max mem: 17714
Eval: data epoch: [12] Total time: 0:00:00 (0.9906 s / it)
2025-01-18 19:02:15,157 [INFO] Saving checkpoint at epoch 12 to outputs_stage1_only/202501181852/checkpoint_12.pth.
2025-01-18 19:02:17,456 [INFO] Training Phase
2025-01-18 19:02:17,464 [INFO] Start training epoch 13, 20 iters per inner epoch.
Train: data epoch: [13]  [ 0/20]  eta: 0:00:30  lr: 0.000001  loss: 0.5639  time: 1.5329  data: 0.0000  max mem: 17714
Train: data epoch: [13]  [ 0/20]  eta: 0:00:30  lr: 0.000001  loss: 0.6543  time: 1.5328  data: 0.0000  max mem: 17520
Train: data epoch: [13]  [ 5/20]  eta: 0:00:21  lr: 0.000001  loss: 0.0786  time: 1.4542  data: 0.0000  max mem: 17714
Train: data epoch: [13]  [ 5/20]  eta: 0:00:21  lr: 0.000001  loss: 0.5285  time: 1.4538  data: 0.0000  max mem: 17520
Train: data epoch: [13]  [10/20]  eta: 0:00:14  lr: 0.000001  loss: 0.4258  time: 1.4997  data: 0.0000  max mem: 17714
Train: data epoch: [13]  [10/20]  eta: 0:00:14  lr: 0.000001  loss: 0.7118  time: 1.4995  data: 0.0000  max mem: 17520
Train: data epoch: [13]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.5108  time: 1.4985  data: 0.0000  max mem: 17714
Train: data epoch: [13]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.6299  time: 1.4983  data: 0.0000  max mem: 17520
Train: data epoch: [13]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.2666  time: 1.5001  data: 0.0000  max mem: 17714
Train: data epoch: [13] Total time: 0:00:30 (1.5003 s / it)
Train: data epoch: [13]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.6771  time: 1.4999  data: 0.0000  max mem: 17520
Train: data epoch: [13] Total time: 0:00:30 (1.5003 s / it)
2025-01-18 19:02:47,472 [INFO] Averaged stats: lr: 0.0000  loss: 0.5532
2025-01-18 19:02:47,475 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [13]  [0/1]  eta: 0:00:00    time: 0.6997  data: 0.4234  max mem: 17520
Eval: data epoch: [13] Total time: 0:00:00 (0.8363 s / it)
Eval: data epoch: [13]  [0/1]  eta: 0:00:00    time: 0.8384  data: 0.5630  max mem: 17714
Eval: data epoch: [13] Total time: 0:00:00 (0.9485 s / it)
2025-01-18 19:02:48,448 [INFO] Saving checkpoint at epoch 13 to outputs_stage1_only/202501181852/checkpoint_13.pth.
2025-01-18 19:02:50,758 [INFO] Training Phase
2025-01-18 19:02:50,766 [INFO] Start training epoch 14, 20 iters per inner epoch.
Train: data epoch: [14]  [ 0/20]  eta: 0:00:27  lr: 0.000001  loss: 0.2563  time: 1.3835  data: 0.0001  max mem: 17714
Train: data epoch: [14]  [ 0/20]  eta: 0:00:27  lr: 0.000001  loss: 0.5569  time: 1.3839  data: 0.0000  max mem: 17520
Train: data epoch: [14]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.5356  time: 1.5301  data: 0.0000  max mem: 17714
Train: data epoch: [14]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.9795  time: 1.5300  data: 0.0000  max mem: 17520
Train: data epoch: [14]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.2584  time: 1.5266  data: 0.0000  max mem: 17714
Train: data epoch: [14]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.7022  time: 1.5265  data: 0.0000  max mem: 17520
Train: data epoch: [14]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.5740  time: 1.5028  data: 0.0000  max mem: 17714
Train: data epoch: [14]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.3824  time: 1.5026  data: 0.0000  max mem: 17520
Train: data epoch: [14]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.3208  time: 1.4969  data: 0.0000  max mem: 17714
Train: data epoch: [14] Total time: 0:00:29 (1.4971 s / it)
Train: data epoch: [14]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.4078  time: 1.4967  data: 0.0000  max mem: 17520
Train: data epoch: [14] Total time: 0:00:29 (1.4972 s / it)
2025-01-18 19:03:20,711 [INFO] Averaged stats: lr: 0.0000  loss: 0.5671
2025-01-18 19:03:20,715 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [14]  [0/1]  eta: 0:00:00    time: 0.7185  data: 0.4408  max mem: 17520
Eval: data epoch: [14] Total time: 0:00:00 (0.8146 s / it)
Eval: data epoch: [14]  [0/1]  eta: 0:00:00    time: 0.8311  data: 0.5595  max mem: 17714
Eval: data epoch: [14] Total time: 0:00:00 (0.9725 s / it)
2025-01-18 19:03:21,712 [INFO] Saving checkpoint at epoch 14 to outputs_stage1_only/202501181852/checkpoint_14.pth.
2025-01-18 19:03:24,067 [INFO] Training Phase
2025-01-18 19:03:24,075 [INFO] Start training epoch 15, 20 iters per inner epoch.
Train: data epoch: [15]  [ 0/20]  eta: 0:00:26  lr: 0.000001  loss: 0.5198  time: 1.3340  data: 0.0000  max mem: 17714
Train: data epoch: [15]  [ 0/20]  eta: 0:00:26  lr: 0.000001  loss: 0.4454  time: 1.3345  data: 0.0000  max mem: 17520
Train: data epoch: [15]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.2853  time: 1.4796  data: 0.0000  max mem: 17714
Train: data epoch: [15]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.2955  time: 1.4794  data: 0.0000  max mem: 17520
Train: data epoch: [15]  [10/20]  eta: 0:00:14  lr: 0.000001  loss: 0.1822  time: 1.4971  data: 0.0000  max mem: 17714
Train: data epoch: [15]  [10/20]  eta: 0:00:14  lr: 0.000001  loss: 0.3801  time: 1.4969  data: 0.0000  max mem: 17520
Train: data epoch: [15]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.5041  time: 1.5154  data: 0.0000  max mem: 17714
Train: data epoch: [15]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.4293  time: 1.5151  data: 0.0000  max mem: 17520
Train: data epoch: [15]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.9448  time: 1.5042  data: 0.0000  max mem: 17714
Train: data epoch: [15] Total time: 0:00:30 (1.5043 s / it)
Train: data epoch: [15]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.2663  time: 1.5040  data: 0.0000  max mem: 17520
Train: data epoch: [15] Total time: 0:00:30 (1.5044 s / it)
2025-01-18 19:03:54,165 [INFO] Averaged stats: lr: 0.0000  loss: 0.5708
2025-01-18 19:03:54,169 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [15]  [0/1]  eta: 0:00:00    time: 0.7116  data: 0.4404  max mem: 17520
Eval: data epoch: [15] Total time: 0:00:00 (0.8137 s / it)
Eval: data epoch: [15]  [0/1]  eta: 0:00:00    time: 0.8353  data: 0.5535  max mem: 17714
Eval: data epoch: [15] Total time: 0:00:00 (0.9656 s / it)
2025-01-18 19:03:55,158 [INFO] Saving checkpoint at epoch 15 to outputs_stage1_only/202501181852/checkpoint_best.pth.
2025-01-18 19:04:02,934 [INFO] Saving checkpoint at epoch 15 to outputs_stage1_only/202501181852/checkpoint_15.pth.
2025-01-18 19:04:05,293 [INFO] Training Phase
2025-01-18 19:04:05,300 [INFO] Start training epoch 16, 20 iters per inner epoch.
Train: data epoch: [16]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 0.7071  time: 1.6160  data: 0.0000  max mem: 17714
Train: data epoch: [16]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 0.2350  time: 1.6155  data: 0.0000  max mem: 17520
Train: data epoch: [16]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 1.0624  time: 1.5996  data: 0.0000  max mem: 17714
Train: data epoch: [16]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 0.8461  time: 1.5996  data: 0.0000  max mem: 17520
Train: data epoch: [16]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.6237  time: 1.5444  data: 0.0000  max mem: 17714
Train: data epoch: [16]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.6677  time: 1.5441  data: 0.0000  max mem: 17520
Train: data epoch: [16]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.5297  time: 1.5395  data: 0.0000  max mem: 17714
Train: data epoch: [16]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 1.1356  time: 1.5393  data: 0.0000  max mem: 17520
Train: data epoch: [16]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.6037  time: 1.5433  data: 0.0000  max mem: 17714
Train: data epoch: [16] Total time: 0:00:30 (1.5436 s / it)
Train: data epoch: [16]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.2915  time: 1.5431  data: 0.0000  max mem: 17520
Train: data epoch: [16] Total time: 0:00:30 (1.5436 s / it)
2025-01-18 19:04:36,175 [INFO] Averaged stats: lr: 0.0000  loss: 0.5764
2025-01-18 19:04:36,179 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [16]  [0/1]  eta: 0:00:00    time: 0.6922  data: 0.4203  max mem: 17520
Eval: data epoch: [16] Total time: 0:00:00 (0.7950 s / it)
Eval: data epoch: [16]  [0/1]  eta: 0:00:00    time: 0.8481  data: 0.5761  max mem: 17714
Eval: data epoch: [16] Total time: 0:00:00 (0.9734 s / it)
2025-01-18 19:04:37,177 [INFO] Saving checkpoint at epoch 16 to outputs_stage1_only/202501181852/checkpoint_16.pth.
2025-01-18 19:04:39,519 [INFO] Training Phase
2025-01-18 19:04:39,527 [INFO] Start training epoch 17, 20 iters per inner epoch.
Train: data epoch: [17]  [ 0/20]  eta: 0:00:31  lr: 0.000001  loss: 0.6893  time: 1.5939  data: 0.0000  max mem: 17714
Train: data epoch: [17]  [ 0/20]  eta: 0:00:31  lr: 0.000001  loss: 0.7219  time: 1.5930  data: 0.0000  max mem: 17520
Train: data epoch: [17]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 0.9136  time: 1.5648  data: 0.0000  max mem: 17714
Train: data epoch: [17]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 0.6799  time: 1.5646  data: 0.0000  max mem: 17520
Train: data epoch: [17]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.8051  time: 1.5224  data: 0.0000  max mem: 17714
Train: data epoch: [17]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.6228  time: 1.5222  data: 0.0000  max mem: 17520
Train: data epoch: [17]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.4320  time: 1.5409  data: 0.0000  max mem: 17714
Train: data epoch: [17]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 1.3370  time: 1.5406  data: 0.0000  max mem: 17520
Train: data epoch: [17]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 1.1722  time: 1.5314  data: 0.0000  max mem: 17714
Train: data epoch: [17] Total time: 0:00:30 (1.5316 s / it)
Train: data epoch: [17]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.4555  time: 1.5311  data: 0.0000  max mem: 17520
Train: data epoch: [17] Total time: 0:00:30 (1.5316 s / it)
2025-01-18 19:05:10,160 [INFO] Averaged stats: lr: 0.0000  loss: 0.6928
2025-01-18 19:05:10,165 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [17]  [0/1]  eta: 0:00:00    time: 0.7019  data: 0.4252  max mem: 17520
Eval: data epoch: [17] Total time: 0:00:00 (0.7887 s / it)
Eval: data epoch: [17]  [0/1]  eta: 0:00:00    time: 0.8565  data: 0.5795  max mem: 17714
Eval: data epoch: [17] Total time: 0:00:00 (0.9849 s / it)
2025-01-18 19:05:11,174 [INFO] Saving checkpoint at epoch 17 to outputs_stage1_only/202501181852/checkpoint_17.pth.
2025-01-18 19:05:13,517 [INFO] Training Phase
2025-01-18 19:05:13,525 [INFO] Start training epoch 18, 20 iters per inner epoch.
Train: data epoch: [18]  [ 0/20]  eta: 0:00:31  lr: 0.000001  loss: 0.2928  time: 1.5959  data: 0.0000  max mem: 17714
Train: data epoch: [18]  [ 0/20]  eta: 0:00:31  lr: 0.000001  loss: 1.4104  time: 1.5953  data: 0.0001  max mem: 17520
Train: data epoch: [18]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 0.6155  time: 1.5947  data: 0.0000  max mem: 17714
Train: data epoch: [18]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 0.3949  time: 1.5945  data: 0.0000  max mem: 17520
Train: data epoch: [18]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 1.1744  time: 1.5469  data: 0.0000  max mem: 17714
Train: data epoch: [18]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.6095  time: 1.5468  data: 0.0000  max mem: 17520
Train: data epoch: [18]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.6726  time: 1.5425  data: 0.0000  max mem: 17714
Train: data epoch: [18]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.6138  time: 1.5423  data: 0.0000  max mem: 17520
Train: data epoch: [18]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.1645  time: 1.5366  data: 0.0000  max mem: 17714
Train: data epoch: [18] Total time: 0:00:30 (1.5368 s / it)
Train: data epoch: [18]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.5431  time: 1.5364  data: 0.0000  max mem: 17520
Train: data epoch: [18] Total time: 0:00:30 (1.5369 s / it)
2025-01-18 19:05:44,264 [INFO] Averaged stats: lr: 0.0000  loss: 0.5166
2025-01-18 19:05:44,268 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [18]  [0/1]  eta: 0:00:00    time: 0.6957  data: 0.4236  max mem: 17520
Eval: data epoch: [18] Total time: 0:00:00 (0.7869 s / it)
Eval: data epoch: [18]  [0/1]  eta: 0:00:00    time: 0.8631  data: 0.5855  max mem: 17714
Eval: data epoch: [18] Total time: 0:00:00 (0.9791 s / it)
2025-01-18 19:05:45,273 [INFO] Saving checkpoint at epoch 18 to outputs_stage1_only/202501181852/checkpoint_18.pth.
2025-01-18 19:05:47,682 [INFO] Training Phase
2025-01-18 19:05:47,690 [INFO] Start training epoch 19, 20 iters per inner epoch.
Train: data epoch: [19]  [ 0/20]  eta: 0:00:29  lr: 0.000001  loss: 0.1286  time: 1.4734  data: 0.0000  max mem: 17714
Train: data epoch: [19]  [ 0/20]  eta: 0:00:29  lr: 0.000001  loss: 1.1086  time: 1.4731  data: 0.0000  max mem: 17520
Train: data epoch: [19]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.2993  time: 1.5194  data: 0.0000  max mem: 17714
Train: data epoch: [19]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.7006  time: 1.5191  data: 0.0000  max mem: 17520
Train: data epoch: [19]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.5916  time: 1.5254  data: 0.0000  max mem: 17714
Train: data epoch: [19]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 1.2755  time: 1.5250  data: 0.0000  max mem: 17520
Train: data epoch: [19]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.3359  time: 1.5005  data: 0.0000  max mem: 17714
Train: data epoch: [19]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.4373  time: 1.5001  data: 0.0000  max mem: 17520
Train: data epoch: [19]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.7875  time: 1.5127  data: 0.0000  max mem: 17714
Train: data epoch: [19] Total time: 0:00:30 (1.5128 s / it)
Train: data epoch: [19]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.3372  time: 1.5123  data: 0.0000  max mem: 17520
Train: data epoch: [19] Total time: 0:00:30 (1.5129 s / it)
2025-01-18 19:06:17,948 [INFO] Averaged stats: lr: 0.0000  loss: 0.5982
2025-01-18 19:06:17,953 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [19]  [0/1]  eta: 0:00:00    time: 0.7214  data: 0.4496  max mem: 17520
Eval: data epoch: [19] Total time: 0:00:00 (0.8445 s / it)
Eval: data epoch: [19]  [0/1]  eta: 0:00:00    time: 0.8662  data: 0.5806  max mem: 17714
Eval: data epoch: [19] Total time: 0:00:00 (0.9889 s / it)
2025-01-18 19:06:18,966 [INFO] Saving checkpoint at epoch 19 to outputs_stage1_only/202501181852/checkpoint_19.pth.
2025-01-18 19:06:21,360 [INFO] Training Phase
2025-01-18 19:06:21,367 [INFO] Start training epoch 20, 20 iters per inner epoch.
Train: data epoch: [20]  [ 0/20]  eta: 0:00:27  lr: 0.000001  loss: 0.0945  time: 1.3695  data: 0.0000  max mem: 17714
Train: data epoch: [20]  [ 0/20]  eta: 0:00:27  lr: 0.000001  loss: 0.4663  time: 1.3681  data: 0.0000  max mem: 17520
Train: data epoch: [20]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.1499  time: 1.5028  data: 0.0000  max mem: 17714
Train: data epoch: [20]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.5644  time: 1.5024  data: 0.0000  max mem: 17520
Train: data epoch: [20]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.7429  time: 1.5216  data: 0.0000  max mem: 17714
Train: data epoch: [20]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.9778  time: 1.5213  data: 0.0000  max mem: 17520
Train: data epoch: [20]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.9558  time: 1.5204  data: 0.0000  max mem: 17714
Train: data epoch: [20]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.5019  time: 1.5201  data: 0.0000  max mem: 17520
Train: data epoch: [20]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.5928  time: 1.5274  data: 0.0000  max mem: 17714
Train: data epoch: [20] Total time: 0:00:30 (1.5276 s / it)
Train: data epoch: [20]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.3411  time: 1.5271  data: 0.0000  max mem: 17520
Train: data epoch: [20] Total time: 0:00:30 (1.5276 s / it)
2025-01-18 19:06:51,921 [INFO] Averaged stats: lr: 0.0000  loss: 0.5619
2025-01-18 19:06:51,925 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [20]  [0/1]  eta: 0:00:00    time: 0.6966  data: 0.4248  max mem: 17520
Eval: data epoch: [20] Total time: 0:00:00 (0.8009 s / it)
Eval: data epoch: [20]  [0/1]  eta: 0:00:00    time: 0.8337  data: 0.5586  max mem: 17714
Eval: data epoch: [20] Total time: 0:00:00 (0.9502 s / it)
2025-01-18 19:06:52,899 [INFO] Saving checkpoint at epoch 20 to outputs_stage1_only/202501181852/checkpoint_20.pth.
2025-01-18 19:06:55,209 [INFO] Training Phase
2025-01-18 19:06:55,216 [INFO] Start training epoch 21, 20 iters per inner epoch.
Train: data epoch: [21]  [ 0/20]  eta: 0:00:30  lr: 0.000001  loss: 0.2303  time: 1.5096  data: 0.0001  max mem: 17714
Train: data epoch: [21]  [ 0/20]  eta: 0:00:30  lr: 0.000001  loss: 1.4760  time: 1.5095  data: 0.0000  max mem: 17520
Train: data epoch: [21]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.2958  time: 1.4974  data: 0.0000  max mem: 17714
Train: data epoch: [21]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.1291  time: 1.4973  data: 0.0000  max mem: 17520
Train: data epoch: [21]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 1.0941  time: 1.5141  data: 0.0000  max mem: 17714
Train: data epoch: [21]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.5817  time: 1.5140  data: 0.0000  max mem: 17520
Train: data epoch: [21]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.2826  time: 1.5183  data: 0.0000  max mem: 17714
Train: data epoch: [21]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.2491  time: 1.5181  data: 0.0000  max mem: 17520
Train: data epoch: [21]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.5396  time: 1.5118  data: 0.0000  max mem: 17714
Train: data epoch: [21] Total time: 0:00:30 (1.5120 s / it)
Train: data epoch: [21]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 1.3194  time: 1.5116  data: 0.0000  max mem: 17520
Train: data epoch: [21] Total time: 0:00:30 (1.5121 s / it)
2025-01-18 19:07:25,459 [INFO] Averaged stats: lr: 0.0000  loss: 0.5736
2025-01-18 19:07:25,463 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [21]  [0/1]  eta: 0:00:00    time: 0.7045  data: 0.4283  max mem: 17520
Eval: data epoch: [21] Total time: 0:00:00 (0.8196 s / it)
Eval: data epoch: [21]  [0/1]  eta: 0:00:00    time: 0.8541  data: 0.5805  max mem: 17714
Eval: data epoch: [21] Total time: 0:00:00 (0.9794 s / it)
2025-01-18 19:07:26,468 [INFO] Saving checkpoint at epoch 21 to outputs_stage1_only/202501181852/checkpoint_21.pth.
2025-01-18 19:07:28,799 [INFO] Training Phase
2025-01-18 19:07:28,807 [INFO] Start training epoch 22, 20 iters per inner epoch.
Train: data epoch: [22]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 0.3293  time: 1.6137  data: 0.0000  max mem: 17714
Train: data epoch: [22]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 0.1481  time: 1.6130  data: 0.0000  max mem: 17520
Train: data epoch: [22]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 0.3352  time: 1.5435  data: 0.0000  max mem: 17714
Train: data epoch: [22]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 0.2354  time: 1.5431  data: 0.0000  max mem: 17520
Train: data epoch: [22]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.8903  time: 1.5299  data: 0.0000  max mem: 17714
Train: data epoch: [22]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.8183  time: 1.5297  data: 0.0000  max mem: 17520
Train: data epoch: [22]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.5631  time: 1.5225  data: 0.0000  max mem: 17714
Train: data epoch: [22]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 1.1463  time: 1.5223  data: 0.0000  max mem: 17520
Train: data epoch: [22]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.3013  time: 1.5389  data: 0.0000  max mem: 17714
Train: data epoch: [22] Total time: 0:00:30 (1.5391 s / it)
Train: data epoch: [22]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.8399  time: 1.5386  data: 0.0000  max mem: 17520
Train: data epoch: [22] Total time: 0:00:30 (1.5391 s / it)
2025-01-18 19:07:59,590 [INFO] Averaged stats: lr: 0.0000  loss: 0.5334
2025-01-18 19:07:59,594 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [22]  [0/1]  eta: 0:00:00    time: 0.7202  data: 0.4414  max mem: 17520
Eval: data epoch: [22]  [0/1]  eta: 0:00:00    time: 0.8349  data: 0.5616  max mem: 17714
Eval: data epoch: [22] Total time: 0:00:00 (0.8617 s / it)
Eval: data epoch: [22] Total time: 0:00:00 (0.9523 s / it)
2025-01-18 19:08:00,570 [INFO] Saving checkpoint at epoch 22 to outputs_stage1_only/202501181852/checkpoint_22.pth.
2025-01-18 19:08:02,891 [INFO] Training Phase
2025-01-18 19:08:02,899 [INFO] Start training epoch 23, 20 iters per inner epoch.
Train: data epoch: [23]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 0.4359  time: 1.6238  data: 0.0000  max mem: 17714
Train: data epoch: [23]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 0.1992  time: 1.6248  data: 0.0000  max mem: 17520
Train: data epoch: [23]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 0.5619  time: 1.5657  data: 0.0000  max mem: 17714
Train: data epoch: [23]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 0.1804  time: 1.5654  data: 0.0000  max mem: 17520
Train: data epoch: [23]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.4097  time: 1.5346  data: 0.0000  max mem: 17714
Train: data epoch: [23]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.2549  time: 1.5343  data: 0.0000  max mem: 17520
Train: data epoch: [23]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.3010  time: 1.5001  data: 0.0000  max mem: 17714
Train: data epoch: [23]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.1407  time: 1.4998  data: 0.0000  max mem: 17520
Train: data epoch: [23]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.2913  time: 1.4973  data: 0.0000  max mem: 17714
Train: data epoch: [23] Total time: 0:00:29 (1.4975 s / it)
Train: data epoch: [23]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.4500  time: 1.4969  data: 0.0000  max mem: 17520
Train: data epoch: [23] Total time: 0:00:29 (1.4975 s / it)
2025-01-18 19:08:32,850 [INFO] Averaged stats: lr: 0.0000  loss: 0.5151
2025-01-18 19:08:32,854 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [23]  [0/1]  eta: 0:00:00    time: 0.7292  data: 0.4508  max mem: 17520
Eval: data epoch: [23] Total time: 0:00:00 (0.8265 s / it)
Eval: data epoch: [23]  [0/1]  eta: 0:00:00    time: 0.8344  data: 0.5576  max mem: 17714
Eval: data epoch: [23] Total time: 0:00:00 (0.9583 s / it)
2025-01-18 19:08:33,836 [INFO] Saving checkpoint at epoch 23 to outputs_stage1_only/202501181852/checkpoint_23.pth.
2025-01-18 19:08:36,209 [INFO] Training Phase
2025-01-18 19:08:36,217 [INFO] Start training epoch 24, 20 iters per inner epoch.
Train: data epoch: [24]  [ 0/20]  eta: 0:00:33  lr: 0.000001  loss: 0.3766  time: 1.6517  data: 0.0000  max mem: 17714
Train: data epoch: [24]  [ 0/20]  eta: 0:00:33  lr: 0.000001  loss: 0.2963  time: 1.6521  data: 0.0000  max mem: 17520
Train: data epoch: [24]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 0.4460  time: 1.5564  data: 0.0000  max mem: 17714
Train: data epoch: [24]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 0.1809  time: 1.5562  data: 0.0000  max mem: 17520
Train: data epoch: [24]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.9740  time: 1.5695  data: 0.0000  max mem: 17714
Train: data epoch: [24]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.2037  time: 1.5694  data: 0.0000  max mem: 17520
Train: data epoch: [24]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 1.0423  time: 1.5705  data: 0.0000  max mem: 17714
Train: data epoch: [24]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.5106  time: 1.5704  data: 0.0000  max mem: 17520
Train: data epoch: [24]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.3013  time: 1.5714  data: 0.0000  max mem: 17714
Train: data epoch: [24] Total time: 0:00:31 (1.5716 s / it)
Train: data epoch: [24]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.2992  time: 1.5712  data: 0.0000  max mem: 17520
Train: data epoch: [24] Total time: 0:00:31 (1.5717 s / it)
2025-01-18 19:09:07,652 [INFO] Averaged stats: lr: 0.0000  loss: 0.5028
2025-01-18 19:09:07,656 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [24]  [0/1]  eta: 0:00:00    time: 0.7180  data: 0.4411  max mem: 17520
Eval: data epoch: [24] Total time: 0:00:00 (0.8263 s / it)
Eval: data epoch: [24]  [0/1]  eta: 0:00:00    time: 0.8282  data: 0.5548  max mem: 17714
Eval: data epoch: [24] Total time: 0:00:00 (0.9503 s / it)
2025-01-18 19:09:08,631 [INFO] Saving checkpoint at epoch 24 to outputs_stage1_only/202501181852/checkpoint_24.pth.
2025-01-18 19:09:11,021 [INFO] Training Phase
2025-01-18 19:09:11,029 [INFO] Start training epoch 25, 20 iters per inner epoch.
Train: data epoch: [25]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 0.5624  time: 1.6481  data: 0.0000  max mem: 17714
Train: data epoch: [25]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 1.0733  time: 1.6484  data: 0.0000  max mem: 17520
Train: data epoch: [25]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.0946  time: 1.4960  data: 0.0000  max mem: 17714
Train: data epoch: [25]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.2025  time: 1.4959  data: 0.0000  max mem: 17520
Train: data epoch: [25]  [10/20]  eta: 0:00:14  lr: 0.000001  loss: 0.5049  time: 1.4810  data: 0.0000  max mem: 17714
Train: data epoch: [25]  [10/20]  eta: 0:00:14  lr: 0.000001  loss: 0.2997  time: 1.4808  data: 0.0000  max mem: 17520
Train: data epoch: [25]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.5346  time: 1.4835  data: 0.0000  max mem: 17714
Train: data epoch: [25]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.3096  time: 1.4832  data: 0.0000  max mem: 17627
Train: data epoch: [25]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.8008  time: 1.4800  data: 0.0000  max mem: 17714
Train: data epoch: [25] Total time: 0:00:29 (1.4802 s / it)
Train: data epoch: [25]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.4346  time: 1.4798  data: 0.0000  max mem: 17627
Train: data epoch: [25] Total time: 0:00:29 (1.4803 s / it)
2025-01-18 19:09:40,636 [INFO] Averaged stats: lr: 0.0000  loss: 0.4497
2025-01-18 19:09:40,641 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [25]  [0/1]  eta: 0:00:00    time: 0.7149  data: 0.4393  max mem: 17627
Eval: data epoch: [25] Total time: 0:00:00 (0.8107 s / it)
Eval: data epoch: [25]  [0/1]  eta: 0:00:00    time: 0.8489  data: 0.5734  max mem: 17714
Eval: data epoch: [25] Total time: 0:00:00 (0.9572 s / it)
2025-01-18 19:09:41,622 [INFO] Saving checkpoint at epoch 25 to outputs_stage1_only/202501181852/checkpoint_25.pth.
2025-01-18 19:09:44,002 [INFO] Training Phase
2025-01-18 19:09:44,010 [INFO] Start training epoch 26, 20 iters per inner epoch.
Train: data epoch: [26]  [ 0/20]  eta: 0:00:30  lr: 0.000001  loss: 1.1258  time: 1.5438  data: 0.0000  max mem: 17714
Train: data epoch: [26]  [ 0/20]  eta: 0:00:30  lr: 0.000001  loss: 0.2417  time: 1.5430  data: 0.0000  max mem: 17627
Train: data epoch: [26]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.5389  time: 1.5201  data: 0.0000  max mem: 17714
Train: data epoch: [26]  [ 5/20]  eta: 0:00:22  lr: 0.000001  loss: 0.9984  time: 1.5199  data: 0.0000  max mem: 17627
Train: data epoch: [26]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.9980  time: 1.5234  data: 0.0000  max mem: 17714
Train: data epoch: [26]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.1436  time: 1.5231  data: 0.0000  max mem: 17627
Train: data epoch: [26]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.3083  time: 1.5170  data: 0.0000  max mem: 17714
Train: data epoch: [26]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 1.2522  time: 1.5167  data: 0.0000  max mem: 17627
Train: data epoch: [26]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.2221  time: 1.5176  data: 0.0000  max mem: 17714
Train: data epoch: [26] Total time: 0:00:30 (1.5178 s / it)
Train: data epoch: [26]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.2983  time: 1.5173  data: 0.0000  max mem: 17627
Train: data epoch: [26] Total time: 0:00:30 (1.5179 s / it)
2025-01-18 19:10:14,368 [INFO] Averaged stats: lr: 0.0000  loss: 0.5184
2025-01-18 19:10:14,373 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [26]  [0/1]  eta: 0:00:00    time: 0.7133  data: 0.4177  max mem: 17627
Eval: data epoch: [26] Total time: 0:00:00 (0.8057 s / it)
Eval: data epoch: [26]  [0/1]  eta: 0:00:00    time: 0.8205  data: 0.5513  max mem: 17714
Eval: data epoch: [26] Total time: 0:00:00 (0.9357 s / it)
2025-01-18 19:10:15,334 [INFO] Saving checkpoint at epoch 26 to outputs_stage1_only/202501181852/checkpoint_26.pth.
2025-01-18 19:10:17,669 [INFO] Training Phase
2025-01-18 19:10:17,677 [INFO] Start training epoch 27, 20 iters per inner epoch.
Train: data epoch: [27]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 0.2537  time: 1.6014  data: 0.0000  max mem: 17714
Train: data epoch: [27]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 0.6050  time: 1.6018  data: 0.0000  max mem: 17627
Train: data epoch: [27]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 0.5347  time: 1.5784  data: 0.0000  max mem: 17714
Train: data epoch: [27]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 0.8290  time: 1.5782  data: 0.0000  max mem: 17627
Train: data epoch: [27]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.3312  time: 1.5621  data: 0.0000  max mem: 17714
Train: data epoch: [27]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.4636  time: 1.5619  data: 0.0000  max mem: 17627
Train: data epoch: [27]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.3472  time: 1.5042  data: 0.0000  max mem: 17714
Train: data epoch: [27]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.3954  time: 1.5040  data: 0.0000  max mem: 17627
Train: data epoch: [27]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.6358  time: 1.4986  data: 0.0000  max mem: 17714
Train: data epoch: [27] Total time: 0:00:29 (1.4988 s / it)
Train: data epoch: [27]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 1.0336  time: 1.4984  data: 0.0000  max mem: 17627
Train: data epoch: [27] Total time: 0:00:29 (1.4989 s / it)
2025-01-18 19:10:47,656 [INFO] Averaged stats: lr: 0.0000  loss: 0.5025
2025-01-18 19:10:47,660 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [27]  [0/1]  eta: 0:00:00    time: 0.6883  data: 0.4140  max mem: 17627
Eval: data epoch: [27] Total time: 0:00:00 (0.7952 s / it)
Eval: data epoch: [27]  [0/1]  eta: 0:00:00    time: 0.8336  data: 0.5593  max mem: 17714
Eval: data epoch: [27] Total time: 0:00:00 (0.9784 s / it)
2025-01-18 19:10:48,662 [INFO] Saving checkpoint at epoch 27 to outputs_stage1_only/202501181852/checkpoint_27.pth.
2025-01-18 19:10:50,996 [INFO] Training Phase
2025-01-18 19:10:51,004 [INFO] Start training epoch 28, 20 iters per inner epoch.
Train: data epoch: [28]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 0.7616  time: 1.6329  data: 0.0000  max mem: 17714
Train: data epoch: [28]  [ 0/20]  eta: 0:00:32  lr: 0.000001  loss: 0.3421  time: 1.6328  data: 0.0000  max mem: 17627
Train: data epoch: [28]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 0.5604  time: 1.5951  data: 0.0000  max mem: 17714
Train: data epoch: [28]  [ 5/20]  eta: 0:00:23  lr: 0.000001  loss: 1.0772  time: 1.5948  data: 0.0000  max mem: 17627
Train: data epoch: [28]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 1.4096  time: 1.5862  data: 0.0000  max mem: 17714
Train: data epoch: [28]  [10/20]  eta: 0:00:15  lr: 0.000001  loss: 0.7796  time: 1.5859  data: 0.0000  max mem: 17627
Train: data epoch: [28]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.1040  time: 1.5707  data: 0.0000  max mem: 17714
Train: data epoch: [28]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.7333  time: 1.5704  data: 0.0000  max mem: 17627
Train: data epoch: [28]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.5284  time: 1.5722  data: 0.0000  max mem: 17714
Train: data epoch: [28] Total time: 0:00:31 (1.5724 s / it)
Train: data epoch: [28]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.6325  time: 1.5720  data: 0.0000  max mem: 17627
Train: data epoch: [28] Total time: 0:00:31 (1.5725 s / it)
2025-01-18 19:11:22,455 [INFO] Averaged stats: lr: 0.0000  loss: 0.5814
2025-01-18 19:11:22,459 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [28]  [0/1]  eta: 0:00:00    time: 0.7160  data: 0.4389  max mem: 17627
Eval: data epoch: [28] Total time: 0:00:00 (0.8299 s / it)
Eval: data epoch: [28]  [0/1]  eta: 0:00:00    time: 0.8375  data: 0.5599  max mem: 17714
Eval: data epoch: [28] Total time: 0:00:00 (0.9797 s / it)
2025-01-18 19:11:23,463 [INFO] Saving checkpoint at epoch 28 to outputs_stage1_only/202501181852/checkpoint_28.pth.
2025-01-18 19:11:25,826 [INFO] Training Phase
2025-01-18 19:11:25,834 [INFO] Start training epoch 29, 20 iters per inner epoch.
Train: data epoch: [29]  [ 0/20]  eta: 0:00:27  lr: 0.000001  loss: 0.5220  time: 1.3733  data: 0.0000  max mem: 17714
Train: data epoch: [29]  [ 0/20]  eta: 0:00:27  lr: 0.000001  loss: 0.4132  time: 1.3734  data: 0.0000  max mem: 17627
Train: data epoch: [29]  [ 5/20]  eta: 0:00:21  lr: 0.000001  loss: 0.2421  time: 1.4399  data: 0.0000  max mem: 17714
Train: data epoch: [29]  [ 5/20]  eta: 0:00:21  lr: 0.000001  loss: 0.8570  time: 1.4397  data: 0.0000  max mem: 17627
Train: data epoch: [29]  [10/20]  eta: 0:00:14  lr: 0.000001  loss: 0.2317  time: 1.4497  data: 0.0000  max mem: 17714
Train: data epoch: [29]  [10/20]  eta: 0:00:14  lr: 0.000001  loss: 0.3971  time: 1.4494  data: 0.0000  max mem: 17627
Train: data epoch: [29]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.7379  time: 1.4821  data: 0.0000  max mem: 17714
Train: data epoch: [29]  [15/20]  eta: 0:00:07  lr: 0.000001  loss: 0.5298  time: 1.4818  data: 0.0000  max mem: 17627
Train: data epoch: [29]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.7684  time: 1.4948  data: 0.0000  max mem: 17714
Train: data epoch: [29] Total time: 0:00:29 (1.4950 s / it)
Train: data epoch: [29]  [19/20]  eta: 0:00:01  lr: 0.000001  loss: 0.5463  time: 1.4945  data: 0.0000  max mem: 17627
Train: data epoch: [29] Total time: 0:00:29 (1.4950 s / it)
2025-01-18 19:11:55,736 [INFO] Averaged stats: lr: 0.0000  loss: 0.5157
2025-01-18 19:11:55,740 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [29]  [0/1]  eta: 0:00:00    time: 0.7131  data: 0.4417  max mem: 17627
Eval: data epoch: [29] Total time: 0:00:00 (0.7983 s / it)
Eval: data epoch: [29]  [0/1]  eta: 0:00:00    time: 0.8511  data: 0.5775  max mem: 17714
Eval: data epoch: [29] Total time: 0:00:00 (0.9709 s / it)
2025-01-18 19:11:56,736 [INFO] Saving checkpoint at epoch 29 to outputs_stage1_only/202501181852/checkpoint_29.pth.
2025-01-18 19:11:59,107 [INFO] Training time 0:17:21
[1;34mwandb[0m: üöÄ View run [33mllama-3.2-1B-1percent-stage1[0m at: [34mhttps://wandb.ai/kihoon090/audio_lm/runs/d5tiqnc4[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250118_185239-d5tiqnc4/logs[0m
[rank0]:[W118 19:12:01.340449682 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
Traceback (most recent call last):
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 105, in <module>
    main()
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 70, in main
    init_distributed_mode(run_config)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/dist_utils.py", line 72, in init_distributed_mode
    torch.cuda.set_device(args.gpu)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch._C._cuda_setDevice(device)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/cuda/__init__.py", line 319, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
Traceback (most recent call last):
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 105, in <module>
    main()
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 70, in main
    init_distributed_mode(run_config)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/dist_utils.py", line 72, in init_distributed_mode
    torch.cuda.set_device(args.gpu)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch._C._cuda_setDevice(device)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/cuda/__init__.py", line 319, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
E0118 19:16:53.569023 956627 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 956678) of binary: /data/anaconda3/envs/salmonn_env/bin/python
Traceback (most recent call last):
  File "/data/anaconda3/envs/salmonn_env/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-01-18_19:16:53
  host      : nota-gpu-svr004
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 956679)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-01-18_19:16:53
  host      : nota-gpu-svr004
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 956678)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
Traceback (most recent call last):
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 105, in <module>
    main()
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 70, in main
    init_distributed_mode(run_config)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/dist_utils.py", line 72, in init_distributed_mode
    torch.cuda.set_device(args.gpu)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch._C._cuda_setDevice(device)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/cuda/__init__.py", line 319, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
Traceback (most recent call last):
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 105, in <module>
    main()
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 70, in main
    init_distributed_mode(run_config)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/dist_utils.py", line 72, in init_distributed_mode
    torch.cuda.set_device(args.gpu)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch._C._cuda_setDevice(device)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/cuda/__init__.py", line 319, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
E0118 19:27:36.630563 957326 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 957374) of binary: /data/anaconda3/envs/salmonn_env/bin/python
Traceback (most recent call last):
  File "/data/anaconda3/envs/salmonn_env/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-01-18_19:27:36
  host      : nota-gpu-svr004
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 957375)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-01-18_19:27:36
  host      : nota-gpu-svr004
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 957374)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
Traceback (most recent call last):
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 105, in <module>
    main()
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 70, in main
    init_distributed_mode(run_config)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/dist_utils.py", line 72, in init_distributed_mode
    torch.cuda.set_device(args.gpu)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch._C._cuda_setDevice(device)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/cuda/__init__.py", line 319, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
Traceback (most recent call last):
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 105, in <module>
    main()
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 70, in main
    init_distributed_mode(run_config)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/dist_utils.py", line 72, in init_distributed_mode
    torch.cuda.set_device(args.gpu)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch._C._cuda_setDevice(device)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/cuda/__init__.py", line 319, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
E0118 19:34:48.049647 957865 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 957913) of binary: /data/anaconda3/envs/salmonn_env/bin/python
Traceback (most recent call last):
  File "/data/anaconda3/envs/salmonn_env/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-01-18_19:34:48
  host      : nota-gpu-svr004
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 957914)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-01-18_19:34:48
  host      : nota-gpu-svr004
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 957913)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
| distributed init (rank 1, world 2): env://
| distributed init (rank 0, world 2): env://
[rank0]:[W118 19:36:17.559615455 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W118 19:36:17.559704949 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: kihoon090. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/wandb/run-20250118_193618-hvp07neu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama-3.2-1B-1percent-stage2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kihoon090/audio_lm
wandb: üöÄ View run at https://wandb.ai/kihoon090/audio_lm/runs/hvp07neu
2025-01-18 19:36:18,898 [INFO] 
=====  Running Parameters    =====
2025-01-18 19:36:18,900 [INFO] {
    "accum_grad_iters": 1,
    "amp": true,
    "batch_size_eval": 4,
    "batch_size_train": 4,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "epoch_based": false,
    "evaluate": false,
    "exp_name": "llama-3.2-1B-1percent-stage2",
    "gpu": 0,
    "iters_per_epoch": 20,
    "log_freq": 5,
    "num_workers": 8,
    "optims": {
        "beta2": 0.999,
        "init_lr": 3e-05,
        "max_epoch": 30,
        "min_lr": 1e-05,
        "warmup_start_lr": 1e-06,
        "warmup_steps": 3000,
        "weight_decay": 0.05
    },
    "output_dir": "outputs_stage2",
    "rank": 0,
    "seed": 42,
    "use_distributed": true,
    "world_size": 2
}
2025-01-18 19:36:18,900 [INFO] 
======  Dataset Attributes  ======
2025-01-18 19:36:18,900 [INFO] {
    "prefix": "/data/data",
    "test_ann_path": "/data/data/stage2_test.json",
    "train_ann_path": "/data/data/stage2_train.json",
    "valid_ann_path": "/data/data/stage2_valid.json",
    "whisper_path": "/data/ckp/whisper"
}
2025-01-18 19:36:18,900 [INFO] 
======  Model Attributes  ======
2025-01-18 19:36:18,901 [INFO] {
    "beats_path": "/data/ckp/beats/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt",
    "ckpt": "/data/audiolm-trainer/outputs_stage1_only/202501171604/checkpoint_best.pth",
    "end_sym": "</s>",
    "freeze_beats": true,
    "freeze_speech_QFormer": false,
    "freeze_speech_llama_proj": false,
    "freeze_whisper": true,
    "llama_path": "/data/ckp/llama",
    "lora": true,
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "lora_rank": 8,
    "max_txt_len": 300,
    "multi_prompt": true,
    "num_speech_query_token": 1,
    "prompt_path": "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/data/prompts/train_prompt.json",
    "prompt_template": "USER: {}\nASSISTANT:",
    "second_per_window": 0.333333,
    "second_stride": 0.333333,
    "speech_llama_proj_model": "",
    "test_prompt_path": "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/data/prompts/test_prompt.json",
    "token": "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",
    "use_speech_Qformer": true,
    "whisper_path": "/data/ckp/whisper",
    "window_level_Qformer": true
}
Traceback (most recent call last):
[rank1]: Traceback (most recent call last):
[rank1]:   File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 105, in <module>
[rank1]:     main()
[rank1]:   File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 86, in main
[rank1]:     "valid": SALMONNDataset(data_config.prefix, data_config.valid_ann_path, data_config.whisper_path),
[rank1]:   File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/dataset.py", line 32, in __init__
[rank1]:     self.annotation = json.load(open(ann_path, "r"))["annotation"]
[rank1]: FileNotFoundError: [Errno 2] No such file or directory: '/data/data/stage2_valid.json'
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 105, in <module>
    main()
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 86, in main
    "valid": SALMONNDataset(data_config.prefix, data_config.valid_ann_path, data_config.whisper_path),
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/dataset.py", line 32, in __init__
    self.annotation = json.load(open(ann_path, "r"))["annotation"]
FileNotFoundError: [Errno 2] No such file or directory: '/data/data/stage2_valid.json'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 105, in <module>
[rank0]:     main()
[rank0]:   File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 86, in main
[rank0]:     "valid": SALMONNDataset(data_config.prefix, data_config.valid_ann_path, data_config.whisper_path),
[rank0]:   File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/dataset.py", line 32, in __init__
[rank0]:     self.annotation = json.load(open(ann_path, "r"))["annotation"]
[rank0]: FileNotFoundError: [Errno 2] No such file or directory: '/data/data/stage2_valid.json'
[1;34mwandb[0m: üöÄ View run [33mllama-3.2-1B-1percent-stage2[0m at: [34mhttps://wandb.ai/kihoon090/audio_lm/runs/hvp07neu[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250118_193618-hvp07neu/logs[0m
W0118 19:36:23.828851 958108 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 958158 closing signal SIGTERM
E0118 19:36:23.993608 958108 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 958159) of binary: /data/anaconda3/envs/salmonn_env/bin/python
Traceback (most recent call last):
  File "/data/anaconda3/envs/salmonn_env/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-01-18_19:36:23
  host      : nota-gpu-svr004
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 958159)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
| distributed init (rank 0, world 2): env://
| distributed init (rank 1, world 2): env://
[rank0]:[W118 19:40:01.426261912 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W118 19:40:01.428143416 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: kihoon090. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/wandb/run-20250118_194003-dcoty5ar
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama-3.2-1B-1percent-stage2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kihoon090/audio_lm
wandb: üöÄ View run at https://wandb.ai/kihoon090/audio_lm/runs/dcoty5ar
2025-01-18 19:40:03,668 [INFO] 
=====  Running Parameters    =====
2025-01-18 19:40:03,669 [INFO] {
    "accum_grad_iters": 1,
    "amp": true,
    "batch_size_eval": 4,
    "batch_size_train": 4,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "epoch_based": false,
    "evaluate": false,
    "exp_name": "llama-3.2-1B-1percent-stage2",
    "gpu": 0,
    "iters_per_epoch": 20,
    "log_freq": 5,
    "num_workers": 8,
    "optims": {
        "beta2": 0.999,
        "init_lr": 3e-05,
        "max_epoch": 30,
        "min_lr": 1e-05,
        "warmup_start_lr": 1e-06,
        "warmup_steps": 3000,
        "weight_decay": 0.05
    },
    "output_dir": "outputs_stage2",
    "rank": 0,
    "seed": 42,
    "use_distributed": true,
    "world_size": 2
}
2025-01-18 19:40:03,669 [INFO] 
======  Dataset Attributes  ======
2025-01-18 19:40:03,670 [INFO] {
    "prefix": "/data/data",
    "test_ann_path": "/data/data/stage1_test.json",
    "train_ann_path": "/data/data/stage1_train.json",
    "valid_ann_path": "/data/data/stage1_valid.json",
    "whisper_path": "/data/ckp/whisper"
}
2025-01-18 19:40:03,670 [INFO] 
======  Model Attributes  ======
2025-01-18 19:40:03,670 [INFO] {
    "beats_path": "/data/ckp/beats/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt",
    "ckpt": "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/outputs_stage1_only/202501181852/checkpoint_best.pth",
    "end_sym": "</s>",
    "freeze_beats": true,
    "freeze_speech_QFormer": false,
    "freeze_speech_llama_proj": false,
    "freeze_whisper": true,
    "llama_path": "/data/ckp/llama",
    "lora": true,
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "lora_rank": 8,
    "max_txt_len": 300,
    "multi_prompt": true,
    "num_speech_query_token": 1,
    "prompt_path": "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/data/prompts/train_prompt.json",
    "prompt_template": "USER: {}\nASSISTANT:",
    "second_per_window": 0.333333,
    "second_stride": 0.333333,
    "speech_llama_proj_model": "",
    "test_prompt_path": "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/data/prompts/test_prompt.json",
    "token": "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",
    "use_speech_Qformer": true,
    "whisper_path": "/data/ckp/whisper",
    "window_level_Qformer": true
}
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  3.75it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.93it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.90it/s]
2025-01-18 19:40:05,951 [INFO] Loading LLaMA Tokenizer
2025-01-18 19:40:06,512 [INFO] Loading LLaMA Model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.82s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.06s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.48s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading training prompts done!
2025-01-18 19:41:45,085 [INFO] Loading LLaMA Done
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
2025-01-18 19:41:50,622 [INFO] LoRA Training
2025-01-18 19:41:50,622 [INFO] Loading Whisper Model
2025-01-18 19:41:51,386 [INFO] freeze Whisper
2025-01-18 19:41:51,387 [INFO] Loading BEATs Model
2025-01-18 19:41:51,703 [INFO] BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 0.6, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': True, 'predictor_dropout': 0.0, 'predictor_class': 527}
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
2025-01-18 19:41:54,467 [INFO] freeze BEATs
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
2025-01-18 19:41:56,119 [INFO] Loading speech LLAMA proj
Loading training prompts done!
2025-01-18 19:41:56,137 [INFO] Load SALMONN ckpt from: /data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/outputs_stage1_only/202501181852/checkpoint_best.pth
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
module.speech_query_tokens
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
module.ln_speech.weight
module.ln_speech.bias
module.ln_audio.weight
module.ln_audio.bias
module.speech_Qformer.bert.embeddings.LayerNorm.weight
module.speech_Qformer.bert.embeddings.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias
module.speech_llama_proj.weight
module.speech_llama_proj.bias
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
module.speech_query_tokens
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
module.ln_speech.weight
module.ln_speech.bias
module.ln_audio.weight
module.ln_audio.bias
module.speech_Qformer.bert.embeddings.LayerNorm.weight
module.speech_Qformer.bert.embeddings.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias
module.speech_llama_proj.weight
module.speech_llama_proj.bias
2025-01-18 19:42:00,520 [INFO] number of trainable parameters: 27498240
2025-01-18 19:42:00,524 [INFO] Training Phase
2025-01-18 19:42:00,533 [INFO] Start training epoch 0, 20 iters per inner epoch.
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.use_amp):
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.use_amp):
Train: data epoch: [0]  [ 0/20]  eta: 0:00:40  lr: 0.000001  loss: 0.1058  time: 2.0373  data: 0.0001  max mem: 13049
Train: data epoch: [0]  [ 0/20]  eta: 0:00:39  lr: 0.000001  loss: 0.8660  time: 1.9662  data: 0.0001  max mem: 12128
Train: data epoch: [0]  [ 5/20]  eta: 0:00:15  lr: 0.000001  loss: 0.1048  time: 1.0102  data: 0.0000  max mem: 13077
Train: data epoch: [0]  [ 5/20]  eta: 0:00:14  lr: 0.000001  loss: 0.1875  time: 0.9958  data: 0.0000  max mem: 12999
Train: data epoch: [0]  [10/20]  eta: 0:00:09  lr: 0.000001  loss: 0.5460  time: 0.9286  data: 0.0000  max mem: 13164
Train: data epoch: [0]  [10/20]  eta: 0:00:09  lr: 0.000001  loss: 0.6768  time: 0.9206  data: 0.0000  max mem: 13036
Train: data epoch: [0]  [15/20]  eta: 0:00:04  lr: 0.000001  loss: 1.3306  time: 0.8955  data: 0.0000  max mem: 13164
Train: data epoch: [0]  [15/20]  eta: 0:00:04  lr: 0.000001  loss: 0.5584  time: 0.8899  data: 0.0000  max mem: 13036
Train: data epoch: [0]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.4881  time: 0.8837  data: 0.0000  max mem: 13200
Train: data epoch: [0] Total time: 0:00:17 (0.8839 s / it)
Train: data epoch: [0]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.3390  time: 0.8792  data: 0.0000  max mem: 13036
Train: data epoch: [0] Total time: 0:00:17 (0.8798 s / it)
2025-01-18 19:42:18,131 [INFO] Averaged stats: lr: 0.0000  loss: 0.5636
2025-01-18 19:42:18,135 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:179: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.use_amp):
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:179: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.use_amp):
Eval: data epoch: [0]  [0/1]  eta: 0:00:00    time: 0.8896  data: 0.5670  max mem: 13200
Eval: data epoch: [0]  [0/1]  eta: 0:00:00    time: 0.9431  data: 0.6230  max mem: 13036
Eval: data epoch: [0] Total time: 0:00:01 (1.0018 s / it)
Eval: data epoch: [0] Total time: 0:00:01 (1.0786 s / it)
2025-01-18 19:42:19,241 [INFO] Saving checkpoint at epoch 0 to outputs_stage2/202501181940/checkpoint_best.pth.
2025-01-18 19:42:21,576 [INFO] Saving checkpoint at epoch 0 to outputs_stage2/202501181940/checkpoint_0.pth.
2025-01-18 19:42:23,892 [INFO] Training Phase
2025-01-18 19:42:23,899 [INFO] Start training epoch 1, 20 iters per inner epoch.
Train: data epoch: [1]  [ 0/20]  eta: 0:00:17  lr: 0.000001  loss: 1.2697  time: 0.8951  data: 0.0000  max mem: 13200
Train: data epoch: [1]  [ 0/20]  eta: 0:00:17  lr: 0.000001  loss: 0.6215  time: 0.8945  data: 0.0000  max mem: 13036
Train: data epoch: [1]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 1.1857  time: 0.7432  data: 0.0000  max mem: 13200
Train: data epoch: [1]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.5443  time: 0.7429  data: 0.0000  max mem: 13036
Train: data epoch: [1]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.6914  time: 0.7839  data: 0.0000  max mem: 13058Train: data epoch: [1]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 1.0478  time: 0.7843  data: 0.0000  max mem: 13200

Train: data epoch: [1]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.1523  time: 0.7845  data: 0.0000  max mem: 13200
Train: data epoch: [1]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 1.0824  time: 0.7843  data: 0.0000  max mem: 13058
Train: data epoch: [1]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.1790  time: 0.7843  data: 0.0000  max mem: 13200
Train: data epoch: [1] Total time: 0:00:15 (0.7845 s / it)
Train: data epoch: [1]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.7649  time: 0.7841  data: 0.0000  max mem: 13058
Train: data epoch: [1] Total time: 0:00:15 (0.7846 s / it)
2025-01-18 19:42:39,592 [INFO] Averaged stats: lr: 0.0000  loss: 0.6983
2025-01-18 19:42:39,596 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [1]  [0/1]  eta: 0:00:00    time: 0.8631  data: 0.5861  max mem: 13200
Eval: data epoch: [1]  [0/1]  eta: 0:00:00    time: 0.8701  data: 0.5952  max mem: 13058
Eval: data epoch: [1] Total time: 0:00:00 (0.9730 s / it)
Eval: data epoch: [1] Total time: 0:00:01 (1.0107 s / it)
2025-01-18 19:42:40,632 [INFO] Saving checkpoint at epoch 1 to outputs_stage2/202501181940/checkpoint_1.pth.
2025-01-18 19:42:42,954 [INFO] Training Phase
2025-01-18 19:42:42,961 [INFO] Start training epoch 2, 20 iters per inner epoch.
Train: data epoch: [2]  [ 0/20]  eta: 0:00:16  lr: 0.000001  loss: 0.5324  time: 0.8369  data: 0.0001  max mem: 13200
Train: data epoch: [2]  [ 0/20]  eta: 0:00:16  lr: 0.000001  loss: 0.4990  time: 0.8371  data: 0.0000  max mem: 13058
Train: data epoch: [2]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.1364  time: 0.7943  data: 0.0000  max mem: 13200
Train: data epoch: [2]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.2157  time: 0.7939  data: 0.0000  max mem: 13058
Train: data epoch: [2]  [10/20]  eta: 0:00:08  lr: 0.000001  loss: 0.0771  time: 0.8083  data: 0.0000  max mem: 13200
Train: data epoch: [2]  [10/20]  eta: 0:00:08  lr: 0.000001  loss: 0.0899  time: 0.8081  data: 0.0000  max mem: 13319
Train: data epoch: [2]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.0854  time: 0.7990  data: 0.0000  max mem: 13424
Train: data epoch: [2]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 2.0252  time: 0.7988  data: 0.0000  max mem: 13319
Train: data epoch: [2]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.4290  time: 0.7907  data: 0.0000  max mem: 13424
Train: data epoch: [2] Total time: 0:00:15 (0.7909 s / it)
Train: data epoch: [2]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.1009  time: 0.7905  data: 0.0000  max mem: 13319
Train: data epoch: [2] Total time: 0:00:15 (0.7910 s / it)
2025-01-18 19:42:58,783 [INFO] Averaged stats: lr: 0.0000  loss: 0.5610
2025-01-18 19:42:58,786 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [2]  [0/1]  eta: 0:00:00    time: 0.8400  data: 0.5675  max mem: 13424
Eval: data epoch: [2]  [0/1]  eta: 0:00:00    time: 0.8760  data: 0.6023  max mem: 13319
Eval: data epoch: [2] Total time: 0:00:00 (0.9516 s / it)
Eval: data epoch: [2] Total time: 0:00:01 (1.0286 s / it)
2025-01-18 19:42:59,840 [INFO] Saving checkpoint at epoch 2 to outputs_stage2/202501181940/checkpoint_2.pth.
2025-01-18 19:43:02,160 [INFO] Training Phase
2025-01-18 19:43:02,168 [INFO] Start training epoch 3, 20 iters per inner epoch.
Train: data epoch: [3]  [ 0/20]  eta: 0:00:17  lr: 0.000001  loss: 0.0806  time: 0.8779  data: 0.0000  max mem: 13424
Train: data epoch: [3]  [ 0/20]  eta: 0:00:17  lr: 0.000001  loss: 1.0721  time: 0.8783  data: 0.0000  max mem: 13319
Train: data epoch: [3]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.1370  time: 0.7857  data: 0.0000  max mem: 13424
Train: data epoch: [3]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 1.4315  time: 0.7857  data: 0.0000  max mem: 13319
Train: data epoch: [3]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.9761  time: 0.7860  data: 0.0000  max mem: 13424
Train: data epoch: [3]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.6656  time: 0.7860  data: 0.0000  max mem: 13319
Train: data epoch: [3]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 1.1322  time: 0.7873  data: 0.0000  max mem: 13424
Train: data epoch: [3]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.1483  time: 0.7871  data: 0.0000  max mem: 13319
Train: data epoch: [3]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.6811  time: 0.7852  data: 0.0000  max mem: 13424
Train: data epoch: [3] Total time: 0:00:15 (0.7854 s / it)
Train: data epoch: [3]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.8394  time: 0.7850  data: 0.0000  max mem: 13319
Train: data epoch: [3] Total time: 0:00:15 (0.7855 s / it)
2025-01-18 19:43:17,878 [INFO] Averaged stats: lr: 0.0000  loss: 0.5888
2025-01-18 19:43:17,882 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [3]  [0/1]  eta: 0:00:00    time: 0.8274  data: 0.5562  max mem: 13424
Eval: data epoch: [3]  [0/1]  eta: 0:00:00    time: 0.8661  data: 0.5746  max mem: 13319
Eval: data epoch: [3] Total time: 0:00:00 (0.9425 s / it)
Eval: data epoch: [3] Total time: 0:00:01 (1.0180 s / it)
2025-01-18 19:43:18,927 [INFO] Saving checkpoint at epoch 3 to outputs_stage2/202501181940/checkpoint_3.pth.
2025-01-18 19:43:21,239 [INFO] Training Phase
2025-01-18 19:43:21,247 [INFO] Start training epoch 4, 20 iters per inner epoch.
Train: data epoch: [4]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.2179  time: 0.7297  data: 0.0000  max mem: 13424
Train: data epoch: [4]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.2201  time: 0.7286  data: 0.0001  max mem: 13319
Train: data epoch: [4]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.8474  time: 0.7962  data: 0.0000  max mem: 13424
Train: data epoch: [4]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.1879  time: 0.7959  data: 0.0000  max mem: 13319
Train: data epoch: [4]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.9800  time: 0.7906  data: 0.0000  max mem: 13424
Train: data epoch: [4]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.2562  time: 0.7902  data: 0.0000  max mem: 13319
Train: data epoch: [4]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.1183  time: 0.7803  data: 0.0000  max mem: 13424
Train: data epoch: [4]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.1713  time: 0.7799  data: 0.0000  max mem: 13319
Train: data epoch: [4]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.3841  time: 0.7719  data: 0.0000  max mem: 13424
Train: data epoch: [4] Total time: 0:00:15 (0.7721 s / it)
Train: data epoch: [4]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.6820  time: 0.7716  data: 0.0000  max mem: 13319
Train: data epoch: [4] Total time: 0:00:15 (0.7722 s / it)
2025-01-18 19:43:36,692 [INFO] Averaged stats: lr: 0.0000  loss: 0.4974
2025-01-18 19:43:36,696 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [4]  [0/1]  eta: 0:00:00    time: 0.8365  data: 0.5676  max mem: 13424
Eval: data epoch: [4]  [0/1]  eta: 0:00:00    time: 0.8632  data: 0.5905  max mem: 13319
Eval: data epoch: [4] Total time: 0:00:00 (0.9500 s / it)
Eval: data epoch: [4] Total time: 0:00:01 (1.0061 s / it)
2025-01-18 19:43:37,728 [INFO] Saving checkpoint at epoch 4 to outputs_stage2/202501181940/checkpoint_4.pth.
2025-01-18 19:43:40,081 [INFO] Training Phase
2025-01-18 19:43:40,088 [INFO] Start training epoch 5, 20 iters per inner epoch.
Train: data epoch: [5]  [ 0/20]  eta: 0:00:17  lr: 0.000001  loss: 0.4863  time: 0.8886  data: 0.0001  max mem: 13424
Train: data epoch: [5]  [ 0/20]  eta: 0:00:17  lr: 0.000001  loss: 0.7133  time: 0.8882  data: 0.0000  max mem: 13319
Train: data epoch: [5]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 1.1389  time: 0.8188  data: 0.0000  max mem: 13424
Train: data epoch: [5]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.2094  time: 0.8186  data: 0.0000  max mem: 13319
Train: data epoch: [5]  [10/20]  eta: 0:00:08  lr: 0.000001  loss: 0.5677  time: 0.8053  data: 0.0000  max mem: 13424
Train: data epoch: [5]  [10/20]  eta: 0:00:08  lr: 0.000001  loss: 0.2073  time: 0.8052  data: 0.0000  max mem: 13319
Train: data epoch: [5]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.5426  time: 0.7993  data: 0.0000  max mem: 13424
Train: data epoch: [5]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.2910  time: 0.7990  data: 0.0000  max mem: 13319
Train: data epoch: [5]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.2363  time: 0.7981  data: 0.0000  max mem: 13424
Train: data epoch: [5] Total time: 0:00:15 (0.7982 s / it)
Train: data epoch: [5]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 1.1003  time: 0.7978  data: 0.0000  max mem: 13319
Train: data epoch: [5] Total time: 0:00:15 (0.7983 s / it)
2025-01-18 19:43:56,057 [INFO] Averaged stats: lr: 0.0000  loss: 0.5890
2025-01-18 19:43:56,061 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [5]  [0/1]  eta: 0:00:00    time: 0.8258  data: 0.5590  max mem: 13424
Eval: data epoch: [5]  [0/1]  eta: 0:00:00    time: 0.8615  data: 0.5856  max mem: 13319
Eval: data epoch: [5] Total time: 0:00:00 (0.9391 s / it)
Eval: data epoch: [5] Total time: 0:00:01 (1.0066 s / it)
2025-01-18 19:43:57,093 [INFO] Saving checkpoint at epoch 5 to outputs_stage2/202501181940/checkpoint_5.pth.
2025-01-18 19:43:59,433 [INFO] Training Phase
2025-01-18 19:43:59,440 [INFO] Start training epoch 6, 20 iters per inner epoch.
Train: data epoch: [6]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.3742  time: 0.7302  data: 0.0000  max mem: 13424
Train: data epoch: [6]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.2059  time: 0.7295  data: 0.0000  max mem: 13319
Train: data epoch: [6]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.1303  time: 0.7597  data: 0.0000  max mem: 13424
Train: data epoch: [6]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.5461  time: 0.7595  data: 0.0000  max mem: 13319
Train: data epoch: [6]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.6455  time: 0.7576  data: 0.0000  max mem: 13424
Train: data epoch: [6]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.3380  time: 0.7575  data: 0.0000  max mem: 13319
Train: data epoch: [6]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.4208  time: 0.7610  data: 0.0000  max mem: 13424
Train: data epoch: [6]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.4659  time: 0.7607  data: 0.0000  max mem: 13319
Train: data epoch: [6]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.5576  time: 0.7563  data: 0.0000  max mem: 13424
Train: data epoch: [6] Total time: 0:00:15 (0.7565 s / it)
Train: data epoch: [6]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.1481  time: 0.7561  data: 0.0000  max mem: 13319
Train: data epoch: [6] Total time: 0:00:15 (0.7566 s / it)
2025-01-18 19:44:14,573 [INFO] Averaged stats: lr: 0.0000  loss: 0.4661
2025-01-18 19:44:14,577 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [6]  [0/1]  eta: 0:00:00    time: 0.8402  data: 0.5706  max mem: 13424
Eval: data epoch: [6]  [0/1]  eta: 0:00:00    time: 0.8613  data: 0.5859  max mem: 13319
Eval: data epoch: [6] Total time: 0:00:00 (0.9530 s / it)
Eval: data epoch: [6] Total time: 0:00:01 (1.0115 s / it)
2025-01-18 19:44:15,615 [INFO] Saving checkpoint at epoch 6 to outputs_stage2/202501181940/checkpoint_6.pth.
2025-01-18 19:44:17,956 [INFO] Training Phase
2025-01-18 19:44:17,963 [INFO] Start training epoch 7, 20 iters per inner epoch.
Train: data epoch: [7]  [ 0/20]  eta: 0:00:13  lr: 0.000001  loss: 0.3253  time: 0.6886  data: 0.0000  max mem: 13424
Train: data epoch: [7]  [ 0/20]  eta: 0:00:13  lr: 0.000001  loss: 0.1297  time: 0.6888  data: 0.0000  max mem: 13319
Train: data epoch: [7]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 1.0838  time: 0.8122  data: 0.0000  max mem: 13424
Train: data epoch: [7]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.5858  time: 0.8118  data: 0.0000  max mem: 13319
Train: data epoch: [7]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.4125  time: 0.7804  data: 0.0000  max mem: 13424
Train: data epoch: [7]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.7846  time: 0.7798  data: 0.0000  max mem: 13319
Train: data epoch: [7]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.1998  time: 0.7773  data: 0.0000  max mem: 13424
Train: data epoch: [7]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.9183  time: 0.7770  data: 0.0000  max mem: 13319
Train: data epoch: [7]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.3769  time: 0.7731  data: 0.0000  max mem: 13424
Train: data epoch: [7] Total time: 0:00:15 (0.7733 s / it)
Train: data epoch: [7]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.4262  time: 0.7726  data: 0.0000  max mem: 13319
Train: data epoch: [7] Total time: 0:00:15 (0.7733 s / it)
2025-01-18 19:44:33,431 [INFO] Averaged stats: lr: 0.0000  loss: 0.5271
2025-01-18 19:44:33,435 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [7]  [0/1]  eta: 0:00:00    time: 0.8255  data: 0.5540  max mem: 13424
Eval: data epoch: [7]  [0/1]  eta: 0:00:00    time: 0.8666  data: 0.5930  max mem: 13319
Eval: data epoch: [7] Total time: 0:00:00 (0.9531 s / it)
Eval: data epoch: [7] Total time: 0:00:01 (1.0148 s / it)
2025-01-18 19:44:34,476 [INFO] Saving checkpoint at epoch 7 to outputs_stage2/202501181940/checkpoint_7.pth.
2025-01-18 19:44:36,809 [INFO] Training Phase
2025-01-18 19:44:36,816 [INFO] Start training epoch 8, 20 iters per inner epoch.
Train: data epoch: [8]  [ 0/20]  eta: 0:00:13  lr: 0.000001  loss: 0.7382  time: 0.6891  data: 0.0000  max mem: 13424
Train: data epoch: [8]  [ 0/20]  eta: 0:00:13  lr: 0.000001  loss: 0.3711  time: 0.6880  data: 0.0000  max mem: 13319
Train: data epoch: [8]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.6405  time: 0.8058  data: 0.0000  max mem: 13424
Train: data epoch: [8]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.3976  time: 0.8056  data: 0.0000  max mem: 13319
Train: data epoch: [8]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.3584  time: 0.7887  data: 0.0000  max mem: 13424
Train: data epoch: [8]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.3822  time: 0.7885  data: 0.0000  max mem: 13521
Train: data epoch: [8]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.2610  time: 0.7836  data: 0.0000  max mem: 13424
Train: data epoch: [8]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.4088  time: 0.7834  data: 0.0000  max mem: 13521
Train: data epoch: [8]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.1845  time: 0.7859  data: 0.0000  max mem: 13424
Train: data epoch: [8] Total time: 0:00:15 (0.7861 s / it)
Train: data epoch: [8]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 1.1176  time: 0.7856  data: 0.0000  max mem: 13521
Train: data epoch: [8] Total time: 0:00:15 (0.7861 s / it)
2025-01-18 19:44:52,540 [INFO] Averaged stats: lr: 0.0000  loss: 0.4396
2025-01-18 19:44:52,543 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [8]  [0/1]  eta: 0:00:00    time: 0.8515  data: 0.5765  max mem: 13424
Eval: data epoch: [8]  [0/1]  eta: 0:00:00    time: 0.8776  data: 0.6026  max mem: 13521
Eval: data epoch: [8] Total time: 0:00:00 (0.9647 s / it)
Eval: data epoch: [8] Total time: 0:00:01 (1.0551 s / it)
2025-01-18 19:44:53,623 [INFO] Saving checkpoint at epoch 8 to outputs_stage2/202501181940/checkpoint_8.pth.
2025-01-18 19:44:55,974 [INFO] Training Phase
2025-01-18 19:44:55,981 [INFO] Start training epoch 9, 20 iters per inner epoch.
Train: data epoch: [9]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.6516  time: 0.7259  data: 0.0000  max mem: 13424
Train: data epoch: [9]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.3108  time: 0.7261  data: 0.0000  max mem: 13521
Train: data epoch: [9]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.9448  time: 0.8001  data: 0.0000  max mem: 13424
Train: data epoch: [9]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.5586  time: 0.7998  data: 0.0000  max mem: 13521
Train: data epoch: [9]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.1937  time: 0.7733  data: 0.0000  max mem: 13424
Train: data epoch: [9]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.3038  time: 0.7731  data: 0.0000  max mem: 13521
Train: data epoch: [9]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 2.0315  time: 0.7836  data: 0.0000  max mem: 13424
Train: data epoch: [9]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.4105  time: 0.7825  data: 0.0000  max mem: 13521
Train: data epoch: [9]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.0955  time: 0.7731  data: 0.0000  max mem: 13424
Train: data epoch: [9] Total time: 0:00:15 (0.7733 s / it)
Train: data epoch: [9]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.0500  time: 0.7721  data: 0.0000  max mem: 13521
Train: data epoch: [9] Total time: 0:00:15 (0.7733 s / it)
2025-01-18 19:45:11,449 [INFO] Averaged stats: lr: 0.0000  loss: 0.5066
2025-01-18 19:45:11,453 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [9]  [0/1]  eta: 0:00:00    time: 0.7443  data: 0.4621  max mem: 13521
Eval: data epoch: [9]  [0/1]  eta: 0:00:00    time: 0.8349  data: 0.5592  max mem: 13424
Eval: data epoch: [9] Total time: 0:00:00 (0.8370 s / it)
Eval: data epoch: [9] Total time: 0:00:00 (0.9486 s / it)
2025-01-18 19:45:12,427 [INFO] Saving checkpoint at epoch 9 to outputs_stage2/202501181940/checkpoint_9.pth.
2025-01-18 19:45:14,794 [INFO] Training Phase
2025-01-18 19:45:14,801 [INFO] Start training epoch 10, 20 iters per inner epoch.
Train: data epoch: [10]  [ 0/20]  eta: 0:00:17  lr: 0.000001  loss: 0.9023  time: 0.8993  data: 0.0000  max mem: 13424
Train: data epoch: [10]  [ 0/20]  eta: 0:00:17  lr: 0.000001  loss: 0.5045  time: 0.8994  data: 0.0001  max mem: 13521
Train: data epoch: [10]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.2015  time: 0.7928  data: 0.0000  max mem: 13424
Train: data epoch: [10]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.2090  time: 0.7925  data: 0.0000  max mem: 13521
Train: data epoch: [10]  [10/20]  eta: 0:00:08  lr: 0.000001  loss: 1.0019  time: 0.8103  data: 0.0000  max mem: 13424
Train: data epoch: [10]  [10/20]  eta: 0:00:08  lr: 0.000001  loss: 1.0706  time: 0.8100  data: 0.0000  max mem: 13521
Train: data epoch: [10]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 1.0491  time: 0.7922  data: 0.0000  max mem: 13424
Train: data epoch: [10]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.8782  time: 0.7920  data: 0.0000  max mem: 13521
Train: data epoch: [10]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.9471  time: 0.8061  data: 0.0000  max mem: 13424
Train: data epoch: [10] Total time: 0:00:16 (0.8063 s / it)
Train: data epoch: [10]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.2179  time: 0.8058  data: 0.0000  max mem: 13521
Train: data epoch: [10] Total time: 0:00:16 (0.8063 s / it)
2025-01-18 19:45:30,928 [INFO] Averaged stats: lr: 0.0000  loss: 0.5625
2025-01-18 19:45:30,932 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [10]  [0/1]  eta: 0:00:00    time: 0.7174  data: 0.4423  max mem: 13521
Eval: data epoch: [10] Total time: 0:00:00 (0.8105 s / it)
Eval: data epoch: [10]  [0/1]  eta: 0:00:00    time: 0.8575  data: 0.5860  max mem: 13424
Eval: data epoch: [10] Total time: 0:00:00 (0.9773 s / it)
2025-01-18 19:45:31,934 [INFO] Saving checkpoint at epoch 10 to outputs_stage2/202501181940/checkpoint_10.pth.
2025-01-18 19:45:34,262 [INFO] Training Phase
2025-01-18 19:45:34,269 [INFO] Start training epoch 11, 20 iters per inner epoch.
Train: data epoch: [11]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.2749  time: 0.7336  data: 0.0001  max mem: 13424
Train: data epoch: [11]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.1172  time: 0.7339  data: 0.0001  max mem: 13521
Train: data epoch: [11]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.2366  time: 0.8024  data: 0.0000  max mem: 13424
Train: data epoch: [11]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.2427  time: 0.8021  data: 0.0000  max mem: 13521
Train: data epoch: [11]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.3216  time: 0.7789  data: 0.0000  max mem: 13424
Train: data epoch: [11]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.1214  time: 0.7787  data: 0.0000  max mem: 13521
Train: data epoch: [11]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.3320  time: 0.7883  data: 0.0000  max mem: 13424
Train: data epoch: [11]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.3475  time: 0.7871  data: 0.0000  max mem: 13521
Train: data epoch: [11]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 1.2032  time: 0.7724  data: 0.0000  max mem: 13424
Train: data epoch: [11] Total time: 0:00:15 (0.7726 s / it)
Train: data epoch: [11]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 1.0396  time: 0.7715  data: 0.0000  max mem: 13521
Train: data epoch: [11] Total time: 0:00:15 (0.7727 s / it)
2025-01-18 19:45:49,724 [INFO] Averaged stats: lr: 0.0000  loss: 0.3805
2025-01-18 19:45:49,729 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [11]  [0/1]  eta: 0:00:00    time: 0.7146  data: 0.4384  max mem: 13521
Eval: data epoch: [11] Total time: 0:00:00 (0.8117 s / it)
Eval: data epoch: [11]  [0/1]  eta: 0:00:00    time: 0.8548  data: 0.5746  max mem: 13424
Eval: data epoch: [11] Total time: 0:00:00 (0.9744 s / it)
2025-01-18 19:45:50,727 [INFO] Saving checkpoint at epoch 11 to outputs_stage2/202501181940/checkpoint_11.pth.
2025-01-18 19:45:53,066 [INFO] Training Phase
2025-01-18 19:45:53,074 [INFO] Start training epoch 12, 20 iters per inner epoch.
Train: data epoch: [12]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.1004  time: 0.7416  data: 0.0000  max mem: 13424
Train: data epoch: [12]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.2143  time: 0.7413  data: 0.0000  max mem: 13521
Train: data epoch: [12]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.2608  time: 0.7923  data: 0.0000  max mem: 13424
Train: data epoch: [12]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.4906  time: 0.7921  data: 0.0000  max mem: 13521
Train: data epoch: [12]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.4790  time: 0.7846  data: 0.0000  max mem: 13424
Train: data epoch: [12]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.0750  time: 0.7844  data: 0.0000  max mem: 13521
Train: data epoch: [12]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.1121  time: 0.7901  data: 0.0000  max mem: 13424
Train: data epoch: [12]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.3437  time: 0.7899  data: 0.0000  max mem: 13521
Train: data epoch: [12]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.4075  time: 0.7819  data: 0.0000  max mem: 13424
Train: data epoch: [12] Total time: 0:00:15 (0.7821 s / it)
Train: data epoch: [12]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.1708  time: 0.7817  data: 0.0000  max mem: 13521
Train: data epoch: [12] Total time: 0:00:15 (0.7822 s / it)
2025-01-18 19:46:08,720 [INFO] Averaged stats: lr: 0.0000  loss: 0.3766
2025-01-18 19:46:08,724 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [12]  [0/1]  eta: 0:00:00    time: 0.7054  data: 0.4317  max mem: 13521
Eval: data epoch: [12] Total time: 0:00:00 (0.8003 s / it)
Eval: data epoch: [12]  [0/1]  eta: 0:00:00    time: 0.8267  data: 0.5557  max mem: 13424
Eval: data epoch: [12] Total time: 0:00:00 (0.9420 s / it)
2025-01-18 19:46:09,690 [INFO] Saving checkpoint at epoch 12 to outputs_stage2/202501181940/checkpoint_12.pth.
2025-01-18 19:46:12,027 [INFO] Training Phase
2025-01-18 19:46:12,035 [INFO] Start training epoch 13, 20 iters per inner epoch.
Train: data epoch: [13]  [ 0/20]  eta: 0:00:13  lr: 0.000001  loss: 0.3073  time: 0.6821  data: 0.0000  max mem: 13424
Train: data epoch: [13]  [ 0/20]  eta: 0:00:13  lr: 0.000001  loss: 0.3862  time: 0.6825  data: 0.0000  max mem: 13521
Train: data epoch: [13]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.7196  time: 0.7632  data: 0.0000  max mem: 13424
Train: data epoch: [13]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.1866  time: 0.7631  data: 0.0000  max mem: 13521
Train: data epoch: [13]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.2535  time: 0.7518  data: 0.0000  max mem: 13424
Train: data epoch: [13]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.9995  time: 0.7516  data: 0.0000  max mem: 13521
Train: data epoch: [13]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 1.1421  time: 0.7717  data: 0.0000  max mem: 13424
Train: data epoch: [13]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.4431  time: 0.7715  data: 0.0000  max mem: 13521
Train: data epoch: [13]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.7750  time: 0.7718  data: 0.0000  max mem: 13424
Train: data epoch: [13] Total time: 0:00:15 (0.7721 s / it)
Train: data epoch: [13]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.5913  time: 0.7716  data: 0.0000  max mem: 13521
Train: data epoch: [13] Total time: 0:00:15 (0.7721 s / it)
2025-01-18 19:46:27,478 [INFO] Averaged stats: lr: 0.0000  loss: 0.5319
2025-01-18 19:46:27,482 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [13]  [0/1]  eta: 0:00:00    time: 0.7061  data: 0.4311  max mem: 13521
Eval: data epoch: [13] Total time: 0:00:00 (0.8064 s / it)
Eval: data epoch: [13]  [0/1]  eta: 0:00:00    time: 0.8814  data: 0.6066  max mem: 13424
Eval: data epoch: [13] Total time: 0:00:01 (1.0079 s / it)
2025-01-18 19:46:28,516 [INFO] Saving checkpoint at epoch 13 to outputs_stage2/202501181940/checkpoint_13.pth.
2025-01-18 19:46:30,862 [INFO] Training Phase
2025-01-18 19:46:30,870 [INFO] Start training epoch 14, 20 iters per inner epoch.
Train: data epoch: [14]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.3544  time: 0.7046  data: 0.0000  max mem: 13424
Train: data epoch: [14]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.2896  time: 0.7051  data: 0.0000  max mem: 13521
Train: data epoch: [14]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.1762  time: 0.7809  data: 0.0000  max mem: 13424
Train: data epoch: [14]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.4340  time: 0.7806  data: 0.0000  max mem: 13521
Train: data epoch: [14]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.2880  time: 0.7880  data: 0.0000  max mem: 13424
Train: data epoch: [14]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.2894  time: 0.7878  data: 0.0000  max mem: 13521
Train: data epoch: [14]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.1167  time: 0.7892  data: 0.0000  max mem: 13424
Train: data epoch: [14]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.5372  time: 0.7890  data: 0.0000  max mem: 13521
Train: data epoch: [14]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.1151  time: 0.7833  data: 0.0000  max mem: 13424
Train: data epoch: [14] Total time: 0:00:15 (0.7835 s / it)
Train: data epoch: [14]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.4369  time: 0.7831  data: 0.0000  max mem: 13521
Train: data epoch: [14] Total time: 0:00:15 (0.7836 s / it)
2025-01-18 19:46:46,543 [INFO] Averaged stats: lr: 0.0000  loss: 0.4639
2025-01-18 19:46:46,547 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [14]  [0/1]  eta: 0:00:00    time: 0.7026  data: 0.4308  max mem: 13521
Eval: data epoch: [14] Total time: 0:00:00 (0.8165 s / it)
Eval: data epoch: [14]  [0/1]  eta: 0:00:00    time: 0.8324  data: 0.5560  max mem: 13424
Eval: data epoch: [14] Total time: 0:00:00 (0.9725 s / it)
2025-01-18 19:46:47,543 [INFO] Saving checkpoint at epoch 14 to outputs_stage2/202501181940/checkpoint_14.pth.
2025-01-18 19:46:49,889 [INFO] Training Phase
2025-01-18 19:46:49,896 [INFO] Start training epoch 15, 20 iters per inner epoch.
Train: data epoch: [15]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.1154  time: 0.7290  data: 0.0000  max mem: 13424
Train: data epoch: [15]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.0620  time: 0.7295  data: 0.0000  max mem: 13521
Train: data epoch: [15]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.5246  time: 0.8006  data: 0.0000  max mem: 13424
Train: data epoch: [15]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.2854  time: 0.8003  data: 0.0000  max mem: 13521
Train: data epoch: [15]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.6676  time: 0.7864  data: 0.0000  max mem: 13424
Train: data epoch: [15]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.6129  time: 0.7861  data: 0.0000  max mem: 13521
Train: data epoch: [15]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.2115  time: 0.7727  data: 0.0000  max mem: 13424
Train: data epoch: [15]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.3402  time: 0.7716  data: 0.0000  max mem: 13521
Train: data epoch: [15]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.6033  time: 0.7836  data: 0.0000  max mem: 13424
Train: data epoch: [15] Total time: 0:00:15 (0.7838 s / it)
Train: data epoch: [15]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.5796  time: 0.7827  data: 0.0000  max mem: 13521
Train: data epoch: [15] Total time: 0:00:15 (0.7838 s / it)
2025-01-18 19:47:05,574 [INFO] Averaged stats: lr: 0.0000  loss: 0.4459
2025-01-18 19:47:05,578 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [15]  [0/1]  eta: 0:00:00    time: 0.6995  data: 0.4277  max mem: 13521
Eval: data epoch: [15] Total time: 0:00:00 (0.8086 s / it)
Eval: data epoch: [15]  [0/1]  eta: 0:00:00    time: 0.8163  data: 0.5466  max mem: 13424
Eval: data epoch: [15] Total time: 0:00:00 (0.9329 s / it)
2025-01-18 19:47:06,536 [INFO] Saving checkpoint at epoch 15 to outputs_stage2/202501181940/checkpoint_15.pth.
2025-01-18 19:47:08,841 [INFO] Training Phase
2025-01-18 19:47:08,848 [INFO] Start training epoch 16, 20 iters per inner epoch.
Train: data epoch: [16]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.2286  time: 0.7025  data: 0.0000  max mem: 13424
Train: data epoch: [16]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.1700  time: 0.7013  data: 0.0000  max mem: 13521
Train: data epoch: [16]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.3010  time: 0.7850  data: 0.0000  max mem: 13424
Train: data epoch: [16]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.1836  time: 0.7847  data: 0.0000  max mem: 13521
Train: data epoch: [16]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.0730  time: 0.7579  data: 0.0000  max mem: 13424
Train: data epoch: [16]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 1.7011  time: 0.7577  data: 0.0000  max mem: 13521
Train: data epoch: [16]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.4824  time: 0.7833  data: 0.0000  max mem: 13424
Train: data epoch: [16]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.0901  time: 0.7831  data: 0.0000  max mem: 13521
Train: data epoch: [16]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.1370  time: 0.7845  data: 0.0000  max mem: 13424
Train: data epoch: [16]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 1.0351  time: 0.7842  data: 0.0000  max mem: 13521Train: data epoch: [16] Total time: 0:00:15 (0.7848 s / it)

Train: data epoch: [16] Total time: 0:00:15 (0.7848 s / it)
2025-01-18 19:47:24,544 [INFO] Averaged stats: lr: 0.0000  loss: 0.5062
2025-01-18 19:47:24,547 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [16]  [0/1]  eta: 0:00:00    time: 0.7046  data: 0.4289  max mem: 13521
Eval: data epoch: [16] Total time: 0:00:00 (0.8257 s / it)
Eval: data epoch: [16]  [0/1]  eta: 0:00:00    time: 0.8267  data: 0.5539  max mem: 13424
Eval: data epoch: [16] Total time: 0:00:00 (0.9562 s / it)
2025-01-18 19:47:25,529 [INFO] Saving checkpoint at epoch 16 to outputs_stage2/202501181940/checkpoint_16.pth.
2025-01-18 19:47:27,866 [INFO] Training Phase
2025-01-18 19:47:27,873 [INFO] Start training epoch 17, 20 iters per inner epoch.
Train: data epoch: [17]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.1920  time: 0.7361  data: 0.0000  max mem: 13424
Train: data epoch: [17]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.3495  time: 0.7356  data: 0.0000  max mem: 13521
Train: data epoch: [17]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.0541  time: 0.7773  data: 0.0000  max mem: 13424
Train: data epoch: [17]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.2327  time: 0.7772  data: 0.0000  max mem: 13521
Train: data epoch: [17]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.3033  time: 0.7725  data: 0.0000  max mem: 13424
Train: data epoch: [17]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.0790  time: 0.7722  data: 0.0000  max mem: 13521
Train: data epoch: [17]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 1.4342  time: 0.7790  data: 0.0000  max mem: 13424
Train: data epoch: [17]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.7816  time: 0.7788  data: 0.0000  max mem: 13521
Train: data epoch: [17]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.2512  time: 0.7898  data: 0.0000  max mem: 13424
Train: data epoch: [17] Total time: 0:00:15 (0.7900 s / it)
Train: data epoch: [17]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.0784  time: 0.7895  data: 0.0000  max mem: 13521
Train: data epoch: [17] Total time: 0:00:15 (0.7900 s / it)
2025-01-18 19:47:43,675 [INFO] Averaged stats: lr: 0.0000  loss: 0.4890
2025-01-18 19:47:43,679 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [17]  [0/1]  eta: 0:00:00    time: 0.7072  data: 0.4317  max mem: 13521
Eval: data epoch: [17] Total time: 0:00:00 (0.8047 s / it)
Eval: data epoch: [17]  [0/1]  eta: 0:00:00    time: 0.8223  data: 0.5505  max mem: 13424
Eval: data epoch: [17] Total time: 0:00:00 (0.9436 s / it)
2025-01-18 19:47:44,647 [INFO] Saving checkpoint at epoch 17 to outputs_stage2/202501181940/checkpoint_17.pth.
2025-01-18 19:47:46,949 [INFO] Training Phase
2025-01-18 19:47:46,957 [INFO] Start training epoch 18, 20 iters per inner epoch.
Train: data epoch: [18]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.0717  time: 0.7353  data: 0.0000  max mem: 13424
Train: data epoch: [18]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.0299  time: 0.7342  data: 0.0000  max mem: 13521
Train: data epoch: [18]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.1504  time: 0.8205  data: 0.0000  max mem: 13424
Train: data epoch: [18]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.3131  time: 0.8203  data: 0.0000  max mem: 13521
Train: data epoch: [18]  [10/20]  eta: 0:00:08  lr: 0.000001  loss: 1.0174  time: 0.8089  data: 0.0000  max mem: 13424
Train: data epoch: [18]  [10/20]  eta: 0:00:08  lr: 0.000001  loss: 0.7517  time: 0.8087  data: 0.0000  max mem: 13521
Train: data epoch: [18]  [15/20]  eta: 0:00:04  lr: 0.000001  loss: 1.2804  time: 0.8130  data: 0.0000  max mem: 13424
Train: data epoch: [18]  [15/20]  eta: 0:00:04  lr: 0.000001  loss: 1.1853  time: 0.8128  data: 0.0000  max mem: 13521
Train: data epoch: [18]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.2142  time: 0.7998  data: 0.0000  max mem: 13424
Train: data epoch: [18] Total time: 0:00:15 (0.7999 s / it)
Train: data epoch: [18]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.2654  time: 0.7996  data: 0.0000  max mem: 13521
Train: data epoch: [18] Total time: 0:00:16 (0.8000 s / it)
2025-01-18 19:48:02,958 [INFO] Averaged stats: lr: 0.0000  loss: 0.4915
2025-01-18 19:48:02,962 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [18]  [0/1]  eta: 0:00:00    time: 0.7050  data: 0.4367  max mem: 13521
Eval: data epoch: [18] Total time: 0:00:00 (0.8228 s / it)
Eval: data epoch: [18]  [0/1]  eta: 0:00:00    time: 0.8428  data: 0.5641  max mem: 13424
Eval: data epoch: [18] Total time: 0:00:00 (0.9711 s / it)
2025-01-18 19:48:03,958 [INFO] Saving checkpoint at epoch 18 to outputs_stage2/202501181940/checkpoint_18.pth.
2025-01-18 19:48:06,269 [INFO] Training Phase
2025-01-18 19:48:06,277 [INFO] Start training epoch 19, 20 iters per inner epoch.
Train: data epoch: [19]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.2959  time: 0.7135  data: 0.0000  max mem: 13424
Train: data epoch: [19]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.2065  time: 0.7144  data: 0.0000  max mem: 13521
Train: data epoch: [19]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.1269  time: 0.7425  data: 0.0000  max mem: 13424
Train: data epoch: [19]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.3321  time: 0.7424  data: 0.0000  max mem: 13521
Train: data epoch: [19]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.3761  time: 0.7556  data: 0.0000  max mem: 13424
Train: data epoch: [19]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.5890  time: 0.7554  data: 0.0000  max mem: 13521
Train: data epoch: [19]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.0875  time: 0.7648  data: 0.0000  max mem: 13424
Train: data epoch: [19]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.2978  time: 0.7646  data: 0.0000  max mem: 13521
Train: data epoch: [19]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.3455  time: 0.7709  data: 0.0000  max mem: 13424
Train: data epoch: [19] Total time: 0:00:15 (0.7711 s / it)
Train: data epoch: [19]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.2155  time: 0.7707  data: 0.0000  max mem: 13521
Train: data epoch: [19] Total time: 0:00:15 (0.7712 s / it)
2025-01-18 19:48:21,702 [INFO] Averaged stats: lr: 0.0000  loss: 0.3147
2025-01-18 19:48:21,705 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [19]  [0/1]  eta: 0:00:00    time: 0.7481  data: 0.4789  max mem: 13521
Eval: data epoch: [19]  [0/1]  eta: 0:00:00    time: 0.8425  data: 0.5673  max mem: 13424
Eval: data epoch: [19] Total time: 0:00:00 (0.8546 s / it)
Eval: data epoch: [19] Total time: 0:00:00 (0.9502 s / it)
2025-01-18 19:48:22,683 [INFO] Saving checkpoint at epoch 19 to outputs_stage2/202501181940/checkpoint_19.pth.
2025-01-18 19:48:25,009 [INFO] Training Phase
2025-01-18 19:48:25,017 [INFO] Start training epoch 20, 20 iters per inner epoch.
Train: data epoch: [20]  [ 0/20]  eta: 0:00:18  lr: 0.000001  loss: 0.1468  time: 0.9047  data: 0.0000  max mem: 13424
Train: data epoch: [20]  [ 0/20]  eta: 0:00:18  lr: 0.000001  loss: 0.9879  time: 0.9054  data: 0.0000  max mem: 13521
Train: data epoch: [20]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.2103  time: 0.8171  data: 0.0000  max mem: 13424
Train: data epoch: [20]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.4295  time: 0.8169  data: 0.0000  max mem: 13521
Train: data epoch: [20]  [10/20]  eta: 0:00:08  lr: 0.000001  loss: 0.4787  time: 0.8185  data: 0.0000  max mem: 13424
Train: data epoch: [20]  [10/20]  eta: 0:00:08  lr: 0.000001  loss: 1.1768  time: 0.8182  data: 0.0000  max mem: 13521
Train: data epoch: [20]  [15/20]  eta: 0:00:04  lr: 0.000001  loss: 0.0988  time: 0.8184  data: 0.0000  max mem: 13424
Train: data epoch: [20]  [15/20]  eta: 0:00:04  lr: 0.000001  loss: 1.7523  time: 0.8182  data: 0.0000  max mem: 13521
Train: data epoch: [20]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 1.1428  time: 0.8150  data: 0.0000  max mem: 13424
Train: data epoch: [20] Total time: 0:00:16 (0.8152 s / it)
Train: data epoch: [20]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.6394  time: 0.8148  data: 0.0000  max mem: 13521
Train: data epoch: [20] Total time: 0:00:16 (0.8153 s / it)
2025-01-18 19:48:41,323 [INFO] Averaged stats: lr: 0.0000  loss: 0.5185
2025-01-18 19:48:41,328 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [20]  [0/1]  eta: 0:00:00    time: 0.7359  data: 0.4473  max mem: 13521
Eval: data epoch: [20]  [0/1]  eta: 0:00:00    time: 0.8243  data: 0.5528  max mem: 13424
Eval: data epoch: [20] Total time: 0:00:00 (0.8503 s / it)
Eval: data epoch: [20] Total time: 0:00:00 (0.9432 s / it)
2025-01-18 19:48:42,295 [INFO] Saving checkpoint at epoch 20 to outputs_stage2/202501181940/checkpoint_20.pth.
2025-01-18 19:48:44,619 [INFO] Training Phase
2025-01-18 19:48:44,627 [INFO] Start training epoch 21, 20 iters per inner epoch.
Train: data epoch: [21]  [ 0/20]  eta: 0:00:18  lr: 0.000001  loss: 0.8476  time: 0.9279  data: 0.0000  max mem: 13424
Train: data epoch: [21]  [ 0/20]  eta: 0:00:18  lr: 0.000001  loss: 0.2384  time: 0.9273  data: 0.0000  max mem: 13521
Train: data epoch: [21]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.1292  time: 0.8192  data: 0.0000  max mem: 13424
Train: data epoch: [21]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.4294  time: 0.8190  data: 0.0000  max mem: 13521
Train: data epoch: [21]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.4250  time: 0.7823  data: 0.0000  max mem: 13424
Train: data epoch: [21]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.2305  time: 0.7821  data: 0.0000  max mem: 13521
Train: data epoch: [21]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.6465  time: 0.7904  data: 0.0000  max mem: 13424
Train: data epoch: [21]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.2600  time: 0.7902  data: 0.0000  max mem: 13521
Train: data epoch: [21]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 1.3961  time: 0.7917  data: 0.0000  max mem: 13424
Train: data epoch: [21] Total time: 0:00:15 (0.7919 s / it)
Train: data epoch: [21]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.3952  time: 0.7915  data: 0.0000  max mem: 13521
Train: data epoch: [21] Total time: 0:00:15 (0.7920 s / it)
2025-01-18 19:49:00,468 [INFO] Averaged stats: lr: 0.0000  loss: 0.5050
2025-01-18 19:49:00,471 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [21]  [0/1]  eta: 0:00:00    time: 0.7126  data: 0.4364  max mem: 13521
Eval: data epoch: [21] Total time: 0:00:00 (0.8068 s / it)
Eval: data epoch: [21]  [0/1]  eta: 0:00:00    time: 0.8490  data: 0.5766  max mem: 13424
Eval: data epoch: [21] Total time: 0:00:00 (0.9731 s / it)
2025-01-18 19:49:01,470 [INFO] Saving checkpoint at epoch 21 to outputs_stage2/202501181940/checkpoint_21.pth.
2025-01-18 19:49:03,787 [INFO] Training Phase
2025-01-18 19:49:03,795 [INFO] Start training epoch 22, 20 iters per inner epoch.
Train: data epoch: [22]  [ 0/20]  eta: 0:00:15  lr: 0.000001  loss: 0.2100  time: 0.7822  data: 0.0000  max mem: 13424
Train: data epoch: [22]  [ 0/20]  eta: 0:00:15  lr: 0.000001  loss: 1.1904  time: 0.7815  data: 0.0000  max mem: 13521
Train: data epoch: [22]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.0640  time: 0.8168  data: 0.0000  max mem: 13424
Train: data epoch: [22]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.5357  time: 0.8165  data: 0.0000  max mem: 13521
Train: data epoch: [22]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.4623  time: 0.7978  data: 0.0000  max mem: 13424
Train: data epoch: [22]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.3159  time: 0.7975  data: 0.0000  max mem: 13521
Train: data epoch: [22]  [15/20]  eta: 0:00:04  lr: 0.000001  loss: 0.6283  time: 0.8063  data: 0.0000  max mem: 13424
Train: data epoch: [22]  [15/20]  eta: 0:00:04  lr: 0.000001  loss: 0.4803  time: 0.8060  data: 0.0000  max mem: 13521
Train: data epoch: [22]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 1.1820  time: 0.8034  data: 0.0000  max mem: 13424
Train: data epoch: [22] Total time: 0:00:16 (0.8036 s / it)
Train: data epoch: [22]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.1662  time: 0.8032  data: 0.0000  max mem: 13521
Train: data epoch: [22] Total time: 0:00:16 (0.8037 s / it)
2025-01-18 19:49:19,870 [INFO] Averaged stats: lr: 0.0000  loss: 0.5405
2025-01-18 19:49:19,874 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [22]  [0/1]  eta: 0:00:00    time: 0.7363  data: 0.4566  max mem: 13521
Eval: data epoch: [22] Total time: 0:00:00 (0.8408 s / it)
Eval: data epoch: [22]  [0/1]  eta: 0:00:00    time: 0.8591  data: 0.5631  max mem: 13424
Eval: data epoch: [22] Total time: 0:00:00 (0.9981 s / it)
2025-01-18 19:49:20,895 [INFO] Saving checkpoint at epoch 22 to outputs_stage2/202501181940/checkpoint_22.pth.
2025-01-18 19:49:23,266 [INFO] Training Phase
2025-01-18 19:49:23,274 [INFO] Start training epoch 23, 20 iters per inner epoch.
Train: data epoch: [23]  [ 0/20]  eta: 0:00:18  lr: 0.000001  loss: 0.8095  time: 0.9215  data: 0.0000  max mem: 13424
Train: data epoch: [23]  [ 0/20]  eta: 0:00:18  lr: 0.000001  loss: 0.7144  time: 0.9204  data: 0.0000  max mem: 13521
Train: data epoch: [23]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.2274  time: 0.8196  data: 0.0000  max mem: 13424
Train: data epoch: [23]  [ 5/20]  eta: 0:00:12  lr: 0.000001  loss: 0.5898  time: 0.8194  data: 0.0000  max mem: 13521
Train: data epoch: [23]  [10/20]  eta: 0:00:08  lr: 0.000001  loss: 0.2532  time: 0.8101  data: 0.0000  max mem: 13424
Train: data epoch: [23]  [10/20]  eta: 0:00:08  lr: 0.000001  loss: 1.3873  time: 0.8090  data: 0.0000  max mem: 13521
Train: data epoch: [23]  [15/20]  eta: 0:00:04  lr: 0.000001  loss: 0.5194  time: 0.8073  data: 0.0000  max mem: 13424
Train: data epoch: [23]  [15/20]  eta: 0:00:04  lr: 0.000001  loss: 0.2625  time: 0.8065  data: 0.0000  max mem: 13521
Train: data epoch: [23]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.6135  time: 0.8130  data: 0.0000  max mem: 13620
Train: data epoch: [23] Total time: 0:00:16 (0.8132 s / it)
Train: data epoch: [23]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.0296  time: 0.8123  data: 0.0000  max mem: 13521
Train: data epoch: [23] Total time: 0:00:16 (0.8132 s / it)
2025-01-18 19:49:39,539 [INFO] Averaged stats: lr: 0.0000  loss: 0.5079
2025-01-18 19:49:39,544 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [23]  [0/1]  eta: 0:00:00    time: 0.7242  data: 0.4507  max mem: 13521
Eval: data epoch: [23] Total time: 0:00:00 (0.8241 s / it)
Eval: data epoch: [23]  [0/1]  eta: 0:00:00    time: 0.8337  data: 0.5573  max mem: 13620
Eval: data epoch: [23] Total time: 0:00:00 (0.9464 s / it)
2025-01-18 19:49:40,515 [INFO] Saving checkpoint at epoch 23 to outputs_stage2/202501181940/checkpoint_23.pth.
2025-01-18 19:49:42,893 [INFO] Training Phase
2025-01-18 19:49:42,900 [INFO] Start training epoch 24, 20 iters per inner epoch.
Train: data epoch: [24]  [ 0/20]  eta: 0:00:18  lr: 0.000001  loss: 0.4167  time: 0.9295  data: 0.0000  max mem: 13620
Train: data epoch: [24]  [ 0/20]  eta: 0:00:18  lr: 0.000001  loss: 0.4828  time: 0.9282  data: 0.0000  max mem: 13521
Train: data epoch: [24]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.2291  time: 0.7820  data: 0.0000  max mem: 13620
Train: data epoch: [24]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.1742  time: 0.7817  data: 0.0000  max mem: 13521
Train: data epoch: [24]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.4313  time: 0.7718  data: 0.0000  max mem: 13620
Train: data epoch: [24]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.1397  time: 0.7716  data: 0.0000  max mem: 13521
Train: data epoch: [24]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.5757  time: 0.7887  data: 0.0000  max mem: 13620
Train: data epoch: [24]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.6895  time: 0.7885  data: 0.0000  max mem: 13521
Train: data epoch: [24]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 1.0655  time: 0.7894  data: 0.0000  max mem: 13620
Train: data epoch: [24] Total time: 0:00:15 (0.7895 s / it)
Train: data epoch: [24]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.4719  time: 0.7891  data: 0.0000  max mem: 13521
Train: data epoch: [24] Total time: 0:00:15 (0.7896 s / it)
2025-01-18 19:49:58,694 [INFO] Averaged stats: lr: 0.0000  loss: 0.4466
2025-01-18 19:49:58,698 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [24]  [0/1]  eta: 0:00:00    time: 0.7213  data: 0.4501  max mem: 13521
Eval: data epoch: [24] Total time: 0:00:00 (0.8278 s / it)
Eval: data epoch: [24]  [0/1]  eta: 0:00:00    time: 0.8395  data: 0.5684  max mem: 13620
Eval: data epoch: [24] Total time: 0:00:00 (0.9689 s / it)
2025-01-18 19:49:59,692 [INFO] Saving checkpoint at epoch 24 to outputs_stage2/202501181940/checkpoint_24.pth.
2025-01-18 19:50:02,017 [INFO] Training Phase
2025-01-18 19:50:02,025 [INFO] Start training epoch 25, 20 iters per inner epoch.
Train: data epoch: [25]  [ 0/20]  eta: 0:00:13  lr: 0.000001  loss: 0.1877  time: 0.6908  data: 0.0000  max mem: 13620
Train: data epoch: [25]  [ 0/20]  eta: 0:00:13  lr: 0.000001  loss: 0.9628  time: 0.6896  data: 0.0000  max mem: 13521
Train: data epoch: [25]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.3617  time: 0.7924  data: 0.0000  max mem: 13620
Train: data epoch: [25]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.3148  time: 0.7900  data: 0.0000  max mem: 13521
Train: data epoch: [25]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.8943  time: 0.7687  data: 0.0000  max mem: 13620
Train: data epoch: [25]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.2337  time: 0.7674  data: 0.0000  max mem: 13521
Train: data epoch: [25]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.1715  time: 0.7884  data: 0.0000  max mem: 13620
Train: data epoch: [25]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.3258  time: 0.7874  data: 0.0000  max mem: 13521
Train: data epoch: [25]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.7123  time: 0.7825  data: 0.0000  max mem: 13620
Train: data epoch: [25] Total time: 0:00:15 (0.7827 s / it)
Train: data epoch: [25]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.9185  time: 0.7816  data: 0.0000  max mem: 13521
Train: data epoch: [25] Total time: 0:00:15 (0.7827 s / it)
2025-01-18 19:50:17,681 [INFO] Averaged stats: lr: 0.0000  loss: 0.5171
2025-01-18 19:50:17,685 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [25]  [0/1]  eta: 0:00:00    time: 0.7087  data: 0.4336  max mem: 13521
Eval: data epoch: [25] Total time: 0:00:00 (0.8061 s / it)
Eval: data epoch: [25]  [0/1]  eta: 0:00:00    time: 0.8375  data: 0.5569  max mem: 13620
Eval: data epoch: [25] Total time: 0:00:00 (0.9759 s / it)
2025-01-18 19:50:18,685 [INFO] Saving checkpoint at epoch 25 to outputs_stage2/202501181940/checkpoint_25.pth.
2025-01-18 19:50:21,031 [INFO] Training Phase
2025-01-18 19:50:21,039 [INFO] Start training epoch 26, 20 iters per inner epoch.
Train: data epoch: [26]  [ 0/20]  eta: 0:00:16  lr: 0.000001  loss: 0.1500  time: 0.8047  data: 0.0000  max mem: 13620
Train: data epoch: [26]  [ 0/20]  eta: 0:00:16  lr: 0.000001  loss: 0.6596  time: 0.8048  data: 0.0000  max mem: 13521
Train: data epoch: [26]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.3486  time: 0.7713  data: 0.0000  max mem: 13620
Train: data epoch: [26]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.4095  time: 0.7711  data: 0.0000  max mem: 13521
Train: data epoch: [26]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.0440  time: 0.7601  data: 0.0000  max mem: 13620
Train: data epoch: [26]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.3541  time: 0.7600  data: 0.0000  max mem: 13521
Train: data epoch: [26]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.4629  time: 0.7695  data: 0.0000  max mem: 13620
Train: data epoch: [26]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.2445  time: 0.7692  data: 0.0000  max mem: 13521
Train: data epoch: [26]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.1739  time: 0.7682  data: 0.0000  max mem: 13620
Train: data epoch: [26] Total time: 0:00:15 (0.7684 s / it)
Train: data epoch: [26]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.1109  time: 0.7671  data: 0.0000  max mem: 13521
Train: data epoch: [26] Total time: 0:00:15 (0.7684 s / it)
2025-01-18 19:50:36,408 [INFO] Averaged stats: lr: 0.0000  loss: 0.3879
2025-01-18 19:50:36,412 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [26]  [0/1]  eta: 0:00:00    time: 0.7140  data: 0.4435  max mem: 13521
Eval: data epoch: [26] Total time: 0:00:00 (0.8244 s / it)
Eval: data epoch: [26]  [0/1]  eta: 0:00:00    time: 0.8308  data: 0.5564  max mem: 13620
Eval: data epoch: [26] Total time: 0:00:00 (0.9563 s / it)
2025-01-18 19:50:37,392 [INFO] Saving checkpoint at epoch 26 to outputs_stage2/202501181940/checkpoint_26.pth.
2025-01-18 19:50:39,716 [INFO] Training Phase
2025-01-18 19:50:39,724 [INFO] Start training epoch 27, 20 iters per inner epoch.
Train: data epoch: [27]  [ 0/20]  eta: 0:00:17  lr: 0.000001  loss: 0.8885  time: 0.8951  data: 0.0000  max mem: 13620
Train: data epoch: [27]  [ 0/20]  eta: 0:00:17  lr: 0.000001  loss: 0.5083  time: 0.8948  data: 0.0000  max mem: 13521
Train: data epoch: [27]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.4928  time: 0.7891  data: 0.0000  max mem: 13620
Train: data epoch: [27]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.4327  time: 0.7891  data: 0.0000  max mem: 13521
Train: data epoch: [27]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.3068  time: 0.7962  data: 0.0000  max mem: 13620
Train: data epoch: [27]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.2767  time: 0.7960  data: 0.0000  max mem: 13521
Train: data epoch: [27]  [15/20]  eta: 0:00:04  lr: 0.000001  loss: 0.0896  time: 0.8002  data: 0.0000  max mem: 13620
Train: data epoch: [27]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 1.1141  time: 0.8000  data: 0.0000  max mem: 13521
Train: data epoch: [27]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.2214  time: 0.7917  data: 0.0000  max mem: 13620
Train: data epoch: [27] Total time: 0:00:15 (0.7919 s / it)
Train: data epoch: [27]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.3529  time: 0.7915  data: 0.0000  max mem: 13521
Train: data epoch: [27] Total time: 0:00:15 (0.7920 s / it)
2025-01-18 19:50:55,566 [INFO] Averaged stats: lr: 0.0000  loss: 0.4371
2025-01-18 19:50:55,570 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [27]  [0/1]  eta: 0:00:00    time: 0.7344  data: 0.4506  max mem: 13521
Eval: data epoch: [27] Total time: 0:00:00 (0.8330 s / it)
Eval: data epoch: [27]  [0/1]  eta: 0:00:00    time: 0.8483  data: 0.5751  max mem: 13620
Eval: data epoch: [27] Total time: 0:00:00 (0.9620 s / it)
2025-01-18 19:50:56,557 [INFO] Saving checkpoint at epoch 27 to outputs_stage2/202501181940/checkpoint_27.pth.
2025-01-18 19:50:58,907 [INFO] Training Phase
2025-01-18 19:50:58,914 [INFO] Start training epoch 28, 20 iters per inner epoch.
Train: data epoch: [28]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.0322  time: 0.7489  data: 0.0000  max mem: 13620
Train: data epoch: [28]  [ 0/20]  eta: 0:00:14  lr: 0.000001  loss: 0.1328  time: 0.7492  data: 0.0000  max mem: 13521
Train: data epoch: [28]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 1.0141  time: 0.7930  data: 0.0000  max mem: 13620
Train: data epoch: [28]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.9250  time: 0.7929  data: 0.0000  max mem: 13521
Train: data epoch: [28]  [10/20]  eta: 0:00:08  lr: 0.000001  loss: 0.1900  time: 0.8192  data: 0.0000  max mem: 13620
Train: data epoch: [28]  [10/20]  eta: 0:00:08  lr: 0.000001  loss: 0.6692  time: 0.8190  data: 0.0000  max mem: 13521
Train: data epoch: [28]  [15/20]  eta: 0:00:04  lr: 0.000001  loss: 0.9664  time: 0.8167  data: 0.0000  max mem: 13620
Train: data epoch: [28]  [15/20]  eta: 0:00:04  lr: 0.000001  loss: 0.5404  time: 0.8164  data: 0.0000  max mem: 13521
Train: data epoch: [28]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.4937  time: 0.8055  data: 0.0000  max mem: 13620
Train: data epoch: [28] Total time: 0:00:16 (0.8056 s / it)
Train: data epoch: [28]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.7556  time: 0.8052  data: 0.0000  max mem: 13521
Train: data epoch: [28] Total time: 0:00:16 (0.8057 s / it)
2025-01-18 19:51:15,030 [INFO] Averaged stats: lr: 0.0000  loss: 0.4936
2025-01-18 19:51:15,035 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [28]  [0/1]  eta: 0:00:00    time: 0.7177  data: 0.4498  max mem: 13521
Eval: data epoch: [28]  [0/1]  eta: 0:00:00    time: 0.8265  data: 0.5535  max mem: 13620
Eval: data epoch: [28] Total time: 0:00:00 (0.8338 s / it)
Eval: data epoch: [28] Total time: 0:00:00 (0.9417 s / it)
2025-01-18 19:51:16,000 [INFO] Saving checkpoint at epoch 28 to outputs_stage2/202501181940/checkpoint_28.pth.
2025-01-18 19:51:18,398 [INFO] Training Phase
2025-01-18 19:51:18,406 [INFO] Start training epoch 29, 20 iters per inner epoch.
Train: data epoch: [29]  [ 0/20]  eta: 0:00:16  lr: 0.000001  loss: 0.0753  time: 0.8464  data: 0.0000  max mem: 13620
Train: data epoch: [29]  [ 0/20]  eta: 0:00:16  lr: 0.000001  loss: 1.1821  time: 0.8466  data: 0.0001  max mem: 13521
Train: data epoch: [29]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.5871  time: 0.7934  data: 0.0000  max mem: 13620
Train: data epoch: [29]  [ 5/20]  eta: 0:00:11  lr: 0.000001  loss: 0.2966  time: 0.7931  data: 0.0000  max mem: 13521
Train: data epoch: [29]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.4791  time: 0.7628  data: 0.0000  max mem: 13620
Train: data epoch: [29]  [10/20]  eta: 0:00:07  lr: 0.000001  loss: 0.5660  time: 0.7625  data: 0.0000  max mem: 13521
Train: data epoch: [29]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 1.5619  time: 0.7679  data: 0.0000  max mem: 13620
Train: data epoch: [29]  [15/20]  eta: 0:00:03  lr: 0.000001  loss: 0.3179  time: 0.7677  data: 0.0000  max mem: 13521
Train: data epoch: [29]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.1484  time: 0.7575  data: 0.0000  max mem: 13620
Train: data epoch: [29] Total time: 0:00:15 (0.7577 s / it)
Train: data epoch: [29]  [19/20]  eta: 0:00:00  lr: 0.000001  loss: 0.3890  time: 0.7573  data: 0.0000  max mem: 13521
Train: data epoch: [29] Total time: 0:00:15 (0.7578 s / it)
2025-01-18 19:51:33,563 [INFO] Averaged stats: lr: 0.0000  loss: 0.3873
2025-01-18 19:51:33,566 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [29]  [0/1]  eta: 0:00:00    time: 0.7185  data: 0.4467  max mem: 13521
Eval: data epoch: [29]  [0/1]  eta: 0:00:00    time: 0.8252  data: 0.5538  max mem: 13620
Eval: data epoch: [29] Total time: 0:00:00 (0.8268 s / it)
Eval: data epoch: [29] Total time: 0:00:00 (0.9392 s / it)
2025-01-18 19:51:34,530 [INFO] Saving checkpoint at epoch 29 to outputs_stage2/202501181940/checkpoint_29.pth.
2025-01-18 19:51:36,865 [INFO] Training time 0:09:36
[1;34mwandb[0m: üöÄ View run [33mllama-3.2-1B-1percent-stage2[0m at: [34mhttps://wandb.ai/kihoon090/audio_lm/runs/dcoty5ar[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250118_194003-dcoty5ar/logs[0m
[rank0]:[W118 19:51:39.069635072 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Traceback (most recent call last):
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_efficiency_salmonn.py", line 230, in <module>
    main(args)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_efficiency_salmonn.py", line 170, in main
    cfg = Config(args)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/config.py", line 27, in __init__
    config = OmegaConf.load(self.args.cfg_path)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/omegaconf/omegaconf.py", line 189, in load
    with io.open(os.path.abspath(file_), "r", encoding="utf-8") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/salmonn_eval_config.yaml'
Force batch size as 1
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  1.04it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.57it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.46it/s]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
Loading training prompts done!
  0%|          | 0/110 [00:00<?, ?it/s]  1%|          | 1/110 [00:00<01:38,  1.11it/s]  2%|‚ñè         | 2/110 [00:01<01:06,  1.61it/s]  3%|‚ñé         | 3/110 [00:01<00:56,  1.89it/s]  4%|‚ñé         | 4/110 [00:02<00:52,  2.03it/s]  5%|‚ñç         | 5/110 [00:02<00:51,  2.04it/s]  5%|‚ñå         | 6/110 [00:03<00:48,  2.12it/s]  6%|‚ñã         | 7/110 [00:03<00:46,  2.19it/s]  7%|‚ñã         | 8/110 [00:03<00:45,  2.24it/s]  8%|‚ñä         | 9/110 [00:04<00:44,  2.28it/s]  9%|‚ñâ         | 10/110 [00:04<00:43,  2.30it/s] 10%|‚ñà         | 11/110 [00:05<00:44,  2.24it/s] 11%|‚ñà         | 12/110 [00:05<00:42,  2.28it/s] 12%|‚ñà‚ñè        | 13/110 [00:06<00:41,  2.31it/s] 13%|‚ñà‚ñé        | 14/110 [00:06<00:41,  2.33it/s] 14%|‚ñà‚ñé        | 15/110 [00:06<00:40,  2.35it/s] 15%|‚ñà‚ñç        | 16/110 [00:07<00:39,  2.35it/s] 15%|‚ñà‚ñå        | 17/110 [00:07<00:39,  2.37it/s] 16%|‚ñà‚ñã        | 18/110 [00:08<00:38,  2.37it/s] 17%|‚ñà‚ñã        | 19/110 [00:08<00:39,  2.31it/s] 18%|‚ñà‚ñä        | 20/110 [00:09<00:38,  2.32it/s] 19%|‚ñà‚ñâ        | 21/110 [00:09<00:38,  2.34it/s] 20%|‚ñà‚ñà        | 22/110 [00:09<00:37,  2.35it/s] 21%|‚ñà‚ñà        | 23/110 [00:10<00:36,  2.37it/s] 22%|‚ñà‚ñà‚ñè       | 24/110 [00:10<00:36,  2.37it/s] 23%|‚ñà‚ñà‚ñé       | 25/110 [00:11<00:35,  2.38it/s] 24%|‚ñà‚ñà‚ñé       | 26/110 [00:11<00:35,  2.38it/s] 25%|‚ñà‚ñà‚ñç       | 27/110 [00:12<00:34,  2.38it/s] 25%|‚ñà‚ñà‚ñå       | 28/110 [00:12<00:34,  2.37it/s] 26%|‚ñà‚ñà‚ñã       | 29/110 [00:12<00:34,  2.37it/s] 27%|‚ñà‚ñà‚ñã       | 30/110 [00:13<00:33,  2.37it/s] 28%|‚ñà‚ñà‚ñä       | 31/110 [00:13<00:33,  2.38it/s] 29%|‚ñà‚ñà‚ñâ       | 32/110 [00:14<00:32,  2.38it/s] 30%|‚ñà‚ñà‚ñà       | 33/110 [00:14<00:32,  2.38it/s] 31%|‚ñà‚ñà‚ñà       | 34/110 [00:14<00:31,  2.38it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 35/110 [00:15<00:31,  2.38it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 36/110 [00:15<00:31,  2.37it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 37/110 [00:16<00:30,  2.38it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 38/110 [00:16<00:30,  2.38it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 39/110 [00:17<00:29,  2.38it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 40/110 [00:17<00:29,  2.37it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 41/110 [00:17<00:29,  2.37it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 42/110 [00:18<00:28,  2.36it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 43/110 [00:18<00:28,  2.36it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 44/110 [00:19<00:27,  2.36it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 45/110 [00:19<00:27,  2.36it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 46/110 [00:20<00:27,  2.36it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 47/110 [00:20<00:26,  2.34it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 48/110 [00:20<00:26,  2.33it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 49/110 [00:21<00:26,  2.34it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 50/110 [00:21<00:26,  2.26it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 51/110 [00:22<00:25,  2.29it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 52/110 [00:22<00:25,  2.31it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 53/110 [00:23<00:24,  2.33it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 54/110 [00:23<00:23,  2.34it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 55/110 [00:23<00:23,  2.35it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 56/110 [00:24<00:22,  2.36it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 57/110 [00:24<00:22,  2.36it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 58/110 [00:25<00:22,  2.36it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 59/110 [00:25<00:21,  2.37it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 60/110 [00:26<00:21,  2.37it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 61/110 [00:26<00:20,  2.38it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 62/110 [00:26<00:20,  2.38it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 63/110 [00:27<00:19,  2.38it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 64/110 [00:27<00:19,  2.37it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 65/110 [00:28<00:18,  2.37it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 66/110 [00:28<00:18,  2.37it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 67/110 [00:28<00:18,  2.38it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 68/110 [00:29<00:17,  2.38it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 69/110 [00:29<00:17,  2.38it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 70/110 [00:30<00:16,  2.38it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 71/110 [00:30<00:16,  2.38it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 72/110 [00:31<00:15,  2.38it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 73/110 [00:31<00:15,  2.38it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 74/110 [00:31<00:15,  2.38it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 75/110 [00:32<00:14,  2.38it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 76/110 [00:32<00:14,  2.38it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 77/110 [00:33<00:13,  2.38it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 78/110 [00:33<00:13,  2.38it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 79/110 [00:34<00:13,  2.38it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 80/110 [00:34<00:12,  2.38it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 81/110 [00:34<00:12,  2.38it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 82/110 [00:35<00:11,  2.38it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 83/110 [00:35<00:11,  2.38it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 84/110 [00:36<00:10,  2.38it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 85/110 [00:36<00:10,  2.38it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 86/110 [00:36<00:10,  2.37it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 87/110 [00:37<00:09,  2.37it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 88/110 [00:37<00:09,  2.37it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 89/110 [00:38<00:08,  2.38it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 90/110 [00:38<00:08,  2.37it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 91/110 [00:39<00:08,  2.37it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 92/110 [00:39<00:07,  2.37it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 93/110 [00:39<00:07,  2.38it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 94/110 [00:40<00:06,  2.37it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 95/110 [00:40<00:06,  2.37it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 96/110 [00:41<00:05,  2.37it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 97/110 [00:41<00:05,  2.37it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 98/110 [00:42<00:05,  2.37it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 99/110 [00:42<00:04,  2.38it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 100/110 [00:42<00:04,  2.38it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 101/110 [00:43<00:03,  2.37it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 102/110 [00:43<00:03,  2.36it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 103/110 [00:44<00:02,  2.37it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 104/110 [00:44<00:02,  2.37it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 105/110 [00:44<00:02,  2.36it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 106/110 [00:45<00:01,  2.37it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 107/110 [00:45<00:01,  2.37it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 108/110 [00:46<00:00,  2.37it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 109/110 [00:46<00:00,  2.37it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 110/110 [00:47<00:00,  2.36it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 110/110 [00:47<00:00,  2.34it/s]
Average memory used during inference: 9.1761 GB
Average inference time: 0.2388 seconds
Average TTFT: 0.2020 seconds
Average TPOT: 0.0368 seconds
Traceback (most recent call last):
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 184, in <module>
    main(args)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 94, in main
    cfg = Config(args)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/config.py", line 27, in __init__
    config = OmegaConf.load(self.args.cfg_path)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/omegaconf/omegaconf.py", line 189, in load
    with io.open(os.path.abspath(file_), "r", encoding="utf-8") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/salmonn_eval_config.yaml'
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  1.09it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.64it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.52it/s]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
Loading training prompts done!
Traceback (most recent call last):
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 184, in <module>
    main(args)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 102, in main
    dataloader = get_dataset(cfg.config.datasets, cfg.config.run, args.task, args.make_submission)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 79, in get_dataset
    dataset_cfg.prefix, dataset_cfg.test_ann_path, dataset_cfg.whisper_path, task, submission
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/omegaconf/dictconfig.py", line 355, in __getattr__
    self._format_and_raise(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/omegaconf/base.py", line 231, in _format_and_raise
    format_and_raise(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/omegaconf/_utils.py", line 899, in format_and_raise
    _raise(ex, cause)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/omegaconf/_utils.py", line 797, in _raise
    raise ex.with_traceback(sys.exc_info()[2])  # set env var OC_CAUSE=1 for full trace
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/omegaconf/dictconfig.py", line 351, in __getattr__
    return self._get_impl(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/omegaconf/dictconfig.py", line 442, in _get_impl
    node = self._get_child(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/omegaconf/basecontainer.py", line 73, in _get_child
    child = self._get_node(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/omegaconf/dictconfig.py", line 480, in _get_node
    raise ConfigKeyError(f"Missing key {key!s}")
omegaconf.errors.ConfigAttributeError: Missing key test_ann_path
    full_key: datasets.test_ann_path
    object_type=dict
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  1.08it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.62it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.51it/s]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
Loading training prompts done!
Traceback (most recent call last):
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 184, in <module>
    main(args)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 102, in main
    dataloader = get_dataset(cfg.config.datasets, cfg.config.run, args.task, args.make_submission)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 79, in get_dataset
    dataset_cfg.prefix, dataset_cfg.test_ann_path, dataset_cfg.whisper_path, task, submission
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/omegaconf/dictconfig.py", line 355, in __getattr__
    self._format_and_raise(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/omegaconf/base.py", line 231, in _format_and_raise
    format_and_raise(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/omegaconf/_utils.py", line 899, in format_and_raise
    _raise(ex, cause)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/omegaconf/_utils.py", line 797, in _raise
    raise ex.with_traceback(sys.exc_info()[2])  # set env var OC_CAUSE=1 for full trace
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/omegaconf/dictconfig.py", line 351, in __getattr__
    return self._get_impl(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/omegaconf/dictconfig.py", line 442, in _get_impl
    node = self._get_child(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/omegaconf/basecontainer.py", line 73, in _get_child
    child = self._get_node(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/omegaconf/dictconfig.py", line 480, in _get_node
    raise ConfigKeyError(f"Missing key {key!s}")
omegaconf.errors.ConfigAttributeError: Missing key test_ann_path
    full_key: datasets.test_ann_path
    object_type=dict
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:01<00:01,  1.02s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.51it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.40it/s]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
Loading training prompts done!
Traceback (most recent call last):
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 184, in <module>
    main(args)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 102, in main
    dataloader = get_dataset(cfg.config.datasets, cfg.config.run, args.task, args.make_submission)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 78, in get_dataset
    testset = SALMONNTestDataset(
TypeError: __init__() takes from 4 to 5 positional arguments but 6 were given
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:01<00:01,  1.07s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.42it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.32it/s]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
Loading training prompts done!
  0%|          | 0/545 [00:00<?, ?it/s]  0%|          | 0/545 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 184, in <module>
    main(args)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 109, in main
    for samples in tqdm(dataloader):
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
    data = self._next_data()
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1465, in _next_data
    return self._process_data(data)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1491, in _process_data
    data.reraise()
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
KeyError: Caught KeyError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py", line 351, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/salmonn_utils.py", line 112, in __getitem__
    entity['text'] = ann['text']
KeyError: 'text'

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  1.08it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.65it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.53it/s]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
Loading training prompts done!
  0%|          | 0/545 [00:00<?, ?it/s]  0%|          | 0/545 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 184, in <module>
    main(args)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 109, in main
    for samples in tqdm(dataloader):
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
    data = self._next_data()
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1465, in _next_data
    return self._process_data(data)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1491, in _process_data
    data.reraise()
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
KeyError: Caught KeyError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py", line 351, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/salmonn_utils.py", line 71, in collater
    entity['text'] = [s["text"] for s in samples]
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/salmonn_utils.py", line 71, in <listcomp>
    entity['text'] = [s["text"] for s in samples]
KeyError: 'text'

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  1.11it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.68it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.56it/s]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
Loading training prompts done!
  0%|          | 0/545 [00:00<?, ?it/s]/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  0%|          | 1/545 [00:05<46:22,  5.11s/it]  0%|          | 2/545 [00:07<33:41,  3.72s/it]  1%|          | 3/545 [00:10<28:34,  3.16s/it]  1%|          | 4/545 [00:12<25:58,  2.88s/it]  1%|          | 5/545 [00:15<25:37,  2.85s/it]  1%|          | 6/545 [00:18<25:31,  2.84s/it]  1%|‚ñè         | 7/545 [00:21<24:53,  2.78s/it]  1%|‚ñè         | 8/545 [00:23<24:19,  2.72s/it]  2%|‚ñè         | 9/545 [00:26<24:02,  2.69s/it]  2%|‚ñè         | 10/545 [00:28<23:25,  2.63s/it]  2%|‚ñè         | 11/545 [00:31<22:52,  2.57s/it]  2%|‚ñè         | 12/545 [00:33<22:38,  2.55s/it]  2%|‚ñè         | 13/545 [00:36<23:15,  2.62s/it]  3%|‚ñé         | 14/545 [00:39<23:29,  2.65s/it]  3%|‚ñé         | 15/545 [00:41<23:07,  2.62s/it]  3%|‚ñé         | 16/545 [00:44<23:01,  2.61s/it]  3%|‚ñé         | 17/545 [00:47<23:27,  2.67s/it]  3%|‚ñé         | 18/545 [00:49<23:12,  2.64s/it]  3%|‚ñé         | 19/545 [00:52<23:14,  2.65s/it]  4%|‚ñé         | 20/545 [00:55<23:03,  2.64s/it]  4%|‚ñç         | 21/545 [00:57<22:47,  2.61s/it]  4%|‚ñç         | 22/545 [01:00<23:18,  2.67s/it]  4%|‚ñç         | 23/545 [01:02<22:31,  2.59s/it]  4%|‚ñç         | 24/545 [01:05<22:35,  2.60s/it]  5%|‚ñç         | 25/545 [01:08<24:05,  2.78s/it]  5%|‚ñç         | 26/545 [01:11<23:56,  2.77s/it]  5%|‚ñç         | 27/545 [01:13<23:17,  2.70s/it]  5%|‚ñå         | 28/545 [01:16<23:11,  2.69s/it]  5%|‚ñå         | 29/545 [01:19<22:36,  2.63s/it]  6%|‚ñå         | 30/545 [01:21<22:56,  2.67s/it]  6%|‚ñå         | 31/545 [01:24<23:08,  2.70s/it]  6%|‚ñå         | 32/545 [01:27<22:38,  2.65s/it]  6%|‚ñå         | 33/545 [01:29<22:11,  2.60s/it]  6%|‚ñå         | 34/545 [01:32<21:44,  2.55s/it]  6%|‚ñã         | 35/545 [01:34<21:52,  2.57s/it]  7%|‚ñã         | 36/545 [01:37<21:43,  2.56s/it]  7%|‚ñã         | 37/545 [01:39<21:51,  2.58s/it]  7%|‚ñã         | 38/545 [01:42<21:48,  2.58s/it]  7%|‚ñã         | 39/545 [01:44<21:23,  2.54s/it]  7%|‚ñã         | 40/545 [01:47<21:05,  2.51s/it]  8%|‚ñä         | 41/545 [01:49<21:13,  2.53s/it]  8%|‚ñä         | 42/545 [01:52<21:03,  2.51s/it]  8%|‚ñä         | 43/545 [01:54<21:03,  2.52s/it]  8%|‚ñä         | 44/545 [01:57<21:11,  2.54s/it]  8%|‚ñä         | 45/545 [02:00<21:29,  2.58s/it]  8%|‚ñä         | 46/545 [02:02<21:17,  2.56s/it]  9%|‚ñä         | 47/545 [02:05<21:46,  2.62s/it]  9%|‚ñâ         | 48/545 [02:08<21:51,  2.64s/it]  9%|‚ñâ         | 49/545 [02:10<21:56,  2.65s/it]  9%|‚ñâ         | 50/545 [02:13<21:30,  2.61s/it]  9%|‚ñâ         | 51/545 [02:16<21:55,  2.66s/it] 10%|‚ñâ         | 52/545 [02:18<21:35,  2.63s/it] 10%|‚ñâ         | 53/545 [02:21<21:45,  2.65s/it] 10%|‚ñâ         | 54/545 [02:23<21:39,  2.65s/it] 10%|‚ñà         | 55/545 [02:26<21:31,  2.64s/it] 10%|‚ñà         | 56/545 [02:29<21:30,  2.64s/it] 10%|‚ñà         | 57/545 [02:31<21:34,  2.65s/it] 11%|‚ñà         | 58/545 [02:34<21:00,  2.59s/it] 11%|‚ñà         | 59/545 [02:36<20:35,  2.54s/it] 11%|‚ñà         | 60/545 [02:39<20:46,  2.57s/it] 11%|‚ñà         | 61/545 [02:42<21:00,  2.60s/it] 11%|‚ñà‚ñè        | 62/545 [02:44<20:54,  2.60s/it] 12%|‚ñà‚ñè        | 63/545 [02:47<20:55,  2.60s/it] 12%|‚ñà‚ñè        | 64/545 [02:49<20:27,  2.55s/it] 12%|‚ñà‚ñè        | 65/545 [02:52<20:14,  2.53s/it] 12%|‚ñà‚ñè        | 66/545 [02:54<20:12,  2.53s/it] 12%|‚ñà‚ñè        | 67/545 [02:57<20:02,  2.52s/it] 12%|‚ñà‚ñè        | 68/545 [02:59<20:17,  2.55s/it] 13%|‚ñà‚ñé        | 69/545 [03:02<20:11,  2.55s/it] 13%|‚ñà‚ñé        | 70/545 [03:04<20:15,  2.56s/it] 13%|‚ñà‚ñé        | 71/545 [03:07<20:23,  2.58s/it] 13%|‚ñà‚ñé        | 72/545 [03:10<20:34,  2.61s/it] 13%|‚ñà‚ñé        | 73/545 [03:13<20:48,  2.64s/it] 14%|‚ñà‚ñé        | 74/545 [03:15<20:44,  2.64s/it] 14%|‚ñà‚ñç        | 75/545 [03:18<20:26,  2.61s/it] 14%|‚ñà‚ñç        | 76/545 [03:20<20:20,  2.60s/it] 14%|‚ñà‚ñç        | 77/545 [03:23<20:01,  2.57s/it] 14%|‚ñà‚ñç        | 78/545 [03:26<21:27,  2.76s/it] 14%|‚ñà‚ñç        | 79/545 [03:29<21:25,  2.76s/it] 15%|‚ñà‚ñç        | 80/545 [03:31<21:24,  2.76s/it] 15%|‚ñà‚ñç        | 81/545 [03:34<20:58,  2.71s/it] 15%|‚ñà‚ñå        | 82/545 [03:37<20:23,  2.64s/it] 15%|‚ñà‚ñå        | 83/545 [03:39<20:25,  2.65s/it] 15%|‚ñà‚ñå        | 84/545 [03:42<20:26,  2.66s/it] 16%|‚ñà‚ñå        | 85/545 [03:44<20:13,  2.64s/it] 16%|‚ñà‚ñå        | 86/545 [03:47<19:54,  2.60s/it] 16%|‚ñà‚ñå        | 87/545 [03:50<19:43,  2.58s/it] 16%|‚ñà‚ñå        | 88/545 [03:52<19:54,  2.61s/it] 16%|‚ñà‚ñã        | 89/545 [03:55<20:01,  2.63s/it] 17%|‚ñà‚ñã        | 90/545 [03:58<19:57,  2.63s/it] 17%|‚ñà‚ñã        | 91/545 [04:00<19:28,  2.57s/it] 17%|‚ñà‚ñã        | 92/545 [04:02<19:07,  2.53s/it] 17%|‚ñà‚ñã        | 93/545 [04:05<19:04,  2.53s/it] 17%|‚ñà‚ñã        | 94/545 [04:08<19:11,  2.55s/it] 17%|‚ñà‚ñã        | 95/545 [04:10<19:28,  2.60s/it] 18%|‚ñà‚ñä        | 96/545 [04:13<19:22,  2.59s/it] 18%|‚ñà‚ñä        | 97/545 [04:15<19:25,  2.60s/it] 18%|‚ñà‚ñä        | 98/545 [04:18<19:46,  2.66s/it] 18%|‚ñà‚ñä        | 99/545 [04:21<19:21,  2.60s/it] 18%|‚ñà‚ñä        | 100/545 [04:23<19:02,  2.57s/it] 19%|‚ñà‚ñä        | 101/545 [04:26<19:02,  2.57s/it] 19%|‚ñà‚ñä        | 102/545 [04:28<19:14,  2.61s/it] 19%|‚ñà‚ñâ        | 103/545 [04:31<19:08,  2.60s/it] 19%|‚ñà‚ñâ        | 104/545 [04:34<19:04,  2.59s/it] 19%|‚ñà‚ñâ        | 105/545 [04:36<19:06,  2.60s/it] 19%|‚ñà‚ñâ        | 106/545 [04:39<18:52,  2.58s/it] 20%|‚ñà‚ñâ        | 107/545 [04:41<18:56,  2.60s/it] 20%|‚ñà‚ñâ        | 108/545 [04:53<38:05,  5.23s/it] 20%|‚ñà‚ñà        | 109/545 [04:55<32:01,  4.41s/it] 20%|‚ñà‚ñà        | 110/545 [04:58<27:55,  3.85s/it] 20%|‚ñà‚ñà        | 111/545 [05:00<24:59,  3.46s/it] 21%|‚ñà‚ñà        | 112/545 [05:03<23:28,  3.25s/it] 21%|‚ñà‚ñà        | 113/545 [05:06<22:01,  3.06s/it] 21%|‚ñà‚ñà        | 114/545 [05:08<20:56,  2.91s/it] 21%|‚ñà‚ñà        | 115/545 [05:11<19:58,  2.79s/it] 21%|‚ñà‚ñà‚ñè       | 116/545 [05:14<19:54,  2.78s/it] 21%|‚ñà‚ñà‚ñè       | 117/545 [05:16<19:19,  2.71s/it] 22%|‚ñà‚ñà‚ñè       | 118/545 [05:19<19:07,  2.69s/it] 22%|‚ñà‚ñà‚ñè       | 119/545 [05:22<19:11,  2.70s/it] 22%|‚ñà‚ñà‚ñè       | 120/545 [05:24<19:19,  2.73s/it] 22%|‚ñà‚ñà‚ñè       | 121/545 [05:27<19:06,  2.70s/it] 22%|‚ñà‚ñà‚ñè       | 122/545 [05:29<18:30,  2.63s/it] 23%|‚ñà‚ñà‚ñé       | 123/545 [05:32<19:00,  2.70s/it] 23%|‚ñà‚ñà‚ñé       | 124/545 [05:35<18:54,  2.69s/it] 23%|‚ñà‚ñà‚ñé       | 125/545 [05:38<18:45,  2.68s/it] 23%|‚ñà‚ñà‚ñé       | 126/545 [05:40<18:26,  2.64s/it] 23%|‚ñà‚ñà‚ñé       | 127/545 [05:43<18:22,  2.64s/it] 23%|‚ñà‚ñà‚ñé       | 128/545 [05:45<18:01,  2.59s/it] 24%|‚ñà‚ñà‚ñé       | 129/545 [05:48<18:08,  2.62s/it] 24%|‚ñà‚ñà‚ñç       | 130/545 [05:51<18:20,  2.65s/it] 24%|‚ñà‚ñà‚ñç       | 131/545 [05:53<18:14,  2.64s/it] 24%|‚ñà‚ñà‚ñç       | 132/545 [05:56<17:52,  2.60s/it] 24%|‚ñà‚ñà‚ñç       | 133/545 [05:59<18:07,  2.64s/it] 25%|‚ñà‚ñà‚ñç       | 134/545 [06:01<17:40,  2.58s/it] 25%|‚ñà‚ñà‚ñç       | 135/545 [06:03<17:26,  2.55s/it] 25%|‚ñà‚ñà‚ñç       | 136/545 [06:06<17:27,  2.56s/it] 25%|‚ñà‚ñà‚ñå       | 137/545 [06:09<17:28,  2.57s/it] 25%|‚ñà‚ñà‚ñå       | 138/545 [06:11<17:33,  2.59s/it] 26%|‚ñà‚ñà‚ñå       | 139/545 [06:14<17:48,  2.63s/it] 26%|‚ñà‚ñà‚ñå       | 140/545 [06:16<17:28,  2.59s/it] 26%|‚ñà‚ñà‚ñå       | 141/545 [06:19<17:37,  2.62s/it] 26%|‚ñà‚ñà‚ñå       | 142/545 [06:22<17:36,  2.62s/it] 26%|‚ñà‚ñà‚ñå       | 143/545 [06:24<17:17,  2.58s/it] 26%|‚ñà‚ñà‚ñã       | 144/545 [06:27<16:58,  2.54s/it] 27%|‚ñà‚ñà‚ñã       | 145/545 [06:29<16:54,  2.54s/it] 27%|‚ñà‚ñà‚ñã       | 146/545 [06:32<17:14,  2.59s/it] 27%|‚ñà‚ñà‚ñã       | 147/545 [06:35<17:22,  2.62s/it] 27%|‚ñà‚ñà‚ñã       | 148/545 [06:37<16:59,  2.57s/it] 27%|‚ñà‚ñà‚ñã       | 149/545 [06:40<17:17,  2.62s/it] 28%|‚ñà‚ñà‚ñä       | 150/545 [06:43<17:33,  2.67s/it] 28%|‚ñà‚ñà‚ñä       | 151/545 [06:45<17:38,  2.69s/it] 28%|‚ñà‚ñà‚ñä       | 152/545 [06:48<17:34,  2.68s/it] 28%|‚ñà‚ñà‚ñä       | 153/545 [06:50<17:02,  2.61s/it] 28%|‚ñà‚ñà‚ñä       | 154/545 [06:53<17:13,  2.64s/it] 28%|‚ñà‚ñà‚ñä       | 155/545 [06:56<16:58,  2.61s/it] 29%|‚ñà‚ñà‚ñä       | 156/545 [06:58<16:58,  2.62s/it] 29%|‚ñà‚ñà‚ñâ       | 157/545 [07:01<17:05,  2.64s/it] 29%|‚ñà‚ñà‚ñâ       | 158/545 [07:04<17:08,  2.66s/it] 29%|‚ñà‚ñà‚ñâ       | 159/545 [07:06<16:58,  2.64s/it] 29%|‚ñà‚ñà‚ñâ       | 160/545 [07:09<16:50,  2.62s/it] 30%|‚ñà‚ñà‚ñâ       | 161/545 [07:12<17:06,  2.67s/it] 30%|‚ñà‚ñà‚ñâ       | 162/545 [07:14<16:54,  2.65s/it] 30%|‚ñà‚ñà‚ñâ       | 163/545 [07:17<16:34,  2.60s/it] 30%|‚ñà‚ñà‚ñà       | 164/545 [07:19<16:19,  2.57s/it] 30%|‚ñà‚ñà‚ñà       | 165/545 [07:22<16:19,  2.58s/it] 30%|‚ñà‚ñà‚ñà       | 166/545 [07:24<16:06,  2.55s/it] 31%|‚ñà‚ñà‚ñà       | 167/545 [07:27<15:52,  2.52s/it] 31%|‚ñà‚ñà‚ñà       | 168/545 [07:29<15:51,  2.52s/it] 31%|‚ñà‚ñà‚ñà       | 169/545 [07:32<16:03,  2.56s/it] 31%|‚ñà‚ñà‚ñà       | 170/545 [07:35<16:09,  2.59s/it] 31%|‚ñà‚ñà‚ñà‚ñè      | 171/545 [07:37<16:13,  2.60s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 172/545 [07:40<16:56,  2.73s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 173/545 [07:43<16:38,  2.68s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 174/545 [07:45<16:07,  2.61s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 175/545 [07:48<16:02,  2.60s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 176/545 [07:50<15:53,  2.58s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 177/545 [07:53<16:08,  2.63s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 178/545 [07:56<16:01,  2.62s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 179/545 [07:58<16:05,  2.64s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 180/545 [08:01<16:28,  2.71s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 181/545 [08:04<16:13,  2.67s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 182/545 [08:07<16:01,  2.65s/it] 34%|‚ñà‚ñà‚ñà‚ñé      | 183/545 [08:09<15:51,  2.63s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 184/545 [08:12<15:24,  2.56s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 185/545 [08:14<15:19,  2.55s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 186/545 [08:17<15:10,  2.54s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 187/545 [08:19<15:27,  2.59s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 188/545 [08:22<15:44,  2.65s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 189/545 [08:24<15:18,  2.58s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 190/545 [08:27<15:26,  2.61s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 191/545 [08:30<15:10,  2.57s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 192/545 [08:32<15:04,  2.56s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 193/545 [08:35<15:13,  2.60s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 194/545 [08:37<14:59,  2.56s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 195/545 [08:40<15:03,  2.58s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 196/545 [08:42<14:55,  2.57s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 197/545 [08:45<14:38,  2.53s/it] 36%|‚ñà‚ñà‚ñà‚ñã      | 198/545 [08:48<14:46,  2.55s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 199/545 [08:50<15:05,  2.62s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 200/545 [08:53<14:44,  2.56s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 201/545 [08:55<14:43,  2.57s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 202/545 [08:58<14:26,  2.53s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 203/545 [09:00<14:30,  2.54s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 204/545 [09:03<14:16,  2.51s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 205/545 [09:05<14:26,  2.55s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 206/545 [09:08<14:32,  2.57s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 207/545 [09:11<14:40,  2.60s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 208/545 [09:13<14:43,  2.62s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 209/545 [09:16<14:42,  2.63s/it] 39%|‚ñà‚ñà‚ñà‚ñä      | 210/545 [09:19<14:41,  2.63s/it] 39%|‚ñà‚ñà‚ñà‚ñä      | 211/545 [09:21<14:35,  2.62s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 212/545 [09:24<14:45,  2.66s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 213/545 [09:27<14:36,  2.64s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 214/545 [09:29<14:24,  2.61s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 215/545 [09:32<14:42,  2.68s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 216/545 [09:34<14:12,  2.59s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 217/545 [09:37<14:19,  2.62s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 218/545 [09:40<14:03,  2.58s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 219/545 [09:42<13:36,  2.50s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 220/545 [09:44<13:46,  2.54s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 221/545 [09:47<13:47,  2.55s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 222/545 [09:50<13:57,  2.59s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 223/545 [09:52<13:58,  2.61s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 224/545 [09:55<13:53,  2.60s/it] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 225/545 [09:57<13:35,  2.55s/it] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 226/545 [10:00<13:36,  2.56s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 227/545 [10:02<13:21,  2.52s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 228/545 [10:05<13:29,  2.56s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 229/545 [10:08<13:25,  2.55s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 230/545 [10:10<13:08,  2.50s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 231/545 [10:13<13:17,  2.54s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 232/545 [10:15<13:23,  2.57s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 233/545 [10:18<13:31,  2.60s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 234/545 [10:21<13:27,  2.60s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 235/545 [10:23<13:28,  2.61s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 236/545 [10:26<13:23,  2.60s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 237/545 [10:28<13:19,  2.60s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 238/545 [10:31<13:18,  2.60s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 239/545 [10:33<13:05,  2.57s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 240/545 [10:36<13:04,  2.57s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 241/545 [10:39<12:58,  2.56s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 242/545 [10:41<13:01,  2.58s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 243/545 [10:44<12:46,  2.54s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 244/545 [10:46<12:57,  2.58s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 245/545 [10:49<12:46,  2.55s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 246/545 [10:51<12:55,  2.59s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 247/545 [10:54<12:46,  2.57s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 248/545 [10:57<12:45,  2.58s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 249/545 [10:59<12:31,  2.54s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 250/545 [11:02<12:29,  2.54s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 251/545 [11:04<12:47,  2.61s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 252/545 [11:07<12:58,  2.66s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 253/545 [11:10<12:44,  2.62s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 254/545 [11:12<12:39,  2.61s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 255/545 [11:15<12:25,  2.57s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 256/545 [11:17<12:26,  2.58s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 257/545 [11:20<12:28,  2.60s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 258/545 [11:23<12:28,  2.61s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 259/545 [11:25<12:32,  2.63s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 260/545 [11:28<13:19,  2.81s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 261/545 [11:31<13:05,  2.76s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 262/545 [11:34<12:39,  2.68s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 263/545 [11:36<12:19,  2.62s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 264/545 [11:39<12:33,  2.68s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 265/545 [11:41<12:19,  2.64s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 266/545 [11:44<11:59,  2.58s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 267/545 [11:47<12:01,  2.59s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 268/545 [11:49<12:14,  2.65s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 269/545 [11:52<12:14,  2.66s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 270/545 [11:54<11:53,  2.59s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 271/545 [11:57<11:49,  2.59s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 272/545 [12:00<11:38,  2.56s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 273/545 [12:02<11:33,  2.55s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 274/545 [12:05<11:49,  2.62s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 275/545 [12:08<12:07,  2.69s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 276/545 [12:10<11:44,  2.62s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 277/545 [12:13<12:29,  2.80s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 278/545 [12:16<12:13,  2.75s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 279/545 [12:19<12:41,  2.86s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 280/545 [12:22<12:29,  2.83s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 281/545 [12:24<12:11,  2.77s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 282/545 [12:27<11:48,  2.69s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 283/545 [12:30<11:44,  2.69s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 284/545 [12:32<11:33,  2.66s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 285/545 [12:35<11:21,  2.62s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 286/545 [12:37<11:23,  2.64s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 287/545 [12:40<11:27,  2.67s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 288/545 [12:43<11:11,  2.61s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 289/545 [12:45<11:06,  2.60s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 290/545 [12:48<11:07,  2.62s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 291/545 [12:51<11:02,  2.61s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 292/545 [12:53<10:58,  2.60s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 293/545 [12:56<10:59,  2.62s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 294/545 [12:59<11:33,  2.76s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 295/545 [13:02<11:34,  2.78s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 296/545 [13:04<11:25,  2.75s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 297/545 [13:07<11:01,  2.67s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 298/545 [13:09<10:57,  2.66s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 299/545 [13:12<10:49,  2.64s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 300/545 [13:15<10:43,  2.63s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 301/545 [13:17<10:27,  2.57s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 302/545 [13:20<10:26,  2.58s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 303/545 [13:22<10:24,  2.58s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 304/545 [13:25<10:28,  2.61s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 305/545 [13:28<10:24,  2.60s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 306/545 [13:30<10:31,  2.64s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 307/545 [13:33<10:21,  2.61s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 308/545 [13:36<10:29,  2.66s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 309/545 [13:38<10:15,  2.61s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 310/545 [13:41<10:07,  2.59s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 311/545 [13:43<09:57,  2.55s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 312/545 [13:46<09:46,  2.52s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 313/545 [13:48<09:54,  2.56s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 314/545 [13:51<09:53,  2.57s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 315/545 [13:53<09:56,  2.59s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 316/545 [13:56<09:52,  2.59s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 317/545 [13:58<09:36,  2.53s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 318/545 [14:01<09:24,  2.49s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 319/545 [14:03<09:32,  2.53s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 320/545 [14:06<09:32,  2.55s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 321/545 [14:09<09:32,  2.56s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 322/545 [14:11<09:34,  2.58s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 323/545 [14:14<09:19,  2.52s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 324/545 [14:16<09:14,  2.51s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 325/545 [14:19<09:13,  2.52s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 326/545 [14:21<09:21,  2.57s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 327/545 [14:24<09:23,  2.58s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 328/545 [14:27<09:21,  2.59s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 329/545 [14:29<09:18,  2.58s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 330/545 [14:32<09:15,  2.58s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 331/545 [14:34<09:15,  2.60s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 332/545 [14:37<09:09,  2.58s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 333/545 [14:39<09:07,  2.58s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 334/545 [14:42<09:17,  2.64s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 335/545 [14:45<09:16,  2.65s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 336/545 [14:48<09:13,  2.65s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 337/545 [14:50<09:06,  2.63s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 338/545 [14:53<09:10,  2.66s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 339/545 [14:55<08:59,  2.62s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 340/545 [14:58<08:54,  2.61s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 341/545 [15:01<08:54,  2.62s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 342/545 [15:03<08:49,  2.61s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 343/545 [15:06<08:57,  2.66s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 344/545 [15:08<08:43,  2.61s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 345/545 [15:11<08:31,  2.56s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 346/545 [15:13<08:30,  2.57s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 347/545 [15:16<08:17,  2.51s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 348/545 [15:18<08:21,  2.55s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 349/545 [15:21<08:31,  2.61s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 350/545 [15:24<08:27,  2.60s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 351/545 [15:27<08:37,  2.67s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 352/545 [15:29<08:32,  2.66s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 353/545 [15:32<08:17,  2.59s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 354/545 [15:34<08:11,  2.58s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 355/545 [15:37<08:04,  2.55s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 356/545 [15:39<08:10,  2.60s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 357/545 [15:42<08:15,  2.64s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 358/545 [15:45<08:07,  2.61s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 359/545 [15:48<08:27,  2.73s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 360/545 [15:51<08:40,  2.81s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 361/545 [15:53<08:20,  2.72s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 362/545 [15:56<08:20,  2.74s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 363/545 [15:59<08:10,  2.69s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 364/545 [16:01<08:04,  2.68s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 365/545 [16:04<07:59,  2.66s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 366/545 [16:06<07:49,  2.62s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 367/545 [16:09<07:50,  2.64s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 368/545 [16:12<07:42,  2.61s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 369/545 [16:14<07:53,  2.69s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 370/545 [16:17<07:56,  2.72s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 371/545 [16:20<07:52,  2.72s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 372/545 [16:23<07:42,  2.68s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 373/545 [16:25<07:35,  2.65s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 374/545 [16:28<07:31,  2.64s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 375/545 [16:31<07:39,  2.70s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 376/545 [16:33<07:30,  2.67s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 377/545 [16:36<07:23,  2.64s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 378/545 [16:39<07:25,  2.67s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 379/545 [16:41<07:23,  2.67s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 380/545 [16:44<07:13,  2.63s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 381/545 [16:46<07:16,  2.66s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 382/545 [16:49<07:12,  2.65s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 383/545 [16:52<07:11,  2.66s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 384/545 [16:54<07:08,  2.66s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 385/545 [16:57<07:03,  2.65s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 386/545 [17:00<06:53,  2.60s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 387/545 [17:02<06:47,  2.58s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 388/545 [17:05<06:48,  2.60s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 389/545 [17:08<06:53,  2.65s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 390/545 [17:10<06:45,  2.62s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 391/545 [17:13<06:37,  2.58s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 392/545 [17:16<07:02,  2.76s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 393/545 [17:27<13:28,  5.32s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 394/545 [17:30<11:21,  4.51s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 395/545 [17:32<09:50,  3.94s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 396/545 [17:35<08:48,  3.55s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 397/545 [17:38<08:07,  3.29s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 398/545 [17:40<07:40,  3.13s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 399/545 [17:43<07:22,  3.03s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 400/545 [17:46<07:00,  2.90s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 401/545 [17:48<06:43,  2.80s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 402/545 [17:51<06:29,  2.73s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 403/545 [17:53<06:21,  2.68s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 404/545 [17:56<06:18,  2.68s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 405/545 [17:59<06:11,  2.65s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 406/545 [18:01<06:09,  2.66s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 407/545 [18:04<06:04,  2.64s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 408/545 [18:07<06:16,  2.75s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 409/545 [18:10<06:09,  2.72s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 410/545 [18:12<06:00,  2.67s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 411/545 [18:15<05:52,  2.63s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 412/545 [18:17<05:49,  2.63s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 413/545 [18:20<05:45,  2.62s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 414/545 [18:23<05:45,  2.64s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 415/545 [18:25<05:39,  2.61s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 416/545 [18:28<05:31,  2.57s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 417/545 [18:30<05:31,  2.59s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 418/545 [18:33<05:28,  2.59s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 419/545 [18:35<05:20,  2.54s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 420/545 [18:38<05:26,  2.61s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 421/545 [18:41<05:30,  2.66s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 422/545 [18:43<05:24,  2.64s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 423/545 [18:46<05:18,  2.61s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 424/545 [18:48<05:11,  2.57s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 425/545 [18:51<05:16,  2.63s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 426/545 [18:54<05:19,  2.68s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 427/545 [18:57<05:09,  2.62s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 428/545 [18:59<05:12,  2.67s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 429/545 [19:02<05:03,  2.62s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 430/545 [19:04<05:01,  2.62s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 431/545 [19:07<04:51,  2.55s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 432/545 [19:09<04:46,  2.54s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 433/545 [19:12<04:42,  2.52s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 434/545 [19:14<04:41,  2.54s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 435/545 [19:17<04:36,  2.51s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 436/545 [19:20<04:45,  2.62s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 437/545 [19:22<04:45,  2.64s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 438/545 [19:25<04:39,  2.61s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 439/545 [19:28<04:36,  2.61s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 440/545 [19:30<04:42,  2.69s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 441/545 [19:33<04:38,  2.68s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 442/545 [19:35<04:28,  2.61s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 443/545 [19:38<04:31,  2.66s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 444/545 [19:41<04:28,  2.66s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 445/545 [19:43<04:20,  2.61s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 446/545 [19:46<04:16,  2.59s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 447/545 [19:49<04:16,  2.62s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 448/545 [19:51<04:14,  2.63s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 449/545 [20:03<08:28,  5.30s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 450/545 [20:05<07:06,  4.49s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 451/545 [20:08<06:18,  4.02s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 452/545 [20:11<05:39,  3.65s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 453/545 [20:14<05:07,  3.34s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 454/545 [20:16<04:42,  3.10s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 455/545 [20:19<04:33,  3.03s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 456/545 [20:22<04:15,  2.88s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 457/545 [20:24<04:05,  2.79s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 458/545 [20:27<04:01,  2.77s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 459/545 [20:29<03:49,  2.67s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 460/545 [20:32<03:44,  2.65s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 461/545 [20:35<03:39,  2.61s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 462/545 [20:37<03:36,  2.61s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 463/545 [20:40<03:33,  2.60s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 464/545 [20:42<03:28,  2.57s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 465/545 [20:45<03:28,  2.61s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 466/545 [20:47<03:22,  2.56s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 467/545 [20:50<03:22,  2.60s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 468/545 [20:52<03:15,  2.54s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 469/545 [20:55<03:10,  2.51s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 470/545 [20:57<03:08,  2.52s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 471/545 [21:00<03:09,  2.57s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 472/545 [21:03<03:08,  2.59s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 473/545 [21:05<03:07,  2.61s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 474/545 [21:08<03:09,  2.66s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 475/545 [21:11<03:05,  2.65s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 476/545 [21:14<03:02,  2.65s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 477/545 [21:16<03:02,  2.69s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 478/545 [21:19<02:58,  2.66s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 479/545 [21:22<02:54,  2.65s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 480/545 [21:24<02:50,  2.62s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 481/545 [21:27<02:48,  2.63s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 482/545 [21:29<02:46,  2.65s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 483/545 [21:32<02:43,  2.64s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 484/545 [21:35<02:39,  2.61s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 485/545 [21:37<02:37,  2.62s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 486/545 [21:40<02:34,  2.62s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 487/545 [21:42<02:31,  2.61s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 488/545 [21:45<02:26,  2.58s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 489/545 [21:47<02:22,  2.55s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 490/545 [21:50<02:20,  2.56s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 491/545 [21:53<02:21,  2.63s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 492/545 [21:55<02:17,  2.59s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 493/545 [21:58<02:18,  2.66s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 494/545 [22:01<02:13,  2.61s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 495/545 [22:03<02:07,  2.56s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 496/545 [22:06<02:05,  2.57s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 497/545 [22:08<02:03,  2.57s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 498/545 [22:11<02:00,  2.56s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 499/545 [22:13<01:58,  2.58s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 500/545 [22:25<03:54,  5.22s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 501/545 [22:27<03:15,  4.43s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 502/545 [22:30<02:49,  3.95s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 503/545 [22:33<02:32,  3.63s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 504/545 [22:36<02:21,  3.46s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 505/545 [22:39<02:11,  3.29s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 506/545 [22:42<02:00,  3.08s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 507/545 [22:44<01:51,  2.93s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 508/545 [22:47<01:47,  2.91s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 509/545 [22:50<01:40,  2.80s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 510/545 [22:52<01:33,  2.68s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 511/545 [22:55<01:30,  2.66s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 512/545 [22:57<01:26,  2.61s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 513/545 [23:00<01:24,  2.63s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 514/545 [23:02<01:20,  2.59s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 515/545 [23:05<01:17,  2.58s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 516/545 [23:08<01:15,  2.61s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 517/545 [23:10<01:12,  2.59s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 518/545 [23:13<01:10,  2.61s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 519/545 [23:15<01:07,  2.60s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 520/545 [23:18<01:04,  2.60s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 521/545 [23:21<01:02,  2.62s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 522/545 [23:23<01:00,  2.64s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 523/545 [23:26<00:58,  2.66s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 524/545 [23:29<00:55,  2.64s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 525/545 [23:31<00:53,  2.65s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 526/545 [23:34<00:49,  2.59s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 527/545 [23:36<00:46,  2.60s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 528/545 [23:39<00:44,  2.61s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 529/545 [23:42<00:42,  2.64s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 530/545 [23:44<00:39,  2.66s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 531/545 [23:47<00:38,  2.73s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 532/545 [23:50<00:35,  2.75s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 533/545 [23:52<00:31,  2.66s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 534/545 [23:55<00:29,  2.64s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 535/545 [23:58<00:25,  2.59s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 536/545 [24:00<00:23,  2.57s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 537/545 [24:03<00:20,  2.58s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 538/545 [24:05<00:17,  2.55s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 539/545 [24:08<00:15,  2.53s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 540/545 [24:10<00:12,  2.57s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 541/545 [24:13<00:10,  2.64s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 542/545 [24:16<00:07,  2.64s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 543/545 [24:18<00:05,  2.61s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 544/545 [24:21<00:02,  2.77s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 545/545 [24:24<00:00,  2.60s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 545/545 [24:24<00:00,  2.69s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  1.07it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.62it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.51it/s]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
Loading training prompts done!
  0%|          | 0/545 [00:00<?, ?it/s]/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  0%|          | 1/545 [00:05<45:41,  5.04s/it]  0%|          | 2/545 [00:07<33:25,  3.69s/it]  1%|          | 3/545 [00:10<28:19,  3.14s/it]  1%|          | 4/545 [00:12<25:45,  2.86s/it]  1%|          | 5/545 [00:15<25:27,  2.83s/it]  1%|          | 6/545 [00:18<25:30,  2.84s/it]  1%|‚ñè         | 7/545 [00:21<25:00,  2.79s/it]  1%|‚ñè         | 8/545 [00:23<24:13,  2.71s/it]  2%|‚ñè         | 9/545 [00:26<24:02,  2.69s/it]  2%|‚ñè         | 10/545 [00:28<23:24,  2.63s/it]  2%|‚ñè         | 11/545 [00:31<22:50,  2.57s/it]  2%|‚ñè         | 12/545 [00:33<22:36,  2.54s/it]  2%|‚ñè         | 13/545 [00:36<23:09,  2.61s/it]  3%|‚ñé         | 14/545 [00:39<23:23,  2.64s/it]  3%|‚ñé         | 15/545 [00:41<22:54,  2.59s/it]  3%|‚ñé         | 16/545 [00:44<22:48,  2.59s/it]  3%|‚ñé         | 17/545 [00:46<23:20,  2.65s/it]  3%|‚ñé         | 18/545 [00:49<23:36,  2.69s/it]  3%|‚ñé         | 19/545 [00:52<23:32,  2.69s/it]  4%|‚ñé         | 20/545 [00:54<23:15,  2.66s/it]  4%|‚ñç         | 21/545 [00:57<22:54,  2.62s/it]  4%|‚ñç         | 22/545 [01:00<23:20,  2.68s/it]  4%|‚ñç         | 23/545 [01:02<22:34,  2.59s/it]  4%|‚ñç         | 24/545 [01:05<22:44,  2.62s/it]  5%|‚ñç         | 25/545 [01:08<24:12,  2.79s/it]  5%|‚ñç         | 26/545 [01:11<24:00,  2.78s/it]  5%|‚ñç         | 27/545 [01:13<23:19,  2.70s/it]  5%|‚ñå         | 28/545 [01:16<23:13,  2.69s/it]  5%|‚ñå         | 29/545 [01:19<22:45,  2.65s/it]  6%|‚ñå         | 30/545 [01:21<23:00,  2.68s/it]  6%|‚ñå         | 31/545 [01:24<23:23,  2.73s/it]  6%|‚ñå         | 32/545 [01:27<22:48,  2.67s/it]  6%|‚ñå         | 33/545 [01:29<22:12,  2.60s/it]  6%|‚ñå         | 34/545 [01:32<21:44,  2.55s/it]  6%|‚ñã         | 35/545 [01:34<21:50,  2.57s/it]  7%|‚ñã         | 36/545 [01:37<21:43,  2.56s/it]  7%|‚ñã         | 37/545 [01:39<21:54,  2.59s/it]  7%|‚ñã         | 38/545 [01:42<21:52,  2.59s/it]  7%|‚ñã         | 39/545 [01:44<21:25,  2.54s/it]  7%|‚ñã         | 40/545 [01:47<21:07,  2.51s/it]  8%|‚ñä         | 41/545 [01:49<21:16,  2.53s/it]  8%|‚ñä         | 42/545 [01:52<21:11,  2.53s/it]  8%|‚ñä         | 43/545 [01:54<21:09,  2.53s/it]  8%|‚ñä         | 44/545 [01:57<21:22,  2.56s/it]  8%|‚ñä         | 45/545 [02:00<21:37,  2.59s/it]  8%|‚ñä         | 46/545 [02:02<21:23,  2.57s/it]  9%|‚ñä         | 47/545 [02:05<21:50,  2.63s/it]  9%|‚ñâ         | 48/545 [02:08<21:53,  2.64s/it]  9%|‚ñâ         | 49/545 [02:10<21:50,  2.64s/it]  9%|‚ñâ         | 50/545 [02:13<21:30,  2.61s/it]  9%|‚ñâ         | 51/545 [02:16<21:50,  2.65s/it] 10%|‚ñâ         | 52/545 [02:18<21:29,  2.62s/it] 10%|‚ñâ         | 53/545 [02:21<21:29,  2.62s/it] 10%|‚ñâ         | 54/545 [02:23<21:27,  2.62s/it] 10%|‚ñà         | 55/545 [02:26<21:17,  2.61s/it] 10%|‚ñà         | 56/545 [02:29<21:17,  2.61s/it] 10%|‚ñà         | 57/545 [02:31<21:24,  2.63s/it] 11%|‚ñà         | 58/545 [02:34<20:52,  2.57s/it] 11%|‚ñà         | 59/545 [02:36<20:29,  2.53s/it] 11%|‚ñà         | 60/545 [02:39<20:41,  2.56s/it] 11%|‚ñà         | 61/545 [02:42<21:04,  2.61s/it] 11%|‚ñà‚ñè        | 62/545 [02:44<20:55,  2.60s/it] 12%|‚ñà‚ñè        | 63/545 [02:47<20:58,  2.61s/it] 12%|‚ñà‚ñè        | 64/545 [02:49<20:38,  2.57s/it] 12%|‚ñà‚ñè        | 65/545 [02:52<20:21,  2.54s/it] 12%|‚ñà‚ñè        | 66/545 [02:54<20:17,  2.54s/it] 12%|‚ñà‚ñè        | 67/545 [02:57<20:06,  2.52s/it] 12%|‚ñà‚ñè        | 68/545 [02:59<20:19,  2.56s/it] 13%|‚ñà‚ñé        | 69/545 [03:02<20:12,  2.55s/it] 13%|‚ñà‚ñé        | 70/545 [03:05<20:15,  2.56s/it] 13%|‚ñà‚ñé        | 71/545 [03:07<20:22,  2.58s/it] 13%|‚ñà‚ñé        | 72/545 [03:10<20:33,  2.61s/it] 13%|‚ñà‚ñé        | 73/545 [03:13<20:46,  2.64s/it] 14%|‚ñà‚ñé        | 74/545 [03:15<20:43,  2.64s/it] 14%|‚ñà‚ñç        | 75/545 [03:18<20:24,  2.60s/it] 14%|‚ñà‚ñç        | 76/545 [03:20<20:18,  2.60s/it] 14%|‚ñà‚ñç        | 77/545 [03:23<19:58,  2.56s/it] 14%|‚ñà‚ñç        | 78/545 [03:26<21:24,  2.75s/it] 14%|‚ñà‚ñç        | 79/545 [03:29<21:23,  2.75s/it] 15%|‚ñà‚ñç        | 80/545 [03:31<21:22,  2.76s/it] 15%|‚ñà‚ñç        | 81/545 [03:34<20:55,  2.71s/it] 15%|‚ñà‚ñå        | 82/545 [03:36<20:14,  2.62s/it] 15%|‚ñà‚ñå        | 83/545 [03:39<20:17,  2.64s/it] 15%|‚ñà‚ñå        | 84/545 [03:42<20:19,  2.65s/it] 16%|‚ñà‚ñå        | 85/545 [03:44<20:08,  2.63s/it] 16%|‚ñà‚ñå        | 86/545 [03:47<19:45,  2.58s/it] 16%|‚ñà‚ñå        | 87/545 [03:49<19:36,  2.57s/it] 16%|‚ñà‚ñå        | 88/545 [03:52<19:48,  2.60s/it] 16%|‚ñà‚ñã        | 89/545 [03:55<19:56,  2.62s/it] 17%|‚ñà‚ñã        | 90/545 [03:57<19:53,  2.62s/it] 17%|‚ñà‚ñã        | 91/545 [04:00<19:25,  2.57s/it] 17%|‚ñà‚ñã        | 92/545 [04:02<19:04,  2.53s/it] 17%|‚ñà‚ñã        | 93/545 [04:05<19:01,  2.53s/it] 17%|‚ñà‚ñã        | 94/545 [04:07<19:06,  2.54s/it] 17%|‚ñà‚ñã        | 95/545 [04:10<18:49,  2.51s/it] 18%|‚ñà‚ñä        | 96/545 [04:12<18:49,  2.52s/it] 18%|‚ñà‚ñä        | 97/545 [04:15<19:02,  2.55s/it] 18%|‚ñà‚ñä        | 98/545 [04:18<19:29,  2.62s/it] 18%|‚ñà‚ñä        | 99/545 [04:20<19:08,  2.58s/it] 18%|‚ñà‚ñä        | 100/545 [04:23<18:53,  2.55s/it] 19%|‚ñà‚ñä        | 101/545 [04:25<18:55,  2.56s/it] 19%|‚ñà‚ñä        | 102/545 [04:28<19:08,  2.59s/it] 19%|‚ñà‚ñâ        | 103/545 [04:31<19:03,  2.59s/it] 19%|‚ñà‚ñâ        | 104/545 [04:33<19:00,  2.59s/it] 19%|‚ñà‚ñâ        | 105/545 [04:36<18:56,  2.58s/it] 19%|‚ñà‚ñâ        | 106/545 [04:38<18:40,  2.55s/it] 20%|‚ñà‚ñâ        | 107/545 [04:41<18:47,  2.57s/it] 20%|‚ñà‚ñâ        | 108/545 [04:52<37:46,  5.19s/it] 20%|‚ñà‚ñà        | 109/545 [04:55<31:46,  4.37s/it] 20%|‚ñà‚ñà        | 110/545 [04:57<27:41,  3.82s/it] 20%|‚ñà‚ñà        | 111/545 [05:00<24:49,  3.43s/it] 21%|‚ñà‚ñà        | 112/545 [05:02<23:20,  3.23s/it] 21%|‚ñà‚ñà        | 113/545 [05:05<21:45,  3.02s/it] 21%|‚ñà‚ñà        | 114/545 [05:07<20:44,  2.89s/it] 21%|‚ñà‚ñà        | 115/545 [05:10<19:50,  2.77s/it] 21%|‚ñà‚ñà‚ñè       | 116/545 [05:13<19:48,  2.77s/it] 21%|‚ñà‚ñà‚ñè       | 117/545 [05:15<19:14,  2.70s/it] 22%|‚ñà‚ñà‚ñè       | 118/545 [05:18<18:58,  2.67s/it] 22%|‚ñà‚ñà‚ñè       | 119/545 [05:21<19:02,  2.68s/it] 22%|‚ñà‚ñà‚ñè       | 120/545 [05:23<19:11,  2.71s/it] 22%|‚ñà‚ñà‚ñè       | 121/545 [05:26<18:58,  2.69s/it] 22%|‚ñà‚ñà‚ñè       | 122/545 [05:28<18:24,  2.61s/it] 23%|‚ñà‚ñà‚ñé       | 123/545 [05:31<18:53,  2.69s/it] 23%|‚ñà‚ñà‚ñé       | 124/545 [05:34<18:47,  2.68s/it] 23%|‚ñà‚ñà‚ñé       | 125/545 [05:37<18:38,  2.66s/it] 23%|‚ñà‚ñà‚ñé       | 126/545 [05:39<18:19,  2.62s/it] 23%|‚ñà‚ñà‚ñé       | 127/545 [05:42<18:11,  2.61s/it] 23%|‚ñà‚ñà‚ñé       | 128/545 [05:44<17:52,  2.57s/it] 24%|‚ñà‚ñà‚ñé       | 129/545 [05:47<18:01,  2.60s/it] 24%|‚ñà‚ñà‚ñç       | 130/545 [05:50<18:14,  2.64s/it] 24%|‚ñà‚ñà‚ñç       | 131/545 [05:52<18:09,  2.63s/it] 24%|‚ñà‚ñà‚ñç       | 132/545 [05:55<17:48,  2.59s/it] 24%|‚ñà‚ñà‚ñç       | 133/545 [05:57<17:57,  2.61s/it] 25%|‚ñà‚ñà‚ñç       | 134/545 [06:00<17:32,  2.56s/it] 25%|‚ñà‚ñà‚ñç       | 135/545 [06:02<17:19,  2.54s/it] 25%|‚ñà‚ñà‚ñç       | 136/545 [06:05<17:21,  2.55s/it] 25%|‚ñà‚ñà‚ñå       | 137/545 [06:07<17:23,  2.56s/it] 25%|‚ñà‚ñà‚ñå       | 138/545 [06:10<17:29,  2.58s/it] 26%|‚ñà‚ñà‚ñå       | 139/545 [06:13<17:43,  2.62s/it] 26%|‚ñà‚ñà‚ñå       | 140/545 [06:15<17:29,  2.59s/it] 26%|‚ñà‚ñà‚ñå       | 141/545 [06:18<17:37,  2.62s/it] 26%|‚ñà‚ñà‚ñå       | 142/545 [06:21<17:35,  2.62s/it] 26%|‚ñà‚ñà‚ñå       | 143/545 [06:23<17:15,  2.58s/it] 26%|‚ñà‚ñà‚ñã       | 144/545 [06:25<16:55,  2.53s/it] 27%|‚ñà‚ñà‚ñã       | 145/545 [06:28<16:52,  2.53s/it] 27%|‚ñà‚ñà‚ñã       | 146/545 [06:31<17:12,  2.59s/it] 27%|‚ñà‚ñà‚ñã       | 147/545 [06:33<17:20,  2.61s/it] 27%|‚ñà‚ñà‚ñã       | 148/545 [06:36<16:59,  2.57s/it] 27%|‚ñà‚ñà‚ñã       | 149/545 [06:39<17:14,  2.61s/it] 28%|‚ñà‚ñà‚ñä       | 150/545 [06:41<17:29,  2.66s/it] 28%|‚ñà‚ñà‚ñä       | 151/545 [06:44<17:33,  2.67s/it] 28%|‚ñà‚ñà‚ñä       | 152/545 [06:47<17:29,  2.67s/it] 28%|‚ñà‚ñà‚ñä       | 153/545 [06:49<16:57,  2.60s/it] 28%|‚ñà‚ñà‚ñä       | 154/545 [06:52<17:08,  2.63s/it] 28%|‚ñà‚ñà‚ñä       | 155/545 [06:54<16:47,  2.58s/it] 29%|‚ñà‚ñà‚ñä       | 156/545 [06:57<16:49,  2.60s/it] 29%|‚ñà‚ñà‚ñâ       | 157/545 [07:00<16:56,  2.62s/it] 29%|‚ñà‚ñà‚ñâ       | 158/545 [07:02<16:59,  2.63s/it] 29%|‚ñà‚ñà‚ñâ       | 159/545 [07:05<16:49,  2.61s/it] 29%|‚ñà‚ñà‚ñâ       | 160/545 [07:07<16:41,  2.60s/it] 30%|‚ñà‚ñà‚ñâ       | 161/545 [07:10<16:57,  2.65s/it] 30%|‚ñà‚ñà‚ñâ       | 162/545 [07:13<16:45,  2.63s/it] 30%|‚ñà‚ñà‚ñâ       | 163/545 [07:15<16:33,  2.60s/it] 30%|‚ñà‚ñà‚ñà       | 164/545 [07:18<16:19,  2.57s/it] 30%|‚ñà‚ñà‚ñà       | 165/545 [07:20<16:18,  2.57s/it] 30%|‚ñà‚ñà‚ñà       | 166/545 [07:23<16:04,  2.55s/it] 31%|‚ñà‚ñà‚ñà       | 167/545 [07:25<15:49,  2.51s/it] 31%|‚ñà‚ñà‚ñà       | 168/545 [07:28<15:48,  2.52s/it] 31%|‚ñà‚ñà‚ñà       | 169/545 [07:30<15:52,  2.53s/it] 31%|‚ñà‚ñà‚ñà       | 170/545 [07:33<15:59,  2.56s/it] 31%|‚ñà‚ñà‚ñà‚ñè      | 171/545 [07:36<16:04,  2.58s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 172/545 [07:39<16:50,  2.71s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 173/545 [07:41<16:32,  2.67s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 174/545 [07:44<16:02,  2.59s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 175/545 [07:46<15:57,  2.59s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 176/545 [07:49<15:48,  2.57s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 177/545 [07:51<15:57,  2.60s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 178/545 [07:54<15:52,  2.60s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 179/545 [07:57<15:59,  2.62s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 180/545 [08:00<16:22,  2.69s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 181/545 [08:02<16:07,  2.66s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 182/545 [08:05<15:56,  2.63s/it] 34%|‚ñà‚ñà‚ñà‚ñé      | 183/545 [08:07<15:51,  2.63s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 184/545 [08:10<15:23,  2.56s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 185/545 [08:12<15:18,  2.55s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 186/545 [08:15<15:09,  2.53s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 187/545 [08:17<15:23,  2.58s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 188/545 [08:20<15:42,  2.64s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 189/545 [08:23<15:18,  2.58s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 190/545 [08:25<15:27,  2.61s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 191/545 [08:28<15:12,  2.58s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 192/545 [08:30<15:05,  2.57s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 193/545 [08:33<15:15,  2.60s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 194/545 [08:36<15:01,  2.57s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 195/545 [08:38<15:06,  2.59s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 196/545 [08:41<14:58,  2.58s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 197/545 [08:43<14:49,  2.56s/it] 36%|‚ñà‚ñà‚ñà‚ñã      | 198/545 [08:46<14:53,  2.58s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 199/545 [08:49<15:10,  2.63s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 200/545 [08:51<14:47,  2.57s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 201/545 [08:54<14:45,  2.57s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 202/545 [08:56<14:27,  2.53s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 203/545 [08:59<14:29,  2.54s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 204/545 [09:01<14:16,  2.51s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 205/545 [09:04<14:25,  2.55s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 206/545 [09:06<14:31,  2.57s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 207/545 [09:09<14:39,  2.60s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 208/545 [09:12<14:38,  2.61s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 209/545 [09:14<14:36,  2.61s/it] 39%|‚ñà‚ñà‚ñà‚ñä      | 210/545 [09:17<14:35,  2.61s/it] 39%|‚ñà‚ñà‚ñà‚ñä      | 211/545 [09:19<14:29,  2.60s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 212/545 [09:22<14:37,  2.64s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 213/545 [09:25<14:30,  2.62s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 214/545 [09:27<14:18,  2.59s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 215/545 [09:30<14:37,  2.66s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 216/545 [09:32<14:03,  2.56s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 217/545 [09:35<14:11,  2.60s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 218/545 [09:38<13:57,  2.56s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 219/545 [09:40<13:31,  2.49s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 220/545 [09:43<13:42,  2.53s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 221/545 [09:45<13:44,  2.54s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 222/545 [09:48<13:54,  2.58s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 223/545 [09:50<13:55,  2.60s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 224/545 [09:53<13:50,  2.59s/it] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 225/545 [09:55<13:33,  2.54s/it] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 226/545 [09:58<13:33,  2.55s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 227/545 [10:00<13:18,  2.51s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 228/545 [10:03<13:26,  2.55s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 229/545 [10:06<13:21,  2.54s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 230/545 [10:08<13:04,  2.49s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 231/545 [10:11<13:16,  2.54s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 232/545 [10:13<13:22,  2.56s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 233/545 [10:16<13:30,  2.60s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 234/545 [10:18<13:26,  2.59s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 235/545 [10:21<13:26,  2.60s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 236/545 [10:24<13:21,  2.59s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 237/545 [10:26<13:17,  2.59s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 238/545 [10:29<13:13,  2.58s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 239/545 [10:31<13:01,  2.55s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 240/545 [10:34<13:00,  2.56s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 241/545 [10:36<12:55,  2.55s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 242/545 [10:39<12:59,  2.57s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 243/545 [10:41<12:44,  2.53s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 244/545 [10:44<12:55,  2.58s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 245/545 [10:47<12:44,  2.55s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 246/545 [10:49<12:53,  2.59s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 247/545 [10:52<12:49,  2.58s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 248/545 [10:54<12:52,  2.60s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 249/545 [10:57<12:35,  2.55s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 250/545 [10:59<12:31,  2.55s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 251/545 [11:02<12:49,  2.62s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 252/545 [11:05<13:00,  2.66s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 253/545 [11:08<12:48,  2.63s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 254/545 [11:10<12:41,  2.62s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 255/545 [11:13<12:22,  2.56s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 256/545 [11:15<12:17,  2.55s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 257/545 [11:18<12:21,  2.57s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 258/545 [11:20<12:23,  2.59s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 259/545 [11:23<12:27,  2.61s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 260/545 [11:26<13:15,  2.79s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 261/545 [11:29<13:01,  2.75s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 262/545 [11:31<12:35,  2.67s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 263/545 [11:34<12:16,  2.61s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 264/545 [11:37<12:30,  2.67s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 265/545 [11:39<12:16,  2.63s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 266/545 [11:42<11:57,  2.57s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 267/545 [11:44<11:59,  2.59s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 268/545 [11:47<12:12,  2.64s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 269/545 [11:50<12:11,  2.65s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 270/545 [11:52<11:50,  2.58s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 271/545 [11:55<11:54,  2.61s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 272/545 [11:58<12:03,  2.65s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 273/545 [12:00<11:50,  2.61s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 274/545 [12:03<12:00,  2.66s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 275/545 [12:06<12:14,  2.72s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 276/545 [12:08<11:48,  2.63s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 277/545 [12:11<12:30,  2.80s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 278/545 [12:14<12:13,  2.75s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 279/545 [12:17<12:12,  2.75s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 280/545 [12:19<12:02,  2.73s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 281/545 [12:22<11:48,  2.68s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 282/545 [12:24<11:29,  2.62s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 283/545 [12:27<11:30,  2.64s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 284/545 [12:30<11:23,  2.62s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 285/545 [12:32<11:14,  2.59s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 286/545 [12:35<11:18,  2.62s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 287/545 [12:38<11:20,  2.64s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 288/545 [12:40<11:05,  2.59s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 289/545 [12:43<11:01,  2.59s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 290/545 [12:45<11:03,  2.60s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 291/545 [12:48<10:58,  2.59s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 292/545 [12:50<10:55,  2.59s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 293/545 [12:53<10:55,  2.60s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 294/545 [12:56<10:57,  2.62s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 295/545 [12:58<10:48,  2.59s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 296/545 [13:01<10:51,  2.62s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 297/545 [13:03<10:35,  2.56s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 298/545 [13:06<10:37,  2.58s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 299/545 [13:09<10:34,  2.58s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 300/545 [13:11<10:32,  2.58s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 301/545 [13:14<10:18,  2.53s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 302/545 [13:16<10:19,  2.55s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 303/545 [13:19<10:18,  2.56s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 304/545 [13:21<10:24,  2.59s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 305/545 [13:24<10:20,  2.59s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 306/545 [13:27<10:27,  2.63s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 307/545 [13:29<10:18,  2.60s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 308/545 [13:32<10:25,  2.64s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 309/545 [13:34<10:11,  2.59s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 310/545 [13:37<10:04,  2.57s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 311/545 [13:39<09:55,  2.54s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 312/545 [13:42<09:47,  2.52s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 313/545 [13:45<09:55,  2.57s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 314/545 [13:47<09:53,  2.57s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 315/545 [13:50<09:55,  2.59s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 316/545 [13:52<09:49,  2.57s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 317/545 [13:55<09:33,  2.52s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 318/545 [13:57<09:22,  2.48s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 319/545 [14:00<09:33,  2.54s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 320/545 [14:02<09:34,  2.55s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 321/545 [14:05<09:33,  2.56s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 322/545 [14:08<09:34,  2.58s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 323/545 [14:10<09:20,  2.52s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 324/545 [14:12<09:14,  2.51s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 325/545 [14:15<09:13,  2.52s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 326/545 [14:18<09:21,  2.57s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 327/545 [14:20<09:23,  2.58s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 328/545 [14:23<09:24,  2.60s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 329/545 [14:25<09:20,  2.59s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 330/545 [14:28<09:16,  2.59s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 331/545 [14:31<09:16,  2.60s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 332/545 [14:33<09:12,  2.60s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 333/545 [14:36<09:06,  2.58s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 334/545 [14:39<09:15,  2.63s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 335/545 [14:41<09:16,  2.65s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 336/545 [14:44<09:12,  2.65s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 337/545 [14:46<09:06,  2.63s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 338/545 [14:49<09:09,  2.66s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 339/545 [14:52<08:59,  2.62s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 340/545 [14:54<08:55,  2.61s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 341/545 [14:57<08:54,  2.62s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 342/545 [15:00<08:51,  2.62s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 343/545 [15:02<08:57,  2.66s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 344/545 [15:05<08:41,  2.59s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 345/545 [15:07<08:29,  2.55s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 346/545 [15:10<08:29,  2.56s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 347/545 [15:12<08:16,  2.51s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 348/545 [15:15<08:20,  2.54s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 349/545 [15:18<08:35,  2.63s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 350/545 [15:20<08:29,  2.61s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 351/545 [15:23<08:35,  2.66s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 352/545 [15:26<08:31,  2.65s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 353/545 [15:28<08:16,  2.58s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 354/545 [15:31<08:11,  2.57s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 355/545 [15:33<08:06,  2.56s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 356/545 [15:36<08:09,  2.59s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 357/545 [15:39<08:14,  2.63s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 358/545 [15:41<08:04,  2.59s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 359/545 [15:44<08:25,  2.72s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 360/545 [15:47<08:39,  2.81s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 361/545 [15:50<08:18,  2.71s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 362/545 [15:52<08:19,  2.73s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 363/545 [15:55<08:08,  2.69s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 364/545 [15:58<08:04,  2.67s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 365/545 [16:00<07:58,  2.66s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 366/545 [16:03<07:50,  2.63s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 367/545 [16:05<07:50,  2.64s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 368/545 [16:08<07:42,  2.61s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 369/545 [16:11<07:55,  2.70s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 370/545 [16:14<07:56,  2.72s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 371/545 [16:16<07:49,  2.70s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 372/545 [16:19<07:40,  2.66s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 373/545 [16:21<07:33,  2.64s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 374/545 [16:24<07:28,  2.62s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 375/545 [16:27<07:33,  2.67s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 376/545 [16:29<07:26,  2.64s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 377/545 [16:32<07:21,  2.63s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 378/545 [16:35<07:26,  2.67s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 379/545 [16:37<07:23,  2.67s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 380/545 [16:40<07:11,  2.61s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 381/545 [16:43<07:14,  2.65s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 382/545 [16:45<07:10,  2.64s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 383/545 [16:48<07:09,  2.65s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 384/545 [16:51<07:03,  2.63s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 385/545 [16:53<07:03,  2.65s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 386/545 [16:56<06:53,  2.60s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 387/545 [16:58<06:47,  2.58s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 388/545 [17:01<06:47,  2.60s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 389/545 [17:04<06:52,  2.65s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 390/545 [17:06<06:45,  2.61s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 391/545 [17:09<06:36,  2.58s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 392/545 [17:12<07:02,  2.76s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 393/545 [17:23<13:27,  5.31s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 394/545 [17:26<11:19,  4.50s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 395/545 [17:29<10:00,  4.01s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 396/545 [17:31<09:09,  3.68s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 397/545 [17:34<08:35,  3.48s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 398/545 [17:38<08:14,  3.37s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 399/545 [17:41<08:00,  3.29s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 400/545 [17:44<07:39,  3.17s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 401/545 [17:46<07:10,  2.99s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 402/545 [17:49<06:49,  2.87s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 403/545 [17:51<06:34,  2.78s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 404/545 [17:54<06:27,  2.75s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 405/545 [17:57<06:17,  2.70s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 406/545 [17:59<06:14,  2.69s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 407/545 [18:02<06:06,  2.66s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 408/545 [18:04<06:04,  2.66s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 409/545 [18:07<05:58,  2.64s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 410/545 [18:10<05:51,  2.61s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 411/545 [18:12<05:56,  2.66s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 412/545 [18:15<05:52,  2.65s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 413/545 [18:18<05:47,  2.63s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 414/545 [18:20<05:46,  2.64s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 415/545 [18:23<05:40,  2.62s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 416/545 [18:25<05:30,  2.56s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 417/545 [18:28<05:28,  2.57s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 418/545 [18:30<05:26,  2.57s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 419/545 [18:33<05:18,  2.53s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 420/545 [18:36<05:26,  2.62s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 421/545 [18:38<05:30,  2.66s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 422/545 [18:41<05:24,  2.64s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 423/545 [18:44<05:18,  2.61s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 424/545 [18:46<05:10,  2.57s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 425/545 [18:49<05:15,  2.63s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 426/545 [18:52<05:18,  2.67s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 427/545 [18:54<05:08,  2.61s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 428/545 [18:57<05:11,  2.66s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 429/545 [18:59<05:02,  2.61s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 430/545 [19:02<05:00,  2.61s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 431/545 [19:04<04:51,  2.56s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 432/545 [19:07<04:46,  2.54s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 433/545 [19:09<04:42,  2.52s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 434/545 [19:12<04:41,  2.54s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 435/545 [19:15<04:44,  2.58s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 436/545 [19:18<04:51,  2.68s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 437/545 [19:20<04:49,  2.68s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 438/545 [19:23<04:41,  2.63s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 439/545 [19:25<04:37,  2.62s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 440/545 [19:28<04:43,  2.70s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 441/545 [19:31<04:38,  2.68s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 442/545 [19:33<04:28,  2.61s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 443/545 [19:36<04:31,  2.67s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 444/545 [19:39<04:28,  2.66s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 445/545 [19:41<04:20,  2.61s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 446/545 [19:44<04:16,  2.59s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 447/545 [19:46<04:16,  2.62s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 448/545 [19:49<04:14,  2.63s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 449/545 [20:00<08:25,  5.26s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 450/545 [20:03<07:03,  4.46s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 451/545 [20:06<06:07,  3.91s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 452/545 [20:08<05:30,  3.55s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 453/545 [20:11<05:01,  3.28s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 454/545 [20:14<04:37,  3.05s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 455/545 [20:16<04:29,  3.00s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 456/545 [20:19<04:13,  2.85s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 457/545 [20:22<04:04,  2.77s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 458/545 [20:24<04:00,  2.76s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 459/545 [20:27<03:48,  2.66s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 460/545 [20:29<03:44,  2.64s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 461/545 [20:32<03:39,  2.61s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 462/545 [20:34<03:36,  2.60s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 463/545 [20:37<03:32,  2.60s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 464/545 [20:40<03:27,  2.56s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 465/545 [20:42<03:29,  2.62s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 466/545 [20:45<03:22,  2.57s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 467/545 [20:47<03:22,  2.60s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 468/545 [20:50<03:15,  2.54s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 469/545 [20:52<03:11,  2.52s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 470/545 [20:55<03:10,  2.54s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 471/545 [20:57<03:10,  2.58s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 472/545 [21:00<03:09,  2.60s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 473/545 [21:03<03:08,  2.62s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 474/545 [21:06<03:09,  2.66s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 475/545 [21:08<03:05,  2.65s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 476/545 [21:11<03:02,  2.65s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 477/545 [21:14<03:02,  2.69s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 478/545 [21:16<02:57,  2.65s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 479/545 [21:19<02:54,  2.64s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 480/545 [21:21<02:49,  2.61s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 481/545 [21:24<02:47,  2.61s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 482/545 [21:27<02:45,  2.63s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 483/545 [21:29<02:42,  2.63s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 484/545 [21:32<02:38,  2.60s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 485/545 [21:34<02:36,  2.61s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 486/545 [21:37<02:33,  2.61s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 487/545 [21:40<02:30,  2.60s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 488/545 [21:42<02:26,  2.56s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 489/545 [21:45<02:22,  2.54s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 490/545 [21:47<02:20,  2.55s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 491/545 [21:50<02:26,  2.72s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 492/545 [21:53<02:21,  2.67s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 493/545 [21:56<02:20,  2.71s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 494/545 [21:58<02:15,  2.65s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 495/545 [22:01<02:10,  2.60s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 496/545 [22:03<02:07,  2.60s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 497/545 [22:06<02:04,  2.59s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 498/545 [22:08<02:01,  2.58s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 499/545 [22:11<01:58,  2.57s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 500/545 [22:22<03:53,  5.19s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 501/545 [22:25<03:13,  4.40s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 502/545 [22:28<02:48,  3.91s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 503/545 [22:30<02:31,  3.60s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 504/545 [22:33<02:17,  3.35s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 505/545 [22:36<02:04,  3.12s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 506/545 [22:38<01:55,  2.95s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 507/545 [22:41<01:47,  2.84s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 508/545 [22:44<01:45,  2.85s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 509/545 [22:46<01:39,  2.76s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 510/545 [22:49<01:32,  2.65s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 511/545 [22:51<01:29,  2.65s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 512/545 [22:54<01:25,  2.60s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 513/545 [22:56<01:23,  2.62s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 514/545 [22:59<01:19,  2.57s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 515/545 [23:02<01:19,  2.65s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 516/545 [23:05<01:19,  2.75s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 517/545 [23:08<01:17,  2.76s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 518/545 [23:10<01:16,  2.82s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 519/545 [23:13<01:13,  2.83s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 520/545 [23:16<01:11,  2.84s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 521/545 [23:19<01:09,  2.89s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 522/545 [23:22<01:07,  2.91s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 523/545 [23:25<01:05,  2.96s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 524/545 [23:28<01:01,  2.93s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 525/545 [23:31<00:59,  2.95s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 526/545 [23:34<00:54,  2.87s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 527/545 [23:37<00:51,  2.87s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 528/545 [23:40<00:49,  2.89s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 529/545 [23:43<00:46,  2.92s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 530/545 [23:46<00:44,  2.94s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 531/545 [23:49<00:42,  3.03s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 532/545 [23:52<00:39,  3.05s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 533/545 [23:55<00:35,  2.94s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 534/545 [23:57<00:32,  2.92s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 535/545 [24:00<00:28,  2.87s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 536/545 [24:03<00:25,  2.85s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 537/545 [24:06<00:22,  2.86s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 538/545 [24:09<00:19,  2.83s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 539/545 [24:11<00:16,  2.80s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 540/545 [24:14<00:14,  2.86s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 541/545 [24:18<00:11,  2.94s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 542/545 [24:20<00:08,  2.94s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 543/545 [24:23<00:05,  2.90s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 544/545 [24:26<00:02,  2.96s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 545/545 [24:29<00:00,  2.72s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 545/545 [24:29<00:00,  2.70s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  1.08it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.64it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.52it/s]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
Loading training prompts done!
  0%|          | 0/545 [00:00<?, ?it/s]/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  0%|          | 1/545 [00:05<45:38,  5.03s/it]  0%|          | 2/545 [00:07<33:23,  3.69s/it]  1%|          | 3/545 [00:10<28:19,  3.14s/it]  1%|          | 4/545 [00:12<26:10,  2.90s/it]  1%|          | 5/545 [00:15<25:42,  2.86s/it]  1%|          | 6/545 [00:18<25:30,  2.84s/it]  1%|‚ñè         | 7/545 [00:21<24:55,  2.78s/it]  1%|‚ñè         | 8/545 [00:23<24:15,  2.71s/it]  2%|‚ñè         | 9/545 [00:26<23:57,  2.68s/it]  2%|‚ñè         | 10/545 [00:28<23:20,  2.62s/it]  2%|‚ñè         | 11/545 [00:31<22:47,  2.56s/it]  2%|‚ñè         | 12/545 [00:33<22:42,  2.56s/it]  2%|‚ñè         | 13/545 [00:36<23:12,  2.62s/it]  3%|‚ñé         | 14/545 [00:39<23:24,  2.64s/it]  3%|‚ñé         | 15/545 [00:41<23:00,  2.61s/it]  3%|‚ñé         | 16/545 [00:44<22:52,  2.59s/it]  3%|‚ñé         | 17/545 [00:47<23:20,  2.65s/it]  3%|‚ñé         | 18/545 [00:49<23:21,  2.66s/it]  3%|‚ñé         | 19/545 [00:52<23:19,  2.66s/it]  4%|‚ñé         | 20/545 [00:54<23:03,  2.64s/it]  4%|‚ñç         | 21/545 [00:57<22:46,  2.61s/it]  4%|‚ñç         | 22/545 [01:00<23:15,  2.67s/it]  4%|‚ñç         | 23/545 [01:02<22:29,  2.58s/it]  4%|‚ñç         | 24/545 [01:05<22:32,  2.60s/it]  5%|‚ñç         | 25/545 [01:08<24:03,  2.78s/it]  5%|‚ñç         | 26/545 [01:11<23:53,  2.76s/it]  5%|‚ñç         | 27/545 [01:13<23:14,  2.69s/it]  5%|‚ñå         | 28/545 [01:16<23:09,  2.69s/it]  5%|‚ñå         | 29/545 [01:18<22:33,  2.62s/it]  6%|‚ñå         | 30/545 [01:21<22:58,  2.68s/it]  6%|‚ñå         | 31/545 [01:24<23:09,  2.70s/it]  6%|‚ñå         | 32/545 [01:26<22:38,  2.65s/it]  6%|‚ñå         | 33/545 [01:29<22:03,  2.58s/it]  6%|‚ñå         | 34/545 [01:31<21:37,  2.54s/it]  6%|‚ñã         | 35/545 [01:34<21:42,  2.55s/it]  7%|‚ñã         | 36/545 [01:37<21:42,  2.56s/it]  7%|‚ñã         | 37/545 [01:39<21:50,  2.58s/it]  7%|‚ñã         | 38/545 [01:42<22:31,  2.66s/it]  7%|‚ñã         | 39/545 [01:44<22:00,  2.61s/it]  7%|‚ñã         | 40/545 [01:47<21:32,  2.56s/it]  8%|‚ñä         | 41/545 [01:49<21:31,  2.56s/it]  8%|‚ñä         | 42/545 [01:52<21:16,  2.54s/it]  8%|‚ñä         | 43/545 [01:55<21:12,  2.54s/it]  8%|‚ñä         | 44/545 [01:57<21:17,  2.55s/it]  8%|‚ñä         | 45/545 [02:00<21:33,  2.59s/it]  8%|‚ñä         | 46/545 [02:02<21:25,  2.58s/it]  9%|‚ñä         | 47/545 [02:05<21:59,  2.65s/it]  9%|‚ñâ         | 48/545 [02:08<22:03,  2.66s/it]  9%|‚ñâ         | 49/545 [02:10<21:56,  2.66s/it]  9%|‚ñâ         | 50/545 [02:13<21:30,  2.61s/it]  9%|‚ñâ         | 51/545 [02:16<22:41,  2.76s/it] 10%|‚ñâ         | 52/545 [02:19<22:11,  2.70s/it] 10%|‚ñâ         | 53/545 [02:21<21:58,  2.68s/it] 10%|‚ñâ         | 54/545 [02:24<21:47,  2.66s/it] 10%|‚ñà         | 55/545 [02:26<21:32,  2.64s/it] 10%|‚ñà         | 56/545 [02:29<21:29,  2.64s/it] 10%|‚ñà         | 57/545 [02:32<21:34,  2.65s/it] 11%|‚ñà         | 58/545 [02:35<21:47,  2.69s/it] 11%|‚ñà         | 59/545 [02:37<21:48,  2.69s/it] 11%|‚ñà         | 60/545 [02:40<22:23,  2.77s/it] 11%|‚ñà         | 61/545 [02:43<22:56,  2.84s/it] 11%|‚ñà‚ñè        | 62/545 [02:46<22:57,  2.85s/it] 12%|‚ñà‚ñè        | 63/545 [02:49<23:06,  2.88s/it] 12%|‚ñà‚ñè        | 64/545 [02:51<22:01,  2.75s/it] 12%|‚ñà‚ñè        | 65/545 [02:54<21:20,  2.67s/it] 12%|‚ñà‚ñè        | 66/545 [02:57<20:59,  2.63s/it] 12%|‚ñà‚ñè        | 67/545 [02:59<20:37,  2.59s/it] 12%|‚ñà‚ñè        | 68/545 [03:02<20:42,  2.61s/it] 13%|‚ñà‚ñé        | 69/545 [03:04<20:30,  2.59s/it] 13%|‚ñà‚ñé        | 70/545 [03:07<20:28,  2.59s/it] 13%|‚ñà‚ñé        | 71/545 [03:09<20:32,  2.60s/it] 13%|‚ñà‚ñé        | 72/545 [03:12<20:40,  2.62s/it] 13%|‚ñà‚ñé        | 73/545 [03:15<20:51,  2.65s/it] 14%|‚ñà‚ñé        | 74/545 [03:17<20:51,  2.66s/it] 14%|‚ñà‚ñç        | 75/545 [03:20<20:32,  2.62s/it] 14%|‚ñà‚ñç        | 76/545 [03:23<20:25,  2.61s/it] 14%|‚ñà‚ñç        | 77/545 [03:25<20:05,  2.58s/it] 14%|‚ñà‚ñç        | 78/545 [03:28<21:31,  2.76s/it] 14%|‚ñà‚ñç        | 79/545 [03:31<21:29,  2.77s/it] 15%|‚ñà‚ñç        | 80/545 [03:34<21:29,  2.77s/it] 15%|‚ñà‚ñç        | 81/545 [03:36<21:08,  2.73s/it] 15%|‚ñà‚ñå        | 82/545 [03:39<20:24,  2.65s/it] 15%|‚ñà‚ñå        | 83/545 [03:42<20:29,  2.66s/it] 15%|‚ñà‚ñå        | 84/545 [03:44<20:30,  2.67s/it] 16%|‚ñà‚ñå        | 85/545 [03:47<20:18,  2.65s/it] 16%|‚ñà‚ñå        | 86/545 [03:49<19:53,  2.60s/it] 16%|‚ñà‚ñå        | 87/545 [03:52<19:43,  2.58s/it] 16%|‚ñà‚ñå        | 88/545 [03:55<19:56,  2.62s/it] 16%|‚ñà‚ñã        | 89/545 [03:57<20:03,  2.64s/it] 17%|‚ñà‚ñã        | 90/545 [04:00<20:00,  2.64s/it] 17%|‚ñà‚ñã        | 91/545 [04:02<19:33,  2.58s/it] 17%|‚ñà‚ñã        | 92/545 [04:05<19:14,  2.55s/it] 17%|‚ñà‚ñã        | 93/545 [04:07<19:14,  2.55s/it] 17%|‚ñà‚ñã        | 94/545 [04:10<19:15,  2.56s/it] 17%|‚ñà‚ñã        | 95/545 [04:12<18:55,  2.52s/it] 18%|‚ñà‚ñä        | 96/545 [04:15<18:54,  2.53s/it] 18%|‚ñà‚ñä        | 97/545 [04:18<19:05,  2.56s/it] 18%|‚ñà‚ñä        | 98/545 [04:20<19:31,  2.62s/it] 18%|‚ñà‚ñä        | 99/545 [04:23<19:10,  2.58s/it] 18%|‚ñà‚ñä        | 100/545 [04:25<18:57,  2.56s/it] 19%|‚ñà‚ñä        | 101/545 [04:28<18:58,  2.56s/it] 19%|‚ñà‚ñä        | 102/545 [04:31<19:17,  2.61s/it] 19%|‚ñà‚ñâ        | 103/545 [04:33<19:14,  2.61s/it] 19%|‚ñà‚ñâ        | 104/545 [04:36<19:13,  2.62s/it] 19%|‚ñà‚ñâ        | 105/545 [04:39<19:07,  2.61s/it] 19%|‚ñà‚ñâ        | 106/545 [04:41<18:49,  2.57s/it] 20%|‚ñà‚ñâ        | 107/545 [04:44<18:54,  2.59s/it] 20%|‚ñà‚ñâ        | 108/545 [04:55<37:53,  5.20s/it] 20%|‚ñà‚ñà        | 109/545 [04:57<32:00,  4.40s/it] 20%|‚ñà‚ñà        | 110/545 [05:00<27:57,  3.86s/it] 20%|‚ñà‚ñà        | 111/545 [05:03<25:01,  3.46s/it] 21%|‚ñà‚ñà        | 112/545 [05:05<23:30,  3.26s/it] 21%|‚ñà‚ñà        | 113/545 [05:08<21:52,  3.04s/it] 21%|‚ñà‚ñà        | 114/545 [05:10<20:49,  2.90s/it] 21%|‚ñà‚ñà        | 115/545 [05:13<19:53,  2.78s/it] 21%|‚ñà‚ñà‚ñè       | 116/545 [05:16<19:51,  2.78s/it] 21%|‚ñà‚ñà‚ñè       | 117/545 [05:18<19:25,  2.72s/it] 22%|‚ñà‚ñà‚ñè       | 118/545 [05:21<19:07,  2.69s/it] 22%|‚ñà‚ñà‚ñè       | 119/545 [05:24<19:09,  2.70s/it] 22%|‚ñà‚ñà‚ñè       | 120/545 [05:26<19:16,  2.72s/it] 22%|‚ñà‚ñà‚ñè       | 121/545 [05:29<19:04,  2.70s/it] 22%|‚ñà‚ñà‚ñè       | 122/545 [05:32<18:32,  2.63s/it] 23%|‚ñà‚ñà‚ñé       | 123/545 [05:34<18:58,  2.70s/it] 23%|‚ñà‚ñà‚ñé       | 124/545 [05:37<18:51,  2.69s/it] 23%|‚ñà‚ñà‚ñé       | 125/545 [05:40<18:48,  2.69s/it] 23%|‚ñà‚ñà‚ñé       | 126/545 [05:42<18:28,  2.64s/it] 23%|‚ñà‚ñà‚ñé       | 127/545 [05:45<18:18,  2.63s/it] 23%|‚ñà‚ñà‚ñé       | 128/545 [05:47<17:59,  2.59s/it] 24%|‚ñà‚ñà‚ñé       | 129/545 [05:50<18:08,  2.62s/it] 24%|‚ñà‚ñà‚ñç       | 130/545 [05:53<18:27,  2.67s/it] 24%|‚ñà‚ñà‚ñç       | 131/545 [05:56<18:27,  2.68s/it] 24%|‚ñà‚ñà‚ñç       | 132/545 [05:58<18:02,  2.62s/it] 24%|‚ñà‚ñà‚ñç       | 133/545 [06:01<18:08,  2.64s/it] 25%|‚ñà‚ñà‚ñç       | 134/545 [06:03<17:41,  2.58s/it] 25%|‚ñà‚ñà‚ñç       | 135/545 [06:06<17:27,  2.55s/it] 25%|‚ñà‚ñà‚ñç       | 136/545 [06:08<17:28,  2.56s/it] 25%|‚ñà‚ñà‚ñå       | 137/545 [06:11<17:29,  2.57s/it] 25%|‚ñà‚ñà‚ñå       | 138/545 [06:14<17:35,  2.59s/it] 26%|‚ñà‚ñà‚ñå       | 139/545 [06:16<17:48,  2.63s/it] 26%|‚ñà‚ñà‚ñå       | 140/545 [06:19<17:27,  2.59s/it] 26%|‚ñà‚ñà‚ñå       | 141/545 [06:21<17:35,  2.61s/it] 26%|‚ñà‚ñà‚ñå       | 142/545 [06:24<17:33,  2.62s/it] 26%|‚ñà‚ñà‚ñå       | 143/545 [06:27<17:19,  2.59s/it] 26%|‚ñà‚ñà‚ñã       | 144/545 [06:29<17:00,  2.54s/it] 27%|‚ñà‚ñà‚ñã       | 145/545 [06:32<17:01,  2.55s/it] 27%|‚ñà‚ñà‚ñã       | 146/545 [06:34<17:18,  2.60s/it] 27%|‚ñà‚ñà‚ñã       | 147/545 [06:37<17:24,  2.62s/it] 27%|‚ñà‚ñà‚ñã       | 148/545 [06:39<16:59,  2.57s/it] 27%|‚ñà‚ñà‚ñã       | 149/545 [06:42<17:15,  2.62s/it] 28%|‚ñà‚ñà‚ñä       | 150/545 [06:45<17:35,  2.67s/it] 28%|‚ñà‚ñà‚ñä       | 151/545 [06:48<17:38,  2.69s/it] 28%|‚ñà‚ñà‚ñä       | 152/545 [06:50<17:34,  2.68s/it] 28%|‚ñà‚ñà‚ñä       | 153/545 [06:53<17:01,  2.61s/it] 28%|‚ñà‚ñà‚ñä       | 154/545 [06:55<17:11,  2.64s/it] 28%|‚ñà‚ñà‚ñä       | 155/545 [06:58<16:49,  2.59s/it] 29%|‚ñà‚ñà‚ñä       | 156/545 [07:01<16:57,  2.62s/it] 29%|‚ñà‚ñà‚ñâ       | 157/545 [07:03<17:02,  2.63s/it] 29%|‚ñà‚ñà‚ñâ       | 158/545 [07:06<17:04,  2.65s/it] 29%|‚ñà‚ñà‚ñâ       | 159/545 [07:09<16:53,  2.62s/it] 29%|‚ñà‚ñà‚ñâ       | 160/545 [07:11<16:44,  2.61s/it] 30%|‚ñà‚ñà‚ñâ       | 161/545 [07:14<17:00,  2.66s/it] 30%|‚ñà‚ñà‚ñâ       | 162/545 [07:16<16:47,  2.63s/it] 30%|‚ñà‚ñà‚ñâ       | 163/545 [07:19<16:26,  2.58s/it] 30%|‚ñà‚ñà‚ñà       | 164/545 [07:21<16:14,  2.56s/it] 30%|‚ñà‚ñà‚ñà       | 165/545 [07:24<16:15,  2.57s/it] 30%|‚ñà‚ñà‚ñà       | 166/545 [07:26<16:04,  2.54s/it] 31%|‚ñà‚ñà‚ñà       | 167/545 [07:29<15:56,  2.53s/it] 31%|‚ñà‚ñà‚ñà       | 168/545 [07:32<16:30,  2.63s/it] 31%|‚ñà‚ñà‚ñà       | 169/545 [07:34<16:22,  2.61s/it] 31%|‚ñà‚ñà‚ñà       | 170/545 [07:37<16:22,  2.62s/it] 31%|‚ñà‚ñà‚ñà‚ñè      | 171/545 [07:40<16:26,  2.64s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 172/545 [07:43<17:04,  2.75s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 173/545 [07:45<16:43,  2.70s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 174/545 [07:48<16:11,  2.62s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 175/545 [07:50<16:09,  2.62s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 176/545 [07:53<15:58,  2.60s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 177/545 [07:56<16:05,  2.62s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 178/545 [07:58<16:01,  2.62s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 179/545 [08:01<16:00,  2.62s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 180/545 [08:04<16:23,  2.70s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 181/545 [08:06<16:08,  2.66s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 182/545 [08:09<15:57,  2.64s/it] 34%|‚ñà‚ñà‚ñà‚ñé      | 183/545 [08:12<15:54,  2.64s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 184/545 [08:14<15:25,  2.56s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 185/545 [08:16<15:24,  2.57s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 186/545 [08:19<15:12,  2.54s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 187/545 [08:22<15:25,  2.58s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 188/545 [08:24<15:42,  2.64s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 189/545 [08:27<15:17,  2.58s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 190/545 [08:30<15:25,  2.61s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 191/545 [08:32<15:09,  2.57s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 192/545 [08:35<15:04,  2.56s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 193/545 [08:37<15:13,  2.60s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 194/545 [08:40<14:59,  2.56s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 195/545 [08:42<15:03,  2.58s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 196/545 [08:45<14:56,  2.57s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 197/545 [08:47<14:39,  2.53s/it] 36%|‚ñà‚ñà‚ñà‚ñã      | 198/545 [08:50<14:47,  2.56s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 199/545 [08:53<15:06,  2.62s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 200/545 [08:55<14:44,  2.56s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 201/545 [08:58<14:43,  2.57s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 202/545 [09:00<14:26,  2.53s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 203/545 [09:03<14:29,  2.54s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 204/545 [09:05<14:16,  2.51s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 205/545 [09:08<14:25,  2.55s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 206/545 [09:10<14:31,  2.57s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 207/545 [09:13<14:40,  2.60s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 208/545 [09:16<14:34,  2.60s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 209/545 [09:18<14:34,  2.60s/it] 39%|‚ñà‚ñà‚ñà‚ñä      | 210/545 [09:21<14:34,  2.61s/it] 39%|‚ñà‚ñà‚ñà‚ñä      | 211/545 [09:23<14:28,  2.60s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 212/545 [09:26<14:43,  2.65s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 213/545 [09:29<14:40,  2.65s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 214/545 [09:31<14:25,  2.62s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 215/545 [09:34<14:43,  2.68s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 216/545 [09:37<14:08,  2.58s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 217/545 [09:39<14:15,  2.61s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 218/545 [09:42<14:03,  2.58s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 219/545 [09:44<13:35,  2.50s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 220/545 [09:47<13:49,  2.55s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 221/545 [09:49<13:53,  2.57s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 222/545 [09:52<14:03,  2.61s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 223/545 [09:55<14:02,  2.62s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 224/545 [09:57<14:00,  2.62s/it] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 225/545 [10:00<13:40,  2.56s/it] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 226/545 [10:02<13:38,  2.57s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 227/545 [10:05<13:26,  2.54s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 228/545 [10:08<13:35,  2.57s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 229/545 [10:10<13:28,  2.56s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 230/545 [10:12<13:10,  2.51s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 231/545 [10:15<13:19,  2.54s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 232/545 [10:18<13:25,  2.57s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 233/545 [10:20<13:32,  2.60s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 234/545 [10:23<13:27,  2.60s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 235/545 [10:26<13:28,  2.61s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 236/545 [10:28<13:23,  2.60s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 237/545 [10:31<13:19,  2.60s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 238/545 [10:33<13:15,  2.59s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 239/545 [10:36<13:02,  2.56s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 240/545 [10:38<13:02,  2.57s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 241/545 [10:41<12:57,  2.56s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 242/545 [10:44<13:01,  2.58s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 243/545 [10:46<12:46,  2.54s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 244/545 [10:49<13:02,  2.60s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 245/545 [10:51<12:52,  2.58s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 246/545 [10:54<12:59,  2.61s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 247/545 [10:56<12:50,  2.59s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 248/545 [10:59<12:50,  2.59s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 249/545 [11:02<12:36,  2.56s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 250/545 [11:04<12:37,  2.57s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 251/545 [11:07<12:52,  2.63s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 252/545 [11:10<13:04,  2.68s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 253/545 [11:12<12:48,  2.63s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 254/545 [11:15<12:42,  2.62s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 255/545 [11:17<12:25,  2.57s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 256/545 [11:20<12:22,  2.57s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 257/545 [11:23<12:25,  2.59s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 258/545 [11:25<12:26,  2.60s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 259/545 [11:28<12:37,  2.65s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 260/545 [11:31<13:24,  2.82s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 261/545 [11:34<13:08,  2.78s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 262/545 [11:36<12:42,  2.69s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 263/545 [11:39<12:21,  2.63s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 264/545 [11:42<12:35,  2.69s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 265/545 [11:44<12:20,  2.65s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 266/545 [11:47<12:01,  2.58s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 267/545 [11:49<12:02,  2.60s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 268/545 [11:52<12:15,  2.66s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 269/545 [11:55<12:40,  2.76s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 270/545 [11:57<12:12,  2.66s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 271/545 [12:00<12:02,  2.64s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 272/545 [12:03<11:49,  2.60s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 273/545 [12:05<11:47,  2.60s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 274/545 [12:08<12:01,  2.66s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 275/545 [12:11<12:18,  2.73s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 276/545 [12:13<11:50,  2.64s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 277/545 [12:16<12:32,  2.81s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 278/545 [12:19<12:14,  2.75s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 279/545 [12:22<12:12,  2.75s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 280/545 [12:25<12:03,  2.73s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 281/545 [12:27<11:49,  2.69s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 282/545 [12:30<11:30,  2.62s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 283/545 [12:32<11:31,  2.64s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 284/545 [12:35<11:23,  2.62s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 285/545 [12:37<11:14,  2.59s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 286/545 [12:40<11:18,  2.62s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 287/545 [12:43<11:21,  2.64s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 288/545 [12:45<11:07,  2.60s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 289/545 [12:48<11:03,  2.59s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 290/545 [12:51<11:09,  2.63s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 291/545 [12:53<11:03,  2.61s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 292/545 [12:56<10:58,  2.60s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 293/545 [12:58<10:57,  2.61s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 294/545 [13:01<10:59,  2.63s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 295/545 [13:04<10:49,  2.60s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 296/545 [13:06<10:51,  2.62s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 297/545 [13:09<10:35,  2.56s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 298/545 [13:11<10:37,  2.58s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 299/545 [13:14<10:34,  2.58s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 300/545 [13:16<10:32,  2.58s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 301/545 [13:19<10:18,  2.54s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 302/545 [13:21<10:19,  2.55s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 303/545 [13:24<10:18,  2.56s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 304/545 [13:27<10:24,  2.59s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 305/545 [13:29<10:21,  2.59s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 306/545 [13:32<10:28,  2.63s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 307/545 [13:34<10:18,  2.60s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 308/545 [13:37<10:25,  2.64s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 309/545 [13:40<10:11,  2.59s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 310/545 [13:42<10:04,  2.57s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 311/545 [13:45<09:54,  2.54s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 312/545 [13:47<09:44,  2.51s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 313/545 [13:50<09:52,  2.56s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 314/545 [13:52<09:51,  2.56s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 315/545 [13:55<09:53,  2.58s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 316/545 [13:57<09:46,  2.56s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 317/545 [14:00<09:31,  2.51s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 318/545 [14:02<09:20,  2.47s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 319/545 [14:05<09:28,  2.52s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 320/545 [14:07<09:29,  2.53s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 321/545 [14:10<09:30,  2.55s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 322/545 [14:13<09:33,  2.57s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 323/545 [14:15<09:17,  2.51s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 324/545 [14:18<09:13,  2.50s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 325/545 [14:20<09:12,  2.51s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 326/545 [14:23<09:20,  2.56s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 327/545 [14:26<09:44,  2.68s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 328/545 [14:28<09:36,  2.66s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 329/545 [14:31<09:28,  2.63s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 330/545 [14:33<09:23,  2.62s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 331/545 [14:36<09:21,  2.62s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 332/545 [14:39<09:13,  2.60s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 333/545 [14:41<09:23,  2.66s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 334/545 [14:44<09:30,  2.70s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 335/545 [14:47<09:25,  2.69s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 336/545 [14:50<09:18,  2.67s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 337/545 [14:52<09:10,  2.64s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 338/545 [14:55<09:11,  2.67s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 339/545 [14:57<09:00,  2.62s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 340/545 [15:00<08:57,  2.62s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 341/545 [15:03<08:55,  2.63s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 342/545 [15:05<08:50,  2.61s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 343/545 [15:08<08:57,  2.66s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 344/545 [15:10<08:41,  2.59s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 345/545 [15:13<08:44,  2.62s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 346/545 [15:16<08:39,  2.61s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 347/545 [15:18<08:24,  2.55s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 348/545 [15:21<08:26,  2.57s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 349/545 [15:23<08:30,  2.60s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 350/545 [15:26<08:28,  2.61s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 351/545 [15:29<08:35,  2.66s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 352/545 [15:31<08:31,  2.65s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 353/545 [15:34<08:16,  2.59s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 354/545 [15:36<08:13,  2.59s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 355/545 [15:39<08:05,  2.55s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 356/545 [15:42<08:09,  2.59s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 357/545 [15:44<08:14,  2.63s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 358/545 [15:47<08:02,  2.58s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 359/545 [15:50<08:27,  2.73s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 360/545 [15:53<08:40,  2.81s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 361/545 [15:55<08:19,  2.72s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 362/545 [15:58<08:20,  2.74s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 363/545 [16:01<08:09,  2.69s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 364/545 [16:03<08:03,  2.67s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 365/545 [16:06<07:58,  2.66s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 366/545 [16:08<07:48,  2.62s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 367/545 [16:11<07:49,  2.64s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 368/545 [16:14<07:44,  2.63s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 369/545 [16:17<07:54,  2.70s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 370/545 [16:19<07:56,  2.72s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 371/545 [16:22<07:49,  2.70s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 372/545 [16:25<07:40,  2.66s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 373/545 [16:27<07:34,  2.64s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 374/545 [16:30<07:28,  2.63s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 375/545 [16:33<07:33,  2.67s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 376/545 [16:35<07:26,  2.64s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 377/545 [16:38<07:20,  2.62s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 378/545 [16:41<07:39,  2.75s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 379/545 [16:43<07:32,  2.73s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 380/545 [16:46<07:17,  2.65s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 381/545 [16:49<07:18,  2.68s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 382/545 [16:51<07:13,  2.66s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 383/545 [16:54<07:12,  2.67s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 384/545 [16:57<07:06,  2.65s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 385/545 [16:59<07:02,  2.64s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 386/545 [17:02<06:52,  2.59s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 387/545 [17:04<06:49,  2.59s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 388/545 [17:07<06:49,  2.61s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 389/545 [17:10<06:54,  2.66s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 390/545 [17:12<06:46,  2.62s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 391/545 [17:15<06:37,  2.58s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 392/545 [17:18<07:02,  2.76s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 393/545 [17:29<13:28,  5.32s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 394/545 [17:32<11:19,  4.50s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 395/545 [17:34<09:46,  3.91s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 396/545 [17:37<08:44,  3.52s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 397/545 [17:40<08:03,  3.27s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 398/545 [17:42<07:37,  3.11s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 399/545 [17:45<07:19,  3.01s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 400/545 [17:48<06:58,  2.89s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 401/545 [17:50<06:47,  2.83s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 402/545 [17:53<06:33,  2.75s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 403/545 [17:56<06:22,  2.70s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 404/545 [17:58<06:19,  2.69s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 405/545 [18:01<06:11,  2.66s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 406/545 [18:03<06:10,  2.66s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 407/545 [18:06<06:04,  2.64s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 408/545 [18:09<06:02,  2.65s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 409/545 [18:11<05:57,  2.63s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 410/545 [18:14<05:50,  2.60s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 411/545 [18:16<05:45,  2.58s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 412/545 [18:19<05:43,  2.58s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 413/545 [18:22<05:40,  2.58s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 414/545 [18:24<05:41,  2.61s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 415/545 [18:27<05:36,  2.59s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 416/545 [18:29<05:27,  2.54s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 417/545 [18:32<05:29,  2.57s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 418/545 [18:34<05:26,  2.57s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 419/545 [18:37<05:19,  2.53s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 420/545 [18:40<05:25,  2.60s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 421/545 [18:42<05:29,  2.65s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 422/545 [18:45<05:23,  2.63s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 423/545 [18:47<05:17,  2.60s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 424/545 [18:50<05:10,  2.57s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 425/545 [18:53<05:15,  2.63s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 426/545 [18:56<05:18,  2.68s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 427/545 [18:58<05:10,  2.63s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 428/545 [19:01<05:13,  2.68s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 429/545 [19:03<05:05,  2.64s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 430/545 [19:06<05:02,  2.63s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 431/545 [19:08<04:51,  2.56s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 432/545 [19:11<04:48,  2.55s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 433/545 [19:13<04:43,  2.53s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 434/545 [19:16<04:42,  2.54s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 435/545 [19:18<04:36,  2.51s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 436/545 [19:21<04:45,  2.62s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 437/545 [19:24<04:44,  2.64s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 438/545 [19:27<04:38,  2.61s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 439/545 [19:29<04:35,  2.60s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 440/545 [19:32<04:41,  2.68s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 441/545 [19:35<04:37,  2.66s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 442/545 [19:37<04:27,  2.60s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 443/545 [19:40<04:30,  2.65s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 444/545 [19:42<04:27,  2.64s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 445/545 [19:45<04:19,  2.59s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 446/545 [19:47<04:15,  2.58s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 447/545 [19:50<04:15,  2.61s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 448/545 [19:53<04:13,  2.62s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 449/545 [20:04<08:24,  5.26s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 450/545 [20:07<07:03,  4.46s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 451/545 [20:09<06:07,  3.91s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 452/545 [20:12<05:30,  3.55s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 453/545 [20:15<05:01,  3.27s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 454/545 [20:17<04:37,  3.05s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 455/545 [20:20<04:29,  3.00s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 456/545 [20:23<04:12,  2.84s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 457/545 [20:25<04:03,  2.77s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 458/545 [20:28<03:59,  2.75s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 459/545 [20:30<03:48,  2.66s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 460/545 [20:33<03:43,  2.63s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 461/545 [20:35<03:38,  2.60s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 462/545 [20:38<03:35,  2.60s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 463/545 [20:41<03:32,  2.59s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 464/545 [20:43<03:27,  2.56s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 465/545 [20:46<03:27,  2.60s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 466/545 [20:48<03:24,  2.58s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 467/545 [20:51<03:23,  2.61s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 468/545 [20:53<03:15,  2.54s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 469/545 [20:56<03:10,  2.51s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 470/545 [20:58<03:08,  2.52s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 471/545 [21:01<03:09,  2.56s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 472/545 [21:04<03:08,  2.58s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 473/545 [21:06<03:07,  2.60s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 474/545 [21:09<03:07,  2.64s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 475/545 [21:12<03:04,  2.63s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 476/545 [21:14<03:01,  2.63s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 477/545 [21:17<03:01,  2.68s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 478/545 [21:20<02:57,  2.65s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 479/545 [21:22<02:54,  2.64s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 480/545 [21:25<02:49,  2.61s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 481/545 [21:27<02:47,  2.61s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 482/545 [21:30<02:45,  2.63s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 483/545 [21:33<02:43,  2.63s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 484/545 [21:35<02:38,  2.60s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 485/545 [21:38<02:36,  2.61s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 486/545 [21:41<02:34,  2.61s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 487/545 [21:43<02:30,  2.60s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 488/545 [21:46<02:26,  2.57s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 489/545 [21:48<02:22,  2.54s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 490/545 [21:51<02:20,  2.55s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 491/545 [21:53<02:21,  2.62s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 492/545 [21:56<02:16,  2.58s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 493/545 [21:59<02:17,  2.65s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 494/545 [22:01<02:12,  2.60s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 495/545 [22:04<02:07,  2.55s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 496/545 [22:06<02:05,  2.56s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 497/545 [22:09<02:03,  2.56s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 498/545 [22:11<02:00,  2.55s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 499/545 [22:14<01:57,  2.55s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 500/545 [22:25<03:52,  5.18s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 501/545 [22:28<03:13,  4.40s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 502/545 [22:31<02:48,  3.91s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 503/545 [22:33<02:30,  3.59s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 504/545 [22:36<02:17,  3.35s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 505/545 [22:39<02:04,  3.12s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 506/545 [22:41<01:55,  2.96s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 507/545 [22:44<01:47,  2.84s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 508/545 [22:47<01:45,  2.85s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 509/545 [22:49<01:39,  2.75s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 510/545 [22:52<01:32,  2.64s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 511/545 [22:54<01:29,  2.64s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 512/545 [22:57<01:25,  2.59s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 513/545 [22:59<01:23,  2.60s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 514/545 [23:02<01:19,  2.56s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 515/545 [23:04<01:16,  2.55s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 516/545 [23:07<01:15,  2.59s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 517/545 [23:10<01:11,  2.57s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 518/545 [23:12<01:09,  2.59s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 519/545 [23:15<01:07,  2.58s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 520/545 [23:17<01:04,  2.58s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 521/545 [23:20<01:02,  2.61s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 522/545 [23:23<01:00,  2.63s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 523/545 [23:25<00:58,  2.65s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 524/545 [23:28<00:55,  2.63s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 525/545 [23:31<00:52,  2.64s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 526/545 [23:33<00:49,  2.58s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 527/545 [23:36<00:46,  2.58s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 528/545 [23:38<00:44,  2.59s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 529/545 [23:41<00:41,  2.62s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 530/545 [23:44<00:39,  2.63s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 531/545 [23:47<00:37,  2.70s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 532/545 [23:49<00:35,  2.72s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 533/545 [23:52<00:31,  2.64s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 534/545 [23:54<00:28,  2.62s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 535/545 [23:57<00:25,  2.58s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 536/545 [23:59<00:23,  2.56s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 537/545 [24:02<00:20,  2.57s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 538/545 [24:04<00:17,  2.54s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 539/545 [24:07<00:15,  2.52s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 540/545 [24:10<00:12,  2.57s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 541/545 [24:12<00:10,  2.63s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 542/545 [24:15<00:07,  2.63s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 543/545 [24:17<00:05,  2.60s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 544/545 [24:20<00:02,  2.65s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 545/545 [24:22<00:00,  2.41s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 545/545 [24:22<00:00,  2.68s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  1.11it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.67it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.55it/s]
Traceback (most recent call last):
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 184, in <module>
    main(args)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 97, in main
    salmonn_preprocessor = load_preprocessor(cfg)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/salmonn_utils.py", line 17, in load_preprocessor
    salmonn_preprocessor = SALMONN.from_config(cfg.config.model)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/models/salmonn.py", line 487, in from_config
    model = cls(
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/models/salmonn.py", line 132, in __init__
    self.llama_model.resize_token_embeddings(len(self.llama_tokenizer))
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2117, in resize_token_embeddings
    model_embeds = self._resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2142, in _resize_token_embeddings
    new_embeddings = self._get_resized_embeddings(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2266, in _get_resized_embeddings
    new_embeddings = nn.Embedding(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/modules/sparse.py", line 170, in __init__
    self.reset_parameters()
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/modules/sparse.py", line 181, in reset_parameters
    init.normal_(self.weight)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/init.py", line 193, in normal_
    return _no_grad_normal_(tensor, mean, std, generator)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/init.py", line 22, in _no_grad_normal_
    return tensor.normal_(mean, std, generator=generator)
KeyboardInterrupt
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  1.12it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.69it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.57it/s]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
Loading training prompts done!
  0%|          | 0/368 [00:00<?, ?it/s]/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  0%|          | 1/368 [00:05<34:03,  5.57s/it]  1%|          | 2/368 [00:08<26:16,  4.31s/it]  1%|          | 3/368 [00:12<23:48,  3.91s/it]  1%|          | 4/368 [00:16<23:41,  3.91s/it]  1%|‚ñè         | 5/368 [00:20<23:25,  3.87s/it]  2%|‚ñè         | 6/368 [00:24<24:40,  4.09s/it]  2%|‚ñè         | 7/368 [00:27<22:33,  3.75s/it]  2%|‚ñè         | 8/368 [00:30<21:10,  3.53s/it]  2%|‚ñè         | 9/368 [00:33<19:39,  3.28s/it]  3%|‚ñé         | 10/368 [00:36<18:53,  3.17s/it]  3%|‚ñé         | 11/368 [00:38<17:38,  2.97s/it]  3%|‚ñé         | 12/368 [00:42<17:55,  3.02s/it]  4%|‚ñé         | 13/368 [00:45<18:55,  3.20s/it]  4%|‚ñç         | 14/368 [00:48<18:49,  3.19s/it]  4%|‚ñç         | 15/368 [00:51<17:56,  3.05s/it]  4%|‚ñç         | 16/368 [00:54<18:22,  3.13s/it]  5%|‚ñç         | 17/368 [00:58<18:19,  3.13s/it]  5%|‚ñç         | 18/368 [01:00<16:53,  2.89s/it]  5%|‚ñå         | 19/368 [01:03<16:29,  2.83s/it]  5%|‚ñå         | 20/368 [01:05<15:35,  2.69s/it]  6%|‚ñå         | 21/368 [01:07<15:18,  2.65s/it]  6%|‚ñå         | 22/368 [01:11<16:08,  2.80s/it]  6%|‚ñã         | 23/368 [01:14<16:15,  2.83s/it]  7%|‚ñã         | 24/368 [01:16<16:10,  2.82s/it]  7%|‚ñã         | 25/368 [01:21<18:56,  3.31s/it]  7%|‚ñã         | 26/368 [01:24<17:53,  3.14s/it]  7%|‚ñã         | 27/368 [01:27<18:03,  3.18s/it]  8%|‚ñä         | 28/368 [01:31<19:21,  3.42s/it]  8%|‚ñä         | 29/368 [01:35<20:11,  3.57s/it]  8%|‚ñä         | 30/368 [01:41<24:24,  4.33s/it]  8%|‚ñä         | 31/368 [01:45<24:45,  4.41s/it]  9%|‚ñä         | 32/368 [01:50<25:50,  4.61s/it]  9%|‚ñâ         | 33/368 [01:54<24:33,  4.40s/it]  9%|‚ñâ         | 34/368 [01:58<22:59,  4.13s/it] 10%|‚ñâ         | 35/368 [02:00<20:05,  3.62s/it] 10%|‚ñâ         | 36/368 [02:03<18:34,  3.36s/it] 10%|‚ñà         | 37/368 [02:05<16:23,  2.97s/it] 10%|‚ñà         | 38/368 [02:08<16:34,  3.02s/it] 11%|‚ñà         | 39/368 [02:11<15:35,  2.84s/it] 11%|‚ñà         | 40/368 [02:13<14:36,  2.67s/it] 11%|‚ñà         | 41/368 [02:16<15:48,  2.90s/it] 11%|‚ñà‚ñè        | 42/368 [02:20<17:07,  3.15s/it] 12%|‚ñà‚ñè        | 43/368 [02:23<16:51,  3.11s/it] 12%|‚ñà‚ñè        | 44/368 [02:26<15:55,  2.95s/it] 12%|‚ñà‚ñè        | 45/368 [02:29<17:03,  3.17s/it] 12%|‚ñà‚ñé        | 46/368 [02:34<18:46,  3.50s/it] 13%|‚ñà‚ñé        | 47/368 [02:37<19:01,  3.56s/it] 13%|‚ñà‚ñé        | 48/368 [02:41<18:23,  3.45s/it] 13%|‚ñà‚ñé        | 49/368 [02:45<19:09,  3.60s/it] 14%|‚ñà‚ñé        | 50/368 [02:48<18:44,  3.54s/it] 14%|‚ñà‚ñç        | 51/368 [02:52<18:58,  3.59s/it] 14%|‚ñà‚ñç        | 52/368 [02:55<18:19,  3.48s/it] 14%|‚ñà‚ñç        | 53/368 [02:58<17:07,  3.26s/it] 15%|‚ñà‚ñç        | 54/368 [03:01<16:38,  3.18s/it] 15%|‚ñà‚ñç        | 55/368 [03:04<16:37,  3.19s/it] 15%|‚ñà‚ñå        | 56/368 [03:07<16:33,  3.18s/it] 15%|‚ñà‚ñå        | 57/368 [03:11<17:29,  3.38s/it] 16%|‚ñà‚ñå        | 58/368 [03:15<18:37,  3.60s/it] 16%|‚ñà‚ñå        | 59/368 [03:18<18:04,  3.51s/it] 16%|‚ñà‚ñã        | 60/368 [03:22<18:47,  3.66s/it] 17%|‚ñà‚ñã        | 61/368 [03:25<17:51,  3.49s/it] 17%|‚ñà‚ñã        | 62/368 [03:30<19:25,  3.81s/it] 17%|‚ñà‚ñã        | 63/368 [03:34<20:09,  3.96s/it] 17%|‚ñà‚ñã        | 64/368 [03:39<21:07,  4.17s/it] 18%|‚ñà‚ñä        | 65/368 [03:41<18:29,  3.66s/it] 18%|‚ñà‚ñä        | 66/368 [03:44<17:33,  3.49s/it] 18%|‚ñà‚ñä        | 67/368 [03:47<16:51,  3.36s/it] 18%|‚ñà‚ñä        | 68/368 [03:50<15:17,  3.06s/it] 19%|‚ñà‚ñâ        | 69/368 [03:53<15:01,  3.02s/it] 19%|‚ñà‚ñâ        | 70/368 [03:58<17:54,  3.61s/it] 19%|‚ñà‚ñâ        | 71/368 [04:05<23:49,  4.81s/it] 20%|‚ñà‚ñâ        | 72/368 [04:08<21:11,  4.30s/it] 20%|‚ñà‚ñâ        | 73/368 [04:13<21:43,  4.42s/it] 20%|‚ñà‚ñà        | 74/368 [04:17<20:54,  4.27s/it] 20%|‚ñà‚ñà        | 75/368 [04:21<19:39,  4.03s/it] 21%|‚ñà‚ñà        | 76/368 [04:24<19:07,  3.93s/it] 21%|‚ñà‚ñà        | 77/368 [04:29<19:38,  4.05s/it] 21%|‚ñà‚ñà        | 78/368 [04:33<20:26,  4.23s/it] 21%|‚ñà‚ñà‚ñè       | 79/368 [04:37<20:07,  4.18s/it] 22%|‚ñà‚ñà‚ñè       | 80/368 [04:41<20:07,  4.19s/it] 22%|‚ñà‚ñà‚ñè       | 81/368 [04:45<19:22,  4.05s/it] 22%|‚ñà‚ñà‚ñè       | 82/368 [04:48<17:19,  3.63s/it] 23%|‚ñà‚ñà‚ñé       | 83/368 [04:53<18:49,  3.96s/it] 23%|‚ñà‚ñà‚ñé       | 84/368 [04:58<21:22,  4.52s/it] 23%|‚ñà‚ñà‚ñé       | 85/368 [05:04<22:15,  4.72s/it] 23%|‚ñà‚ñà‚ñé       | 86/368 [05:09<23:45,  5.06s/it] 24%|‚ñà‚ñà‚ñé       | 87/368 [05:12<20:21,  4.35s/it] 24%|‚ñà‚ñà‚ñç       | 88/368 [05:15<17:42,  3.79s/it] 24%|‚ñà‚ñà‚ñç       | 89/368 [05:18<17:37,  3.79s/it] 24%|‚ñà‚ñà‚ñç       | 90/368 [05:20<15:09,  3.27s/it] 25%|‚ñà‚ñà‚ñç       | 91/368 [05:23<14:31,  3.15s/it] 25%|‚ñà‚ñà‚ñå       | 92/368 [05:26<14:05,  3.07s/it] 25%|‚ñà‚ñà‚ñå       | 93/368 [05:29<13:44,  3.00s/it] 26%|‚ñà‚ñà‚ñå       | 94/368 [05:33<15:07,  3.31s/it] 26%|‚ñà‚ñà‚ñå       | 95/368 [05:36<14:22,  3.16s/it] 26%|‚ñà‚ñà‚ñå       | 96/368 [05:40<15:04,  3.33s/it] 26%|‚ñà‚ñà‚ñã       | 97/368 [05:43<15:36,  3.45s/it] 27%|‚ñà‚ñà‚ñã       | 98/368 [05:48<16:33,  3.68s/it] 27%|‚ñà‚ñà‚ñã       | 99/368 [05:51<15:38,  3.49s/it] 27%|‚ñà‚ñà‚ñã       | 100/368 [05:53<14:18,  3.20s/it] 27%|‚ñà‚ñà‚ñã       | 101/368 [05:56<13:11,  2.97s/it] 28%|‚ñà‚ñà‚ñä       | 102/368 [05:58<12:53,  2.91s/it] 28%|‚ñà‚ñà‚ñä       | 103/368 [06:01<12:01,  2.72s/it] 28%|‚ñà‚ñà‚ñä       | 104/368 [06:04<13:07,  2.98s/it] 29%|‚ñà‚ñà‚ñä       | 105/368 [06:08<13:30,  3.08s/it] 29%|‚ñà‚ñà‚ñâ       | 106/368 [06:14<17:21,  3.98s/it] 29%|‚ñà‚ñà‚ñâ       | 107/368 [06:18<17:49,  4.10s/it] 29%|‚ñà‚ñà‚ñâ       | 108/368 [06:21<16:28,  3.80s/it] 30%|‚ñà‚ñà‚ñâ       | 109/368 [06:26<17:29,  4.05s/it] 30%|‚ñà‚ñà‚ñâ       | 110/368 [06:29<16:51,  3.92s/it] 30%|‚ñà‚ñà‚ñà       | 111/368 [06:33<16:14,  3.79s/it] 30%|‚ñà‚ñà‚ñà       | 112/368 [06:36<15:28,  3.63s/it] 31%|‚ñà‚ñà‚ñà       | 113/368 [06:39<14:19,  3.37s/it] 31%|‚ñà‚ñà‚ñà       | 114/368 [06:42<14:07,  3.34s/it] 31%|‚ñà‚ñà‚ñà‚ñè      | 115/368 [06:46<14:40,  3.48s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 116/368 [06:50<15:37,  3.72s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 117/368 [06:53<14:50,  3.55s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 118/368 [06:56<14:05,  3.38s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 119/368 [07:01<15:57,  3.84s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 120/368 [07:05<16:17,  3.94s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 121/368 [07:12<19:09,  4.65s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 122/368 [07:17<19:29,  4.76s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 123/368 [07:19<16:36,  4.07s/it] 34%|‚ñà‚ñà‚ñà‚ñé      | 124/368 [07:22<15:32,  3.82s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 125/368 [07:25<14:16,  3.53s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 126/368 [07:29<14:15,  3.53s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 127/368 [07:31<12:57,  3.22s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 128/368 [07:36<14:22,  3.59s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 129/368 [07:39<13:30,  3.39s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 130/368 [07:42<13:37,  3.43s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 131/368 [07:47<15:25,  3.91s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 132/368 [07:50<13:56,  3.54s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 133/368 [07:52<12:29,  3.19s/it] 36%|‚ñà‚ñà‚ñà‚ñã      | 134/368 [07:55<12:09,  3.12s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 135/368 [07:58<11:58,  3.08s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 136/368 [08:01<11:41,  3.02s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 137/368 [08:06<13:29,  3.50s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 138/368 [08:10<14:37,  3.82s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 139/368 [08:15<15:26,  4.05s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 140/368 [08:18<14:04,  3.70s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 141/368 [08:22<14:13,  3.76s/it] 39%|‚ñà‚ñà‚ñà‚ñä      | 142/368 [08:27<15:28,  4.11s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 143/368 [08:30<14:56,  3.99s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 144/368 [08:34<14:30,  3.89s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 145/368 [08:37<13:26,  3.62s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 146/368 [08:41<13:56,  3.77s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 147/368 [08:44<12:41,  3.44s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 148/368 [08:47<12:53,  3.51s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 149/368 [08:50<11:20,  3.11s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 150/368 [08:52<10:41,  2.94s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 151/368 [08:55<10:39,  2.95s/it] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 152/368 [08:58<10:11,  2.83s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 153/368 [09:01<11:06,  3.10s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 154/368 [09:06<12:17,  3.45s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 155/368 [09:09<12:05,  3.41s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 156/368 [09:12<12:09,  3.44s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 157/368 [09:16<12:01,  3.42s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 158/368 [09:19<11:50,  3.38s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 159/368 [09:23<12:15,  3.52s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 160/368 [09:28<13:14,  3.82s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 161/368 [09:39<20:54,  6.06s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 162/368 [09:42<17:59,  5.24s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 163/368 [09:45<15:53,  4.65s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 164/368 [09:49<14:51,  4.37s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 165/368 [09:54<15:27,  4.57s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 166/368 [09:58<14:12,  4.22s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 167/368 [10:00<12:22,  3.69s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 168/368 [10:02<10:52,  3.26s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 169/368 [10:06<11:04,  3.34s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 170/368 [10:08<10:15,  3.11s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 171/368 [10:11<09:42,  2.96s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 172/368 [10:14<09:37,  2.95s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 173/368 [10:17<09:49,  3.02s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 174/368 [10:22<11:08,  3.45s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 175/368 [10:26<12:02,  3.74s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 176/368 [10:31<13:11,  4.12s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 177/368 [10:35<12:48,  4.02s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 178/368 [10:39<12:32,  3.96s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 179/368 [10:42<11:51,  3.76s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 180/368 [10:46<12:20,  3.94s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 181/368 [10:49<11:01,  3.54s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 182/368 [10:53<11:18,  3.65s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 183/368 [10:57<11:33,  3.75s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 184/368 [11:00<11:21,  3.71s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 185/368 [11:04<10:57,  3.59s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 186/368 [11:07<10:55,  3.60s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 187/368 [11:10<10:11,  3.38s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 188/368 [11:13<09:30,  3.17s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 189/368 [11:17<10:00,  3.35s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 190/368 [11:20<10:03,  3.39s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 191/368 [11:24<10:42,  3.63s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 192/368 [11:27<10:07,  3.45s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 193/368 [11:30<09:31,  3.27s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 194/368 [11:33<09:05,  3.14s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 195/368 [11:36<08:47,  3.05s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 196/368 [11:39<08:31,  2.97s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 197/368 [11:42<09:09,  3.22s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 198/368 [11:45<08:34,  3.03s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 199/368 [11:48<08:52,  3.15s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 200/368 [11:51<08:44,  3.12s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 201/368 [11:56<09:31,  3.42s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 202/368 [11:58<08:48,  3.18s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 203/368 [12:02<09:15,  3.37s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 204/368 [12:06<09:53,  3.62s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 205/368 [12:09<08:56,  3.29s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 206/368 [12:13<09:54,  3.67s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 207/368 [12:16<09:02,  3.37s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 208/368 [12:20<09:45,  3.66s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 209/368 [12:25<10:48,  4.08s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 210/368 [12:30<11:04,  4.20s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 211/368 [12:36<12:37,  4.82s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 212/368 [12:40<12:00,  4.62s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 213/368 [12:45<11:50,  4.58s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 214/368 [12:48<10:32,  4.11s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 215/368 [12:53<10:59,  4.31s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 216/368 [12:55<09:46,  3.86s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 217/368 [13:00<10:10,  4.04s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 218/368 [13:04<09:51,  3.94s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 219/368 [13:08<10:25,  4.20s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 220/368 [13:13<10:57,  4.44s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 221/368 [13:17<10:23,  4.24s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 222/368 [13:21<09:42,  3.99s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 223/368 [13:25<09:42,  4.02s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 224/368 [13:30<10:41,  4.45s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 225/368 [13:34<10:31,  4.42s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 226/368 [13:40<11:30,  4.86s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 227/368 [13:44<10:32,  4.48s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 228/368 [13:47<09:25,  4.04s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 229/368 [13:58<14:17,  6.17s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 230/368 [14:01<12:07,  5.27s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 231/368 [14:04<10:08,  4.44s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 232/368 [14:15<14:34,  6.43s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 233/368 [14:18<12:01,  5.35s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 234/368 [14:21<10:41,  4.79s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 235/368 [14:24<09:21,  4.22s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 236/368 [14:27<08:11,  3.72s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 237/368 [14:29<07:23,  3.39s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 238/368 [14:32<07:00,  3.23s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 239/368 [14:36<07:17,  3.39s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 240/368 [14:38<06:40,  3.13s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 241/368 [14:50<11:51,  5.60s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 242/368 [14:53<10:11,  4.85s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 243/368 [14:59<11:11,  5.37s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 244/368 [15:03<10:08,  4.91s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 245/368 [15:07<09:27,  4.61s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 246/368 [15:10<08:20,  4.10s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 247/368 [15:14<08:06,  4.02s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 248/368 [15:20<09:27,  4.73s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 249/368 [15:23<08:11,  4.13s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 250/368 [15:27<08:05,  4.11s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 251/368 [15:32<08:25,  4.32s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 252/368 [15:37<08:33,  4.43s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 253/368 [15:40<08:01,  4.19s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 254/368 [15:44<07:39,  4.03s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 255/368 [15:48<07:25,  3.94s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 256/368 [15:52<07:22,  3.95s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 257/368 [15:54<06:22,  3.44s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 258/368 [15:57<06:07,  3.34s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 259/368 [16:00<05:59,  3.29s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 260/368 [16:03<05:56,  3.30s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 261/368 [16:07<05:49,  3.27s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 262/368 [16:09<05:17,  3.00s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 263/368 [16:13<05:42,  3.26s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 264/368 [16:16<05:26,  3.14s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 265/368 [16:19<05:22,  3.13s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 266/368 [16:22<05:15,  3.09s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 267/368 [16:26<05:37,  3.35s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 268/368 [16:28<05:10,  3.11s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 269/368 [16:31<05:00,  3.04s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 270/368 [16:34<04:59,  3.06s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 271/368 [16:37<04:37,  2.87s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 272/368 [16:40<04:42,  2.94s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 273/368 [16:43<04:41,  2.96s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 274/368 [16:54<08:28,  5.41s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 275/368 [16:59<08:01,  5.18s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 276/368 [17:03<07:47,  5.08s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 277/368 [17:09<08:04,  5.32s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 278/368 [17:14<07:53,  5.27s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 279/368 [17:18<07:08,  4.81s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 280/368 [17:25<08:00,  5.46s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 281/368 [17:29<07:11,  4.96s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 282/368 [17:32<06:23,  4.46s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 283/368 [17:43<09:07,  6.44s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 284/368 [17:46<07:21,  5.26s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 285/368 [17:49<06:36,  4.78s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 286/368 [17:55<06:41,  4.89s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 287/368 [17:59<06:15,  4.64s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 288/368 [18:02<05:29,  4.12s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 289/368 [18:05<05:09,  3.92s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 290/368 [18:09<04:59,  3.84s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 291/368 [18:11<04:31,  3.52s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 292/368 [18:16<04:44,  3.75s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 293/368 [18:19<04:29,  3.60s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 294/368 [18:23<04:41,  3.81s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 295/368 [18:28<05:03,  4.16s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 296/368 [18:31<04:18,  3.59s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 297/368 [18:35<04:34,  3.86s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 298/368 [18:37<04:00,  3.43s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 299/368 [18:40<03:46,  3.28s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 300/368 [18:44<03:41,  3.26s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 301/368 [18:46<03:27,  3.10s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 302/368 [18:49<03:06,  2.83s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 303/368 [18:51<03:05,  2.85s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 304/368 [18:54<03:02,  2.85s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 305/368 [18:57<02:57,  2.82s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 306/368 [19:01<03:22,  3.27s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 307/368 [19:04<03:06,  3.06s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 308/368 [19:08<03:19,  3.33s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 309/368 [19:11<03:17,  3.35s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 310/368 [19:15<03:12,  3.32s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 311/368 [19:18<03:06,  3.28s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 312/368 [19:21<03:09,  3.38s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 313/368 [19:24<03:00,  3.28s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 314/368 [19:27<02:47,  3.11s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 315/368 [19:32<03:09,  3.58s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 316/368 [19:35<03:00,  3.48s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 317/368 [19:38<02:51,  3.36s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 318/368 [19:41<02:46,  3.34s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 319/368 [19:46<02:56,  3.61s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 320/368 [19:48<02:42,  3.39s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 321/368 [19:52<02:37,  3.35s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 322/368 [19:55<02:34,  3.35s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 323/368 [19:58<02:18,  3.08s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 324/368 [20:00<02:06,  2.88s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 325/368 [20:02<01:59,  2.77s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 326/368 [20:05<01:49,  2.62s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 327/368 [20:07<01:47,  2.61s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 328/368 [20:11<01:51,  2.79s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 329/368 [20:13<01:49,  2.82s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 330/368 [20:16<01:43,  2.72s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 331/368 [20:20<02:00,  3.24s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 332/368 [20:31<03:21,  5.58s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 333/368 [20:35<02:52,  4.94s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 334/368 [20:37<02:24,  4.24s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 335/368 [20:40<02:07,  3.87s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 336/368 [20:44<01:55,  3.62s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 337/368 [20:47<01:46,  3.45s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 338/368 [20:50<01:45,  3.52s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 339/368 [20:53<01:35,  3.29s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 340/368 [20:55<01:25,  3.04s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 341/368 [20:58<01:19,  2.93s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 342/368 [21:01<01:15,  2.89s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 343/368 [21:04<01:10,  2.82s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 344/368 [21:06<01:03,  2.67s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 345/368 [21:09<01:01,  2.67s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 346/368 [21:12<01:03,  2.86s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 347/368 [21:14<00:57,  2.75s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 348/368 [21:17<00:53,  2.68s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 349/368 [21:20<00:56,  2.95s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 350/368 [21:24<00:55,  3.06s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 351/368 [21:28<00:55,  3.29s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 352/368 [21:31<00:52,  3.29s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 353/368 [21:35<00:51,  3.43s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 354/368 [21:39<00:50,  3.61s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 355/368 [21:43<00:49,  3.78s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 356/368 [21:46<00:42,  3.56s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 357/368 [21:50<00:40,  3.65s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 358/368 [21:54<00:37,  3.70s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 359/368 [21:58<00:34,  3.88s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 360/368 [22:02<00:30,  3.86s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 361/368 [22:07<00:30,  4.34s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 362/368 [22:11<00:24,  4.13s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 363/368 [22:14<00:19,  3.90s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 364/368 [22:18<00:15,  3.82s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 365/368 [22:22<00:11,  3.79s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 366/368 [22:26<00:08,  4.09s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 367/368 [22:30<00:04,  4.07s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 368/368 [22:33<00:00,  3.53s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 368/368 [22:33<00:00,  3.68s/it]
| distributed init (rank 1, world 2): env://
| distributed init (rank 0, world 2): env://
[rank1]:[W118 23:38:40.341331972 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W118 23:38:40.341827008 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: kihoon090. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/wandb/run-20250118_233842-qklhlv91
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama-3.2-1B-1percent-stage1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kihoon090/audio_lm
wandb: üöÄ View run at https://wandb.ai/kihoon090/audio_lm/runs/qklhlv91
2025-01-18 23:38:42,886 [INFO] 
=====  Running Parameters    =====
2025-01-18 23:38:42,887 [INFO] {
    "accum_grad_iters": 1,
    "amp": true,
    "batch_size_eval": 8,
    "batch_size_train": 8,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "epoch_based": false,
    "evaluate": false,
    "exp_name": "llama-3.2-1B-1percent-stage1",
    "gpu": 0,
    "iters_per_epoch": 3000,
    "log_freq": 5,
    "num_workers": 8,
    "optims": {
        "beta2": 0.999,
        "init_lr": 3e-05,
        "max_epoch": 30,
        "min_lr": 1e-05,
        "warmup_start_lr": 1e-06,
        "warmup_steps": 3000,
        "weight_decay": 0.05
    },
    "output_dir": "outputs_stage1_only",
    "rank": 0,
    "seed": 42,
    "use_distributed": true,
    "world_size": 2
}
2025-01-18 23:38:42,887 [INFO] 
======  Dataset Attributes  ======
2025-01-18 23:38:42,888 [INFO] {
    "prefix": "/data/data",
    "test_ann_path": "/data/data/stage1_test.json",
    "train_ann_path": "/data/data/stage1_train.json",
    "valid_ann_path": "/data/data/stage1_valid.json",
    "whisper_path": "/data/ckp/whisper"
}
2025-01-18 23:38:42,888 [INFO] 
======  Model Attributes  ======
2025-01-18 23:38:42,888 [INFO] {
    "beats_path": "/data/ckp/beats/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt",
    "ckpt": "/data/ckp/salmonn/salmonn_3b_nota.pth",
    "end_sym": "</s>",
    "freeze_beats": true,
    "freeze_speech_QFormer": false,
    "freeze_speech_llama_proj": false,
    "freeze_whisper": true,
    "llama_path": "/data/ckp/llama",
    "lora": true,
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "lora_rank": 8,
    "max_txt_len": 300,
    "multi_prompt": true,
    "num_speech_query_token": 1,
    "prompt_path": "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/data/prompts/train_prompt.json",
    "prompt_template": "USER: {}\nASSISTANT:",
    "second_per_window": 0.333333,
    "second_stride": 0.333333,
    "speech_llama_proj_model": "",
    "test_prompt_path": "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/data/prompts/test_prompt.json",
    "token": "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",
    "use_speech_Qformer": true,
    "whisper_path": "/data/ckp/whisper",
    "window_level_Qformer": true
}
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  3.57it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.79it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.76it/s]
2025-01-18 23:38:45,164 [INFO] Loading LLaMA Tokenizer
2025-01-18 23:38:45,739 [INFO] Loading LLaMA Model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.77s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.05s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.46s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading training prompts done!
2025-01-18 23:40:24,262 [INFO] Loading LLaMA Done
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
2025-01-18 23:40:29,799 [INFO] LoRA Training
2025-01-18 23:40:29,799 [INFO] Loading Whisper Model
2025-01-18 23:40:30,531 [INFO] freeze Whisper
2025-01-18 23:40:30,531 [INFO] Loading BEATs Model
2025-01-18 23:40:30,835 [INFO] BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 0.6, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': True, 'predictor_dropout': 0.0, 'predictor_class': 527}
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
2025-01-18 23:40:33,589 [INFO] freeze BEATs
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
2025-01-18 23:40:35,232 [INFO] Loading speech LLAMA proj
Loading training prompts done!
2025-01-18 23:40:35,250 [INFO] Load SALMONN ckpt from: /data/ckp/salmonn/salmonn_3b_nota.pth
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
module.speech_query_tokens
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
module.ln_speech.weight
module.ln_speech.bias
module.ln_audio.weight
module.ln_audio.bias
module.speech_Qformer.bert.embeddings.LayerNorm.weight
module.speech_Qformer.bert.embeddings.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias
module.speech_llama_proj.weight
module.speech_llama_proj.bias
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
module.speech_query_tokens
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
module.ln_speech.weight
module.ln_speech.bias
module.ln_audio.weight
module.ln_audio.bias
module.speech_Qformer.bert.embeddings.LayerNorm.weight
module.speech_Qformer.bert.embeddings.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias
module.speech_llama_proj.weight
module.speech_llama_proj.bias
2025-01-18 23:40:39,366 [INFO] number of trainable parameters: 27498240
2025-01-18 23:40:39,369 [INFO] Training Phase
2025-01-18 23:40:39,379 [INFO] Start training epoch 0, 3000 iters per inner epoch.
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.use_amp):
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.use_amp):
Train: data epoch: [0]  [   0/3000]  eta: 2:40:31  lr: 0.000001  loss: 1.3924  time: 3.2103  data: 0.0001  max mem: 16781
Train: data epoch: [0]  [   0/3000]  eta: 2:32:45  lr: 0.000001  loss: 3.3616  time: 3.0552  data: 0.0001  max mem: 15808
Train: data epoch: [0]  [   5/3000]  eta: 1:32:06  lr: 0.000001  loss: 2.8345  time: 1.8452  data: 0.0000  max mem: 16781
Train: data epoch: [0]  [   5/3000]  eta: 1:30:44  lr: 0.000001  loss: 2.9820  time: 1.8178  data: 0.0000  max mem: 16613
Train: data epoch: [0]  [  10/3000]  eta: 1:25:41  lr: 0.000001  loss: 3.0064  time: 1.7195  data: 0.0000  max mem: 16790
Train: data epoch: [0]  [  10/3000]  eta: 1:24:56  lr: 0.000001  loss: 3.3952  time: 1.7044  data: 0.0000  max mem: 16613
Train: data epoch: [0]  [  15/3000]  eta: 1:21:50  lr: 0.000001  loss: 3.1221  time: 1.6452  data: 0.0000  max mem: 16790
Train: data epoch: [0]  [  15/3000]  eta: 1:21:19  lr: 0.000001  loss: 2.0444  time: 1.6348  data: 0.0000  max mem: 16791
Train: data epoch: [0]  [  20/3000]  eta: 1:20:40  lr: 0.000001  loss: 2.3513  time: 1.5449  data: 0.0000  max mem: 16790
Train: data epoch: [0]  [  20/3000]  eta: 1:20:16  lr: 0.000001  loss: 4.2007  time: 1.5442  data: 0.0000  max mem: 16791
Train: data epoch: [0]  [  25/3000]  eta: 1:19:34  lr: 0.000001  loss: 2.5995  time: 1.5329  data: 0.0000  max mem: 16790
Train: data epoch: [0]  [  25/3000]  eta: 1:19:15  lr: 0.000001  loss: 2.6030  time: 1.5326  data: 0.0000  max mem: 17000
Train: data epoch: [0]  [  30/3000]  eta: 1:18:41  lr: 0.000001  loss: 1.5973  time: 1.5184  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  30/3000]  eta: 1:18:25  lr: 0.000001  loss: 3.4828  time: 1.5182  data: 0.0000  max mem: 17000
Train: data epoch: [0]  [  35/3000]  eta: 1:17:50  lr: 0.000001  loss: 3.4264  time: 1.5194  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  35/3000]  eta: 1:17:36  lr: 0.000001  loss: 1.9685  time: 1.5191  data: 0.0000  max mem: 17000
Train: data epoch: [0]  [  40/3000]  eta: 1:16:59  lr: 0.000001  loss: 1.7893  time: 1.4942  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  40/3000]  eta: 1:16:47  lr: 0.000001  loss: 2.0397  time: 1.4939  data: 0.0000  max mem: 17000
Train: data epoch: [0]  [  45/3000]  eta: 1:16:38  lr: 0.000001  loss: 2.6357  time: 1.4928  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  45/3000]  eta: 1:16:27  lr: 0.000001  loss: 1.8987  time: 1.4926  data: 0.0000  max mem: 17034
Train: data epoch: [0]  [  50/3000]  eta: 1:16:11  lr: 0.000001  loss: 2.9415  time: 1.4878  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  50/3000]  eta: 1:16:01  lr: 0.000001  loss: 1.6359  time: 1.4876  data: 0.0000  max mem: 17034
Train: data epoch: [0]  [  55/3000]  eta: 1:15:57  lr: 0.000002  loss: 1.8908  time: 1.4976  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  55/3000]  eta: 1:15:48  lr: 0.000002  loss: 1.3335  time: 1.4974  data: 0.0000  max mem: 17034
Train: data epoch: [0]  [  60/3000]  eta: 1:15:54  lr: 0.000002  loss: 1.3897  time: 1.5254  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  60/3000]  eta: 1:15:46  lr: 0.000002  loss: 1.8163  time: 1.5252  data: 0.0000  max mem: 17034
Train: data epoch: [0]  [  65/3000]  eta: 1:15:29  lr: 0.000002  loss: 2.5543  time: 1.5138  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  65/3000]  eta: 1:15:21  lr: 0.000002  loss: 1.5383  time: 1.5135  data: 0.0000  max mem: 17034
Train: data epoch: [0]  [  70/3000]  eta: 1:15:14  lr: 0.000002  loss: 1.6545  time: 1.5182  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  70/3000]  eta: 1:15:07  lr: 0.000002  loss: 1.1934  time: 1.5180  data: 0.0000  max mem: 17034
Train: data epoch: [0]  [  75/3000]  eta: 1:14:57  lr: 0.000002  loss: 2.0117  time: 1.5103  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  75/3000]  eta: 1:14:50  lr: 0.000002  loss: 1.8426  time: 1.5100  data: 0.0000  max mem: 17034
Train: data epoch: [0]  [  80/3000]  eta: 1:14:38  lr: 0.000002  loss: 1.2046  time: 1.4871  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  80/3000]  eta: 1:14:32  lr: 0.000002  loss: 1.5056  time: 1.4868  data: 0.0000  max mem: 17034
Train: data epoch: [0]  [  85/3000]  eta: 1:14:30  lr: 0.000002  loss: 1.2050  time: 1.5015  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  85/3000]  eta: 1:14:24  lr: 0.000002  loss: 0.9667  time: 1.5013  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [  90/3000]  eta: 1:14:21  lr: 0.000002  loss: 1.9766  time: 1.5057  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  90/3000]  eta: 1:14:15  lr: 0.000002  loss: 1.0133  time: 1.5054  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [  95/3000]  eta: 1:14:13  lr: 0.000002  loss: 0.9432  time: 1.5155  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [  95/3000]  eta: 1:14:08  lr: 0.000002  loss: 1.2543  time: 1.5153  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 100/3000]  eta: 1:13:55  lr: 0.000002  loss: 2.5414  time: 1.5114  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 100/3000]  eta: 1:13:49  lr: 0.000002  loss: 0.8760  time: 1.5112  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 105/3000]  eta: 1:13:48  lr: 0.000002  loss: 1.5359  time: 1.5126  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 105/3000]  eta: 1:13:43  lr: 0.000002  loss: 1.4534  time: 1.5123  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 110/3000]  eta: 1:13:40  lr: 0.000002  loss: 1.3946  time: 1.5141  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 110/3000]  eta: 1:13:35  lr: 0.000002  loss: 0.9544  time: 1.5139  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 115/3000]  eta: 1:13:33  lr: 0.000002  loss: 0.5629  time: 1.5145  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 115/3000]  eta: 1:13:28  lr: 0.000002  loss: 1.1214  time: 1.5142  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 120/3000]  eta: 1:13:17  lr: 0.000002  loss: 0.4980  time: 1.5149  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 120/3000]  eta: 1:13:13  lr: 0.000002  loss: 1.5110  time: 1.5147  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 125/3000]  eta: 1:13:11  lr: 0.000002  loss: 1.1339  time: 1.5157  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 125/3000]  eta: 1:13:06  lr: 0.000002  loss: 0.6049  time: 1.5154  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 130/3000]  eta: 1:13:00  lr: 0.000002  loss: 0.2990  time: 1.5077  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 130/3000]  eta: 1:12:56  lr: 0.000002  loss: 0.5359  time: 1.5075  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 135/3000]  eta: 1:12:42  lr: 0.000002  loss: 0.4985  time: 1.4818  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 135/3000]  eta: 1:12:38  lr: 0.000002  loss: 0.9980  time: 1.4816  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 140/3000]  eta: 1:12:32  lr: 0.000002  loss: 0.8984  time: 1.4905  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 140/3000]  eta: 1:12:28  lr: 0.000002  loss: 0.5806  time: 1.4902  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 145/3000]  eta: 1:12:26  lr: 0.000002  loss: 0.4123  time: 1.4909  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 145/3000]  eta: 1:12:22  lr: 0.000002  loss: 0.3905  time: 1.4906  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 150/3000]  eta: 1:12:20  lr: 0.000002  loss: 0.6821  time: 1.5009  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 150/3000]  eta: 1:12:16  lr: 0.000002  loss: 0.1259  time: 1.5005  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 155/3000]  eta: 1:12:09  lr: 0.000002  loss: 0.7748  time: 1.5154  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 155/3000]  eta: 1:12:06  lr: 0.000002  loss: 1.0156  time: 1.5150  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 160/3000]  eta: 1:12:01  lr: 0.000003  loss: 0.4782  time: 1.5216  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 160/3000]  eta: 1:11:58  lr: 0.000003  loss: 0.4178  time: 1.5213  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 165/3000]  eta: 1:11:53  lr: 0.000003  loss: 0.8879  time: 1.5164  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 165/3000]  eta: 1:11:50  lr: 0.000003  loss: 1.0008  time: 1.5161  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 170/3000]  eta: 1:11:47  lr: 0.000003  loss: 0.6152  time: 1.5153  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 170/3000]  eta: 1:11:44  lr: 0.000003  loss: 0.3021  time: 1.5151  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 175/3000]  eta: 1:11:37  lr: 0.000003  loss: 0.3479  time: 1.5157  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 175/3000]  eta: 1:11:33  lr: 0.000003  loss: 0.2904  time: 1.5154  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 180/3000]  eta: 1:11:34  lr: 0.000003  loss: 0.1945  time: 1.5314  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 180/3000]  eta: 1:11:31  lr: 0.000003  loss: 0.3137  time: 1.5312  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 185/3000]  eta: 1:11:28  lr: 0.000003  loss: 1.1723  time: 1.5367  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 185/3000]  eta: 1:11:24  lr: 0.000003  loss: 0.5863  time: 1.5364  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 190/3000]  eta: 1:11:17  lr: 0.000003  loss: 0.4681  time: 1.5248  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 190/3000]  eta: 1:11:14  lr: 0.000003  loss: 0.4534  time: 1.5246  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 195/3000]  eta: 1:11:07  lr: 0.000003  loss: 0.2796  time: 1.5229  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 195/3000]  eta: 1:11:04  lr: 0.000003  loss: 0.3065  time: 1.5227  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 200/3000]  eta: 1:11:05  lr: 0.000003  loss: 0.4317  time: 1.5270  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 200/3000]  eta: 1:11:02  lr: 0.000003  loss: 1.1320  time: 1.5268  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 205/3000]  eta: 1:10:59  lr: 0.000003  loss: 0.6490  time: 1.5304  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 205/3000]  eta: 1:10:56  lr: 0.000003  loss: 0.8640  time: 1.5301  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 210/3000]  eta: 1:10:54  lr: 0.000003  loss: 1.0248  time: 1.5475  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 210/3000]  eta: 1:10:51  lr: 0.000003  loss: 0.3898  time: 1.5473  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 215/3000]  eta: 1:10:46  lr: 0.000003  loss: 1.1372  time: 1.5565  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 215/3000]  eta: 1:10:43  lr: 0.000003  loss: 0.3632  time: 1.5563  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 220/3000]  eta: 1:10:40  lr: 0.000003  loss: 0.2887  time: 1.5449  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 220/3000]  eta: 1:10:37  lr: 0.000003  loss: 1.2111  time: 1.5447  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 225/3000]  eta: 1:10:32  lr: 0.000003  loss: 0.7088  time: 1.5383  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 225/3000]  eta: 1:10:29  lr: 0.000003  loss: 0.2835  time: 1.5381  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 230/3000]  eta: 1:10:28  lr: 0.000003  loss: 0.8570  time: 1.5460  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 230/3000]  eta: 1:10:26  lr: 0.000003  loss: 0.4284  time: 1.5458  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 235/3000]  eta: 1:10:20  lr: 0.000003  loss: 0.2632  time: 1.5474  data: 0.0000  max mem: 17399
Train: data epoch: [0]  [ 235/3000]  eta: 1:10:18  lr: 0.000003  loss: 1.1649  time: 1.5473  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 240/3000]  eta: 1:10:13  lr: 0.000003  loss: 0.4158  time: 1.5432  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 240/3000]  eta: 1:10:11  lr: 0.000003  loss: 0.7660  time: 1.5429  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 245/3000]  eta: 1:10:03  lr: 0.000003  loss: 0.9587  time: 1.5304  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 245/3000]  eta: 1:10:00  lr: 0.000003  loss: 0.1702  time: 1.5301  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 250/3000]  eta: 1:09:55  lr: 0.000003  loss: 0.9075  time: 1.5162  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 250/3000]  eta: 1:09:53  lr: 0.000003  loss: 0.9423  time: 1.5154  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 255/3000]  eta: 1:09:48  lr: 0.000003  loss: 0.5562  time: 1.5158  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 255/3000]  eta: 1:09:45  lr: 0.000003  loss: 0.6770  time: 1.5150  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 260/3000]  eta: 1:09:40  lr: 0.000004  loss: 0.4278  time: 1.5134  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 260/3000]  eta: 1:09:37  lr: 0.000004  loss: 0.4884  time: 1.5124  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 265/3000]  eta: 1:09:27  lr: 0.000004  loss: 0.0710  time: 1.5009  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 265/3000]  eta: 1:09:25  lr: 0.000004  loss: 0.4268  time: 1.4999  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 270/3000]  eta: 1:09:22  lr: 0.000004  loss: 0.3309  time: 1.5093  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 270/3000]  eta: 1:09:19  lr: 0.000004  loss: 0.5748  time: 1.5090  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 275/3000]  eta: 1:09:13  lr: 0.000004  loss: 0.3335  time: 1.5046  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 275/3000]  eta: 1:09:10  lr: 0.000004  loss: 0.4633  time: 1.5042  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 280/3000]  eta: 1:09:04  lr: 0.000004  loss: 0.1879  time: 1.4981  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 280/3000]  eta: 1:09:02  lr: 0.000004  loss: 0.4157  time: 1.4979  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 285/3000]  eta: 1:08:57  lr: 0.000004  loss: 0.4513  time: 1.5259  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 285/3000]  eta: 1:08:55  lr: 0.000004  loss: 0.5615  time: 1.5256  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 290/3000]  eta: 1:08:49  lr: 0.000004  loss: 0.2238  time: 1.5134  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 290/3000]  eta: 1:08:47  lr: 0.000004  loss: 0.5020  time: 1.5133  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 295/3000]  eta: 1:08:38  lr: 0.000004  loss: 0.4989  time: 1.5000  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 295/3000]  eta: 1:08:36  lr: 0.000004  loss: 0.3120  time: 1.4997  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 300/3000]  eta: 1:08:27  lr: 0.000004  loss: 0.3702  time: 1.4883  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 300/3000]  eta: 1:08:25  lr: 0.000004  loss: 0.3186  time: 1.4882  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 305/3000]  eta: 1:08:18  lr: 0.000004  loss: 0.2380  time: 1.4775  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 305/3000]  eta: 1:08:16  lr: 0.000004  loss: 0.2396  time: 1.4772  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 310/3000]  eta: 1:08:10  lr: 0.000004  loss: 0.1588  time: 1.4758  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 310/3000]  eta: 1:08:08  lr: 0.000004  loss: 0.2738  time: 1.4755  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 315/3000]  eta: 1:08:03  lr: 0.000004  loss: 0.4635  time: 1.4967  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 315/3000]  eta: 1:08:01  lr: 0.000004  loss: 0.2630  time: 1.4964  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 320/3000]  eta: 1:07:54  lr: 0.000004  loss: 0.5844  time: 1.5045  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 320/3000]  eta: 1:07:52  lr: 0.000004  loss: 0.1760  time: 1.5040  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 325/3000]  eta: 1:07:49  lr: 0.000004  loss: 0.8991  time: 1.5301  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 325/3000]  eta: 1:07:47  lr: 0.000004  loss: 0.7228  time: 1.5298  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 330/3000]  eta: 1:07:40  lr: 0.000004  loss: 0.4856  time: 1.5204  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 330/3000]  eta: 1:07:38  lr: 0.000004  loss: 0.5098  time: 1.5200  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 335/3000]  eta: 1:07:33  lr: 0.000004  loss: 0.3534  time: 1.5210  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 335/3000]  eta: 1:07:31  lr: 0.000004  loss: 1.0765  time: 1.5207  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 340/3000]  eta: 1:07:27  lr: 0.000004  loss: 0.5300  time: 1.5424  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 340/3000]  eta: 1:07:25  lr: 0.000004  loss: 0.6217  time: 1.5422  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 345/3000]  eta: 1:07:21  lr: 0.000004  loss: 0.7744  time: 1.5339  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 345/3000]  eta: 1:07:19  lr: 0.000004  loss: 0.5700  time: 1.5336  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 350/3000]  eta: 1:07:11  lr: 0.000004  loss: 0.7007  time: 1.5347  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 350/3000]  eta: 1:07:09  lr: 0.000004  loss: 0.5474  time: 1.5345  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 355/3000]  eta: 1:07:06  lr: 0.000004  loss: 0.3894  time: 1.5465  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 355/3000]  eta: 1:07:04  lr: 0.000004  loss: 1.0518  time: 1.5462  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 360/3000]  eta: 1:06:58  lr: 0.000004  loss: 0.2253  time: 1.5305  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 360/3000]  eta: 1:06:56  lr: 0.000004  loss: 1.0813  time: 1.5302  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 365/3000]  eta: 1:06:52  lr: 0.000005  loss: 0.4129  time: 1.5351  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 365/3000]  eta: 1:06:50  lr: 0.000005  loss: 0.2942  time: 1.5347  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 370/3000]  eta: 1:06:43  lr: 0.000005  loss: 0.9662  time: 1.5383  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 370/3000]  eta: 1:06:41  lr: 0.000005  loss: 0.4564  time: 1.5379  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 375/3000]  eta: 1:06:36  lr: 0.000005  loss: 0.5761  time: 1.5267  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 375/3000]  eta: 1:06:34  lr: 0.000005  loss: 0.5466  time: 1.5264  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 380/3000]  eta: 1:06:28  lr: 0.000005  loss: 0.0963  time: 1.5261  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 380/3000]  eta: 1:06:26  lr: 0.000005  loss: 0.9964  time: 1.5258  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 385/3000]  eta: 1:06:21  lr: 0.000005  loss: 0.2430  time: 1.5133  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 385/3000]  eta: 1:06:19  lr: 0.000005  loss: 0.5485  time: 1.5131  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 390/3000]  eta: 1:06:13  lr: 0.000005  loss: 0.4975  time: 1.5256  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 390/3000]  eta: 1:06:12  lr: 0.000005  loss: 1.0656  time: 1.5253  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 395/3000]  eta: 1:06:03  lr: 0.000005  loss: 0.3127  time: 1.5023  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 395/3000]  eta: 1:06:01  lr: 0.000005  loss: 0.3922  time: 1.5020  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 400/3000]  eta: 1:05:55  lr: 0.000005  loss: 0.0862  time: 1.5026  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 400/3000]  eta: 1:05:53  lr: 0.000005  loss: 0.4291  time: 1.5023  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 405/3000]  eta: 1:05:47  lr: 0.000005  loss: 0.1051  time: 1.4977  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 405/3000]  eta: 1:05:45  lr: 0.000005  loss: 0.4532  time: 1.4975  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 410/3000]  eta: 1:05:40  lr: 0.000005  loss: 0.6142  time: 1.4991  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 410/3000]  eta: 1:05:38  lr: 0.000005  loss: 0.7655  time: 1.4989  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 415/3000]  eta: 1:05:32  lr: 0.000005  loss: 0.7852  time: 1.5152  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 415/3000]  eta: 1:05:30  lr: 0.000005  loss: 0.4566  time: 1.5150  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 420/3000]  eta: 1:05:25  lr: 0.000005  loss: 0.2218  time: 1.5253  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 420/3000]  eta: 1:05:23  lr: 0.000005  loss: 1.2790  time: 1.5251  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 425/3000]  eta: 1:05:16  lr: 0.000005  loss: 0.2265  time: 1.5150  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 425/3000]  eta: 1:05:14  lr: 0.000005  loss: 0.1091  time: 1.5148  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 430/3000]  eta: 1:05:09  lr: 0.000005  loss: 0.8982  time: 1.5145  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 430/3000]  eta: 1:05:07  lr: 0.000005  loss: 0.4928  time: 1.5143  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 435/3000]  eta: 1:05:01  lr: 0.000005  loss: 0.2509  time: 1.5186  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 435/3000]  eta: 1:04:59  lr: 0.000005  loss: 0.2057  time: 1.5184  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 440/3000]  eta: 1:04:53  lr: 0.000005  loss: 0.2559  time: 1.5110  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 440/3000]  eta: 1:04:52  lr: 0.000005  loss: 0.1103  time: 1.5107  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 445/3000]  eta: 1:04:46  lr: 0.000005  loss: 0.3202  time: 1.5268  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 445/3000]  eta: 1:04:44  lr: 0.000005  loss: 0.2149  time: 1.5266  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 450/3000]  eta: 1:04:39  lr: 0.000005  loss: 0.8332  time: 1.5234  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 450/3000]  eta: 1:04:37  lr: 0.000005  loss: 0.7151  time: 1.5231  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 455/3000]  eta: 1:04:30  lr: 0.000005  loss: 0.5383  time: 1.5182  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 455/3000]  eta: 1:04:29  lr: 0.000005  loss: 0.9626  time: 1.5179  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 460/3000]  eta: 1:04:25  lr: 0.000005  loss: 0.3448  time: 1.5397  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 460/3000]  eta: 1:04:23  lr: 0.000005  loss: 0.1455  time: 1.5394  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 465/3000]  eta: 1:04:18  lr: 0.000005  loss: 0.4384  time: 1.5461  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 465/3000]  eta: 1:04:17  lr: 0.000005  loss: 0.1415  time: 1.5459  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 470/3000]  eta: 1:04:10  lr: 0.000006  loss: 0.3499  time: 1.5342  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 470/3000]  eta: 1:04:08  lr: 0.000006  loss: 0.2347  time: 1.5340  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 475/3000]  eta: 1:03:59  lr: 0.000006  loss: 0.2603  time: 1.5161  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 475/3000]  eta: 1:03:58  lr: 0.000006  loss: 0.1293  time: 1.5159  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 480/3000]  eta: 1:03:51  lr: 0.000006  loss: 0.3651  time: 1.4907  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 480/3000]  eta: 1:03:50  lr: 0.000006  loss: 0.2968  time: 1.4905  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 485/3000]  eta: 1:03:44  lr: 0.000006  loss: 0.4194  time: 1.4844  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 485/3000]  eta: 1:03:42  lr: 0.000006  loss: 0.1706  time: 1.4842  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 490/3000]  eta: 1:03:38  lr: 0.000006  loss: 0.7490  time: 1.5071  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 490/3000]  eta: 1:03:36  lr: 0.000006  loss: 0.1737  time: 1.5068  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 495/3000]  eta: 1:03:31  lr: 0.000006  loss: 0.9394  time: 1.5423  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 495/3000]  eta: 1:03:30  lr: 0.000006  loss: 0.4230  time: 1.5421  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 500/3000]  eta: 1:03:25  lr: 0.000006  loss: 0.4898  time: 1.5613  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 500/3000]  eta: 1:03:23  lr: 0.000006  loss: 0.9278  time: 1.5611  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 505/3000]  eta: 1:03:16  lr: 0.000006  loss: 0.0823  time: 1.5457  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 505/3000]  eta: 1:03:15  lr: 0.000006  loss: 0.1929  time: 1.5456  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 510/3000]  eta: 1:03:07  lr: 0.000006  loss: 0.4319  time: 1.5176  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 510/3000]  eta: 1:03:05  lr: 0.000006  loss: 0.2768  time: 1.5175  data: 0.0000  max mem: 17599
Train: data epoch: [0]  [ 515/3000]  eta: 1:02:59  lr: 0.000006  loss: 0.4973  time: 1.4996  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 515/3000]  eta: 1:02:57  lr: 0.000006  loss: 0.2851  time: 1.4995  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 520/3000]  eta: 1:02:50  lr: 0.000006  loss: 1.0567  time: 1.4788  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 520/3000]  eta: 1:02:49  lr: 0.000006  loss: 0.1981  time: 1.4786  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 525/3000]  eta: 1:02:43  lr: 0.000006  loss: 0.4756  time: 1.4911  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 525/3000]  eta: 1:02:41  lr: 0.000006  loss: 0.8183  time: 1.4908  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 530/3000]  eta: 1:02:35  lr: 0.000006  loss: 0.8946  time: 1.5109  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 530/3000]  eta: 1:02:34  lr: 0.000006  loss: 0.1187  time: 1.5106  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 535/3000]  eta: 1:02:27  lr: 0.000006  loss: 0.2591  time: 1.5084  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 535/3000]  eta: 1:02:26  lr: 0.000006  loss: 1.1585  time: 1.5080  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 540/3000]  eta: 1:02:20  lr: 0.000006  loss: 0.2558  time: 1.5167  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 540/3000]  eta: 1:02:18  lr: 0.000006  loss: 0.5281  time: 1.5164  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 545/3000]  eta: 1:02:13  lr: 0.000006  loss: 0.4081  time: 1.5277  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 545/3000]  eta: 1:02:12  lr: 0.000006  loss: 0.7650  time: 1.5275  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 550/3000]  eta: 1:02:06  lr: 0.000006  loss: 0.3191  time: 1.5268  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 550/3000]  eta: 1:02:04  lr: 0.000006  loss: 0.4126  time: 1.5266  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 555/3000]  eta: 1:01:55  lr: 0.000006  loss: 0.3224  time: 1.4992  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 555/3000]  eta: 1:01:53  lr: 0.000006  loss: 0.3955  time: 1.4990  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 560/3000]  eta: 1:01:47  lr: 0.000006  loss: 0.6686  time: 1.4931  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 560/3000]  eta: 1:01:45  lr: 0.000006  loss: 0.2963  time: 1.4929  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 565/3000]  eta: 1:01:41  lr: 0.000006  loss: 0.5200  time: 1.4990  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 565/3000]  eta: 1:01:39  lr: 0.000006  loss: 1.0944  time: 1.4988  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 570/3000]  eta: 1:01:34  lr: 0.000007  loss: 1.2426  time: 1.5098  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 570/3000]  eta: 1:01:33  lr: 0.000007  loss: 0.7244  time: 1.5096  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 575/3000]  eta: 1:01:27  lr: 0.000007  loss: 0.1103  time: 1.5481  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 575/3000]  eta: 1:01:26  lr: 0.000007  loss: 0.6541  time: 1.5478  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 580/3000]  eta: 1:01:19  lr: 0.000007  loss: 0.4771  time: 1.5528  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 580/3000]  eta: 1:01:18  lr: 0.000007  loss: 0.3829  time: 1.5526  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 585/3000]  eta: 1:01:10  lr: 0.000007  loss: 0.2185  time: 1.5148  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 585/3000]  eta: 1:01:09  lr: 0.000007  loss: 0.7557  time: 1.5146  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 590/3000]  eta: 1:01:01  lr: 0.000007  loss: 0.2061  time: 1.4850  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 590/3000]  eta: 1:01:00  lr: 0.000007  loss: 0.3230  time: 1.4847  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 595/3000]  eta: 1:00:54  lr: 0.000007  loss: 0.7403  time: 1.4914  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 595/3000]  eta: 1:00:53  lr: 0.000007  loss: 0.5148  time: 1.4911  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 600/3000]  eta: 1:00:47  lr: 0.000007  loss: 0.3365  time: 1.5010  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 600/3000]  eta: 1:00:46  lr: 0.000007  loss: 0.1271  time: 1.5007  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 605/3000]  eta: 1:00:40  lr: 0.000007  loss: 0.3841  time: 1.5243  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 605/3000]  eta: 1:00:39  lr: 0.000007  loss: 0.3670  time: 1.5241  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 610/3000]  eta: 1:00:32  lr: 0.000007  loss: 0.4324  time: 1.5428  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 610/3000]  eta: 1:00:31  lr: 0.000007  loss: 0.2709  time: 1.5426  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 615/3000]  eta: 1:00:25  lr: 0.000007  loss: 0.2266  time: 1.5369  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 615/3000]  eta: 1:00:24  lr: 0.000007  loss: 0.4214  time: 1.5367  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 620/3000]  eta: 1:00:18  lr: 0.000007  loss: 0.5592  time: 1.5329  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 620/3000]  eta: 1:00:17  lr: 0.000007  loss: 0.1711  time: 1.5326  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 625/3000]  eta: 1:00:10  lr: 0.000007  loss: 0.1004  time: 1.5246  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 625/3000]  eta: 1:00:09  lr: 0.000007  loss: 0.2133  time: 1.5244  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 630/3000]  eta: 1:00:02  lr: 0.000007  loss: 0.4653  time: 1.5257  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 630/3000]  eta: 1:00:01  lr: 0.000007  loss: 0.9876  time: 1.5255  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 635/3000]  eta: 0:59:55  lr: 0.000007  loss: 0.2108  time: 1.5273  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 635/3000]  eta: 0:59:54  lr: 0.000007  loss: 0.1132  time: 1.5270  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 640/3000]  eta: 0:59:46  lr: 0.000007  loss: 0.2753  time: 1.4995  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 640/3000]  eta: 0:59:45  lr: 0.000007  loss: 0.2545  time: 1.4993  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 645/3000]  eta: 0:59:39  lr: 0.000007  loss: 0.5262  time: 1.5195  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 645/3000]  eta: 0:59:38  lr: 0.000007  loss: 0.1258  time: 1.5192  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 650/3000]  eta: 0:59:32  lr: 0.000007  loss: 0.7160  time: 1.5195  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 650/3000]  eta: 0:59:31  lr: 0.000007  loss: 0.3667  time: 1.5192  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 655/3000]  eta: 0:59:23  lr: 0.000007  loss: 0.4605  time: 1.5012  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 655/3000]  eta: 0:59:22  lr: 0.000007  loss: 0.4712  time: 1.5009  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 660/3000]  eta: 0:59:16  lr: 0.000007  loss: 1.0140  time: 1.5258  data: 0.0000  max mem: 17595
Train: data epoch: [0]  [ 660/3000]  eta: 0:59:15  lr: 0.000007  loss: 0.1626  time: 1.5255  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 665/3000]  eta: 0:59:08  lr: 0.000007  loss: 0.2926  time: 1.5020  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 665/3000]  eta: 0:59:06  lr: 0.000007  loss: 0.3420  time: 1.5018  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 670/3000]  eta: 0:59:00  lr: 0.000007  loss: 0.2157  time: 1.4965  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 670/3000]  eta: 0:58:59  lr: 0.000007  loss: 0.2818  time: 1.4962  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 675/3000]  eta: 0:58:52  lr: 0.000008  loss: 0.2517  time: 1.5059  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 675/3000]  eta: 0:58:51  lr: 0.000008  loss: 0.2746  time: 1.5056  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 680/3000]  eta: 0:58:45  lr: 0.000008  loss: 0.2757  time: 1.5088  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 680/3000]  eta: 0:58:44  lr: 0.000008  loss: 0.4612  time: 1.5085  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 685/3000]  eta: 0:58:37  lr: 0.000008  loss: 0.2857  time: 1.5133  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 685/3000]  eta: 0:58:36  lr: 0.000008  loss: 0.6102  time: 1.5130  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 690/3000]  eta: 0:58:29  lr: 0.000008  loss: 0.3057  time: 1.5154  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 690/3000]  eta: 0:58:28  lr: 0.000008  loss: 0.2788  time: 1.5152  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 695/3000]  eta: 0:58:21  lr: 0.000008  loss: 0.3472  time: 1.5032  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 695/3000]  eta: 0:58:19  lr: 0.000008  loss: 0.2687  time: 1.5028  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 700/3000]  eta: 0:58:12  lr: 0.000008  loss: 0.4177  time: 1.4858  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 700/3000]  eta: 0:58:11  lr: 0.000008  loss: 0.1675  time: 1.4855  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 705/3000]  eta: 0:58:04  lr: 0.000008  loss: 0.2610  time: 1.4831  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 705/3000]  eta: 0:58:03  lr: 0.000008  loss: 0.4763  time: 1.4828  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 710/3000]  eta: 0:57:57  lr: 0.000008  loss: 0.4399  time: 1.4860  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 710/3000]  eta: 0:57:56  lr: 0.000008  loss: 0.2093  time: 1.4858  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 715/3000]  eta: 0:57:48  lr: 0.000008  loss: 0.1142  time: 1.4835  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 715/3000]  eta: 0:57:47  lr: 0.000008  loss: 0.4366  time: 1.4833  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 720/3000]  eta: 0:57:40  lr: 0.000008  loss: 1.1172  time: 1.4885  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 720/3000]  eta: 0:57:39  lr: 0.000008  loss: 0.4018  time: 1.4883  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 725/3000]  eta: 0:57:31  lr: 0.000008  loss: 0.1447  time: 1.4743  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 725/3000]  eta: 0:57:30  lr: 0.000008  loss: 0.4048  time: 1.4741  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 730/3000]  eta: 0:57:23  lr: 0.000008  loss: 0.9026  time: 1.4593  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 730/3000]  eta: 0:57:22  lr: 0.000008  loss: 0.3346  time: 1.4589  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 735/3000]  eta: 0:57:16  lr: 0.000008  loss: 0.4242  time: 1.4915  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 735/3000]  eta: 0:57:15  lr: 0.000008  loss: 0.2581  time: 1.4910  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 740/3000]  eta: 0:57:07  lr: 0.000008  loss: 0.1097  time: 1.4767  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 740/3000]  eta: 0:57:06  lr: 0.000008  loss: 0.6656  time: 1.4762  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 745/3000]  eta: 0:57:01  lr: 0.000008  loss: 0.7808  time: 1.5154  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 745/3000]  eta: 0:56:59  lr: 0.000008  loss: 0.1396  time: 1.5150  data: 0.0000  max mem: 17631
Train: data epoch: [0]  [ 750/3000]  eta: 0:56:54  lr: 0.000008  loss: 0.1779  time: 1.5398  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 750/3000]  eta: 0:56:53  lr: 0.000008  loss: 0.5470  time: 1.5395  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 755/3000]  eta: 0:56:47  lr: 0.000008  loss: 1.3445  time: 1.5347  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 755/3000]  eta: 0:56:46  lr: 0.000008  loss: 0.4814  time: 1.5345  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 760/3000]  eta: 0:56:39  lr: 0.000008  loss: 0.7387  time: 1.5610  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 760/3000]  eta: 0:56:38  lr: 0.000008  loss: 0.4870  time: 1.5608  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 765/3000]  eta: 0:56:33  lr: 0.000008  loss: 0.5193  time: 1.5598  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 765/3000]  eta: 0:56:32  lr: 0.000008  loss: 0.7205  time: 1.5596  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 770/3000]  eta: 0:56:24  lr: 0.000008  loss: 0.8518  time: 1.5300  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 770/3000]  eta: 0:56:23  lr: 0.000008  loss: 0.7355  time: 1.5297  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 775/3000]  eta: 0:56:16  lr: 0.000008  loss: 0.3082  time: 1.5187  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 775/3000]  eta: 0:56:15  lr: 0.000008  loss: 0.3773  time: 1.5184  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 780/3000]  eta: 0:56:10  lr: 0.000009  loss: 0.4644  time: 1.5315  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 780/3000]  eta: 0:56:09  lr: 0.000009  loss: 1.0758  time: 1.5313  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 785/3000]  eta: 0:56:03  lr: 0.000009  loss: 0.3865  time: 1.5299  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 785/3000]  eta: 0:56:02  lr: 0.000009  loss: 0.2444  time: 1.5296  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 790/3000]  eta: 0:55:55  lr: 0.000009  loss: 0.3630  time: 1.5426  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 790/3000]  eta: 0:55:54  lr: 0.000009  loss: 0.2820  time: 1.5424  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 795/3000]  eta: 0:55:49  lr: 0.000009  loss: 0.3671  time: 1.5634  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 795/3000]  eta: 0:55:47  lr: 0.000009  loss: 0.7615  time: 1.5632  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 800/3000]  eta: 0:55:41  lr: 0.000009  loss: 0.2930  time: 1.5431  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 800/3000]  eta: 0:55:40  lr: 0.000009  loss: 0.2535  time: 1.5429  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 805/3000]  eta: 0:55:33  lr: 0.000009  loss: 0.3228  time: 1.5327  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 805/3000]  eta: 0:55:32  lr: 0.000009  loss: 0.3284  time: 1.5325  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 810/3000]  eta: 0:55:27  lr: 0.000009  loss: 0.5510  time: 1.5529  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 810/3000]  eta: 0:55:26  lr: 0.000009  loss: 0.5120  time: 1.5527  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 815/3000]  eta: 0:55:19  lr: 0.000009  loss: 0.3212  time: 1.5372  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 815/3000]  eta: 0:55:18  lr: 0.000009  loss: 0.4612  time: 1.5370  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 820/3000]  eta: 0:55:10  lr: 0.000009  loss: 0.2072  time: 1.5385  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 820/3000]  eta: 0:55:11  lr: 0.000009  loss: 0.6778  time: 1.5389  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 825/3000]  eta: 0:55:04  lr: 0.000009  loss: 0.4793  time: 1.5433  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 825/3000]  eta: 0:55:03  lr: 0.000009  loss: 0.1781  time: 1.5430  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 830/3000]  eta: 0:54:56  lr: 0.000009  loss: 0.6057  time: 1.5176  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 830/3000]  eta: 0:54:55  lr: 0.000009  loss: 0.8983  time: 1.5174  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 835/3000]  eta: 0:54:49  lr: 0.000009  loss: 0.3462  time: 1.5218  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 835/3000]  eta: 0:54:48  lr: 0.000009  loss: 0.4715  time: 1.5216  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 840/3000]  eta: 0:54:41  lr: 0.000009  loss: 0.2895  time: 1.5190  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 840/3000]  eta: 0:54:40  lr: 0.000009  loss: 0.1329  time: 1.5188  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 845/3000]  eta: 0:54:33  lr: 0.000009  loss: 0.3760  time: 1.5092  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 845/3000]  eta: 0:54:32  lr: 0.000009  loss: 0.5504  time: 1.5091  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 850/3000]  eta: 0:54:25  lr: 0.000009  loss: 0.0891  time: 1.5131  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 850/3000]  eta: 0:54:24  lr: 0.000009  loss: 0.3092  time: 1.5129  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 855/3000]  eta: 0:54:18  lr: 0.000009  loss: 0.4089  time: 1.5044  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 855/3000]  eta: 0:54:17  lr: 0.000009  loss: 0.2944  time: 1.5042  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 860/3000]  eta: 0:54:10  lr: 0.000009  loss: 0.2927  time: 1.5028  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 860/3000]  eta: 0:54:09  lr: 0.000009  loss: 0.3383  time: 1.5026  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 865/3000]  eta: 0:54:02  lr: 0.000009  loss: 0.1949  time: 1.4973  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 865/3000]  eta: 0:54:01  lr: 0.000009  loss: 0.7206  time: 1.4971  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 870/3000]  eta: 0:53:54  lr: 0.000009  loss: 0.4546  time: 1.4888  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 870/3000]  eta: 0:53:53  lr: 0.000009  loss: 0.2006  time: 1.4885  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 875/3000]  eta: 0:53:46  lr: 0.000009  loss: 0.9553  time: 1.4807  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 875/3000]  eta: 0:53:45  lr: 0.000009  loss: 1.0234  time: 1.4804  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 880/3000]  eta: 0:53:39  lr: 0.000010  loss: 0.6387  time: 1.5034  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 880/3000]  eta: 0:53:38  lr: 0.000010  loss: 0.1593  time: 1.5031  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 885/3000]  eta: 0:53:31  lr: 0.000010  loss: 0.2004  time: 1.5158  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 885/3000]  eta: 0:53:30  lr: 0.000010  loss: 0.0980  time: 1.5155  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 890/3000]  eta: 0:53:24  lr: 0.000010  loss: 0.3410  time: 1.5245  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 890/3000]  eta: 0:53:23  lr: 0.000010  loss: 0.1012  time: 1.5242  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 895/3000]  eta: 0:53:17  lr: 0.000010  loss: 0.5196  time: 1.5500  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 895/3000]  eta: 0:53:16  lr: 0.000010  loss: 0.3628  time: 1.5498  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 900/3000]  eta: 0:53:10  lr: 0.000010  loss: 0.4414  time: 1.5452  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 900/3000]  eta: 0:53:09  lr: 0.000010  loss: 0.2709  time: 1.5450  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 905/3000]  eta: 0:53:01  lr: 0.000010  loss: 0.3536  time: 1.5243  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 905/3000]  eta: 0:53:00  lr: 0.000010  loss: 0.5925  time: 1.5241  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 910/3000]  eta: 0:52:54  lr: 0.000010  loss: 0.2484  time: 1.5401  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 910/3000]  eta: 0:52:53  lr: 0.000010  loss: 0.5685  time: 1.5398  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 915/3000]  eta: 0:52:46  lr: 0.000010  loss: 0.6276  time: 1.5214  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 915/3000]  eta: 0:52:45  lr: 0.000010  loss: 0.4322  time: 1.5211  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 920/3000]  eta: 0:52:39  lr: 0.000010  loss: 0.1854  time: 1.5065  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 920/3000]  eta: 0:52:38  lr: 0.000010  loss: 0.0890  time: 1.5061  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 925/3000]  eta: 0:52:31  lr: 0.000010  loss: 0.5471  time: 1.5289  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 925/3000]  eta: 0:52:30  lr: 0.000010  loss: 0.6777  time: 1.5285  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 930/3000]  eta: 0:52:24  lr: 0.000010  loss: 0.2831  time: 1.5263  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 930/3000]  eta: 0:52:23  lr: 0.000010  loss: 0.4266  time: 1.5259  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 935/3000]  eta: 0:52:17  lr: 0.000010  loss: 0.4603  time: 1.5364  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 935/3000]  eta: 0:52:16  lr: 0.000010  loss: 0.4058  time: 1.5360  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 940/3000]  eta: 0:52:09  lr: 0.000010  loss: 0.3689  time: 1.5344  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 940/3000]  eta: 0:52:08  lr: 0.000010  loss: 0.4240  time: 1.5341  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 945/3000]  eta: 0:52:01  lr: 0.000010  loss: 0.2804  time: 1.5268  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 945/3000]  eta: 0:52:00  lr: 0.000010  loss: 0.2441  time: 1.5265  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 950/3000]  eta: 0:51:54  lr: 0.000010  loss: 0.3818  time: 1.5345  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 950/3000]  eta: 0:51:53  lr: 0.000010  loss: 0.5430  time: 1.5342  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 955/3000]  eta: 0:51:46  lr: 0.000010  loss: 0.4324  time: 1.5178  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 955/3000]  eta: 0:51:45  lr: 0.000010  loss: 0.4240  time: 1.5175  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 960/3000]  eta: 0:51:39  lr: 0.000010  loss: 0.3718  time: 1.5183  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 960/3000]  eta: 0:51:38  lr: 0.000010  loss: 0.4224  time: 1.5180  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 965/3000]  eta: 0:51:30  lr: 0.000010  loss: 0.3519  time: 1.5049  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 965/3000]  eta: 0:51:29  lr: 0.000010  loss: 0.1961  time: 1.5046  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 970/3000]  eta: 0:51:24  lr: 0.000010  loss: 0.4232  time: 1.5083  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 970/3000]  eta: 0:51:23  lr: 0.000010  loss: 0.4976  time: 1.5081  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 975/3000]  eta: 0:51:16  lr: 0.000010  loss: 0.5287  time: 1.5200  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 975/3000]  eta: 0:51:15  lr: 0.000010  loss: 0.2108  time: 1.5197  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 980/3000]  eta: 0:51:08  lr: 0.000010  loss: 0.1681  time: 1.5210  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 980/3000]  eta: 0:51:07  lr: 0.000010  loss: 0.1602  time: 1.5208  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 985/3000]  eta: 0:51:01  lr: 0.000011  loss: 0.3830  time: 1.5391  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 985/3000]  eta: 0:51:00  lr: 0.000011  loss: 0.9318  time: 1.5389  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 990/3000]  eta: 0:50:53  lr: 0.000011  loss: 0.3430  time: 1.5259  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 990/3000]  eta: 0:50:53  lr: 0.000011  loss: 0.3046  time: 1.5257  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [ 995/3000]  eta: 0:50:45  lr: 0.000011  loss: 0.3859  time: 1.5120  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [ 995/3000]  eta: 0:50:44  lr: 0.000011  loss: 0.2650  time: 1.5118  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1000/3000]  eta: 0:50:37  lr: 0.000011  loss: 0.1546  time: 1.5083  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1000/3000]  eta: 0:50:37  lr: 0.000011  loss: 0.3037  time: 1.5081  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1005/3000]  eta: 0:50:30  lr: 0.000011  loss: 0.8427  time: 1.5122  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1005/3000]  eta: 0:50:29  lr: 0.000011  loss: 0.5959  time: 1.5119  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1010/3000]  eta: 0:50:23  lr: 0.000011  loss: 0.6858  time: 1.5100  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1010/3000]  eta: 0:50:22  lr: 0.000011  loss: 0.1765  time: 1.5098  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1015/3000]  eta: 0:50:15  lr: 0.000011  loss: 0.1752  time: 1.5088  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1015/3000]  eta: 0:50:14  lr: 0.000011  loss: 0.1811  time: 1.5086  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1020/3000]  eta: 0:50:08  lr: 0.000011  loss: 0.4365  time: 1.5338  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1020/3000]  eta: 0:50:07  lr: 0.000011  loss: 0.1752  time: 1.5336  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1025/3000]  eta: 0:50:00  lr: 0.000011  loss: 0.1809  time: 1.5366  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1025/3000]  eta: 0:50:00  lr: 0.000011  loss: 0.4707  time: 1.5364  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1030/3000]  eta: 0:49:53  lr: 0.000011  loss: 0.3046  time: 1.5333  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1030/3000]  eta: 0:49:52  lr: 0.000011  loss: 1.3892  time: 1.5330  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1035/3000]  eta: 0:49:44  lr: 0.000011  loss: 0.1313  time: 1.5274  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1035/3000]  eta: 0:49:44  lr: 0.000011  loss: 0.5043  time: 1.5279  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1040/3000]  eta: 0:49:37  lr: 0.000011  loss: 0.1163  time: 1.5027  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1040/3000]  eta: 0:49:36  lr: 0.000011  loss: 0.5826  time: 1.5024  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1045/3000]  eta: 0:49:30  lr: 0.000011  loss: 0.6554  time: 1.5066  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1045/3000]  eta: 0:49:29  lr: 0.000011  loss: 0.2911  time: 1.5063  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1050/3000]  eta: 0:49:22  lr: 0.000011  loss: 0.0759  time: 1.5095  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1050/3000]  eta: 0:49:21  lr: 0.000011  loss: 0.2241  time: 1.5093  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1055/3000]  eta: 0:49:14  lr: 0.000011  loss: 0.4811  time: 1.5144  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1055/3000]  eta: 0:49:13  lr: 0.000011  loss: 0.5546  time: 1.5133  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1060/3000]  eta: 0:49:06  lr: 0.000011  loss: 0.5132  time: 1.5047  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1060/3000]  eta: 0:49:05  lr: 0.000011  loss: 0.3164  time: 1.5045  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1065/3000]  eta: 0:48:58  lr: 0.000011  loss: 0.3613  time: 1.4902  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1065/3000]  eta: 0:48:57  lr: 0.000011  loss: 0.4313  time: 1.4900  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1070/3000]  eta: 0:48:51  lr: 0.000011  loss: 0.4514  time: 1.5028  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1070/3000]  eta: 0:48:50  lr: 0.000011  loss: 0.3613  time: 1.5026  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1075/3000]  eta: 0:48:44  lr: 0.000011  loss: 0.3600  time: 1.5225  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1075/3000]  eta: 0:48:43  lr: 0.000011  loss: 0.3709  time: 1.5222  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1080/3000]  eta: 0:48:36  lr: 0.000011  loss: 0.1822  time: 1.5274  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1080/3000]  eta: 0:48:35  lr: 0.000011  loss: 0.4105  time: 1.5271  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1085/3000]  eta: 0:48:27  lr: 0.000011  loss: 0.5237  time: 1.5094  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1085/3000]  eta: 0:48:27  lr: 0.000011  loss: 0.1974  time: 1.5092  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1090/3000]  eta: 0:48:20  lr: 0.000012  loss: 0.2732  time: 1.4986  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1090/3000]  eta: 0:48:19  lr: 0.000012  loss: 0.6138  time: 1.4984  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1095/3000]  eta: 0:48:13  lr: 0.000012  loss: 0.7514  time: 1.4969  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1095/3000]  eta: 0:48:12  lr: 0.000012  loss: 1.0923  time: 1.4966  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1100/3000]  eta: 0:48:05  lr: 0.000012  loss: 0.5113  time: 1.5086  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1100/3000]  eta: 0:48:04  lr: 0.000012  loss: 0.4597  time: 1.5083  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1105/3000]  eta: 0:47:57  lr: 0.000012  loss: 0.6019  time: 1.5152  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1105/3000]  eta: 0:47:56  lr: 0.000012  loss: 0.3819  time: 1.5148  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1110/3000]  eta: 0:47:49  lr: 0.000012  loss: 0.6904  time: 1.4964  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1110/3000]  eta: 0:47:48  lr: 0.000012  loss: 0.2684  time: 1.4962  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1115/3000]  eta: 0:47:41  lr: 0.000012  loss: 0.0714  time: 1.4781  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1115/3000]  eta: 0:47:40  lr: 0.000012  loss: 0.4183  time: 1.4779  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1120/3000]  eta: 0:47:33  lr: 0.000012  loss: 0.2586  time: 1.4645  data: 0.0000  max mem: 17606
Train: data epoch: [0]  [1120/3000]  eta: 0:47:32  lr: 0.000012  loss: 0.2373  time: 1.4644  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1125/3000]  eta: 0:47:25  lr: 0.000012  loss: 1.0804  time: 1.4851  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1125/3000]  eta: 0:47:25  lr: 0.000012  loss: 0.4678  time: 1.4848  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1130/3000]  eta: 0:47:18  lr: 0.000012  loss: 0.2904  time: 1.5082  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1130/3000]  eta: 0:47:17  lr: 0.000012  loss: 0.2173  time: 1.5079  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1135/3000]  eta: 0:47:10  lr: 0.000012  loss: 0.6697  time: 1.5032  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1135/3000]  eta: 0:47:09  lr: 0.000012  loss: 0.8406  time: 1.5030  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1140/3000]  eta: 0:47:02  lr: 0.000012  loss: 0.5647  time: 1.5080  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1140/3000]  eta: 0:47:01  lr: 0.000012  loss: 0.9048  time: 1.5078  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1145/3000]  eta: 0:46:54  lr: 0.000012  loss: 0.2829  time: 1.4985  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1145/3000]  eta: 0:46:54  lr: 0.000012  loss: 0.3442  time: 1.4982  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1150/3000]  eta: 0:46:47  lr: 0.000012  loss: 0.6822  time: 1.4861  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1150/3000]  eta: 0:46:46  lr: 0.000012  loss: 0.2355  time: 1.4858  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1155/3000]  eta: 0:46:39  lr: 0.000012  loss: 0.2056  time: 1.4857  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1155/3000]  eta: 0:46:38  lr: 0.000012  loss: 0.1968  time: 1.4853  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1160/3000]  eta: 0:46:31  lr: 0.000012  loss: 0.3644  time: 1.4919  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1160/3000]  eta: 0:46:30  lr: 0.000012  loss: 0.7070  time: 1.4914  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1165/3000]  eta: 0:46:23  lr: 0.000012  loss: 0.2964  time: 1.4969  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1165/3000]  eta: 0:46:23  lr: 0.000012  loss: 0.2245  time: 1.4967  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1170/3000]  eta: 0:46:16  lr: 0.000012  loss: 0.3064  time: 1.5117  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1170/3000]  eta: 0:46:16  lr: 0.000012  loss: 0.4611  time: 1.5114  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1175/3000]  eta: 0:46:08  lr: 0.000012  loss: 0.2224  time: 1.5260  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1175/3000]  eta: 0:46:08  lr: 0.000012  loss: 0.2262  time: 1.5259  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1180/3000]  eta: 0:46:01  lr: 0.000012  loss: 0.8711  time: 1.5399  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1180/3000]  eta: 0:46:01  lr: 0.000012  loss: 0.1494  time: 1.5397  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1185/3000]  eta: 0:45:54  lr: 0.000012  loss: 0.2818  time: 1.5385  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1185/3000]  eta: 0:45:53  lr: 0.000012  loss: 0.5413  time: 1.5383  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1190/3000]  eta: 0:45:46  lr: 0.000013  loss: 0.4067  time: 1.5226  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1190/3000]  eta: 0:45:45  lr: 0.000013  loss: 0.3370  time: 1.5224  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1195/3000]  eta: 0:45:38  lr: 0.000013  loss: 0.5242  time: 1.5123  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1195/3000]  eta: 0:45:37  lr: 0.000013  loss: 0.5148  time: 1.5120  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1200/3000]  eta: 0:45:30  lr: 0.000013  loss: 0.1454  time: 1.4915  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1200/3000]  eta: 0:45:29  lr: 0.000013  loss: 0.3427  time: 1.4913  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1205/3000]  eta: 0:45:22  lr: 0.000013  loss: 1.2390  time: 1.4808  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1205/3000]  eta: 0:45:22  lr: 0.000013  loss: 0.1242  time: 1.4806  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1210/3000]  eta: 0:45:15  lr: 0.000013  loss: 0.1163  time: 1.4818  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1210/3000]  eta: 0:45:14  lr: 0.000013  loss: 0.3600  time: 1.4816  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1215/3000]  eta: 0:45:07  lr: 0.000013  loss: 0.5110  time: 1.4877  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1215/3000]  eta: 0:45:06  lr: 0.000013  loss: 0.3331  time: 1.4875  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1220/3000]  eta: 0:44:59  lr: 0.000013  loss: 0.7335  time: 1.4929  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1220/3000]  eta: 0:44:58  lr: 0.000013  loss: 0.4907  time: 1.4926  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1225/3000]  eta: 0:44:51  lr: 0.000013  loss: 0.4468  time: 1.4811  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1225/3000]  eta: 0:44:50  lr: 0.000013  loss: 0.2299  time: 1.4808  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1230/3000]  eta: 0:44:44  lr: 0.000013  loss: 0.7882  time: 1.4959  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1230/3000]  eta: 0:44:43  lr: 0.000013  loss: 0.6876  time: 1.4956  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1235/3000]  eta: 0:44:36  lr: 0.000013  loss: 0.4501  time: 1.5058  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1235/3000]  eta: 0:44:35  lr: 0.000013  loss: 0.2470  time: 1.5056  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1240/3000]  eta: 0:44:29  lr: 0.000013  loss: 0.2560  time: 1.5196  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1240/3000]  eta: 0:44:28  lr: 0.000013  loss: 0.3639  time: 1.5194  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1245/3000]  eta: 0:44:22  lr: 0.000013  loss: 0.7982  time: 1.5506  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1245/3000]  eta: 0:44:21  lr: 0.000013  loss: 0.2504  time: 1.5503  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1250/3000]  eta: 0:44:14  lr: 0.000013  loss: 0.6359  time: 1.5444  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1250/3000]  eta: 0:44:13  lr: 0.000013  loss: 0.3046  time: 1.5442  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1255/3000]  eta: 0:44:07  lr: 0.000013  loss: 0.6357  time: 1.5497  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1255/3000]  eta: 0:44:06  lr: 0.000013  loss: 0.1228  time: 1.5494  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1260/3000]  eta: 0:43:59  lr: 0.000013  loss: 0.4436  time: 1.5356  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1260/3000]  eta: 0:43:58  lr: 0.000013  loss: 0.2452  time: 1.5353  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1265/3000]  eta: 0:43:52  lr: 0.000013  loss: 0.3078  time: 1.5353  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1265/3000]  eta: 0:43:51  lr: 0.000013  loss: 0.3989  time: 1.5350  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1270/3000]  eta: 0:43:44  lr: 0.000013  loss: 0.8550  time: 1.5408  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1270/3000]  eta: 0:43:44  lr: 0.000013  loss: 0.5489  time: 1.5406  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1275/3000]  eta: 0:43:37  lr: 0.000013  loss: 0.4671  time: 1.5490  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1275/3000]  eta: 0:43:37  lr: 0.000013  loss: 0.3372  time: 1.5487  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1280/3000]  eta: 0:43:30  lr: 0.000013  loss: 0.2344  time: 1.5519  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1280/3000]  eta: 0:43:29  lr: 0.000013  loss: 0.8497  time: 1.5516  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1285/3000]  eta: 0:43:22  lr: 0.000013  loss: 0.1922  time: 1.5368  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1285/3000]  eta: 0:43:21  lr: 0.000013  loss: 0.5424  time: 1.5366  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1290/3000]  eta: 0:43:15  lr: 0.000013  loss: 0.4407  time: 1.5314  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1290/3000]  eta: 0:43:14  lr: 0.000013  loss: 0.1749  time: 1.5312  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1295/3000]  eta: 0:43:07  lr: 0.000014  loss: 0.2627  time: 1.5177  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1295/3000]  eta: 0:43:06  lr: 0.000014  loss: 0.6290  time: 1.5174  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1300/3000]  eta: 0:42:59  lr: 0.000014  loss: 0.6586  time: 1.5105  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1300/3000]  eta: 0:42:58  lr: 0.000014  loss: 0.3975  time: 1.5103  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1305/3000]  eta: 0:42:51  lr: 0.000014  loss: 0.2975  time: 1.4929  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1305/3000]  eta: 0:42:50  lr: 0.000014  loss: 0.6707  time: 1.4926  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1310/3000]  eta: 0:42:44  lr: 0.000014  loss: 0.2751  time: 1.4975  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1310/3000]  eta: 0:42:43  lr: 0.000014  loss: 0.4497  time: 1.4972  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1315/3000]  eta: 0:42:36  lr: 0.000014  loss: 0.3181  time: 1.5110  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1315/3000]  eta: 0:42:36  lr: 0.000014  loss: 0.1258  time: 1.5108  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1320/3000]  eta: 0:42:29  lr: 0.000014  loss: 0.3529  time: 1.5172  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1320/3000]  eta: 0:42:28  lr: 0.000014  loss: 0.3139  time: 1.5169  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1325/3000]  eta: 0:42:21  lr: 0.000014  loss: 0.2115  time: 1.5319  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1325/3000]  eta: 0:42:20  lr: 0.000014  loss: 0.0811  time: 1.5317  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1330/3000]  eta: 0:42:13  lr: 0.000014  loss: 0.1064  time: 1.5195  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1330/3000]  eta: 0:42:13  lr: 0.000014  loss: 0.4208  time: 1.5192  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1335/3000]  eta: 0:42:06  lr: 0.000014  loss: 0.5283  time: 1.5087  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1335/3000]  eta: 0:42:05  lr: 0.000014  loss: 0.4118  time: 1.5084  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1340/3000]  eta: 0:41:58  lr: 0.000014  loss: 0.4554  time: 1.5162  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1340/3000]  eta: 0:41:58  lr: 0.000014  loss: 0.2670  time: 1.5160  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1345/3000]  eta: 0:41:50  lr: 0.000014  loss: 0.5238  time: 1.4992  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1345/3000]  eta: 0:41:50  lr: 0.000014  loss: 0.6452  time: 1.4989  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1350/3000]  eta: 0:41:42  lr: 0.000014  loss: 0.4717  time: 1.4678  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1350/3000]  eta: 0:41:41  lr: 0.000014  loss: 0.9199  time: 1.4676  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1355/3000]  eta: 0:41:34  lr: 0.000014  loss: 0.6253  time: 1.4601  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1355/3000]  eta: 0:41:33  lr: 0.000014  loss: 0.2182  time: 1.4599  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1360/3000]  eta: 0:41:26  lr: 0.000014  loss: 0.5423  time: 1.4434  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1360/3000]  eta: 0:41:26  lr: 0.000014  loss: 0.0523  time: 1.4431  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1365/3000]  eta: 0:41:19  lr: 0.000014  loss: 0.3895  time: 1.4611  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1365/3000]  eta: 0:41:18  lr: 0.000014  loss: 0.1349  time: 1.4609  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1370/3000]  eta: 0:41:11  lr: 0.000014  loss: 0.2623  time: 1.5027  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1370/3000]  eta: 0:41:11  lr: 0.000014  loss: 0.1562  time: 1.5024  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1375/3000]  eta: 0:41:03  lr: 0.000014  loss: 1.1726  time: 1.4857  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1375/3000]  eta: 0:41:02  lr: 0.000014  loss: 0.1784  time: 1.4854  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1380/3000]  eta: 0:40:55  lr: 0.000014  loss: 0.0931  time: 1.4829  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1380/3000]  eta: 0:40:55  lr: 0.000014  loss: 0.3467  time: 1.4827  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1385/3000]  eta: 0:40:47  lr: 0.000014  loss: 0.3524  time: 1.4833  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1385/3000]  eta: 0:40:47  lr: 0.000014  loss: 0.2980  time: 1.4831  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1390/3000]  eta: 0:40:40  lr: 0.000014  loss: 0.4978  time: 1.4857  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1390/3000]  eta: 0:40:39  lr: 0.000014  loss: 0.2458  time: 1.4854  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1395/3000]  eta: 0:40:32  lr: 0.000014  loss: 0.2163  time: 1.5044  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1395/3000]  eta: 0:40:32  lr: 0.000014  loss: 0.9237  time: 1.5043  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1400/3000]  eta: 0:40:25  lr: 0.000015  loss: 0.3882  time: 1.5144  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1400/3000]  eta: 0:40:24  lr: 0.000015  loss: 0.4152  time: 1.5142  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1405/3000]  eta: 0:40:17  lr: 0.000015  loss: 0.3134  time: 1.5044  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1405/3000]  eta: 0:40:16  lr: 0.000015  loss: 0.2101  time: 1.5041  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1410/3000]  eta: 0:40:09  lr: 0.000015  loss: 0.0936  time: 1.4948  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1410/3000]  eta: 0:40:09  lr: 0.000015  loss: 0.3841  time: 1.4945  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1415/3000]  eta: 0:40:01  lr: 0.000015  loss: 0.8062  time: 1.4797  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1415/3000]  eta: 0:40:01  lr: 0.000015  loss: 0.4776  time: 1.4795  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1420/3000]  eta: 0:39:54  lr: 0.000015  loss: 0.6850  time: 1.4799  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1420/3000]  eta: 0:39:53  lr: 0.000015  loss: 0.6998  time: 1.4796  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1425/3000]  eta: 0:39:46  lr: 0.000015  loss: 0.2556  time: 1.5005  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1425/3000]  eta: 0:39:46  lr: 0.000015  loss: 0.7710  time: 1.5002  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1430/3000]  eta: 0:39:38  lr: 0.000015  loss: 0.5549  time: 1.4888  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1430/3000]  eta: 0:39:38  lr: 0.000015  loss: 0.2669  time: 1.4886  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1435/3000]  eta: 0:39:31  lr: 0.000015  loss: 0.4402  time: 1.4931  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1435/3000]  eta: 0:39:30  lr: 0.000015  loss: 0.6226  time: 1.4928  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1440/3000]  eta: 0:39:23  lr: 0.000015  loss: 0.1878  time: 1.4865  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1440/3000]  eta: 0:39:22  lr: 0.000015  loss: 0.3875  time: 1.4863  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1445/3000]  eta: 0:39:15  lr: 0.000015  loss: 0.3051  time: 1.4792  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1445/3000]  eta: 0:39:15  lr: 0.000015  loss: 0.9013  time: 1.4789  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1450/3000]  eta: 0:39:08  lr: 0.000015  loss: 0.3819  time: 1.4911  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1450/3000]  eta: 0:39:07  lr: 0.000015  loss: 0.0701  time: 1.4909  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1455/3000]  eta: 0:39:00  lr: 0.000015  loss: 0.2915  time: 1.5025  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1455/3000]  eta: 0:38:59  lr: 0.000015  loss: 0.3957  time: 1.5022  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1460/3000]  eta: 0:38:53  lr: 0.000015  loss: 0.3056  time: 1.5191  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1460/3000]  eta: 0:38:52  lr: 0.000015  loss: 0.3720  time: 1.5188  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1465/3000]  eta: 0:38:45  lr: 0.000015  loss: 0.4549  time: 1.5042  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1465/3000]  eta: 0:38:44  lr: 0.000015  loss: 0.3436  time: 1.5039  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1470/3000]  eta: 0:38:37  lr: 0.000015  loss: 0.0751  time: 1.5195  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1470/3000]  eta: 0:38:37  lr: 0.000015  loss: 0.2030  time: 1.5193  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1475/3000]  eta: 0:38:30  lr: 0.000015  loss: 1.2188  time: 1.5169  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1475/3000]  eta: 0:38:29  lr: 0.000015  loss: 0.2162  time: 1.5166  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1480/3000]  eta: 0:38:22  lr: 0.000015  loss: 0.3958  time: 1.5119  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1480/3000]  eta: 0:38:22  lr: 0.000015  loss: 0.1633  time: 1.5116  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1485/3000]  eta: 0:38:15  lr: 0.000015  loss: 0.3093  time: 1.5256  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1485/3000]  eta: 0:38:14  lr: 0.000015  loss: 0.5826  time: 1.5254  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1490/3000]  eta: 0:38:07  lr: 0.000015  loss: 0.5245  time: 1.5071  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1490/3000]  eta: 0:38:06  lr: 0.000015  loss: 0.0644  time: 1.5068  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1495/3000]  eta: 0:37:59  lr: 0.000015  loss: 0.8613  time: 1.5148  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1495/3000]  eta: 0:37:59  lr: 0.000015  loss: 0.1573  time: 1.5146  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1500/3000]  eta: 0:37:52  lr: 0.000015  loss: 0.2382  time: 1.5087  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1500/3000]  eta: 0:37:51  lr: 0.000015  loss: 0.3815  time: 1.5085  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1505/3000]  eta: 0:37:44  lr: 0.000016  loss: 0.6112  time: 1.5180  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1505/3000]  eta: 0:37:44  lr: 0.000016  loss: 0.7626  time: 1.5177  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1510/3000]  eta: 0:37:37  lr: 0.000016  loss: 0.8694  time: 1.5206  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1510/3000]  eta: 0:37:36  lr: 0.000016  loss: 0.1927  time: 1.5203  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1515/3000]  eta: 0:37:29  lr: 0.000016  loss: 0.7336  time: 1.5224  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1515/3000]  eta: 0:37:29  lr: 0.000016  loss: 0.3205  time: 1.5221  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1520/3000]  eta: 0:37:22  lr: 0.000016  loss: 0.4721  time: 1.5418  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1520/3000]  eta: 0:37:21  lr: 0.000016  loss: 1.0470  time: 1.5415  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1525/3000]  eta: 0:37:14  lr: 0.000016  loss: 0.2239  time: 1.5337  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1525/3000]  eta: 0:37:14  lr: 0.000016  loss: 0.3136  time: 1.5334  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1530/3000]  eta: 0:37:07  lr: 0.000016  loss: 0.2225  time: 1.5356  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1530/3000]  eta: 0:37:06  lr: 0.000016  loss: 0.4467  time: 1.5354  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1535/3000]  eta: 0:36:59  lr: 0.000016  loss: 0.7044  time: 1.5274  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1535/3000]  eta: 0:36:59  lr: 0.000016  loss: 0.8789  time: 1.5271  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1540/3000]  eta: 0:36:52  lr: 0.000016  loss: 0.4825  time: 1.5293  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1540/3000]  eta: 0:36:51  lr: 0.000016  loss: 0.5089  time: 1.5290  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1545/3000]  eta: 0:36:44  lr: 0.000016  loss: 0.5008  time: 1.5356  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1545/3000]  eta: 0:36:44  lr: 0.000016  loss: 0.9582  time: 1.5354  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1550/3000]  eta: 0:36:37  lr: 0.000016  loss: 0.3791  time: 1.5350  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1550/3000]  eta: 0:36:36  lr: 0.000016  loss: 0.2786  time: 1.5348  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1555/3000]  eta: 0:36:29  lr: 0.000016  loss: 0.1231  time: 1.5259  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1555/3000]  eta: 0:36:29  lr: 0.000016  loss: 0.3319  time: 1.5257  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1560/3000]  eta: 0:36:22  lr: 0.000016  loss: 0.4895  time: 1.5118  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1560/3000]  eta: 0:36:21  lr: 0.000016  loss: 0.7903  time: 1.5115  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1565/3000]  eta: 0:36:14  lr: 0.000016  loss: 0.4696  time: 1.5048  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1565/3000]  eta: 0:36:13  lr: 0.000016  loss: 0.2122  time: 1.5046  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1570/3000]  eta: 0:36:06  lr: 0.000016  loss: 1.5466  time: 1.4886  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1570/3000]  eta: 0:36:06  lr: 0.000016  loss: 0.0549  time: 1.4884  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1575/3000]  eta: 0:35:59  lr: 0.000016  loss: 0.3091  time: 1.5035  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1575/3000]  eta: 0:35:58  lr: 0.000016  loss: 0.3512  time: 1.5033  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1580/3000]  eta: 0:35:51  lr: 0.000016  loss: 0.3471  time: 1.4977  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1580/3000]  eta: 0:35:50  lr: 0.000016  loss: 0.3460  time: 1.4975  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1585/3000]  eta: 0:35:43  lr: 0.000016  loss: 0.1606  time: 1.4920  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1585/3000]  eta: 0:35:43  lr: 0.000016  loss: 0.3401  time: 1.4918  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1590/3000]  eta: 0:35:36  lr: 0.000016  loss: 0.5343  time: 1.4997  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1590/3000]  eta: 0:35:35  lr: 0.000016  loss: 0.4743  time: 1.4995  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1595/3000]  eta: 0:35:28  lr: 0.000016  loss: 0.3665  time: 1.5070  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1595/3000]  eta: 0:35:28  lr: 0.000016  loss: 0.7672  time: 1.5068  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1600/3000]  eta: 0:35:21  lr: 0.000016  loss: 0.4257  time: 1.5102  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1600/3000]  eta: 0:35:20  lr: 0.000016  loss: 0.1901  time: 1.5100  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1605/3000]  eta: 0:35:13  lr: 0.000017  loss: 0.2223  time: 1.5305  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1605/3000]  eta: 0:35:13  lr: 0.000017  loss: 0.7140  time: 1.5303  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1610/3000]  eta: 0:35:06  lr: 0.000017  loss: 0.4010  time: 1.5428  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1610/3000]  eta: 0:35:05  lr: 0.000017  loss: 0.7073  time: 1.5425  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1615/3000]  eta: 0:34:58  lr: 0.000017  loss: 0.4980  time: 1.5140  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1615/3000]  eta: 0:34:57  lr: 0.000017  loss: 0.4098  time: 1.5138  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1620/3000]  eta: 0:34:50  lr: 0.000017  loss: 0.2476  time: 1.5065  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1620/3000]  eta: 0:34:50  lr: 0.000017  loss: 0.3583  time: 1.5061  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1625/3000]  eta: 0:34:43  lr: 0.000017  loss: 0.4243  time: 1.5042  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1625/3000]  eta: 0:34:42  lr: 0.000017  loss: 0.3881  time: 1.5039  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1630/3000]  eta: 0:34:35  lr: 0.000017  loss: 0.0707  time: 1.5029  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1630/3000]  eta: 0:34:35  lr: 0.000017  loss: 0.0895  time: 1.5027  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1635/3000]  eta: 0:34:27  lr: 0.000017  loss: 0.3347  time: 1.5073  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1635/3000]  eta: 0:34:27  lr: 0.000017  loss: 0.5945  time: 1.5070  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1640/3000]  eta: 0:34:20  lr: 0.000017  loss: 0.3235  time: 1.5101  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1640/3000]  eta: 0:34:19  lr: 0.000017  loss: 0.7330  time: 1.5099  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1645/3000]  eta: 0:34:12  lr: 0.000017  loss: 0.9973  time: 1.4922  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1645/3000]  eta: 0:34:12  lr: 0.000017  loss: 0.4814  time: 1.4919  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1650/3000]  eta: 0:34:04  lr: 0.000017  loss: 0.1358  time: 1.4801  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1650/3000]  eta: 0:34:04  lr: 0.000017  loss: 0.1797  time: 1.4798  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1655/3000]  eta: 0:33:57  lr: 0.000017  loss: 0.6225  time: 1.4849  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1655/3000]  eta: 0:33:56  lr: 0.000017  loss: 0.6997  time: 1.4846  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1660/3000]  eta: 0:33:49  lr: 0.000017  loss: 0.3887  time: 1.5007  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1660/3000]  eta: 0:33:49  lr: 0.000017  loss: 0.5463  time: 1.5002  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1665/3000]  eta: 0:33:42  lr: 0.000017  loss: 0.3046  time: 1.5154  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1665/3000]  eta: 0:33:41  lr: 0.000017  loss: 0.3330  time: 1.5147  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1670/3000]  eta: 0:33:34  lr: 0.000017  loss: 0.6865  time: 1.5068  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1670/3000]  eta: 0:33:33  lr: 0.000017  loss: 0.5844  time: 1.5060  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1675/3000]  eta: 0:33:26  lr: 0.000017  loss: 0.2920  time: 1.5266  data: 0.0000  max mem: 17869
Train: data epoch: [0]  [1675/3000]  eta: 0:33:26  lr: 0.000017  loss: 0.5061  time: 1.5259  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1680/3000]  eta: 0:33:19  lr: 0.000017  loss: 0.5368  time: 1.5289  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1680/3000]  eta: 0:33:19  lr: 0.000017  loss: 0.1188  time: 1.5282  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1685/3000]  eta: 0:33:12  lr: 0.000017  loss: 0.1831  time: 1.5400  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1685/3000]  eta: 0:33:11  lr: 0.000017  loss: 0.1138  time: 1.5398  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1690/3000]  eta: 0:33:04  lr: 0.000017  loss: 0.2999  time: 1.5600  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1690/3000]  eta: 0:33:04  lr: 0.000017  loss: 0.6201  time: 1.5597  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1695/3000]  eta: 0:32:57  lr: 0.000017  loss: 0.2280  time: 1.5545  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1695/3000]  eta: 0:32:56  lr: 0.000017  loss: 0.4089  time: 1.5543  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1700/3000]  eta: 0:32:49  lr: 0.000017  loss: 0.4560  time: 1.5412  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1700/3000]  eta: 0:32:49  lr: 0.000017  loss: 0.7198  time: 1.5410  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1705/3000]  eta: 0:32:42  lr: 0.000017  loss: 0.2183  time: 1.5140  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1705/3000]  eta: 0:32:41  lr: 0.000017  loss: 0.1261  time: 1.5137  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1710/3000]  eta: 0:32:34  lr: 0.000018  loss: 0.2894  time: 1.4954  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1710/3000]  eta: 0:32:33  lr: 0.000018  loss: 0.8715  time: 1.4951  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1715/3000]  eta: 0:32:26  lr: 0.000018  loss: 0.4394  time: 1.4857  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1715/3000]  eta: 0:32:26  lr: 0.000018  loss: 0.1032  time: 1.4854  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1720/3000]  eta: 0:32:19  lr: 0.000018  loss: 0.5025  time: 1.4887  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1720/3000]  eta: 0:32:18  lr: 0.000018  loss: 0.7855  time: 1.4885  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1725/3000]  eta: 0:32:11  lr: 0.000018  loss: 0.2146  time: 1.4927  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1725/3000]  eta: 0:32:10  lr: 0.000018  loss: 0.2160  time: 1.4924  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1730/3000]  eta: 0:32:03  lr: 0.000018  loss: 0.3315  time: 1.5047  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1730/3000]  eta: 0:32:03  lr: 0.000018  loss: 0.4936  time: 1.5045  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1735/3000]  eta: 0:31:56  lr: 0.000018  loss: 0.1117  time: 1.4983  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1735/3000]  eta: 0:31:55  lr: 0.000018  loss: 0.9369  time: 1.4980  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1740/3000]  eta: 0:31:48  lr: 0.000018  loss: 0.4161  time: 1.5013  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1740/3000]  eta: 0:31:48  lr: 0.000018  loss: 0.2288  time: 1.5011  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1745/3000]  eta: 0:31:41  lr: 0.000018  loss: 0.2698  time: 1.5206  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1745/3000]  eta: 0:31:40  lr: 0.000018  loss: 0.2622  time: 1.5204  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1750/3000]  eta: 0:31:33  lr: 0.000018  loss: 0.5365  time: 1.5268  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1750/3000]  eta: 0:31:33  lr: 0.000018  loss: 0.6082  time: 1.5267  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1755/3000]  eta: 0:31:26  lr: 0.000018  loss: 0.2985  time: 1.5398  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1755/3000]  eta: 0:31:25  lr: 0.000018  loss: 1.3525  time: 1.5396  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1760/3000]  eta: 0:31:18  lr: 0.000018  loss: 0.5292  time: 1.5456  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1760/3000]  eta: 0:31:18  lr: 0.000018  loss: 1.0384  time: 1.5454  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1765/3000]  eta: 0:31:11  lr: 0.000018  loss: 0.2806  time: 1.5349  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1765/3000]  eta: 0:31:10  lr: 0.000018  loss: 0.4259  time: 1.5347  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1770/3000]  eta: 0:31:03  lr: 0.000018  loss: 0.2237  time: 1.5157  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1770/3000]  eta: 0:31:02  lr: 0.000018  loss: 0.5757  time: 1.5154  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1775/3000]  eta: 0:30:55  lr: 0.000018  loss: 0.4132  time: 1.5107  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1775/3000]  eta: 0:30:55  lr: 0.000018  loss: 0.1867  time: 1.5104  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1780/3000]  eta: 0:30:48  lr: 0.000018  loss: 0.2720  time: 1.5042  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1780/3000]  eta: 0:30:47  lr: 0.000018  loss: 0.3800  time: 1.5039  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1785/3000]  eta: 0:30:40  lr: 0.000018  loss: 0.8003  time: 1.4928  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1785/3000]  eta: 0:30:40  lr: 0.000018  loss: 0.7462  time: 1.4925  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1790/3000]  eta: 0:30:33  lr: 0.000018  loss: 0.6263  time: 1.5111  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1790/3000]  eta: 0:30:32  lr: 0.000018  loss: 0.2130  time: 1.5108  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1795/3000]  eta: 0:30:25  lr: 0.000018  loss: 0.6528  time: 1.5005  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1795/3000]  eta: 0:30:24  lr: 0.000018  loss: 0.3183  time: 1.5001  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1800/3000]  eta: 0:30:17  lr: 0.000018  loss: 0.0803  time: 1.4893  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1800/3000]  eta: 0:30:17  lr: 0.000018  loss: 0.7190  time: 1.4891  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1805/3000]  eta: 0:30:10  lr: 0.000018  loss: 0.4140  time: 1.5006  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1805/3000]  eta: 0:30:09  lr: 0.000018  loss: 0.4090  time: 1.5003  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1810/3000]  eta: 0:30:02  lr: 0.000018  loss: 0.2373  time: 1.4988  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1810/3000]  eta: 0:30:02  lr: 0.000018  loss: 0.6578  time: 1.4985  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1815/3000]  eta: 0:29:54  lr: 0.000019  loss: 0.3836  time: 1.5086  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1815/3000]  eta: 0:29:54  lr: 0.000019  loss: 0.3880  time: 1.5084  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1820/3000]  eta: 0:29:47  lr: 0.000019  loss: 0.1715  time: 1.5157  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1820/3000]  eta: 0:29:46  lr: 0.000019  loss: 0.3404  time: 1.5154  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1825/3000]  eta: 0:29:39  lr: 0.000019  loss: 0.0760  time: 1.5250  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1825/3000]  eta: 0:29:39  lr: 0.000019  loss: 0.4915  time: 1.5247  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1830/3000]  eta: 0:29:32  lr: 0.000019  loss: 0.7553  time: 1.5245  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1830/3000]  eta: 0:29:31  lr: 0.000019  loss: 0.5411  time: 1.5241  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1835/3000]  eta: 0:29:24  lr: 0.000019  loss: 0.1157  time: 1.5405  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1835/3000]  eta: 0:29:24  lr: 0.000019  loss: 0.4461  time: 1.5401  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1840/3000]  eta: 0:29:17  lr: 0.000019  loss: 1.1340  time: 1.5318  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1840/3000]  eta: 0:29:16  lr: 0.000019  loss: 0.4466  time: 1.5314  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1845/3000]  eta: 0:29:09  lr: 0.000019  loss: 0.4812  time: 1.5186  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1845/3000]  eta: 0:29:09  lr: 0.000019  loss: 0.4755  time: 1.5181  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1850/3000]  eta: 0:29:02  lr: 0.000019  loss: 0.9575  time: 1.5334  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1850/3000]  eta: 0:29:01  lr: 0.000019  loss: 0.3260  time: 1.5331  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1855/3000]  eta: 0:28:54  lr: 0.000019  loss: 0.1614  time: 1.5169  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1855/3000]  eta: 0:28:54  lr: 0.000019  loss: 0.3772  time: 1.5166  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1860/3000]  eta: 0:28:47  lr: 0.000019  loss: 0.3620  time: 1.5293  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1860/3000]  eta: 0:28:46  lr: 0.000019  loss: 0.4286  time: 1.5290  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1865/3000]  eta: 0:28:39  lr: 0.000019  loss: 0.5022  time: 1.5458  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1865/3000]  eta: 0:28:39  lr: 0.000019  loss: 0.3228  time: 1.5461  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1870/3000]  eta: 0:28:32  lr: 0.000019  loss: 0.6378  time: 1.5464  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1870/3000]  eta: 0:28:31  lr: 0.000019  loss: 0.8135  time: 1.5462  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1875/3000]  eta: 0:28:24  lr: 0.000019  loss: 0.7701  time: 1.5565  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1875/3000]  eta: 0:28:24  lr: 0.000019  loss: 0.1088  time: 1.5564  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1880/3000]  eta: 0:28:17  lr: 0.000019  loss: 0.1112  time: 1.5448  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1880/3000]  eta: 0:28:16  lr: 0.000019  loss: 0.3299  time: 1.5446  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1885/3000]  eta: 0:28:09  lr: 0.000019  loss: 0.4587  time: 1.5316  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1885/3000]  eta: 0:28:09  lr: 0.000019  loss: 0.3099  time: 1.5314  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1890/3000]  eta: 0:28:01  lr: 0.000019  loss: 0.7619  time: 1.5092  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1890/3000]  eta: 0:28:01  lr: 0.000019  loss: 0.5091  time: 1.5090  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1895/3000]  eta: 0:27:54  lr: 0.000019  loss: 0.2089  time: 1.5142  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1895/3000]  eta: 0:27:54  lr: 0.000019  loss: 0.2098  time: 1.5140  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1900/3000]  eta: 0:27:46  lr: 0.000019  loss: 0.2901  time: 1.5251  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1900/3000]  eta: 0:27:46  lr: 0.000019  loss: 0.2164  time: 1.5249  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1905/3000]  eta: 0:27:39  lr: 0.000019  loss: 0.3872  time: 1.5225  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1905/3000]  eta: 0:27:39  lr: 0.000019  loss: 0.2405  time: 1.5223  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1910/3000]  eta: 0:27:31  lr: 0.000019  loss: 0.2794  time: 1.5361  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1910/3000]  eta: 0:27:31  lr: 0.000019  loss: 0.5361  time: 1.5358  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1915/3000]  eta: 0:27:24  lr: 0.000020  loss: 1.6891  time: 1.5357  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1915/3000]  eta: 0:27:24  lr: 0.000020  loss: 0.2902  time: 1.5354  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1920/3000]  eta: 0:27:16  lr: 0.000020  loss: 0.3128  time: 1.5163  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1920/3000]  eta: 0:27:16  lr: 0.000020  loss: 0.1012  time: 1.5160  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1925/3000]  eta: 0:27:08  lr: 0.000020  loss: 0.3595  time: 1.4964  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1925/3000]  eta: 0:27:08  lr: 0.000020  loss: 0.2983  time: 1.4961  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1930/3000]  eta: 0:27:01  lr: 0.000020  loss: 0.1346  time: 1.4814  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1930/3000]  eta: 0:27:00  lr: 0.000020  loss: 0.2498  time: 1.4812  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1935/3000]  eta: 0:26:53  lr: 0.000020  loss: 0.8860  time: 1.4752  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1935/3000]  eta: 0:26:53  lr: 0.000020  loss: 0.3575  time: 1.4749  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1940/3000]  eta: 0:26:46  lr: 0.000020  loss: 0.5434  time: 1.4896  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1940/3000]  eta: 0:26:45  lr: 0.000020  loss: 0.1514  time: 1.4894  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1945/3000]  eta: 0:26:38  lr: 0.000020  loss: 1.3124  time: 1.5213  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1945/3000]  eta: 0:26:38  lr: 0.000020  loss: 0.2241  time: 1.5210  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1950/3000]  eta: 0:26:31  lr: 0.000020  loss: 0.1387  time: 1.5397  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1950/3000]  eta: 0:26:30  lr: 0.000020  loss: 0.6257  time: 1.5392  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1955/3000]  eta: 0:26:23  lr: 0.000020  loss: 0.2597  time: 1.5484  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1955/3000]  eta: 0:26:23  lr: 0.000020  loss: 0.6556  time: 1.5481  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1960/3000]  eta: 0:26:15  lr: 0.000020  loss: 0.3967  time: 1.5343  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1960/3000]  eta: 0:26:15  lr: 0.000020  loss: 0.6115  time: 1.5340  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1965/3000]  eta: 0:26:08  lr: 0.000020  loss: 0.6340  time: 1.5220  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1965/3000]  eta: 0:26:08  lr: 0.000020  loss: 0.3697  time: 1.5219  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1970/3000]  eta: 0:26:00  lr: 0.000020  loss: 0.6119  time: 1.5223  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1970/3000]  eta: 0:26:00  lr: 0.000020  loss: 1.0031  time: 1.5221  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1975/3000]  eta: 0:25:53  lr: 0.000020  loss: 0.3040  time: 1.4989  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1975/3000]  eta: 0:25:52  lr: 0.000020  loss: 0.5990  time: 1.4986  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1980/3000]  eta: 0:25:45  lr: 0.000020  loss: 0.3048  time: 1.5165  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1980/3000]  eta: 0:25:45  lr: 0.000020  loss: 0.4180  time: 1.5162  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1985/3000]  eta: 0:25:38  lr: 0.000020  loss: 0.2234  time: 1.5310  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1985/3000]  eta: 0:25:37  lr: 0.000020  loss: 0.8232  time: 1.5307  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1990/3000]  eta: 0:25:30  lr: 0.000020  loss: 1.0116  time: 1.5164  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1990/3000]  eta: 0:25:30  lr: 0.000020  loss: 0.2856  time: 1.5162  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [1995/3000]  eta: 0:25:23  lr: 0.000020  loss: 0.5434  time: 1.5336  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [1995/3000]  eta: 0:25:22  lr: 0.000020  loss: 0.3112  time: 1.5335  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2000/3000]  eta: 0:25:15  lr: 0.000020  loss: 0.2668  time: 1.5403  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2000/3000]  eta: 0:25:15  lr: 0.000020  loss: 0.1031  time: 1.5400  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2005/3000]  eta: 0:25:07  lr: 0.000020  loss: 0.6364  time: 1.5182  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2005/3000]  eta: 0:25:07  lr: 0.000020  loss: 0.3869  time: 1.5180  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2010/3000]  eta: 0:25:00  lr: 0.000020  loss: 0.3839  time: 1.5174  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2010/3000]  eta: 0:25:00  lr: 0.000020  loss: 0.4928  time: 1.5172  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2015/3000]  eta: 0:24:52  lr: 0.000020  loss: 0.5069  time: 1.5111  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2015/3000]  eta: 0:24:52  lr: 0.000020  loss: 0.1966  time: 1.5107  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2020/3000]  eta: 0:24:45  lr: 0.000021  loss: 0.4771  time: 1.5002  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2020/3000]  eta: 0:24:44  lr: 0.000021  loss: 0.4639  time: 1.5000  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2025/3000]  eta: 0:24:37  lr: 0.000021  loss: 0.2655  time: 1.5158  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2025/3000]  eta: 0:24:37  lr: 0.000021  loss: 0.4367  time: 1.5155  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2030/3000]  eta: 0:24:30  lr: 0.000021  loss: 0.6822  time: 1.5307  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2030/3000]  eta: 0:24:29  lr: 0.000021  loss: 0.5221  time: 1.5304  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2035/3000]  eta: 0:24:22  lr: 0.000021  loss: 0.2612  time: 1.5423  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2035/3000]  eta: 0:24:22  lr: 0.000021  loss: 0.4299  time: 1.5421  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2040/3000]  eta: 0:24:14  lr: 0.000021  loss: 0.3440  time: 1.5187  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2040/3000]  eta: 0:24:14  lr: 0.000021  loss: 0.1113  time: 1.5186  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2045/3000]  eta: 0:24:07  lr: 0.000021  loss: 0.1306  time: 1.5100  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2045/3000]  eta: 0:24:06  lr: 0.000021  loss: 0.1350  time: 1.5097  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2050/3000]  eta: 0:23:59  lr: 0.000021  loss: 0.4441  time: 1.4986  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2050/3000]  eta: 0:23:59  lr: 0.000021  loss: 0.2881  time: 1.4984  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2055/3000]  eta: 0:23:52  lr: 0.000021  loss: 0.3390  time: 1.4746  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2055/3000]  eta: 0:23:51  lr: 0.000021  loss: 0.3814  time: 1.4744  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2060/3000]  eta: 0:23:44  lr: 0.000021  loss: 0.3500  time: 1.5160  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2060/3000]  eta: 0:23:44  lr: 0.000021  loss: 0.9984  time: 1.5157  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2065/3000]  eta: 0:23:37  lr: 0.000021  loss: 0.3688  time: 1.5324  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2065/3000]  eta: 0:23:36  lr: 0.000021  loss: 0.5609  time: 1.5322  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2070/3000]  eta: 0:23:29  lr: 0.000021  loss: 0.7610  time: 1.5288  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2070/3000]  eta: 0:23:29  lr: 0.000021  loss: 0.2781  time: 1.5286  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2075/3000]  eta: 0:23:22  lr: 0.000021  loss: 0.5751  time: 1.5578  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2075/3000]  eta: 0:23:21  lr: 0.000021  loss: 0.4353  time: 1.5575  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2080/3000]  eta: 0:23:14  lr: 0.000021  loss: 0.8981  time: 1.5528  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2080/3000]  eta: 0:23:14  lr: 0.000021  loss: 0.3166  time: 1.5527  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2085/3000]  eta: 0:23:07  lr: 0.000021  loss: 0.1843  time: 1.5415  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2085/3000]  eta: 0:23:06  lr: 0.000021  loss: 0.9352  time: 1.5413  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2090/3000]  eta: 0:22:59  lr: 0.000021  loss: 0.0603  time: 1.5530  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2090/3000]  eta: 0:22:59  lr: 0.000021  loss: 0.1558  time: 1.5527  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2095/3000]  eta: 0:22:52  lr: 0.000021  loss: 0.2205  time: 1.5464  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2095/3000]  eta: 0:22:51  lr: 0.000021  loss: 0.7002  time: 1.5461  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2100/3000]  eta: 0:22:44  lr: 0.000021  loss: 0.4123  time: 1.5441  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2100/3000]  eta: 0:22:44  lr: 0.000021  loss: 0.3631  time: 1.5436  data: 0.0000  max mem: 17768
Train: data epoch: [0]  [2105/3000]  eta: 0:22:37  lr: 0.000021  loss: 0.4856  time: 1.5514  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2105/3000]  eta: 0:22:36  lr: 0.000021  loss: 0.1758  time: 1.5511  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2110/3000]  eta: 0:22:29  lr: 0.000021  loss: 0.2028  time: 1.5567  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2110/3000]  eta: 0:22:29  lr: 0.000021  loss: 1.0849  time: 1.5565  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2115/3000]  eta: 0:22:22  lr: 0.000021  loss: 0.7812  time: 1.5660  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2115/3000]  eta: 0:22:21  lr: 0.000021  loss: 0.6068  time: 1.5658  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2120/3000]  eta: 0:22:14  lr: 0.000021  loss: 0.4191  time: 1.5608  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2120/3000]  eta: 0:22:14  lr: 0.000021  loss: 0.2964  time: 1.5606  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2125/3000]  eta: 0:22:06  lr: 0.000022  loss: 0.4125  time: 1.5484  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2125/3000]  eta: 0:22:06  lr: 0.000022  loss: 0.3454  time: 1.5482  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2130/3000]  eta: 0:21:59  lr: 0.000022  loss: 0.4229  time: 1.5439  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2130/3000]  eta: 0:21:59  lr: 0.000022  loss: 0.6629  time: 1.5437  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2135/3000]  eta: 0:21:51  lr: 0.000022  loss: 0.4657  time: 1.5393  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2135/3000]  eta: 0:21:51  lr: 0.000022  loss: 0.2273  time: 1.5391  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2140/3000]  eta: 0:21:44  lr: 0.000022  loss: 0.5779  time: 1.5309  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2140/3000]  eta: 0:21:44  lr: 0.000022  loss: 0.6183  time: 1.5307  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2145/3000]  eta: 0:21:36  lr: 0.000022  loss: 0.3499  time: 1.5138  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2145/3000]  eta: 0:21:36  lr: 0.000022  loss: 0.1921  time: 1.5136  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2150/3000]  eta: 0:21:29  lr: 0.000022  loss: 0.5083  time: 1.5178  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2150/3000]  eta: 0:21:28  lr: 0.000022  loss: 0.4129  time: 1.5175  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2155/3000]  eta: 0:21:21  lr: 0.000022  loss: 0.5063  time: 1.4749  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2155/3000]  eta: 0:21:21  lr: 0.000022  loss: 0.3944  time: 1.4747  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2160/3000]  eta: 0:21:13  lr: 0.000022  loss: 0.4840  time: 1.4765  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2160/3000]  eta: 0:21:13  lr: 0.000022  loss: 0.5241  time: 1.4763  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2165/3000]  eta: 0:21:06  lr: 0.000022  loss: 0.1537  time: 1.4785  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2165/3000]  eta: 0:21:05  lr: 0.000022  loss: 0.7398  time: 1.4782  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2170/3000]  eta: 0:20:58  lr: 0.000022  loss: 0.3891  time: 1.4731  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2170/3000]  eta: 0:20:58  lr: 0.000022  loss: 0.2124  time: 1.4729  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2175/3000]  eta: 0:20:50  lr: 0.000022  loss: 0.6104  time: 1.5143  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2175/3000]  eta: 0:20:50  lr: 0.000022  loss: 0.6303  time: 1.5142  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2180/3000]  eta: 0:20:43  lr: 0.000022  loss: 0.3705  time: 1.5203  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2180/3000]  eta: 0:20:43  lr: 0.000022  loss: 0.2341  time: 1.5202  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2185/3000]  eta: 0:20:35  lr: 0.000022  loss: 0.2176  time: 1.5423  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2185/3000]  eta: 0:20:35  lr: 0.000022  loss: 0.2964  time: 1.5421  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2190/3000]  eta: 0:20:28  lr: 0.000022  loss: 0.3371  time: 1.5186  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2190/3000]  eta: 0:20:27  lr: 0.000022  loss: 0.5212  time: 1.5184  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2195/3000]  eta: 0:20:20  lr: 0.000022  loss: 0.3088  time: 1.5141  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2195/3000]  eta: 0:20:20  lr: 0.000022  loss: 0.1734  time: 1.5138  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2200/3000]  eta: 0:20:13  lr: 0.000022  loss: 0.2063  time: 1.5129  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2200/3000]  eta: 0:20:12  lr: 0.000022  loss: 0.3371  time: 1.5126  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2205/3000]  eta: 0:20:05  lr: 0.000022  loss: 0.5359  time: 1.5163  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2205/3000]  eta: 0:20:05  lr: 0.000022  loss: 0.3192  time: 1.5160  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2210/3000]  eta: 0:19:57  lr: 0.000022  loss: 0.4813  time: 1.5385  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2210/3000]  eta: 0:19:57  lr: 0.000022  loss: 0.6589  time: 1.5389  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2215/3000]  eta: 0:19:50  lr: 0.000022  loss: 0.3183  time: 1.5159  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2215/3000]  eta: 0:19:50  lr: 0.000022  loss: 0.2468  time: 1.5157  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2220/3000]  eta: 0:19:42  lr: 0.000022  loss: 0.3571  time: 1.5171  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2220/3000]  eta: 0:19:42  lr: 0.000022  loss: 0.7140  time: 1.5168  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2225/3000]  eta: 0:19:35  lr: 0.000023  loss: 0.5319  time: 1.4951  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2225/3000]  eta: 0:19:34  lr: 0.000023  loss: 0.5804  time: 1.4949  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2230/3000]  eta: 0:19:27  lr: 0.000023  loss: 0.6974  time: 1.4871  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2230/3000]  eta: 0:19:27  lr: 0.000023  loss: 0.3093  time: 1.4870  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2235/3000]  eta: 0:19:19  lr: 0.000023  loss: 0.0875  time: 1.5008  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2235/3000]  eta: 0:19:19  lr: 0.000023  loss: 0.2662  time: 1.5007  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2240/3000]  eta: 0:19:12  lr: 0.000023  loss: 0.4990  time: 1.5145  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2240/3000]  eta: 0:19:12  lr: 0.000023  loss: 0.4258  time: 1.5143  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2245/3000]  eta: 0:19:04  lr: 0.000023  loss: 0.1778  time: 1.5155  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2245/3000]  eta: 0:19:04  lr: 0.000023  loss: 0.3044  time: 1.5152  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2250/3000]  eta: 0:18:57  lr: 0.000023  loss: 0.3048  time: 1.5019  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2250/3000]  eta: 0:18:56  lr: 0.000023  loss: 0.3966  time: 1.5017  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2255/3000]  eta: 0:18:49  lr: 0.000023  loss: 0.6438  time: 1.4964  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2255/3000]  eta: 0:18:49  lr: 0.000023  loss: 0.1961  time: 1.4973  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2260/3000]  eta: 0:18:41  lr: 0.000023  loss: 0.3335  time: 1.4834  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2260/3000]  eta: 0:18:41  lr: 0.000023  loss: 0.1686  time: 1.4831  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2265/3000]  eta: 0:18:34  lr: 0.000023  loss: 0.5149  time: 1.5126  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2265/3000]  eta: 0:18:34  lr: 0.000023  loss: 0.5046  time: 1.5123  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2270/3000]  eta: 0:18:26  lr: 0.000023  loss: 0.1164  time: 1.5257  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2270/3000]  eta: 0:18:26  lr: 0.000023  loss: 0.2061  time: 1.5254  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2275/3000]  eta: 0:18:19  lr: 0.000023  loss: 0.5215  time: 1.5335  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2275/3000]  eta: 0:18:18  lr: 0.000023  loss: 0.6496  time: 1.5321  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2280/3000]  eta: 0:18:11  lr: 0.000023  loss: 0.4193  time: 1.4954  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2280/3000]  eta: 0:18:11  lr: 0.000023  loss: 0.6104  time: 1.4952  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2285/3000]  eta: 0:18:03  lr: 0.000023  loss: 1.4124  time: 1.4709  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2285/3000]  eta: 0:18:03  lr: 0.000023  loss: 0.1751  time: 1.4706  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2290/3000]  eta: 0:17:56  lr: 0.000023  loss: 0.2583  time: 1.4875  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2290/3000]  eta: 0:17:56  lr: 0.000023  loss: 0.2450  time: 1.4871  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2295/3000]  eta: 0:17:48  lr: 0.000023  loss: 0.4478  time: 1.4798  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2295/3000]  eta: 0:17:48  lr: 0.000023  loss: 0.5870  time: 1.4795  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2300/3000]  eta: 0:17:41  lr: 0.000023  loss: 0.5932  time: 1.5144  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2300/3000]  eta: 0:17:40  lr: 0.000023  loss: 0.1771  time: 1.5140  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2305/3000]  eta: 0:17:33  lr: 0.000023  loss: 1.4499  time: 1.5378  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2305/3000]  eta: 0:17:33  lr: 0.000023  loss: 0.2037  time: 1.5375  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2310/3000]  eta: 0:17:26  lr: 0.000023  loss: 0.7244  time: 1.5358  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2310/3000]  eta: 0:17:25  lr: 0.000023  loss: 0.3626  time: 1.5355  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2315/3000]  eta: 0:17:18  lr: 0.000023  loss: 0.3271  time: 1.5486  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2315/3000]  eta: 0:17:18  lr: 0.000023  loss: 0.3213  time: 1.5482  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2320/3000]  eta: 0:17:10  lr: 0.000023  loss: 0.2097  time: 1.5497  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2320/3000]  eta: 0:17:10  lr: 0.000023  loss: 1.2175  time: 1.5495  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2325/3000]  eta: 0:17:03  lr: 0.000023  loss: 0.5110  time: 1.5156  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2325/3000]  eta: 0:17:03  lr: 0.000023  loss: 0.6095  time: 1.5153  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2330/3000]  eta: 0:16:55  lr: 0.000024  loss: 0.1146  time: 1.4905  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2330/3000]  eta: 0:16:55  lr: 0.000024  loss: 0.4091  time: 1.4902  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2335/3000]  eta: 0:16:47  lr: 0.000024  loss: 0.1510  time: 1.4697  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2335/3000]  eta: 0:16:47  lr: 0.000024  loss: 0.2893  time: 1.4695  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2340/3000]  eta: 0:16:40  lr: 0.000024  loss: 0.3135  time: 1.4634  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2340/3000]  eta: 0:16:40  lr: 0.000024  loss: 0.5978  time: 1.4631  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2345/3000]  eta: 0:16:32  lr: 0.000024  loss: 0.3461  time: 1.4868  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2345/3000]  eta: 0:16:32  lr: 0.000024  loss: 0.5284  time: 1.4865  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2350/3000]  eta: 0:16:25  lr: 0.000024  loss: 0.3862  time: 1.4818  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2350/3000]  eta: 0:16:24  lr: 0.000024  loss: 0.4624  time: 1.4816  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2355/3000]  eta: 0:16:17  lr: 0.000024  loss: 0.8524  time: 1.4837  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2355/3000]  eta: 0:16:17  lr: 0.000024  loss: 0.3524  time: 1.4835  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2360/3000]  eta: 0:16:09  lr: 0.000024  loss: 0.2887  time: 1.4926  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2360/3000]  eta: 0:16:09  lr: 0.000024  loss: 0.4347  time: 1.4924  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2365/3000]  eta: 0:16:02  lr: 0.000024  loss: 0.2778  time: 1.4972  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2365/3000]  eta: 0:16:02  lr: 0.000024  loss: 1.5835  time: 1.4970  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2370/3000]  eta: 0:15:54  lr: 0.000024  loss: 1.0080  time: 1.5273  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2370/3000]  eta: 0:15:54  lr: 0.000024  loss: 0.2294  time: 1.5271  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2375/3000]  eta: 0:15:47  lr: 0.000024  loss: 0.6796  time: 1.5345  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2375/3000]  eta: 0:15:47  lr: 0.000024  loss: 0.1876  time: 1.5343  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2380/3000]  eta: 0:15:39  lr: 0.000024  loss: 0.3274  time: 1.5112  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2380/3000]  eta: 0:15:39  lr: 0.000024  loss: 0.7109  time: 1.5110  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2385/3000]  eta: 0:15:31  lr: 0.000024  loss: 0.3392  time: 1.4985  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2385/3000]  eta: 0:15:31  lr: 0.000024  loss: 0.1915  time: 1.4982  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2390/3000]  eta: 0:15:24  lr: 0.000024  loss: 1.3311  time: 1.4961  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2390/3000]  eta: 0:15:24  lr: 0.000024  loss: 0.4339  time: 1.4958  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2395/3000]  eta: 0:15:16  lr: 0.000024  loss: 0.0917  time: 1.5021  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2395/3000]  eta: 0:15:16  lr: 0.000024  loss: 0.6372  time: 1.5019  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2400/3000]  eta: 0:15:09  lr: 0.000024  loss: 0.3223  time: 1.5365  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2400/3000]  eta: 0:15:09  lr: 0.000024  loss: 0.2284  time: 1.5363  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2405/3000]  eta: 0:15:01  lr: 0.000024  loss: 0.4908  time: 1.5364  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2405/3000]  eta: 0:15:01  lr: 0.000024  loss: 0.3898  time: 1.5362  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2410/3000]  eta: 0:14:54  lr: 0.000024  loss: 0.3571  time: 1.5287  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2410/3000]  eta: 0:14:54  lr: 0.000024  loss: 0.1053  time: 1.5285  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2415/3000]  eta: 0:14:46  lr: 0.000024  loss: 0.3186  time: 1.5308  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2415/3000]  eta: 0:14:46  lr: 0.000024  loss: 0.7171  time: 1.5305  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2420/3000]  eta: 0:14:39  lr: 0.000024  loss: 0.6041  time: 1.5327  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2420/3000]  eta: 0:14:38  lr: 0.000024  loss: 0.4339  time: 1.5324  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2425/3000]  eta: 0:14:31  lr: 0.000024  loss: 0.2790  time: 1.5415  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2425/3000]  eta: 0:14:31  lr: 0.000024  loss: 0.6766  time: 1.5413  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2430/3000]  eta: 0:14:24  lr: 0.000024  loss: 0.6189  time: 1.5397  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2430/3000]  eta: 0:14:23  lr: 0.000024  loss: 0.2582  time: 1.5394  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2435/3000]  eta: 0:14:16  lr: 0.000025  loss: 0.2523  time: 1.5401  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2435/3000]  eta: 0:14:16  lr: 0.000025  loss: 0.1257  time: 1.5398  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2440/3000]  eta: 0:14:08  lr: 0.000025  loss: 0.5050  time: 1.5223  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2440/3000]  eta: 0:14:08  lr: 0.000025  loss: 0.6618  time: 1.5220  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2445/3000]  eta: 0:14:01  lr: 0.000025  loss: 0.7744  time: 1.4960  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2445/3000]  eta: 0:14:01  lr: 0.000025  loss: 0.6507  time: 1.4958  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2450/3000]  eta: 0:13:53  lr: 0.000025  loss: 0.1842  time: 1.4929  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2450/3000]  eta: 0:13:53  lr: 0.000025  loss: 0.4984  time: 1.4927  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2455/3000]  eta: 0:13:45  lr: 0.000025  loss: 0.2880  time: 1.4856  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2455/3000]  eta: 0:13:45  lr: 0.000025  loss: 0.4063  time: 1.4854  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2460/3000]  eta: 0:13:38  lr: 0.000025  loss: 0.3785  time: 1.4838  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2460/3000]  eta: 0:13:38  lr: 0.000025  loss: 0.2830  time: 1.4835  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2465/3000]  eta: 0:13:30  lr: 0.000025  loss: 0.3257  time: 1.4738  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2465/3000]  eta: 0:13:30  lr: 0.000025  loss: 0.3247  time: 1.4736  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2470/3000]  eta: 0:13:23  lr: 0.000025  loss: 0.4253  time: 1.4763  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2470/3000]  eta: 0:13:22  lr: 0.000025  loss: 0.2693  time: 1.4761  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2475/3000]  eta: 0:13:15  lr: 0.000025  loss: 0.2486  time: 1.4861  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2475/3000]  eta: 0:13:15  lr: 0.000025  loss: 0.2703  time: 1.4858  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2480/3000]  eta: 0:13:08  lr: 0.000025  loss: 0.2251  time: 1.4963  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2480/3000]  eta: 0:13:07  lr: 0.000025  loss: 0.2732  time: 1.4961  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2485/3000]  eta: 0:13:00  lr: 0.000025  loss: 0.1507  time: 1.5237  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2485/3000]  eta: 0:13:00  lr: 0.000025  loss: 0.6081  time: 1.5234  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2490/3000]  eta: 0:12:52  lr: 0.000025  loss: 0.2492  time: 1.5244  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2490/3000]  eta: 0:12:52  lr: 0.000025  loss: 0.4926  time: 1.5241  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2495/3000]  eta: 0:12:45  lr: 0.000025  loss: 0.3812  time: 1.5259  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2495/3000]  eta: 0:12:45  lr: 0.000025  loss: 0.5756  time: 1.5257  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2500/3000]  eta: 0:12:37  lr: 0.000025  loss: 0.1218  time: 1.5246  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2500/3000]  eta: 0:12:37  lr: 0.000025  loss: 0.5106  time: 1.5244  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2505/3000]  eta: 0:12:30  lr: 0.000025  loss: 0.1957  time: 1.4979  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2505/3000]  eta: 0:12:29  lr: 0.000025  loss: 0.0868  time: 1.4978  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2510/3000]  eta: 0:12:22  lr: 0.000025  loss: 0.7029  time: 1.5007  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2510/3000]  eta: 0:12:22  lr: 0.000025  loss: 0.9120  time: 1.5005  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2515/3000]  eta: 0:12:14  lr: 0.000025  loss: 0.4956  time: 1.4859  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2515/3000]  eta: 0:12:14  lr: 0.000025  loss: 0.4878  time: 1.4857  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2520/3000]  eta: 0:12:07  lr: 0.000025  loss: 0.2100  time: 1.4815  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2520/3000]  eta: 0:12:07  lr: 0.000025  loss: 1.1052  time: 1.4813  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2525/3000]  eta: 0:11:59  lr: 0.000025  loss: 0.1939  time: 1.5029  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2525/3000]  eta: 0:11:59  lr: 0.000025  loss: 0.8396  time: 1.5026  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2530/3000]  eta: 0:11:52  lr: 0.000025  loss: 0.3091  time: 1.4941  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2530/3000]  eta: 0:11:51  lr: 0.000025  loss: 0.7933  time: 1.4939  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2535/3000]  eta: 0:11:44  lr: 0.000026  loss: 0.1292  time: 1.5065  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2535/3000]  eta: 0:11:44  lr: 0.000026  loss: 0.4508  time: 1.5062  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2540/3000]  eta: 0:11:36  lr: 0.000026  loss: 0.3559  time: 1.5075  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2540/3000]  eta: 0:11:36  lr: 0.000026  loss: 0.2342  time: 1.5073  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2545/3000]  eta: 0:11:29  lr: 0.000026  loss: 0.3287  time: 1.5048  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2545/3000]  eta: 0:11:29  lr: 0.000026  loss: 0.2565  time: 1.5046  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2550/3000]  eta: 0:11:21  lr: 0.000026  loss: 0.2387  time: 1.5097  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2550/3000]  eta: 0:11:21  lr: 0.000026  loss: 0.3273  time: 1.5095  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2555/3000]  eta: 0:11:14  lr: 0.000026  loss: 0.4073  time: 1.4982  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2555/3000]  eta: 0:11:14  lr: 0.000026  loss: 0.3346  time: 1.4980  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2560/3000]  eta: 0:11:06  lr: 0.000026  loss: 0.3441  time: 1.4759  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2560/3000]  eta: 0:11:06  lr: 0.000026  loss: 0.3392  time: 1.4756  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2565/3000]  eta: 0:10:58  lr: 0.000026  loss: 0.8160  time: 1.4753  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2565/3000]  eta: 0:10:58  lr: 0.000026  loss: 0.3134  time: 1.4751  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2570/3000]  eta: 0:10:51  lr: 0.000026  loss: 0.4053  time: 1.4741  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2570/3000]  eta: 0:10:51  lr: 0.000026  loss: 0.9824  time: 1.4739  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2575/3000]  eta: 0:10:43  lr: 0.000026  loss: 0.1669  time: 1.4658  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2575/3000]  eta: 0:10:43  lr: 0.000026  loss: 0.4312  time: 1.4655  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2580/3000]  eta: 0:10:36  lr: 0.000026  loss: 0.9204  time: 1.4907  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2580/3000]  eta: 0:10:36  lr: 0.000026  loss: 0.3595  time: 1.4904  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2585/3000]  eta: 0:10:28  lr: 0.000026  loss: 1.2147  time: 1.4972  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2585/3000]  eta: 0:10:28  lr: 0.000026  loss: 0.4226  time: 1.4970  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2590/3000]  eta: 0:10:20  lr: 0.000026  loss: 0.8656  time: 1.4881  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2590/3000]  eta: 0:10:20  lr: 0.000026  loss: 0.5157  time: 1.4878  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2595/3000]  eta: 0:10:13  lr: 0.000026  loss: 1.0629  time: 1.5041  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2595/3000]  eta: 0:10:13  lr: 0.000026  loss: 0.5019  time: 1.5037  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2600/3000]  eta: 0:10:05  lr: 0.000026  loss: 0.1330  time: 1.5114  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2600/3000]  eta: 0:10:05  lr: 0.000026  loss: 0.5024  time: 1.5110  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2605/3000]  eta: 0:09:58  lr: 0.000026  loss: 0.2793  time: 1.5128  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2605/3000]  eta: 0:09:58  lr: 0.000026  loss: 0.2964  time: 1.5123  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2610/3000]  eta: 0:09:50  lr: 0.000026  loss: 0.1662  time: 1.5264  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2610/3000]  eta: 0:09:50  lr: 0.000026  loss: 0.2523  time: 1.5260  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2615/3000]  eta: 0:09:43  lr: 0.000026  loss: 0.4685  time: 1.5112  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2615/3000]  eta: 0:09:42  lr: 0.000026  loss: 0.2018  time: 1.5110  data: 0.0000  max mem: 17871
Train: data epoch: [0]  [2620/3000]  eta: 0:09:35  lr: 0.000026  loss: 0.1580  time: 1.5101  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2620/3000]  eta: 0:09:35  lr: 0.000026  loss: 0.3064  time: 1.5098  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2625/3000]  eta: 0:09:28  lr: 0.000026  loss: 0.3385  time: 1.5271  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2625/3000]  eta: 0:09:27  lr: 0.000026  loss: 0.1291  time: 1.5269  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2630/3000]  eta: 0:09:20  lr: 0.000026  loss: 0.1677  time: 1.5426  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2630/3000]  eta: 0:09:20  lr: 0.000026  loss: 0.2525  time: 1.5423  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2635/3000]  eta: 0:09:12  lr: 0.000026  loss: 0.7287  time: 1.5412  data: 0.0000  max mem: 17924Train: data epoch: [0]  [2635/3000]  eta: 0:09:12  lr: 0.000026  loss: 0.2349  time: 1.5409  data: 0.0000  max mem: 18051

Train: data epoch: [0]  [2640/3000]  eta: 0:09:05  lr: 0.000027  loss: 0.1876  time: 1.5326  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2640/3000]  eta: 0:09:05  lr: 0.000027  loss: 0.4864  time: 1.5324  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2645/3000]  eta: 0:08:57  lr: 0.000027  loss: 0.2044  time: 1.5010  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2645/3000]  eta: 0:08:57  lr: 0.000027  loss: 0.3297  time: 1.5007  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2650/3000]  eta: 0:08:50  lr: 0.000027  loss: 0.3494  time: 1.5030  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2650/3000]  eta: 0:08:50  lr: 0.000027  loss: 0.4115  time: 1.5028  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2655/3000]  eta: 0:08:42  lr: 0.000027  loss: 0.3797  time: 1.5399  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2655/3000]  eta: 0:08:42  lr: 0.000027  loss: 0.1110  time: 1.5397  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2660/3000]  eta: 0:08:35  lr: 0.000027  loss: 0.3994  time: 1.5282  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2660/3000]  eta: 0:08:34  lr: 0.000027  loss: 0.3956  time: 1.5279  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2665/3000]  eta: 0:08:27  lr: 0.000027  loss: 0.6473  time: 1.5379  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2665/3000]  eta: 0:08:27  lr: 0.000027  loss: 0.4322  time: 1.5377  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2670/3000]  eta: 0:08:19  lr: 0.000027  loss: 0.5675  time: 1.5320  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2670/3000]  eta: 0:08:19  lr: 0.000027  loss: 0.5862  time: 1.5318  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2675/3000]  eta: 0:08:12  lr: 0.000027  loss: 0.1543  time: 1.5058  data: 0.0000  max mem: 17924
Train: data epoch: [0]  [2675/3000]  eta: 0:08:12  lr: 0.000027  loss: 0.3369  time: 1.5056  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2680/3000]  eta: 0:08:04  lr: 0.000027  loss: 0.4554  time: 1.5211  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2680/3000]  eta: 0:08:04  lr: 0.000027  loss: 0.3632  time: 1.5208  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2685/3000]  eta: 0:07:57  lr: 0.000027  loss: 0.5871  time: 1.5423  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2685/3000]  eta: 0:07:57  lr: 0.000027  loss: 0.2689  time: 1.5420  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2690/3000]  eta: 0:07:49  lr: 0.000027  loss: 0.4915  time: 1.5297  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2690/3000]  eta: 0:07:49  lr: 0.000027  loss: 0.4111  time: 1.5293  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2695/3000]  eta: 0:07:42  lr: 0.000027  loss: 1.3278  time: 1.5235  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2695/3000]  eta: 0:07:41  lr: 0.000027  loss: 0.3275  time: 1.5232  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2700/3000]  eta: 0:07:34  lr: 0.000027  loss: 0.3426  time: 1.5097  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2700/3000]  eta: 0:07:34  lr: 0.000027  loss: 0.3186  time: 1.5094  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2705/3000]  eta: 0:07:26  lr: 0.000027  loss: 0.9544  time: 1.4757  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2705/3000]  eta: 0:07:26  lr: 0.000027  loss: 0.3781  time: 1.4754  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2710/3000]  eta: 0:07:19  lr: 0.000027  loss: 0.2153  time: 1.4818  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2710/3000]  eta: 0:07:19  lr: 0.000027  loss: 0.6694  time: 1.4815  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2715/3000]  eta: 0:07:11  lr: 0.000027  loss: 0.5748  time: 1.4847  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2715/3000]  eta: 0:07:11  lr: 0.000027  loss: 0.6040  time: 1.4844  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2720/3000]  eta: 0:07:04  lr: 0.000027  loss: 0.7715  time: 1.4791  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2720/3000]  eta: 0:07:03  lr: 0.000027  loss: 0.2064  time: 1.4789  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2725/3000]  eta: 0:06:56  lr: 0.000027  loss: 0.1637  time: 1.4730  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2725/3000]  eta: 0:06:56  lr: 0.000027  loss: 0.2270  time: 1.4728  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2730/3000]  eta: 0:06:48  lr: 0.000027  loss: 0.3160  time: 1.4536  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2730/3000]  eta: 0:06:48  lr: 0.000027  loss: 0.2467  time: 1.4534  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2735/3000]  eta: 0:06:41  lr: 0.000027  loss: 0.1677  time: 1.4651  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2735/3000]  eta: 0:06:41  lr: 0.000027  loss: 0.3300  time: 1.4649  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2740/3000]  eta: 0:06:33  lr: 0.000027  loss: 0.5401  time: 1.4782  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2740/3000]  eta: 0:06:33  lr: 0.000027  loss: 0.5192  time: 1.4780  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2745/3000]  eta: 0:06:26  lr: 0.000028  loss: 1.3790  time: 1.4978  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2745/3000]  eta: 0:06:26  lr: 0.000028  loss: 0.2902  time: 1.4976  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2750/3000]  eta: 0:06:18  lr: 0.000028  loss: 0.5795  time: 1.5130  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2750/3000]  eta: 0:06:18  lr: 0.000028  loss: 1.0101  time: 1.5127  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2755/3000]  eta: 0:06:11  lr: 0.000028  loss: 0.2788  time: 1.5234  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2755/3000]  eta: 0:06:10  lr: 0.000028  loss: 0.3012  time: 1.5231  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2760/3000]  eta: 0:06:03  lr: 0.000028  loss: 0.4029  time: 1.5072  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2760/3000]  eta: 0:06:03  lr: 0.000028  loss: 0.1782  time: 1.5069  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2765/3000]  eta: 0:05:55  lr: 0.000028  loss: 0.7733  time: 1.5059  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2765/3000]  eta: 0:05:55  lr: 0.000028  loss: 0.2358  time: 1.5055  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2770/3000]  eta: 0:05:48  lr: 0.000028  loss: 0.2889  time: 1.5036  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2770/3000]  eta: 0:05:48  lr: 0.000028  loss: 0.3580  time: 1.5032  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2775/3000]  eta: 0:05:40  lr: 0.000028  loss: 0.5192  time: 1.4720  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2775/3000]  eta: 0:05:40  lr: 0.000028  loss: 0.0400  time: 1.4716  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2780/3000]  eta: 0:05:33  lr: 0.000028  loss: 0.2131  time: 1.4924  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2780/3000]  eta: 0:05:33  lr: 0.000028  loss: 0.4088  time: 1.4920  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2785/3000]  eta: 0:05:25  lr: 0.000028  loss: 0.3721  time: 1.4810  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2785/3000]  eta: 0:05:25  lr: 0.000028  loss: 0.3031  time: 1.4809  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2790/3000]  eta: 0:05:17  lr: 0.000028  loss: 0.2969  time: 1.4875  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2790/3000]  eta: 0:05:17  lr: 0.000028  loss: 0.2504  time: 1.4873  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2795/3000]  eta: 0:05:10  lr: 0.000028  loss: 1.1660  time: 1.4880  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2795/3000]  eta: 0:05:10  lr: 0.000028  loss: 0.2073  time: 1.4877  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2800/3000]  eta: 0:05:02  lr: 0.000028  loss: 0.1850  time: 1.4870  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2800/3000]  eta: 0:05:02  lr: 0.000028  loss: 0.2505  time: 1.4867  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2805/3000]  eta: 0:04:55  lr: 0.000028  loss: 0.4971  time: 1.4943  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2805/3000]  eta: 0:04:55  lr: 0.000028  loss: 1.1601  time: 1.4940  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2810/3000]  eta: 0:04:47  lr: 0.000028  loss: 0.5727  time: 1.4582  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2810/3000]  eta: 0:04:47  lr: 0.000028  loss: 0.3821  time: 1.4579  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2815/3000]  eta: 0:04:40  lr: 0.000028  loss: 0.5000  time: 1.4637  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2815/3000]  eta: 0:04:39  lr: 0.000028  loss: 0.5642  time: 1.4635  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2820/3000]  eta: 0:04:32  lr: 0.000028  loss: 0.5100  time: 1.4738  data: 0.0000  max mem: 18051Train: data epoch: [0]  [2820/3000]  eta: 0:04:32  lr: 0.000028  loss: 0.4310  time: 1.4741  data: 0.0000  max mem: 18432

Train: data epoch: [0]  [2825/3000]  eta: 0:04:24  lr: 0.000028  loss: 0.4178  time: 1.4731  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2825/3000]  eta: 0:04:24  lr: 0.000028  loss: 0.4702  time: 1.4729  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2830/3000]  eta: 0:04:17  lr: 0.000028  loss: 0.4670  time: 1.5089  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2830/3000]  eta: 0:04:17  lr: 0.000028  loss: 0.5257  time: 1.5087  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2835/3000]  eta: 0:04:09  lr: 0.000028  loss: 0.4099  time: 1.5374  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2835/3000]  eta: 0:04:09  lr: 0.000028  loss: 0.8157  time: 1.5371  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2840/3000]  eta: 0:04:02  lr: 0.000028  loss: 0.3617  time: 1.5101  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2840/3000]  eta: 0:04:02  lr: 0.000028  loss: 0.2386  time: 1.5099  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2845/3000]  eta: 0:03:54  lr: 0.000029  loss: 0.6171  time: 1.5187  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2845/3000]  eta: 0:03:54  lr: 0.000029  loss: 0.2324  time: 1.5185  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2850/3000]  eta: 0:03:47  lr: 0.000029  loss: 0.9051  time: 1.5161  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2850/3000]  eta: 0:03:46  lr: 0.000029  loss: 0.2480  time: 1.5159  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2855/3000]  eta: 0:03:39  lr: 0.000029  loss: 0.2285  time: 1.4895  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2855/3000]  eta: 0:03:39  lr: 0.000029  loss: 0.0705  time: 1.4892  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2860/3000]  eta: 0:03:31  lr: 0.000029  loss: 0.7152  time: 1.5049  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2860/3000]  eta: 0:03:31  lr: 0.000029  loss: 0.7546  time: 1.5047  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2865/3000]  eta: 0:03:24  lr: 0.000029  loss: 0.3400  time: 1.5038  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2865/3000]  eta: 0:03:24  lr: 0.000029  loss: 0.8658  time: 1.5035  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2870/3000]  eta: 0:03:16  lr: 0.000029  loss: 1.2195  time: 1.5031  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2870/3000]  eta: 0:03:16  lr: 0.000029  loss: 0.1377  time: 1.5029  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2875/3000]  eta: 0:03:09  lr: 0.000029  loss: 0.7314  time: 1.5067  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2875/3000]  eta: 0:03:09  lr: 0.000029  loss: 0.1108  time: 1.5065  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2880/3000]  eta: 0:03:01  lr: 0.000029  loss: 0.2965  time: 1.5076  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2880/3000]  eta: 0:03:01  lr: 0.000029  loss: 0.3534  time: 1.5071  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2885/3000]  eta: 0:02:54  lr: 0.000029  loss: 0.2802  time: 1.5212  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2885/3000]  eta: 0:02:54  lr: 0.000029  loss: 0.3436  time: 1.5208  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2890/3000]  eta: 0:02:46  lr: 0.000029  loss: 0.5618  time: 1.5241  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2890/3000]  eta: 0:02:46  lr: 0.000029  loss: 0.2320  time: 1.5238  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2895/3000]  eta: 0:02:38  lr: 0.000029  loss: 0.3723  time: 1.5422  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2895/3000]  eta: 0:02:38  lr: 0.000029  loss: 0.5959  time: 1.5418  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2900/3000]  eta: 0:02:31  lr: 0.000029  loss: 0.1492  time: 1.5350  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2900/3000]  eta: 0:02:31  lr: 0.000029  loss: 0.3531  time: 1.5348  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2905/3000]  eta: 0:02:23  lr: 0.000029  loss: 0.1893  time: 1.5293  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2905/3000]  eta: 0:02:23  lr: 0.000029  loss: 0.4225  time: 1.5291  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2910/3000]  eta: 0:02:16  lr: 0.000029  loss: 0.4862  time: 1.5283  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2910/3000]  eta: 0:02:16  lr: 0.000029  loss: 0.2581  time: 1.5281  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2915/3000]  eta: 0:02:08  lr: 0.000029  loss: 0.4140  time: 1.5134  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2915/3000]  eta: 0:02:08  lr: 0.000029  loss: 0.1606  time: 1.5132  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2920/3000]  eta: 0:02:01  lr: 0.000029  loss: 0.4970  time: 1.5256  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2920/3000]  eta: 0:02:01  lr: 0.000029  loss: 0.3694  time: 1.5253  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2925/3000]  eta: 0:01:53  lr: 0.000029  loss: 0.4726  time: 1.5235  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2925/3000]  eta: 0:01:53  lr: 0.000029  loss: 0.5602  time: 1.5232  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2930/3000]  eta: 0:01:45  lr: 0.000029  loss: 0.2932  time: 1.5161  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2930/3000]  eta: 0:01:45  lr: 0.000029  loss: 0.6314  time: 1.5158  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2935/3000]  eta: 0:01:38  lr: 0.000029  loss: 0.8007  time: 1.5344  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2935/3000]  eta: 0:01:38  lr: 0.000029  loss: 0.3479  time: 1.5341  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2940/3000]  eta: 0:01:30  lr: 0.000029  loss: 0.3287  time: 1.5332  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2940/3000]  eta: 0:01:30  lr: 0.000029  loss: 0.1670  time: 1.5329  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2945/3000]  eta: 0:01:23  lr: 0.000029  loss: 0.3937  time: 1.5364  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2945/3000]  eta: 0:01:23  lr: 0.000029  loss: 0.3314  time: 1.5362  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2950/3000]  eta: 0:01:15  lr: 0.000030  loss: 0.3945  time: 1.5120  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2950/3000]  eta: 0:01:15  lr: 0.000030  loss: 0.4718  time: 1.5117  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2955/3000]  eta: 0:01:08  lr: 0.000030  loss: 0.7927  time: 1.5008  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2955/3000]  eta: 0:01:08  lr: 0.000030  loss: 0.3534  time: 1.5006  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2960/3000]  eta: 0:01:00  lr: 0.000030  loss: 0.5367  time: 1.5069  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2960/3000]  eta: 0:01:00  lr: 0.000030  loss: 0.4141  time: 1.5067  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2965/3000]  eta: 0:00:52  lr: 0.000030  loss: 0.2960  time: 1.4859  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2965/3000]  eta: 0:00:52  lr: 0.000030  loss: 0.1900  time: 1.4856  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2970/3000]  eta: 0:00:45  lr: 0.000030  loss: 0.3364  time: 1.5044  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2970/3000]  eta: 0:00:45  lr: 0.000030  loss: 0.2053  time: 1.5042  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2975/3000]  eta: 0:00:37  lr: 0.000030  loss: 0.1351  time: 1.5034  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2975/3000]  eta: 0:00:37  lr: 0.000030  loss: 0.5107  time: 1.5032  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2980/3000]  eta: 0:00:30  lr: 0.000030  loss: 0.6627  time: 1.5032  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2980/3000]  eta: 0:00:30  lr: 0.000030  loss: 1.3407  time: 1.5030  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2985/3000]  eta: 0:00:22  lr: 0.000030  loss: 0.1994  time: 1.5162  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2985/3000]  eta: 0:00:22  lr: 0.000030  loss: 0.4257  time: 1.5159  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2990/3000]  eta: 0:00:15  lr: 0.000030  loss: 0.4595  time: 1.5315  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2990/3000]  eta: 0:00:15  lr: 0.000030  loss: 0.1507  time: 1.5313  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2995/3000]  eta: 0:00:07  lr: 0.000030  loss: 0.6939  time: 1.5129  data: 0.0000  max mem: 18432
Train: data epoch: [0]  [2995/3000]  eta: 0:00:07  lr: 0.000030  loss: 0.1449  time: 1.5125  data: 0.0000  max mem: 18051
Train: data epoch: [0]  [2999/3000]  eta: 0:00:01  lr: 0.000030  loss: 0.4657  time: 1.4979  data: 0.0000  max mem: 18432
Train: data epoch: [0] Total time: 1:15:41 (1.5138 s / it)
Train: data epoch: [0]  [2999/3000]  eta: 0:00:01  lr: 0.000030  loss: 0.4207  time: 1.4974  data: 0.0000  max mem: 18051
Train: data epoch: [0] Total time: 1:15:41 (1.5137 s / it)
2025-01-19 00:56:20,532 [INFO] Averaged stats: lr: 0.0000  loss: 0.5030
2025-01-19 00:56:20,538 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:179: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.use_amp):
/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py:179: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.use_amp):
Eval: data epoch: [0]  [0/1]  eta: 0:00:00    time: 0.8992  data: 0.5557  max mem: 18051
Eval: data epoch: [0]  [0/1]  eta: 0:00:00    time: 0.9785  data: 0.6176  max mem: 18432
Eval: data epoch: [0] Total time: 0:00:01 (1.0097 s / it)
Eval: data epoch: [0] Total time: 0:00:01 (1.1048 s / it)
2025-01-19 00:56:21,669 [INFO] Saving checkpoint at epoch 0 to outputs_stage1_only/202501182338/checkpoint_best.pth.
2025-01-19 00:56:24,050 [INFO] Saving checkpoint at epoch 0 to outputs_stage1_only/202501182338/checkpoint_0.pth.
2025-01-19 00:56:26,400 [INFO] Training Phase
2025-01-19 00:56:26,408 [INFO] Start training epoch 1, 3000 iters per inner epoch.
Train: data epoch: [1]  [   0/3000]  eta: 1:21:46  lr: 0.000030  loss: 0.1897  time: 1.6355  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [   0/3000]  eta: 1:21:46  lr: 0.000030  loss: 0.2074  time: 1.6354  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [   5/3000]  eta: 1:18:05  lr: 0.000030  loss: 0.8319  time: 1.5643  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [   5/3000]  eta: 1:18:04  lr: 0.000030  loss: 0.7399  time: 1.5640  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [  10/3000]  eta: 1:17:48  lr: 0.000030  loss: 0.4704  time: 1.5614  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [  10/3000]  eta: 1:17:46  lr: 0.000030  loss: 0.2589  time: 1.5608  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [  15/3000]  eta: 1:17:15  lr: 0.000030  loss: 0.2745  time: 1.5529  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [  15/3000]  eta: 1:17:13  lr: 0.000030  loss: 0.4374  time: 1.5524  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [  20/3000]  eta: 1:16:32  lr: 0.000030  loss: 0.7753  time: 1.5363  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [  20/3000]  eta: 1:16:31  lr: 0.000030  loss: 0.5729  time: 1.5359  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [  25/3000]  eta: 1:16:27  lr: 0.000030  loss: 0.1771  time: 1.5353  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [  25/3000]  eta: 1:16:26  lr: 0.000030  loss: 0.4843  time: 1.5348  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [  30/3000]  eta: 1:16:14  lr: 0.000030  loss: 0.2966  time: 1.5287  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [  30/3000]  eta: 1:16:13  lr: 0.000030  loss: 0.5209  time: 1.5284  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [  35/3000]  eta: 1:16:04  lr: 0.000030  loss: 0.8915  time: 1.5288  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [  35/3000]  eta: 1:16:03  lr: 0.000030  loss: 0.3592  time: 1.5285  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [  40/3000]  eta: 1:15:32  lr: 0.000030  loss: 0.4127  time: 1.5207  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [  40/3000]  eta: 1:15:30  lr: 0.000030  loss: 0.0777  time: 1.5203  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [  45/3000]  eta: 1:15:26  lr: 0.000030  loss: 0.6653  time: 1.5186  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [  45/3000]  eta: 1:15:25  lr: 0.000030  loss: 0.4126  time: 1.5183  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [  50/3000]  eta: 1:15:26  lr: 0.000030  loss: 0.5581  time: 1.5250  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [  50/3000]  eta: 1:15:25  lr: 0.000030  loss: 0.3202  time: 1.5247  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [  55/3000]  eta: 1:15:18  lr: 0.000030  loss: 0.7572  time: 1.5251  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [  55/3000]  eta: 1:15:17  lr: 0.000030  loss: 0.4799  time: 1.5249  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [  60/3000]  eta: 1:15:18  lr: 0.000030  loss: 0.4726  time: 1.5489  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [  60/3000]  eta: 1:15:17  lr: 0.000030  loss: 0.2371  time: 1.5487  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [  65/3000]  eta: 1:14:57  lr: 0.000030  loss: 0.1971  time: 1.5333  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [  65/3000]  eta: 1:14:55  lr: 0.000030  loss: 1.1825  time: 1.5320  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [  70/3000]  eta: 1:14:50  lr: 0.000030  loss: 0.5292  time: 1.5288  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [  70/3000]  eta: 1:14:49  lr: 0.000030  loss: 1.1402  time: 1.5275  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [  75/3000]  eta: 1:14:41  lr: 0.000030  loss: 0.2203  time: 1.5262  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [  75/3000]  eta: 1:14:40  lr: 0.000030  loss: 0.1061  time: 1.5249  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [  80/3000]  eta: 1:14:19  lr: 0.000030  loss: 0.4859  time: 1.4976  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [  80/3000]  eta: 1:14:17  lr: 0.000030  loss: 0.1721  time: 1.4964  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [  85/3000]  eta: 1:14:04  lr: 0.000030  loss: 0.3553  time: 1.4997  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [  85/3000]  eta: 1:14:02  lr: 0.000030  loss: 0.1951  time: 1.4994  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [  90/3000]  eta: 1:13:50  lr: 0.000030  loss: 0.1451  time: 1.4855  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [  90/3000]  eta: 1:13:48  lr: 0.000030  loss: 0.7066  time: 1.4853  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [  95/3000]  eta: 1:13:42  lr: 0.000030  loss: 0.1526  time: 1.4857  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [  95/3000]  eta: 1:13:41  lr: 0.000030  loss: 0.2924  time: 1.4855  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 100/3000]  eta: 1:13:39  lr: 0.000030  loss: 0.4864  time: 1.5099  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 100/3000]  eta: 1:13:37  lr: 0.000030  loss: 0.7986  time: 1.5096  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 105/3000]  eta: 1:13:32  lr: 0.000030  loss: 0.2387  time: 1.5218  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 105/3000]  eta: 1:13:30  lr: 0.000030  loss: 0.3388  time: 1.5215  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 110/3000]  eta: 1:13:29  lr: 0.000030  loss: 0.2652  time: 1.5419  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 110/3000]  eta: 1:13:28  lr: 0.000030  loss: 0.7940  time: 1.5416  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 115/3000]  eta: 1:13:21  lr: 0.000030  loss: 0.1717  time: 1.5404  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 115/3000]  eta: 1:13:19  lr: 0.000030  loss: 1.5334  time: 1.5401  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 120/3000]  eta: 1:13:17  lr: 0.000030  loss: 0.5316  time: 1.5431  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 120/3000]  eta: 1:13:16  lr: 0.000030  loss: 0.0668  time: 1.5428  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 125/3000]  eta: 1:13:09  lr: 0.000030  loss: 0.5769  time: 1.5405  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 125/3000]  eta: 1:13:08  lr: 0.000030  loss: 0.3506  time: 1.5403  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 130/3000]  eta: 1:13:03  lr: 0.000030  loss: 0.1089  time: 1.5349  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 130/3000]  eta: 1:13:01  lr: 0.000030  loss: 0.4791  time: 1.5347  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 135/3000]  eta: 1:12:57  lr: 0.000030  loss: 0.6384  time: 1.5409  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 135/3000]  eta: 1:12:56  lr: 0.000030  loss: 0.2759  time: 1.5406  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 140/3000]  eta: 1:12:53  lr: 0.000030  loss: 0.2234  time: 1.5435  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 140/3000]  eta: 1:12:52  lr: 0.000030  loss: 0.3904  time: 1.5432  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 145/3000]  eta: 1:12:43  lr: 0.000030  loss: 0.4561  time: 1.5398  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 145/3000]  eta: 1:12:42  lr: 0.000030  loss: 0.2405  time: 1.5396  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 150/3000]  eta: 1:12:37  lr: 0.000030  loss: 0.4918  time: 1.5402  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 150/3000]  eta: 1:12:36  lr: 0.000030  loss: 0.2276  time: 1.5399  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 155/3000]  eta: 1:12:27  lr: 0.000030  loss: 0.5085  time: 1.5298  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 155/3000]  eta: 1:12:26  lr: 0.000030  loss: 0.2129  time: 1.5296  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 160/3000]  eta: 1:12:15  lr: 0.000030  loss: 0.2923  time: 1.5065  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 160/3000]  eta: 1:12:14  lr: 0.000030  loss: 0.5814  time: 1.5063  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 165/3000]  eta: 1:12:06  lr: 0.000030  loss: 0.4682  time: 1.5093  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 165/3000]  eta: 1:12:05  lr: 0.000030  loss: 0.2791  time: 1.5091  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 170/3000]  eta: 1:11:56  lr: 0.000030  loss: 0.2392  time: 1.4985  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 170/3000]  eta: 1:11:55  lr: 0.000030  loss: 0.4142  time: 1.4982  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 175/3000]  eta: 1:11:54  lr: 0.000030  loss: 0.7339  time: 1.5201  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 175/3000]  eta: 1:11:53  lr: 0.000030  loss: 0.6026  time: 1.5199  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 180/3000]  eta: 1:11:39  lr: 0.000030  loss: 0.2239  time: 1.5094  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 180/3000]  eta: 1:11:38  lr: 0.000030  loss: 0.2590  time: 1.5092  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 185/3000]  eta: 1:11:31  lr: 0.000030  loss: 0.4190  time: 1.5108  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 185/3000]  eta: 1:11:30  lr: 0.000030  loss: 0.2090  time: 1.5105  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 190/3000]  eta: 1:11:17  lr: 0.000030  loss: 0.6470  time: 1.4946  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 190/3000]  eta: 1:11:16  lr: 0.000030  loss: 0.5953  time: 1.4944  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 195/3000]  eta: 1:11:03  lr: 0.000030  loss: 0.3379  time: 1.4569  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 195/3000]  eta: 1:11:02  lr: 0.000030  loss: 0.1895  time: 1.4566  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 200/3000]  eta: 1:10:56  lr: 0.000030  loss: 0.3265  time: 1.4814  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 200/3000]  eta: 1:10:55  lr: 0.000030  loss: 0.3400  time: 1.4811  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 205/3000]  eta: 1:10:50  lr: 0.000030  loss: 0.4546  time: 1.4841  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 205/3000]  eta: 1:10:49  lr: 0.000030  loss: 0.1058  time: 1.4838  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 210/3000]  eta: 1:10:42  lr: 0.000030  loss: 1.0516  time: 1.5048  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 210/3000]  eta: 1:10:41  lr: 0.000030  loss: 0.1927  time: 1.5045  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 215/3000]  eta: 1:10:30  lr: 0.000030  loss: 0.4864  time: 1.5091  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 215/3000]  eta: 1:10:29  lr: 0.000030  loss: 0.4398  time: 1.5090  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 220/3000]  eta: 1:10:22  lr: 0.000030  loss: 0.3252  time: 1.5047  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 220/3000]  eta: 1:10:21  lr: 0.000030  loss: 0.4696  time: 1.5045  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 225/3000]  eta: 1:10:19  lr: 0.000030  loss: 0.6345  time: 1.5192  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 225/3000]  eta: 1:10:18  lr: 0.000030  loss: 0.3707  time: 1.5191  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 230/3000]  eta: 1:10:11  lr: 0.000030  loss: 0.3757  time: 1.5180  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 230/3000]  eta: 1:10:10  lr: 0.000030  loss: 0.2358  time: 1.5178  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 235/3000]  eta: 1:10:03  lr: 0.000030  loss: 0.7220  time: 1.5325  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 235/3000]  eta: 1:10:02  lr: 0.000030  loss: 0.2426  time: 1.5322  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 240/3000]  eta: 1:09:55  lr: 0.000030  loss: 0.3635  time: 1.5328  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 240/3000]  eta: 1:09:54  lr: 0.000030  loss: 0.5101  time: 1.5325  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 245/3000]  eta: 1:09:41  lr: 0.000030  loss: 1.0397  time: 1.4869  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 245/3000]  eta: 1:09:40  lr: 0.000030  loss: 0.4489  time: 1.4866  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 250/3000]  eta: 1:09:32  lr: 0.000030  loss: 0.6075  time: 1.4810  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 250/3000]  eta: 1:09:31  lr: 0.000030  loss: 0.2915  time: 1.4808  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 255/3000]  eta: 1:09:22  lr: 0.000030  loss: 0.7054  time: 1.4705  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 255/3000]  eta: 1:09:21  lr: 0.000030  loss: 0.6990  time: 1.4703  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 260/3000]  eta: 1:09:13  lr: 0.000030  loss: 0.3657  time: 1.4668  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 260/3000]  eta: 1:09:12  lr: 0.000030  loss: 0.2037  time: 1.4665  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 265/3000]  eta: 1:09:07  lr: 0.000030  loss: 0.6877  time: 1.5025  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 265/3000]  eta: 1:09:07  lr: 0.000030  loss: 0.1579  time: 1.5022  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 270/3000]  eta: 1:09:01  lr: 0.000030  loss: 0.7202  time: 1.5161  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 270/3000]  eta: 1:09:00  lr: 0.000030  loss: 0.4606  time: 1.5158  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 275/3000]  eta: 1:08:52  lr: 0.000030  loss: 0.1427  time: 1.5211  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 275/3000]  eta: 1:08:51  lr: 0.000030  loss: 0.3638  time: 1.5209  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 280/3000]  eta: 1:08:43  lr: 0.000030  loss: 0.3178  time: 1.5180  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 280/3000]  eta: 1:08:42  lr: 0.000030  loss: 0.1852  time: 1.5177  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 285/3000]  eta: 1:08:32  lr: 0.000030  loss: 0.6374  time: 1.4894  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 285/3000]  eta: 1:08:31  lr: 0.000030  loss: 0.3011  time: 1.4892  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 290/3000]  eta: 1:08:26  lr: 0.000030  loss: 0.1666  time: 1.4888  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 290/3000]  eta: 1:08:25  lr: 0.000030  loss: 0.6622  time: 1.4886  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 295/3000]  eta: 1:08:19  lr: 0.000030  loss: 0.1763  time: 1.5024  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 295/3000]  eta: 1:08:18  lr: 0.000030  loss: 0.7902  time: 1.5022  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 300/3000]  eta: 1:08:11  lr: 0.000030  loss: 0.4671  time: 1.5070  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 300/3000]  eta: 1:08:10  lr: 0.000030  loss: 0.2682  time: 1.5069  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 305/3000]  eta: 1:08:07  lr: 0.000030  loss: 0.4188  time: 1.5435  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 305/3000]  eta: 1:08:06  lr: 0.000030  loss: 0.3329  time: 1.5432  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 310/3000]  eta: 1:08:00  lr: 0.000030  loss: 0.6587  time: 1.5448  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 310/3000]  eta: 1:07:59  lr: 0.000030  loss: 0.1976  time: 1.5445  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 315/3000]  eta: 1:07:52  lr: 0.000030  loss: 0.6431  time: 1.5335  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 315/3000]  eta: 1:07:51  lr: 0.000030  loss: 0.1174  time: 1.5332  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 320/3000]  eta: 1:07:47  lr: 0.000030  loss: 1.2951  time: 1.5494  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 320/3000]  eta: 1:07:46  lr: 0.000030  loss: 0.6209  time: 1.5492  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 325/3000]  eta: 1:07:39  lr: 0.000030  loss: 0.3799  time: 1.5320  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 325/3000]  eta: 1:07:38  lr: 0.000030  loss: 0.3763  time: 1.5318  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 330/3000]  eta: 1:07:31  lr: 0.000030  loss: 0.4897  time: 1.5253  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 330/3000]  eta: 1:07:31  lr: 0.000030  loss: 0.2121  time: 1.5251  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 335/3000]  eta: 1:07:23  lr: 0.000030  loss: 0.4649  time: 1.5254  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 335/3000]  eta: 1:07:22  lr: 0.000030  loss: 0.7549  time: 1.5252  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 340/3000]  eta: 1:07:13  lr: 0.000030  loss: 0.5377  time: 1.4969  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 340/3000]  eta: 1:07:12  lr: 0.000030  loss: 0.3912  time: 1.4966  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 345/3000]  eta: 1:07:07  lr: 0.000030  loss: 0.3255  time: 1.5069  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 345/3000]  eta: 1:07:06  lr: 0.000030  loss: 1.0596  time: 1.5066  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 350/3000]  eta: 1:07:00  lr: 0.000030  loss: 0.4204  time: 1.5093  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 350/3000]  eta: 1:06:59  lr: 0.000030  loss: 0.1567  time: 1.5090  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 355/3000]  eta: 1:06:54  lr: 0.000030  loss: 0.1229  time: 1.5235  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 355/3000]  eta: 1:06:53  lr: 0.000030  loss: 1.0215  time: 1.5233  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 360/3000]  eta: 1:06:47  lr: 0.000030  loss: 0.4836  time: 1.5432  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 360/3000]  eta: 1:06:46  lr: 0.000030  loss: 0.2714  time: 1.5429  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 365/3000]  eta: 1:06:38  lr: 0.000030  loss: 0.2581  time: 1.5290  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 365/3000]  eta: 1:06:38  lr: 0.000030  loss: 0.8194  time: 1.5287  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 370/3000]  eta: 1:06:32  lr: 0.000030  loss: 0.5195  time: 1.5318  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 370/3000]  eta: 1:06:31  lr: 0.000030  loss: 1.1473  time: 1.5315  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 375/3000]  eta: 1:06:24  lr: 0.000030  loss: 0.4876  time: 1.5219  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 375/3000]  eta: 1:06:23  lr: 0.000030  loss: 0.2173  time: 1.5216  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 380/3000]  eta: 1:06:16  lr: 0.000030  loss: 0.1044  time: 1.5157  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 380/3000]  eta: 1:06:15  lr: 0.000030  loss: 0.6285  time: 1.5155  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 385/3000]  eta: 1:06:10  lr: 0.000030  loss: 0.9334  time: 1.5334  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 385/3000]  eta: 1:06:09  lr: 0.000030  loss: 0.2102  time: 1.5331  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 390/3000]  eta: 1:06:02  lr: 0.000030  loss: 0.7719  time: 1.5229  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 390/3000]  eta: 1:06:01  lr: 0.000030  loss: 0.2369  time: 1.5227  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 395/3000]  eta: 1:05:55  lr: 0.000030  loss: 1.2185  time: 1.5313  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 395/3000]  eta: 1:05:55  lr: 0.000030  loss: 0.2928  time: 1.5312  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 400/3000]  eta: 1:05:47  lr: 0.000030  loss: 0.1313  time: 1.5269  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 400/3000]  eta: 1:05:46  lr: 0.000030  loss: 0.4181  time: 1.5266  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 405/3000]  eta: 1:05:37  lr: 0.000030  loss: 0.8607  time: 1.4957  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 405/3000]  eta: 1:05:36  lr: 0.000030  loss: 0.1363  time: 1.4955  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 410/3000]  eta: 1:05:30  lr: 0.000030  loss: 0.3503  time: 1.5045  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 410/3000]  eta: 1:05:29  lr: 0.000030  loss: 0.2924  time: 1.5043  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 415/3000]  eta: 1:05:20  lr: 0.000030  loss: 0.2149  time: 1.4769  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 415/3000]  eta: 1:05:19  lr: 0.000030  loss: 0.2433  time: 1.4767  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 420/3000]  eta: 1:05:14  lr: 0.000030  loss: 0.7542  time: 1.4949  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 420/3000]  eta: 1:05:13  lr: 0.000030  loss: 0.2739  time: 1.4946  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 425/3000]  eta: 1:05:04  lr: 0.000030  loss: 0.1331  time: 1.4965  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 425/3000]  eta: 1:05:03  lr: 0.000030  loss: 0.4764  time: 1.4962  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 430/3000]  eta: 1:04:58  lr: 0.000030  loss: 1.0908  time: 1.5050  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 430/3000]  eta: 1:04:57  lr: 0.000030  loss: 0.7001  time: 1.5048  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 435/3000]  eta: 1:04:52  lr: 0.000030  loss: 0.9243  time: 1.5385  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 435/3000]  eta: 1:04:51  lr: 0.000030  loss: 0.3396  time: 1.5382  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 440/3000]  eta: 1:04:46  lr: 0.000030  loss: 0.0870  time: 1.5376  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 440/3000]  eta: 1:04:45  lr: 0.000030  loss: 0.4026  time: 1.5373  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 445/3000]  eta: 1:04:37  lr: 0.000030  loss: 0.3353  time: 1.5491  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 445/3000]  eta: 1:04:37  lr: 0.000030  loss: 0.5451  time: 1.5489  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 450/3000]  eta: 1:04:27  lr: 0.000030  loss: 0.8144  time: 1.5088  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 450/3000]  eta: 1:04:26  lr: 0.000030  loss: 0.1920  time: 1.5086  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 455/3000]  eta: 1:04:20  lr: 0.000030  loss: 0.7169  time: 1.5002  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 455/3000]  eta: 1:04:19  lr: 0.000030  loss: 0.1640  time: 1.5000  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 460/3000]  eta: 1:04:12  lr: 0.000030  loss: 0.3765  time: 1.4858  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 460/3000]  eta: 1:04:11  lr: 0.000030  loss: 0.8040  time: 1.4855  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 465/3000]  eta: 1:04:04  lr: 0.000030  loss: 0.5875  time: 1.4873  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 465/3000]  eta: 1:04:03  lr: 0.000030  loss: 0.5115  time: 1.4871  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 470/3000]  eta: 1:03:55  lr: 0.000030  loss: 0.0860  time: 1.5050  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 470/3000]  eta: 1:03:54  lr: 0.000030  loss: 0.5094  time: 1.5036  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 475/3000]  eta: 1:03:49  lr: 0.000030  loss: 0.8674  time: 1.5126  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 475/3000]  eta: 1:03:48  lr: 0.000030  loss: 0.4400  time: 1.5114  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 480/3000]  eta: 1:03:40  lr: 0.000030  loss: 0.2687  time: 1.5059  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 480/3000]  eta: 1:03:39  lr: 0.000030  loss: 0.7154  time: 1.5047  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 485/3000]  eta: 1:03:34  lr: 0.000030  loss: 0.3808  time: 1.5176  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 485/3000]  eta: 1:03:33  lr: 0.000030  loss: 0.3830  time: 1.5164  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 490/3000]  eta: 1:03:26  lr: 0.000030  loss: 0.1504  time: 1.5235  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 490/3000]  eta: 1:03:25  lr: 0.000030  loss: 0.1166  time: 1.5233  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 495/3000]  eta: 1:03:17  lr: 0.000030  loss: 0.1712  time: 1.5042  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 495/3000]  eta: 1:03:17  lr: 0.000030  loss: 0.2460  time: 1.5040  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 500/3000]  eta: 1:03:09  lr: 0.000030  loss: 0.2437  time: 1.5077  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 500/3000]  eta: 1:03:08  lr: 0.000030  loss: 0.4557  time: 1.5075  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 505/3000]  eta: 1:03:02  lr: 0.000030  loss: 0.4730  time: 1.5061  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 505/3000]  eta: 1:03:01  lr: 0.000030  loss: 0.0617  time: 1.5058  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 510/3000]  eta: 1:02:56  lr: 0.000030  loss: 0.6924  time: 1.5222  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 510/3000]  eta: 1:02:55  lr: 0.000030  loss: 0.5359  time: 1.5219  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 515/3000]  eta: 1:02:46  lr: 0.000030  loss: 0.0635  time: 1.5090  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 515/3000]  eta: 1:02:46  lr: 0.000030  loss: 0.1953  time: 1.5088  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 520/3000]  eta: 1:02:39  lr: 0.000030  loss: 0.3057  time: 1.5215  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 520/3000]  eta: 1:02:39  lr: 0.000030  loss: 0.3167  time: 1.5213  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 525/3000]  eta: 1:02:32  lr: 0.000030  loss: 0.3926  time: 1.5155  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 525/3000]  eta: 1:02:31  lr: 0.000030  loss: 0.3413  time: 1.5153  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 530/3000]  eta: 1:02:22  lr: 0.000030  loss: 0.1558  time: 1.4813  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 530/3000]  eta: 1:02:21  lr: 0.000030  loss: 0.6067  time: 1.4811  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 535/3000]  eta: 1:02:16  lr: 0.000030  loss: 0.7125  time: 1.5178  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 535/3000]  eta: 1:02:15  lr: 0.000030  loss: 0.4111  time: 1.5175  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 540/3000]  eta: 1:02:08  lr: 0.000030  loss: 0.2409  time: 1.5036  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 540/3000]  eta: 1:02:07  lr: 0.000030  loss: 0.1765  time: 1.5034  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 545/3000]  eta: 1:02:01  lr: 0.000030  loss: 0.2404  time: 1.5100  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 545/3000]  eta: 1:02:00  lr: 0.000030  loss: 0.1868  time: 1.5097  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 550/3000]  eta: 1:01:51  lr: 0.000030  loss: 0.3649  time: 1.5106  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 550/3000]  eta: 1:01:51  lr: 0.000030  loss: 0.2328  time: 1.5108  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 555/3000]  eta: 1:01:43  lr: 0.000030  loss: 0.1838  time: 1.4771  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 555/3000]  eta: 1:01:42  lr: 0.000030  loss: 0.4657  time: 1.4770  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 560/3000]  eta: 1:01:36  lr: 0.000030  loss: 0.2875  time: 1.4986  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 560/3000]  eta: 1:01:35  lr: 0.000030  loss: 0.1785  time: 1.4984  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 565/3000]  eta: 1:01:29  lr: 0.000030  loss: 0.4033  time: 1.4928  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 565/3000]  eta: 1:01:28  lr: 0.000030  loss: 0.3424  time: 1.4927  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 570/3000]  eta: 1:01:21  lr: 0.000030  loss: 0.3085  time: 1.5158  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 570/3000]  eta: 1:01:20  lr: 0.000030  loss: 0.5938  time: 1.5156  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 575/3000]  eta: 1:01:13  lr: 0.000030  loss: 0.3940  time: 1.5260  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 575/3000]  eta: 1:01:12  lr: 0.000030  loss: 1.0953  time: 1.5257  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 580/3000]  eta: 1:01:04  lr: 0.000030  loss: 0.1622  time: 1.5046  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 580/3000]  eta: 1:01:05  lr: 0.000030  loss: 0.4618  time: 1.5049  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 585/3000]  eta: 1:00:58  lr: 0.000030  loss: 0.2470  time: 1.5110  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 585/3000]  eta: 1:00:57  lr: 0.000030  loss: 0.4340  time: 1.5107  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 590/3000]  eta: 1:00:51  lr: 0.000030  loss: 1.3328  time: 1.5173  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 590/3000]  eta: 1:00:50  lr: 0.000030  loss: 0.5741  time: 1.5171  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 595/3000]  eta: 1:00:43  lr: 0.000030  loss: 0.8032  time: 1.5209  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 595/3000]  eta: 1:00:43  lr: 0.000030  loss: 0.9222  time: 1.5208  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 600/3000]  eta: 1:00:36  lr: 0.000030  loss: 0.4935  time: 1.5284  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 600/3000]  eta: 1:00:35  lr: 0.000030  loss: 0.3245  time: 1.5283  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 605/3000]  eta: 1:00:28  lr: 0.000030  loss: 0.4011  time: 1.5224  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 605/3000]  eta: 1:00:28  lr: 0.000030  loss: 0.4694  time: 1.5222  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 610/3000]  eta: 1:00:22  lr: 0.000030  loss: 0.5180  time: 1.5335  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 610/3000]  eta: 1:00:21  lr: 0.000030  loss: 0.3519  time: 1.5332  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 615/3000]  eta: 1:00:15  lr: 0.000030  loss: 0.4344  time: 1.5377  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 615/3000]  eta: 1:00:14  lr: 0.000030  loss: 0.5505  time: 1.5374  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 620/3000]  eta: 1:00:07  lr: 0.000030  loss: 0.4931  time: 1.5386  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 620/3000]  eta: 1:00:07  lr: 0.000030  loss: 0.3059  time: 1.5383  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 625/3000]  eta: 1:00:00  lr: 0.000030  loss: 0.7480  time: 1.5391  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 625/3000]  eta: 0:59:59  lr: 0.000030  loss: 0.1639  time: 1.5388  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 630/3000]  eta: 0:59:51  lr: 0.000030  loss: 0.7863  time: 1.5000  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 630/3000]  eta: 0:59:50  lr: 0.000030  loss: 0.1735  time: 1.4997  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 635/3000]  eta: 0:59:44  lr: 0.000030  loss: 0.7668  time: 1.5087  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 635/3000]  eta: 0:59:43  lr: 0.000030  loss: 0.6347  time: 1.5084  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 640/3000]  eta: 0:59:35  lr: 0.000030  loss: 0.3853  time: 1.4844  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 640/3000]  eta: 0:59:34  lr: 0.000030  loss: 0.4727  time: 1.4842  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 645/3000]  eta: 0:59:28  lr: 0.000030  loss: 0.4134  time: 1.4962  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 645/3000]  eta: 0:59:27  lr: 0.000030  loss: 0.3633  time: 1.4960  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 650/3000]  eta: 0:59:20  lr: 0.000030  loss: 0.2443  time: 1.5071  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 650/3000]  eta: 0:59:19  lr: 0.000030  loss: 0.0915  time: 1.5068  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 655/3000]  eta: 0:59:13  lr: 0.000030  loss: 0.6276  time: 1.5009  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 655/3000]  eta: 0:59:12  lr: 0.000030  loss: 0.5640  time: 1.5006  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 660/3000]  eta: 0:59:06  lr: 0.000030  loss: 0.4192  time: 1.5320  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 660/3000]  eta: 0:59:05  lr: 0.000030  loss: 0.4379  time: 1.5317  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 665/3000]  eta: 0:58:58  lr: 0.000030  loss: 0.4763  time: 1.5227  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 665/3000]  eta: 0:58:58  lr: 0.000030  loss: 0.2876  time: 1.5224  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 670/3000]  eta: 0:58:50  lr: 0.000030  loss: 0.2940  time: 1.5216  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 670/3000]  eta: 0:58:49  lr: 0.000030  loss: 0.8729  time: 1.5212  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 675/3000]  eta: 0:58:43  lr: 0.000030  loss: 0.6846  time: 1.5230  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 675/3000]  eta: 0:58:42  lr: 0.000030  loss: 0.2863  time: 1.5226  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 680/3000]  eta: 0:58:36  lr: 0.000030  loss: 0.7714  time: 1.5199  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 680/3000]  eta: 0:58:35  lr: 0.000030  loss: 0.0839  time: 1.5195  data: 0.0000  max mem: 18051
Train: data epoch: [1]  [ 685/3000]  eta: 0:58:29  lr: 0.000030  loss: 1.1408  time: 1.5315  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 685/3000]  eta: 0:58:28  lr: 0.000030  loss: 0.4218  time: 1.5312  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 690/3000]  eta: 0:58:20  lr: 0.000030  loss: 0.2940  time: 1.5259  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 690/3000]  eta: 0:58:20  lr: 0.000030  loss: 0.2343  time: 1.5256  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 695/3000]  eta: 0:58:13  lr: 0.000030  loss: 0.2218  time: 1.5192  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 695/3000]  eta: 0:58:12  lr: 0.000030  loss: 0.3884  time: 1.5189  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 700/3000]  eta: 0:58:07  lr: 0.000030  loss: 0.1917  time: 1.5363  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 700/3000]  eta: 0:58:06  lr: 0.000030  loss: 0.2930  time: 1.5360  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 705/3000]  eta: 0:58:00  lr: 0.000030  loss: 0.2303  time: 1.5290  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 705/3000]  eta: 0:57:59  lr: 0.000030  loss: 0.4508  time: 1.5288  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 710/3000]  eta: 0:57:52  lr: 0.000030  loss: 0.1769  time: 1.5491  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 710/3000]  eta: 0:57:52  lr: 0.000030  loss: 0.5348  time: 1.5489  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 715/3000]  eta: 0:57:45  lr: 0.000030  loss: 1.2487  time: 1.5517  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 715/3000]  eta: 0:57:44  lr: 0.000030  loss: 0.1564  time: 1.5514  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 720/3000]  eta: 0:57:37  lr: 0.000030  loss: 0.2717  time: 1.5262  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 720/3000]  eta: 0:57:36  lr: 0.000030  loss: 0.4451  time: 1.5260  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 725/3000]  eta: 0:57:29  lr: 0.000030  loss: 0.3186  time: 1.5151  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 725/3000]  eta: 0:57:28  lr: 0.000030  loss: 0.3383  time: 1.5149  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 730/3000]  eta: 0:57:22  lr: 0.000030  loss: 0.3016  time: 1.5160  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 730/3000]  eta: 0:57:21  lr: 0.000030  loss: 0.8547  time: 1.5158  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 735/3000]  eta: 0:57:14  lr: 0.000030  loss: 0.2672  time: 1.5014  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 735/3000]  eta: 0:57:13  lr: 0.000030  loss: 0.7751  time: 1.5012  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 740/3000]  eta: 0:57:06  lr: 0.000030  loss: 0.6183  time: 1.5050  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 740/3000]  eta: 0:57:05  lr: 0.000030  loss: 0.5144  time: 1.5048  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 745/3000]  eta: 0:56:58  lr: 0.000030  loss: 0.7493  time: 1.4948  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 745/3000]  eta: 0:56:57  lr: 0.000030  loss: 0.2557  time: 1.4947  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 750/3000]  eta: 0:56:50  lr: 0.000030  loss: 0.3386  time: 1.4900  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 750/3000]  eta: 0:56:49  lr: 0.000030  loss: 0.5022  time: 1.4898  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 755/3000]  eta: 0:56:42  lr: 0.000030  loss: 0.2085  time: 1.5045  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 755/3000]  eta: 0:56:42  lr: 0.000030  loss: 0.3993  time: 1.5042  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 760/3000]  eta: 0:56:35  lr: 0.000030  loss: 0.7910  time: 1.5020  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 760/3000]  eta: 0:56:34  lr: 0.000030  loss: 0.3639  time: 1.5018  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 765/3000]  eta: 0:56:28  lr: 0.000030  loss: 0.2554  time: 1.5243  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 765/3000]  eta: 0:56:27  lr: 0.000030  loss: 0.2095  time: 1.5240  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 770/3000]  eta: 0:56:21  lr: 0.000030  loss: 0.4352  time: 1.5334  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 770/3000]  eta: 0:56:20  lr: 0.000030  loss: 0.6646  time: 1.5331  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 775/3000]  eta: 0:56:12  lr: 0.000030  loss: 0.1988  time: 1.5041  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 775/3000]  eta: 0:56:11  lr: 0.000030  loss: 0.3095  time: 1.5039  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 780/3000]  eta: 0:56:05  lr: 0.000030  loss: 0.2917  time: 1.5199  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 780/3000]  eta: 0:56:04  lr: 0.000030  loss: 0.4128  time: 1.5195  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 785/3000]  eta: 0:55:57  lr: 0.000030  loss: 0.8517  time: 1.5126  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 785/3000]  eta: 0:55:57  lr: 0.000030  loss: 0.2631  time: 1.5123  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 790/3000]  eta: 0:55:50  lr: 0.000030  loss: 0.5701  time: 1.5137  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 790/3000]  eta: 0:55:50  lr: 0.000030  loss: 0.6968  time: 1.5135  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 795/3000]  eta: 0:55:43  lr: 0.000030  loss: 0.2856  time: 1.5433  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 795/3000]  eta: 0:55:42  lr: 0.000030  loss: 0.2105  time: 1.5431  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 800/3000]  eta: 0:55:34  lr: 0.000030  loss: 0.2235  time: 1.5165  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 800/3000]  eta: 0:55:34  lr: 0.000030  loss: 0.1858  time: 1.5163  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 805/3000]  eta: 0:55:27  lr: 0.000030  loss: 0.5145  time: 1.5182  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 805/3000]  eta: 0:55:26  lr: 0.000030  loss: 0.5218  time: 1.5180  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 810/3000]  eta: 0:55:19  lr: 0.000030  loss: 0.6626  time: 1.5058  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 810/3000]  eta: 0:55:19  lr: 0.000030  loss: 0.9521  time: 1.5055  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 815/3000]  eta: 0:55:12  lr: 0.000030  loss: 0.3802  time: 1.5080  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 815/3000]  eta: 0:55:11  lr: 0.000030  loss: 0.6793  time: 1.5077  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 820/3000]  eta: 0:55:04  lr: 0.000030  loss: 0.4659  time: 1.5123  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 820/3000]  eta: 0:55:03  lr: 0.000030  loss: 0.0963  time: 1.5120  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 825/3000]  eta: 0:54:56  lr: 0.000030  loss: 0.5232  time: 1.4974  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 825/3000]  eta: 0:54:55  lr: 0.000030  loss: 0.3196  time: 1.4970  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 830/3000]  eta: 0:54:48  lr: 0.000030  loss: 0.4815  time: 1.4982  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 830/3000]  eta: 0:54:47  lr: 0.000030  loss: 0.0836  time: 1.4979  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 835/3000]  eta: 0:54:41  lr: 0.000030  loss: 0.4864  time: 1.5045  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 835/3000]  eta: 0:54:40  lr: 0.000030  loss: 0.5321  time: 1.5041  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 840/3000]  eta: 0:54:33  lr: 0.000030  loss: 0.5031  time: 1.5023  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 840/3000]  eta: 0:54:32  lr: 0.000030  loss: 0.4643  time: 1.5020  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 845/3000]  eta: 0:54:25  lr: 0.000030  loss: 0.1819  time: 1.4993  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 845/3000]  eta: 0:54:24  lr: 0.000030  loss: 0.1611  time: 1.4990  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 850/3000]  eta: 0:54:17  lr: 0.000030  loss: 0.0567  time: 1.4913  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 850/3000]  eta: 0:54:16  lr: 0.000030  loss: 0.1071  time: 1.4910  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 855/3000]  eta: 0:54:08  lr: 0.000030  loss: 0.1834  time: 1.4655  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 855/3000]  eta: 0:54:08  lr: 0.000030  loss: 0.2294  time: 1.4652  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 860/3000]  eta: 0:54:01  lr: 0.000030  loss: 0.1994  time: 1.4935  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 860/3000]  eta: 0:54:01  lr: 0.000030  loss: 0.2419  time: 1.4933  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 865/3000]  eta: 0:53:54  lr: 0.000030  loss: 0.3646  time: 1.5095  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 865/3000]  eta: 0:53:53  lr: 0.000030  loss: 0.6584  time: 1.5093  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 870/3000]  eta: 0:53:47  lr: 0.000030  loss: 0.0899  time: 1.5211  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 870/3000]  eta: 0:53:46  lr: 0.000030  loss: 0.4719  time: 1.5208  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 875/3000]  eta: 0:53:40  lr: 0.000030  loss: 0.9321  time: 1.5533  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 875/3000]  eta: 0:53:39  lr: 0.000030  loss: 0.4418  time: 1.5531  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 880/3000]  eta: 0:53:32  lr: 0.000030  loss: 0.2564  time: 1.5309  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 880/3000]  eta: 0:53:31  lr: 0.000030  loss: 0.2537  time: 1.5307  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 885/3000]  eta: 0:53:24  lr: 0.000030  loss: 0.6748  time: 1.5316  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 885/3000]  eta: 0:53:24  lr: 0.000030  loss: 0.7059  time: 1.5313  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 890/3000]  eta: 0:53:17  lr: 0.000030  loss: 0.3752  time: 1.5251  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 890/3000]  eta: 0:53:16  lr: 0.000030  loss: 0.2756  time: 1.5249  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 895/3000]  eta: 0:53:10  lr: 0.000030  loss: 0.3944  time: 1.5236  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 895/3000]  eta: 0:53:09  lr: 0.000030  loss: 0.2796  time: 1.5232  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 900/3000]  eta: 0:53:03  lr: 0.000030  loss: 0.7020  time: 1.5450  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 900/3000]  eta: 0:53:02  lr: 0.000030  loss: 0.1861  time: 1.5447  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 905/3000]  eta: 0:52:55  lr: 0.000030  loss: 0.3329  time: 1.5436  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 905/3000]  eta: 0:52:55  lr: 0.000030  loss: 0.3934  time: 1.5433  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 910/3000]  eta: 0:52:48  lr: 0.000030  loss: 0.5327  time: 1.5541  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 910/3000]  eta: 0:52:48  lr: 0.000030  loss: 0.3627  time: 1.5539  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 915/3000]  eta: 0:52:40  lr: 0.000030  loss: 0.2562  time: 1.5349  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 915/3000]  eta: 0:52:40  lr: 0.000030  loss: 0.2006  time: 1.5346  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 920/3000]  eta: 0:52:33  lr: 0.000030  loss: 0.4565  time: 1.5245  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 920/3000]  eta: 0:52:32  lr: 0.000030  loss: 0.3386  time: 1.5242  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 925/3000]  eta: 0:52:25  lr: 0.000030  loss: 0.3833  time: 1.5135  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 925/3000]  eta: 0:52:24  lr: 0.000030  loss: 0.3010  time: 1.5132  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 930/3000]  eta: 0:52:18  lr: 0.000030  loss: 0.3952  time: 1.5124  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 930/3000]  eta: 0:52:17  lr: 0.000030  loss: 0.2548  time: 1.5121  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 935/3000]  eta: 0:52:11  lr: 0.000030  loss: 0.7479  time: 1.5283  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 935/3000]  eta: 0:52:10  lr: 0.000030  loss: 0.5807  time: 1.5280  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 940/3000]  eta: 0:52:02  lr: 0.000030  loss: 0.5221  time: 1.5051  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 940/3000]  eta: 0:52:02  lr: 0.000030  loss: 0.4569  time: 1.5046  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 945/3000]  eta: 0:51:55  lr: 0.000030  loss: 0.4746  time: 1.5234  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 945/3000]  eta: 0:51:54  lr: 0.000030  loss: 0.3719  time: 1.5229  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 950/3000]  eta: 0:51:47  lr: 0.000030  loss: 0.1576  time: 1.5069  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 950/3000]  eta: 0:51:46  lr: 0.000030  loss: 0.3226  time: 1.5065  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 955/3000]  eta: 0:51:40  lr: 0.000030  loss: 0.4778  time: 1.4976  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 955/3000]  eta: 0:51:39  lr: 0.000030  loss: 0.3696  time: 1.4971  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 960/3000]  eta: 0:51:32  lr: 0.000030  loss: 0.9587  time: 1.5207  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 960/3000]  eta: 0:51:32  lr: 0.000030  loss: 0.1923  time: 1.5205  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 965/3000]  eta: 0:51:25  lr: 0.000030  loss: 0.8854  time: 1.5208  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 965/3000]  eta: 0:51:24  lr: 0.000030  loss: 1.0375  time: 1.5206  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 970/3000]  eta: 0:51:17  lr: 0.000030  loss: 0.2003  time: 1.5233  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 970/3000]  eta: 0:51:16  lr: 0.000030  loss: 0.2141  time: 1.5231  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 975/3000]  eta: 0:51:09  lr: 0.000030  loss: 0.2983  time: 1.5159  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 975/3000]  eta: 0:51:09  lr: 0.000030  loss: 0.8018  time: 1.5157  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 980/3000]  eta: 0:51:02  lr: 0.000030  loss: 0.3033  time: 1.5278  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 980/3000]  eta: 0:51:02  lr: 0.000030  loss: 0.2712  time: 1.5276  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 985/3000]  eta: 0:50:54  lr: 0.000030  loss: 0.2662  time: 1.5056  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 985/3000]  eta: 0:50:54  lr: 0.000030  loss: 0.8525  time: 1.5053  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 990/3000]  eta: 0:50:46  lr: 0.000030  loss: 0.5886  time: 1.5085  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 990/3000]  eta: 0:50:46  lr: 0.000030  loss: 0.3686  time: 1.5083  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [ 995/3000]  eta: 0:50:39  lr: 0.000030  loss: 0.2766  time: 1.5110  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [ 995/3000]  eta: 0:50:38  lr: 0.000030  loss: 0.1458  time: 1.5108  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1000/3000]  eta: 0:50:31  lr: 0.000030  loss: 0.5705  time: 1.4996  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1000/3000]  eta: 0:50:31  lr: 0.000030  loss: 1.2118  time: 1.4993  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1005/3000]  eta: 0:50:24  lr: 0.000030  loss: 0.1830  time: 1.5113  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1005/3000]  eta: 0:50:23  lr: 0.000030  loss: 1.8335  time: 1.5110  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1010/3000]  eta: 0:50:16  lr: 0.000030  loss: 0.3872  time: 1.5139  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1010/3000]  eta: 0:50:15  lr: 0.000030  loss: 0.3607  time: 1.5138  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1015/3000]  eta: 0:50:09  lr: 0.000030  loss: 0.5140  time: 1.5236  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1015/3000]  eta: 0:50:08  lr: 0.000030  loss: 0.8997  time: 1.5234  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1020/3000]  eta: 0:50:01  lr: 0.000030  loss: 0.2049  time: 1.5138  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1020/3000]  eta: 0:50:00  lr: 0.000030  loss: 0.2798  time: 1.5135  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1025/3000]  eta: 0:49:52  lr: 0.000030  loss: 0.1309  time: 1.4908  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1025/3000]  eta: 0:49:52  lr: 0.000030  loss: 0.4326  time: 1.4906  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1030/3000]  eta: 0:49:44  lr: 0.000030  loss: 0.0864  time: 1.4748  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1030/3000]  eta: 0:49:44  lr: 0.000030  loss: 0.0702  time: 1.4743  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1035/3000]  eta: 0:49:36  lr: 0.000030  loss: 0.5383  time: 1.4493  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1035/3000]  eta: 0:49:35  lr: 0.000030  loss: 0.1920  time: 1.4491  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1040/3000]  eta: 0:49:28  lr: 0.000030  loss: 0.1398  time: 1.4443  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1040/3000]  eta: 0:49:27  lr: 0.000030  loss: 0.2184  time: 1.4441  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1045/3000]  eta: 0:49:20  lr: 0.000030  loss: 0.3211  time: 1.4659  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1045/3000]  eta: 0:49:20  lr: 0.000030  loss: 0.1649  time: 1.4656  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1050/3000]  eta: 0:49:13  lr: 0.000030  loss: 0.7908  time: 1.4806  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1050/3000]  eta: 0:49:12  lr: 0.000030  loss: 0.0700  time: 1.4803  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1055/3000]  eta: 0:49:05  lr: 0.000030  loss: 0.3766  time: 1.5102  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1055/3000]  eta: 0:49:05  lr: 0.000030  loss: 0.3224  time: 1.5099  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1060/3000]  eta: 0:48:59  lr: 0.000030  loss: 0.4400  time: 1.5411  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1060/3000]  eta: 0:48:58  lr: 0.000030  loss: 0.4491  time: 1.5408  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1065/3000]  eta: 0:48:51  lr: 0.000030  loss: 0.3801  time: 1.5334  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1065/3000]  eta: 0:48:50  lr: 0.000030  loss: 0.3523  time: 1.5331  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1070/3000]  eta: 0:48:43  lr: 0.000030  loss: 0.6852  time: 1.5430  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1070/3000]  eta: 0:48:43  lr: 0.000030  loss: 0.2119  time: 1.5427  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1075/3000]  eta: 0:48:35  lr: 0.000030  loss: 0.1878  time: 1.5227  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1075/3000]  eta: 0:48:35  lr: 0.000030  loss: 0.4139  time: 1.5224  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1080/3000]  eta: 0:48:28  lr: 0.000030  loss: 0.5314  time: 1.5148  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1080/3000]  eta: 0:48:28  lr: 0.000030  loss: 0.6522  time: 1.5145  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1085/3000]  eta: 0:48:21  lr: 0.000030  loss: 0.2802  time: 1.5297  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1085/3000]  eta: 0:48:20  lr: 0.000030  loss: 0.4666  time: 1.5294  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1090/3000]  eta: 0:48:13  lr: 0.000030  loss: 0.2348  time: 1.5171  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1090/3000]  eta: 0:48:12  lr: 0.000030  loss: 0.1760  time: 1.5169  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1095/3000]  eta: 0:48:06  lr: 0.000030  loss: 0.5025  time: 1.5349  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1095/3000]  eta: 0:48:05  lr: 0.000030  loss: 0.7559  time: 1.5347  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1100/3000]  eta: 0:47:58  lr: 0.000030  loss: 0.6964  time: 1.5018  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1100/3000]  eta: 0:47:57  lr: 0.000030  loss: 0.1923  time: 1.5016  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1105/3000]  eta: 0:47:50  lr: 0.000030  loss: 0.3800  time: 1.4862  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1105/3000]  eta: 0:47:49  lr: 0.000030  loss: 0.2505  time: 1.4860  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1110/3000]  eta: 0:47:42  lr: 0.000030  loss: 0.8489  time: 1.4969  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1110/3000]  eta: 0:47:42  lr: 0.000030  loss: 0.5004  time: 1.4967  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1115/3000]  eta: 0:47:35  lr: 0.000030  loss: 0.6190  time: 1.4938  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1115/3000]  eta: 0:47:34  lr: 0.000030  loss: 0.2372  time: 1.4936  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1120/3000]  eta: 0:47:26  lr: 0.000030  loss: 0.3670  time: 1.4910  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1120/3000]  eta: 0:47:26  lr: 0.000030  loss: 0.0987  time: 1.4906  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1125/3000]  eta: 0:47:19  lr: 0.000030  loss: 0.3509  time: 1.4974  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1125/3000]  eta: 0:47:18  lr: 0.000030  loss: 0.4667  time: 1.4970  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1130/3000]  eta: 0:47:11  lr: 0.000030  loss: 0.6778  time: 1.4993  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1130/3000]  eta: 0:47:11  lr: 0.000030  loss: 0.3625  time: 1.4988  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1135/3000]  eta: 0:47:04  lr: 0.000030  loss: 0.3874  time: 1.4927  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1135/3000]  eta: 0:47:03  lr: 0.000030  loss: 0.3154  time: 1.4923  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1140/3000]  eta: 0:46:56  lr: 0.000030  loss: 0.6437  time: 1.5172  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1140/3000]  eta: 0:46:56  lr: 0.000030  loss: 0.7309  time: 1.5169  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1145/3000]  eta: 0:46:48  lr: 0.000030  loss: 0.2203  time: 1.5106  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1145/3000]  eta: 0:46:48  lr: 0.000030  loss: 0.3195  time: 1.5103  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1150/3000]  eta: 0:46:41  lr: 0.000030  loss: 0.5367  time: 1.5194  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1150/3000]  eta: 0:46:41  lr: 0.000030  loss: 0.5387  time: 1.5191  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1155/3000]  eta: 0:46:33  lr: 0.000030  loss: 1.1856  time: 1.5070  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1155/3000]  eta: 0:46:33  lr: 0.000030  loss: 0.1664  time: 1.5067  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1160/3000]  eta: 0:46:26  lr: 0.000030  loss: 0.2509  time: 1.5053  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1160/3000]  eta: 0:46:25  lr: 0.000030  loss: 0.3860  time: 1.5050  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1165/3000]  eta: 0:46:19  lr: 0.000030  loss: 0.4861  time: 1.5299  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1165/3000]  eta: 0:46:18  lr: 0.000030  loss: 0.3289  time: 1.5296  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1170/3000]  eta: 0:46:11  lr: 0.000030  loss: 0.2346  time: 1.5222  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1170/3000]  eta: 0:46:11  lr: 0.000030  loss: 0.6452  time: 1.5219  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1175/3000]  eta: 0:46:04  lr: 0.000030  loss: 0.7132  time: 1.5378  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1175/3000]  eta: 0:46:03  lr: 0.000030  loss: 0.3463  time: 1.5376  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1180/3000]  eta: 0:45:56  lr: 0.000030  loss: 0.0604  time: 1.5419  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1180/3000]  eta: 0:45:56  lr: 0.000030  loss: 0.6614  time: 1.5416  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1185/3000]  eta: 0:45:49  lr: 0.000030  loss: 0.3554  time: 1.5288  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1185/3000]  eta: 0:45:48  lr: 0.000030  loss: 0.2107  time: 1.5285  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1190/3000]  eta: 0:45:41  lr: 0.000030  loss: 0.7825  time: 1.5320  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1190/3000]  eta: 0:45:41  lr: 0.000030  loss: 0.2296  time: 1.5318  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1195/3000]  eta: 0:45:34  lr: 0.000030  loss: 0.4142  time: 1.5256  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1195/3000]  eta: 0:45:33  lr: 0.000030  loss: 0.6107  time: 1.5254  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1200/3000]  eta: 0:45:26  lr: 0.000030  loss: 0.0787  time: 1.5085  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1200/3000]  eta: 0:45:25  lr: 0.000030  loss: 0.7719  time: 1.5083  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1205/3000]  eta: 0:45:17  lr: 0.000030  loss: 0.4264  time: 1.4789  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1205/3000]  eta: 0:45:17  lr: 0.000030  loss: 0.5126  time: 1.4786  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1210/3000]  eta: 0:45:10  lr: 0.000030  loss: 0.3086  time: 1.4669  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1210/3000]  eta: 0:45:09  lr: 0.000030  loss: 0.3389  time: 1.4666  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1215/3000]  eta: 0:45:02  lr: 0.000030  loss: 0.7172  time: 1.4608  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1215/3000]  eta: 0:45:01  lr: 0.000030  loss: 0.4643  time: 1.4606  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1220/3000]  eta: 0:44:54  lr: 0.000030  loss: 0.7371  time: 1.4775  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1220/3000]  eta: 0:44:54  lr: 0.000030  loss: 0.1631  time: 1.4773  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1225/3000]  eta: 0:44:46  lr: 0.000030  loss: 0.6040  time: 1.4956  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1225/3000]  eta: 0:44:46  lr: 0.000030  loss: 0.3842  time: 1.4954  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1230/3000]  eta: 0:44:38  lr: 0.000030  loss: 0.4205  time: 1.4822  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1230/3000]  eta: 0:44:38  lr: 0.000030  loss: 0.4462  time: 1.4820  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1235/3000]  eta: 0:44:31  lr: 0.000030  loss: 0.7285  time: 1.4806  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1235/3000]  eta: 0:44:30  lr: 0.000030  loss: 0.4653  time: 1.4803  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1240/3000]  eta: 0:44:23  lr: 0.000030  loss: 0.3831  time: 1.4838  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1240/3000]  eta: 0:44:23  lr: 0.000030  loss: 0.1813  time: 1.4835  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1245/3000]  eta: 0:44:16  lr: 0.000030  loss: 0.5413  time: 1.4924  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1245/3000]  eta: 0:44:15  lr: 0.000030  loss: 0.2235  time: 1.4921  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1250/3000]  eta: 0:44:08  lr: 0.000030  loss: 0.2602  time: 1.5165  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1250/3000]  eta: 0:44:08  lr: 0.000030  loss: 0.2762  time: 1.5163  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1255/3000]  eta: 0:44:01  lr: 0.000030  loss: 0.2405  time: 1.5243  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1255/3000]  eta: 0:44:00  lr: 0.000030  loss: 0.3359  time: 1.5240  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1260/3000]  eta: 0:43:53  lr: 0.000030  loss: 0.1546  time: 1.5290  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1260/3000]  eta: 0:43:53  lr: 0.000030  loss: 0.4796  time: 1.5287  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1265/3000]  eta: 0:43:46  lr: 0.000030  loss: 0.4000  time: 1.5465  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1265/3000]  eta: 0:43:46  lr: 0.000030  loss: 0.5301  time: 1.5463  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1270/3000]  eta: 0:43:39  lr: 0.000030  loss: 0.3976  time: 1.5451  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1270/3000]  eta: 0:43:38  lr: 0.000030  loss: 1.5388  time: 1.5448  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1275/3000]  eta: 0:43:32  lr: 0.000030  loss: 0.4237  time: 1.5595  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1275/3000]  eta: 0:43:31  lr: 0.000030  loss: 0.2626  time: 1.5593  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1280/3000]  eta: 0:43:23  lr: 0.000030  loss: 1.7306  time: 1.5242  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1280/3000]  eta: 0:43:23  lr: 0.000030  loss: 0.6081  time: 1.5250  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1285/3000]  eta: 0:43:15  lr: 0.000030  loss: 0.1502  time: 1.4875  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1285/3000]  eta: 0:43:15  lr: 0.000030  loss: 0.4479  time: 1.4873  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1290/3000]  eta: 0:43:08  lr: 0.000030  loss: 0.4631  time: 1.4841  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1290/3000]  eta: 0:43:07  lr: 0.000030  loss: 0.6543  time: 1.4838  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1295/3000]  eta: 0:43:00  lr: 0.000030  loss: 0.7228  time: 1.4687  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1295/3000]  eta: 0:43:00  lr: 0.000030  loss: 0.6718  time: 1.4684  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1300/3000]  eta: 0:42:53  lr: 0.000030  loss: 0.3562  time: 1.4903  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1300/3000]  eta: 0:42:52  lr: 0.000030  loss: 0.2226  time: 1.4890  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1305/3000]  eta: 0:42:45  lr: 0.000030  loss: 0.4222  time: 1.5122  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1305/3000]  eta: 0:42:44  lr: 0.000030  loss: 0.4035  time: 1.5119  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1310/3000]  eta: 0:42:37  lr: 0.000030  loss: 0.4848  time: 1.4907  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1310/3000]  eta: 0:42:36  lr: 0.000030  loss: 0.7526  time: 1.4905  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1315/3000]  eta: 0:42:29  lr: 0.000030  loss: 0.3168  time: 1.4957  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1315/3000]  eta: 0:42:29  lr: 0.000030  loss: 0.2699  time: 1.4955  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1320/3000]  eta: 0:42:22  lr: 0.000030  loss: 0.6051  time: 1.5089  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1320/3000]  eta: 0:42:22  lr: 0.000030  loss: 0.6027  time: 1.5086  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1325/3000]  eta: 0:42:14  lr: 0.000030  loss: 0.3904  time: 1.4989  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1325/3000]  eta: 0:42:14  lr: 0.000030  loss: 0.3092  time: 1.4986  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1330/3000]  eta: 0:42:06  lr: 0.000030  loss: 0.3761  time: 1.5004  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1330/3000]  eta: 0:42:06  lr: 0.000030  loss: 0.1881  time: 1.5001  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1335/3000]  eta: 0:41:58  lr: 0.000030  loss: 0.7886  time: 1.4870  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1335/3000]  eta: 0:41:58  lr: 0.000030  loss: 0.7770  time: 1.4868  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1340/3000]  eta: 0:41:51  lr: 0.000030  loss: 0.2282  time: 1.4673  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1340/3000]  eta: 0:41:50  lr: 0.000030  loss: 0.6587  time: 1.4671  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1345/3000]  eta: 0:41:43  lr: 0.000030  loss: 0.6374  time: 1.4544  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1345/3000]  eta: 0:41:42  lr: 0.000030  loss: 0.1316  time: 1.4542  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1350/3000]  eta: 0:41:36  lr: 0.000030  loss: 0.6729  time: 1.4891  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1350/3000]  eta: 0:41:35  lr: 0.000030  loss: 0.6480  time: 1.4889  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1355/3000]  eta: 0:41:28  lr: 0.000030  loss: 0.1425  time: 1.4897  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1355/3000]  eta: 0:41:27  lr: 0.000030  loss: 0.7019  time: 1.4895  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1360/3000]  eta: 0:41:21  lr: 0.000030  loss: 0.2899  time: 1.5152  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1360/3000]  eta: 0:41:20  lr: 0.000030  loss: 0.4548  time: 1.5150  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1365/3000]  eta: 0:41:13  lr: 0.000030  loss: 0.5538  time: 1.5255  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1365/3000]  eta: 0:41:12  lr: 0.000030  loss: 0.6344  time: 1.5252  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1370/3000]  eta: 0:41:05  lr: 0.000030  loss: 0.1899  time: 1.4897  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1370/3000]  eta: 0:41:04  lr: 0.000030  loss: 0.3859  time: 1.4894  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1375/3000]  eta: 0:40:58  lr: 0.000030  loss: 0.3336  time: 1.5209  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1375/3000]  eta: 0:40:57  lr: 0.000030  loss: 0.3546  time: 1.5206  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1380/3000]  eta: 0:40:50  lr: 0.000030  loss: 0.2636  time: 1.4843  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1380/3000]  eta: 0:40:49  lr: 0.000030  loss: 0.5361  time: 1.4840  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1385/3000]  eta: 0:40:42  lr: 0.000030  loss: 0.1382  time: 1.5058  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1385/3000]  eta: 0:40:42  lr: 0.000030  loss: 0.6763  time: 1.5056  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1390/3000]  eta: 0:40:35  lr: 0.000030  loss: 0.4044  time: 1.5386  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1390/3000]  eta: 0:40:35  lr: 0.000030  loss: 0.4017  time: 1.5383  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1395/3000]  eta: 0:40:28  lr: 0.000030  loss: 0.4098  time: 1.5235  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1395/3000]  eta: 0:40:27  lr: 0.000030  loss: 0.5863  time: 1.5232  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1400/3000]  eta: 0:40:19  lr: 0.000030  loss: 0.3477  time: 1.5172  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1400/3000]  eta: 0:40:19  lr: 0.000030  loss: 0.1988  time: 1.5170  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1405/3000]  eta: 0:40:12  lr: 0.000030  loss: 0.6539  time: 1.5038  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1405/3000]  eta: 0:40:11  lr: 0.000030  loss: 0.1355  time: 1.5036  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1410/3000]  eta: 0:40:04  lr: 0.000030  loss: 0.6537  time: 1.4854  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1410/3000]  eta: 0:40:04  lr: 0.000030  loss: 0.1077  time: 1.4853  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1415/3000]  eta: 0:39:57  lr: 0.000030  loss: 0.9445  time: 1.4892  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1415/3000]  eta: 0:39:56  lr: 0.000030  loss: 0.2239  time: 1.4890  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1420/3000]  eta: 0:39:49  lr: 0.000030  loss: 0.2121  time: 1.5204  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1420/3000]  eta: 0:39:49  lr: 0.000030  loss: 0.3238  time: 1.5202  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1425/3000]  eta: 0:39:42  lr: 0.000030  loss: 0.1542  time: 1.5228  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1425/3000]  eta: 0:39:41  lr: 0.000030  loss: 0.8244  time: 1.5226  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1430/3000]  eta: 0:39:34  lr: 0.000030  loss: 0.8356  time: 1.5263  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1430/3000]  eta: 0:39:34  lr: 0.000030  loss: 0.5856  time: 1.5259  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1435/3000]  eta: 0:39:26  lr: 0.000030  loss: 0.5528  time: 1.4949  data: 0.0000  max mem: 18067Train: data epoch: [1]  [1435/3000]  eta: 0:39:26  lr: 0.000030  loss: 0.1917  time: 1.4953  data: 0.0000  max mem: 18432

Train: data epoch: [1]  [1440/3000]  eta: 0:39:19  lr: 0.000030  loss: 0.4778  time: 1.5074  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1440/3000]  eta: 0:39:19  lr: 0.000030  loss: 0.4039  time: 1.5071  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1445/3000]  eta: 0:39:11  lr: 0.000030  loss: 0.2108  time: 1.5169  data: 0.0000  max mem: 18067Train: data epoch: [1]  [1445/3000]  eta: 0:39:12  lr: 0.000030  loss: 0.6295  time: 1.5172  data: 0.0000  max mem: 18432

Train: data epoch: [1]  [1450/3000]  eta: 0:39:04  lr: 0.000030  loss: 0.2813  time: 1.5088  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1450/3000]  eta: 0:39:03  lr: 0.000030  loss: 0.3680  time: 1.5086  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1455/3000]  eta: 0:38:56  lr: 0.000030  loss: 0.7684  time: 1.5080  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1455/3000]  eta: 0:38:55  lr: 0.000030  loss: 1.1931  time: 1.5079  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1460/3000]  eta: 0:38:49  lr: 0.000030  loss: 0.2462  time: 1.5004  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1460/3000]  eta: 0:38:48  lr: 0.000030  loss: 0.2216  time: 1.5001  data: 0.0000  max mem: 18067
Train: data epoch: [1]  [1465/3000]  eta: 0:38:41  lr: 0.000030  loss: 0.2931  time: 1.4870  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1465/3000]  eta: 0:38:40  lr: 0.000030  loss: 0.1340  time: 1.4868  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1470/3000]  eta: 0:38:34  lr: 0.000030  loss: 0.7663  time: 1.5069  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1470/3000]  eta: 0:38:33  lr: 0.000030  loss: 0.3935  time: 1.5066  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1475/3000]  eta: 0:38:26  lr: 0.000030  loss: 0.5034  time: 1.5486  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1475/3000]  eta: 0:38:26  lr: 0.000030  loss: 0.2085  time: 1.5483  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1480/3000]  eta: 0:38:19  lr: 0.000030  loss: 0.2684  time: 1.5418  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1480/3000]  eta: 0:38:18  lr: 0.000030  loss: 0.3759  time: 1.5415  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1485/3000]  eta: 0:38:11  lr: 0.000030  loss: 0.2422  time: 1.5271  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1485/3000]  eta: 0:38:10  lr: 0.000030  loss: 0.1773  time: 1.5268  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1490/3000]  eta: 0:38:03  lr: 0.000030  loss: 0.2764  time: 1.5052  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1490/3000]  eta: 0:38:03  lr: 0.000030  loss: 0.7975  time: 1.5050  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1495/3000]  eta: 0:37:56  lr: 0.000030  loss: 0.1527  time: 1.4918  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1495/3000]  eta: 0:37:55  lr: 0.000030  loss: 0.3560  time: 1.4916  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1500/3000]  eta: 0:37:48  lr: 0.000030  loss: 0.1406  time: 1.4795  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1500/3000]  eta: 0:37:48  lr: 0.000030  loss: 0.3404  time: 1.4792  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1505/3000]  eta: 0:37:41  lr: 0.000030  loss: 0.4144  time: 1.5080  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1505/3000]  eta: 0:37:40  lr: 0.000030  loss: 0.1843  time: 1.5078  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1510/3000]  eta: 0:37:33  lr: 0.000030  loss: 0.4261  time: 1.5043  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1510/3000]  eta: 0:37:32  lr: 0.000030  loss: 0.2659  time: 1.5041  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1515/3000]  eta: 0:37:25  lr: 0.000030  loss: 0.5049  time: 1.5119  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1515/3000]  eta: 0:37:25  lr: 0.000030  loss: 0.5268  time: 1.5117  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1520/3000]  eta: 0:37:18  lr: 0.000030  loss: 1.6995  time: 1.5091  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1520/3000]  eta: 0:37:17  lr: 0.000030  loss: 0.2129  time: 1.5088  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1525/3000]  eta: 0:37:10  lr: 0.000030  loss: 0.8409  time: 1.5090  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1525/3000]  eta: 0:37:10  lr: 0.000030  loss: 0.2038  time: 1.5088  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1530/3000]  eta: 0:37:03  lr: 0.000030  loss: 0.6319  time: 1.5229  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1530/3000]  eta: 0:37:02  lr: 0.000030  loss: 0.3445  time: 1.5227  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1535/3000]  eta: 0:36:55  lr: 0.000030  loss: 0.1789  time: 1.5161  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1535/3000]  eta: 0:36:55  lr: 0.000030  loss: 0.3993  time: 1.5158  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1540/3000]  eta: 0:36:48  lr: 0.000030  loss: 0.5322  time: 1.5432  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1540/3000]  eta: 0:36:48  lr: 0.000030  loss: 0.1796  time: 1.5430  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1545/3000]  eta: 0:36:40  lr: 0.000030  loss: 0.7497  time: 1.5285  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1545/3000]  eta: 0:36:40  lr: 0.000030  loss: 0.2383  time: 1.5282  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1550/3000]  eta: 0:36:33  lr: 0.000030  loss: 0.3521  time: 1.5369  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1550/3000]  eta: 0:36:33  lr: 0.000030  loss: 0.3156  time: 1.5367  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1555/3000]  eta: 0:36:25  lr: 0.000030  loss: 0.8118  time: 1.5135  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1555/3000]  eta: 0:36:25  lr: 0.000030  loss: 0.2407  time: 1.5132  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1560/3000]  eta: 0:36:18  lr: 0.000030  loss: 0.2834  time: 1.5162  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1560/3000]  eta: 0:36:17  lr: 0.000030  loss: 0.2295  time: 1.5160  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1565/3000]  eta: 0:36:10  lr: 0.000030  loss: 0.1308  time: 1.5185  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1565/3000]  eta: 0:36:10  lr: 0.000030  loss: 0.7541  time: 1.5182  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1570/3000]  eta: 0:36:03  lr: 0.000030  loss: 0.1916  time: 1.5232  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1570/3000]  eta: 0:36:02  lr: 0.000030  loss: 0.4852  time: 1.5230  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1575/3000]  eta: 0:35:55  lr: 0.000030  loss: 0.7705  time: 1.5439  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1575/3000]  eta: 0:35:55  lr: 0.000030  loss: 0.1623  time: 1.5436  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1580/3000]  eta: 0:35:48  lr: 0.000030  loss: 0.2707  time: 1.5344  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1580/3000]  eta: 0:35:48  lr: 0.000030  loss: 0.2072  time: 1.5342  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1585/3000]  eta: 0:35:41  lr: 0.000030  loss: 0.6435  time: 1.5499  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1585/3000]  eta: 0:35:40  lr: 0.000030  loss: 0.4798  time: 1.5498  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1590/3000]  eta: 0:35:33  lr: 0.000030  loss: 0.3074  time: 1.5260  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1590/3000]  eta: 0:35:32  lr: 0.000030  loss: 0.4841  time: 1.5258  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1595/3000]  eta: 0:35:25  lr: 0.000030  loss: 1.0581  time: 1.5198  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1595/3000]  eta: 0:35:25  lr: 0.000030  loss: 0.6733  time: 1.5196  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1600/3000]  eta: 0:35:17  lr: 0.000030  loss: 0.5861  time: 1.4989  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1600/3000]  eta: 0:35:17  lr: 0.000030  loss: 0.2447  time: 1.4987  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1605/3000]  eta: 0:35:10  lr: 0.000030  loss: 0.2969  time: 1.4880  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1605/3000]  eta: 0:35:09  lr: 0.000030  loss: 0.6914  time: 1.4877  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1610/3000]  eta: 0:35:02  lr: 0.000030  loss: 0.8910  time: 1.5077  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1610/3000]  eta: 0:35:02  lr: 0.000030  loss: 0.2941  time: 1.5073  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1615/3000]  eta: 0:34:55  lr: 0.000030  loss: 0.4370  time: 1.4945  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1615/3000]  eta: 0:34:54  lr: 0.000030  loss: 0.4550  time: 1.4942  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1620/3000]  eta: 0:34:47  lr: 0.000030  loss: 0.2069  time: 1.5015  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1620/3000]  eta: 0:34:47  lr: 0.000030  loss: 1.1867  time: 1.5013  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1625/3000]  eta: 0:34:40  lr: 0.000030  loss: 0.5750  time: 1.5199  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1625/3000]  eta: 0:34:39  lr: 0.000030  loss: 0.7423  time: 1.5196  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1630/3000]  eta: 0:34:32  lr: 0.000030  loss: 0.3246  time: 1.5168  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1630/3000]  eta: 0:34:32  lr: 0.000030  loss: 0.4393  time: 1.5165  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1635/3000]  eta: 0:34:25  lr: 0.000030  loss: 0.7639  time: 1.5386  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1635/3000]  eta: 0:34:24  lr: 0.000030  loss: 0.7252  time: 1.5383  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1640/3000]  eta: 0:34:18  lr: 0.000030  loss: 0.5153  time: 1.5635  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1640/3000]  eta: 0:34:17  lr: 0.000030  loss: 0.7203  time: 1.5633  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1645/3000]  eta: 0:34:10  lr: 0.000030  loss: 0.3171  time: 1.5505  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1645/3000]  eta: 0:34:10  lr: 0.000030  loss: 0.3093  time: 1.5502  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1650/3000]  eta: 0:34:02  lr: 0.000030  loss: 0.4755  time: 1.5346  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1650/3000]  eta: 0:34:02  lr: 0.000030  loss: 0.3730  time: 1.5345  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1655/3000]  eta: 0:33:54  lr: 0.000030  loss: 0.4434  time: 1.5090  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1655/3000]  eta: 0:33:54  lr: 0.000030  loss: 0.6429  time: 1.5089  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1660/3000]  eta: 0:33:47  lr: 0.000030  loss: 0.6495  time: 1.4879  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1660/3000]  eta: 0:33:47  lr: 0.000030  loss: 0.5862  time: 1.4876  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1665/3000]  eta: 0:33:39  lr: 0.000030  loss: 0.2174  time: 1.4693  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1665/3000]  eta: 0:33:39  lr: 0.000030  loss: 0.2313  time: 1.4692  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1670/3000]  eta: 0:33:32  lr: 0.000030  loss: 0.5928  time: 1.4914  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1670/3000]  eta: 0:33:31  lr: 0.000030  loss: 0.4840  time: 1.4912  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1675/3000]  eta: 0:33:24  lr: 0.000030  loss: 0.5666  time: 1.5152  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1675/3000]  eta: 0:33:24  lr: 0.000030  loss: 0.6201  time: 1.5150  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1680/3000]  eta: 0:33:17  lr: 0.000030  loss: 0.2558  time: 1.5134  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1680/3000]  eta: 0:33:16  lr: 0.000030  loss: 0.3318  time: 1.5132  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1685/3000]  eta: 0:33:09  lr: 0.000030  loss: 0.4695  time: 1.5246  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1685/3000]  eta: 0:33:09  lr: 0.000030  loss: 0.5408  time: 1.5243  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1690/3000]  eta: 0:33:01  lr: 0.000030  loss: 0.1453  time: 1.5001  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1690/3000]  eta: 0:33:01  lr: 0.000030  loss: 0.2690  time: 1.4999  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1695/3000]  eta: 0:32:53  lr: 0.000030  loss: 0.0413  time: 1.4899  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1695/3000]  eta: 0:32:54  lr: 0.000030  loss: 0.2749  time: 1.4902  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1700/3000]  eta: 0:32:46  lr: 0.000030  loss: 0.7441  time: 1.4917  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1700/3000]  eta: 0:32:46  lr: 0.000030  loss: 0.2499  time: 1.4914  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1705/3000]  eta: 0:32:39  lr: 0.000030  loss: 0.8502  time: 1.5005  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1705/3000]  eta: 0:32:38  lr: 0.000030  loss: 0.8427  time: 1.5003  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1710/3000]  eta: 0:32:31  lr: 0.000030  loss: 0.1561  time: 1.5201  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1710/3000]  eta: 0:32:31  lr: 0.000030  loss: 0.2666  time: 1.5198  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1715/3000]  eta: 0:32:24  lr: 0.000030  loss: 0.3657  time: 1.5272  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1715/3000]  eta: 0:32:23  lr: 0.000030  loss: 0.3741  time: 1.5271  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1720/3000]  eta: 0:32:16  lr: 0.000030  loss: 0.1551  time: 1.4998  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1720/3000]  eta: 0:32:15  lr: 0.000030  loss: 0.2239  time: 1.4997  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1725/3000]  eta: 0:32:08  lr: 0.000030  loss: 0.1292  time: 1.4755  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1725/3000]  eta: 0:32:07  lr: 0.000030  loss: 0.6660  time: 1.4753  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1730/3000]  eta: 0:32:00  lr: 0.000030  loss: 0.5178  time: 1.4613  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1730/3000]  eta: 0:32:00  lr: 0.000030  loss: 0.5584  time: 1.4610  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1735/3000]  eta: 0:31:53  lr: 0.000030  loss: 0.3563  time: 1.4745  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1735/3000]  eta: 0:31:52  lr: 0.000030  loss: 0.3055  time: 1.4742  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1740/3000]  eta: 0:31:45  lr: 0.000030  loss: 0.2221  time: 1.4945  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1740/3000]  eta: 0:31:45  lr: 0.000030  loss: 0.4038  time: 1.4941  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1745/3000]  eta: 0:31:37  lr: 0.000030  loss: 0.5375  time: 1.5047  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1745/3000]  eta: 0:31:37  lr: 0.000030  loss: 0.2662  time: 1.5044  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1750/3000]  eta: 0:31:30  lr: 0.000030  loss: 0.0253  time: 1.5252  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1750/3000]  eta: 0:31:30  lr: 0.000030  loss: 0.3813  time: 1.5249  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1755/3000]  eta: 0:31:23  lr: 0.000030  loss: 0.5528  time: 1.5184  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1755/3000]  eta: 0:31:22  lr: 0.000030  loss: 0.1342  time: 1.5181  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1760/3000]  eta: 0:31:15  lr: 0.000030  loss: 0.7770  time: 1.5416  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1760/3000]  eta: 0:31:15  lr: 0.000030  loss: 0.3155  time: 1.5413  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1765/3000]  eta: 0:31:08  lr: 0.000030  loss: 0.2899  time: 1.5527  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1765/3000]  eta: 0:31:07  lr: 0.000030  loss: 0.4439  time: 1.5524  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1770/3000]  eta: 0:31:00  lr: 0.000030  loss: 0.2982  time: 1.5585  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1770/3000]  eta: 0:31:00  lr: 0.000030  loss: 0.4905  time: 1.5583  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1775/3000]  eta: 0:30:53  lr: 0.000030  loss: 0.4726  time: 1.5639  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1775/3000]  eta: 0:30:53  lr: 0.000030  loss: 0.1278  time: 1.5637  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1780/3000]  eta: 0:30:46  lr: 0.000030  loss: 0.2494  time: 1.5548  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1780/3000]  eta: 0:30:45  lr: 0.000030  loss: 0.4957  time: 1.5546  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1785/3000]  eta: 0:30:38  lr: 0.000030  loss: 0.2082  time: 1.5544  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1785/3000]  eta: 0:30:38  lr: 0.000030  loss: 0.7185  time: 1.5542  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1790/3000]  eta: 0:30:31  lr: 0.000030  loss: 0.3498  time: 1.5507  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1790/3000]  eta: 0:30:30  lr: 0.000030  loss: 0.2982  time: 1.5504  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1795/3000]  eta: 0:30:23  lr: 0.000030  loss: 0.2259  time: 1.5467  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1795/3000]  eta: 0:30:23  lr: 0.000030  loss: 0.2375  time: 1.5464  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1800/3000]  eta: 0:30:16  lr: 0.000030  loss: 0.7314  time: 1.5563  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1800/3000]  eta: 0:30:16  lr: 0.000030  loss: 0.4092  time: 1.5561  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1805/3000]  eta: 0:30:08  lr: 0.000030  loss: 0.4053  time: 1.5606  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1805/3000]  eta: 0:30:08  lr: 0.000030  loss: 0.1315  time: 1.5603  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1810/3000]  eta: 0:30:01  lr: 0.000030  loss: 0.6709  time: 1.5516  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1810/3000]  eta: 0:30:01  lr: 0.000030  loss: 0.4089  time: 1.5514  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1815/3000]  eta: 0:29:53  lr: 0.000030  loss: 0.3361  time: 1.5335  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1815/3000]  eta: 0:29:53  lr: 0.000030  loss: 0.2972  time: 1.5332  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1820/3000]  eta: 0:29:46  lr: 0.000030  loss: 0.1839  time: 1.5295  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1820/3000]  eta: 0:29:46  lr: 0.000030  loss: 0.8876  time: 1.5293  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1825/3000]  eta: 0:29:38  lr: 0.000030  loss: 0.2591  time: 1.5231  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1825/3000]  eta: 0:29:38  lr: 0.000030  loss: 0.4542  time: 1.5229  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1830/3000]  eta: 0:29:31  lr: 0.000030  loss: 0.3010  time: 1.5126  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1830/3000]  eta: 0:29:30  lr: 0.000030  loss: 0.2894  time: 1.5124  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1835/3000]  eta: 0:29:23  lr: 0.000030  loss: 0.3676  time: 1.5277  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1835/3000]  eta: 0:29:23  lr: 0.000030  loss: 0.4233  time: 1.5275  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1840/3000]  eta: 0:29:16  lr: 0.000030  loss: 0.6218  time: 1.5234  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1840/3000]  eta: 0:29:15  lr: 0.000030  loss: 0.1305  time: 1.5231  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1845/3000]  eta: 0:29:08  lr: 0.000030  loss: 0.1446  time: 1.5212  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1845/3000]  eta: 0:29:08  lr: 0.000030  loss: 0.2339  time: 1.5210  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1850/3000]  eta: 0:29:01  lr: 0.000030  loss: 0.8229  time: 1.5304  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1850/3000]  eta: 0:29:00  lr: 0.000030  loss: 0.1236  time: 1.5301  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1855/3000]  eta: 0:28:53  lr: 0.000030  loss: 0.8271  time: 1.5434  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1855/3000]  eta: 0:28:53  lr: 0.000030  loss: 0.3021  time: 1.5432  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1860/3000]  eta: 0:28:46  lr: 0.000030  loss: 0.8461  time: 1.5400  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1860/3000]  eta: 0:28:45  lr: 0.000030  loss: 0.3032  time: 1.5398  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1865/3000]  eta: 0:28:38  lr: 0.000030  loss: 0.3567  time: 1.5352  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1865/3000]  eta: 0:28:38  lr: 0.000030  loss: 0.2263  time: 1.5349  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1870/3000]  eta: 0:28:31  lr: 0.000030  loss: 0.2112  time: 1.5428  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1870/3000]  eta: 0:28:30  lr: 0.000030  loss: 0.6685  time: 1.5426  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1875/3000]  eta: 0:28:23  lr: 0.000030  loss: 0.4352  time: 1.5260  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1875/3000]  eta: 0:28:23  lr: 0.000030  loss: 0.4484  time: 1.5258  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1880/3000]  eta: 0:28:16  lr: 0.000030  loss: 0.7574  time: 1.5213  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1880/3000]  eta: 0:28:15  lr: 0.000030  loss: 0.3427  time: 1.5211  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1885/3000]  eta: 0:28:08  lr: 0.000030  loss: 0.3877  time: 1.5332  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1885/3000]  eta: 0:28:08  lr: 0.000030  loss: 0.3988  time: 1.5329  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1890/3000]  eta: 0:28:00  lr: 0.000030  loss: 0.1733  time: 1.5105  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1890/3000]  eta: 0:28:00  lr: 0.000030  loss: 0.4473  time: 1.5102  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1895/3000]  eta: 0:27:53  lr: 0.000030  loss: 0.1112  time: 1.5102  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1895/3000]  eta: 0:27:52  lr: 0.000030  loss: 0.7308  time: 1.5099  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1900/3000]  eta: 0:27:45  lr: 0.000030  loss: 0.1742  time: 1.5038  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1900/3000]  eta: 0:27:45  lr: 0.000030  loss: 0.2756  time: 1.5034  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1905/3000]  eta: 0:27:38  lr: 0.000030  loss: 0.0654  time: 1.5123  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1905/3000]  eta: 0:27:37  lr: 0.000030  loss: 0.2769  time: 1.5120  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1910/3000]  eta: 0:27:30  lr: 0.000030  loss: 0.3147  time: 1.5369  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1910/3000]  eta: 0:27:30  lr: 0.000030  loss: 0.2294  time: 1.5367  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1915/3000]  eta: 0:27:23  lr: 0.000030  loss: 0.9011  time: 1.5215  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1915/3000]  eta: 0:27:22  lr: 0.000030  loss: 0.4756  time: 1.5212  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1920/3000]  eta: 0:27:15  lr: 0.000030  loss: 0.3342  time: 1.5169  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1920/3000]  eta: 0:27:15  lr: 0.000030  loss: 0.3298  time: 1.5167  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1925/3000]  eta: 0:27:07  lr: 0.000030  loss: 0.2936  time: 1.5099  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1925/3000]  eta: 0:27:07  lr: 0.000030  loss: 0.6152  time: 1.5097  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1930/3000]  eta: 0:27:00  lr: 0.000030  loss: 0.2840  time: 1.4896  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1930/3000]  eta: 0:26:59  lr: 0.000030  loss: 0.6308  time: 1.4893  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1935/3000]  eta: 0:26:52  lr: 0.000030  loss: 0.2825  time: 1.4879  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1935/3000]  eta: 0:26:52  lr: 0.000030  loss: 0.3931  time: 1.4877  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1940/3000]  eta: 0:26:44  lr: 0.000030  loss: 0.3348  time: 1.5073  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1940/3000]  eta: 0:26:44  lr: 0.000030  loss: 0.8865  time: 1.5070  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1945/3000]  eta: 0:26:37  lr: 0.000030  loss: 0.7556  time: 1.5066  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1945/3000]  eta: 0:26:37  lr: 0.000030  loss: 0.7688  time: 1.5064  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1950/3000]  eta: 0:26:29  lr: 0.000030  loss: 0.3084  time: 1.5104  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1950/3000]  eta: 0:26:29  lr: 0.000030  loss: 0.4478  time: 1.5102  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1955/3000]  eta: 0:26:22  lr: 0.000030  loss: 0.1854  time: 1.5370  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1955/3000]  eta: 0:26:22  lr: 0.000030  loss: 0.6849  time: 1.5368  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1960/3000]  eta: 0:26:14  lr: 0.000030  loss: 0.1541  time: 1.5123  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1960/3000]  eta: 0:26:14  lr: 0.000030  loss: 0.2446  time: 1.5122  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1965/3000]  eta: 0:26:06  lr: 0.000030  loss: 0.8908  time: 1.4936  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1965/3000]  eta: 0:26:06  lr: 0.000030  loss: 0.0934  time: 1.4933  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1970/3000]  eta: 0:25:59  lr: 0.000030  loss: 1.0112  time: 1.4826  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1970/3000]  eta: 0:25:59  lr: 0.000030  loss: 0.3583  time: 1.4824  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1975/3000]  eta: 0:25:51  lr: 0.000030  loss: 0.2657  time: 1.4771  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1975/3000]  eta: 0:25:51  lr: 0.000030  loss: 0.0550  time: 1.4768  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1980/3000]  eta: 0:25:44  lr: 0.000030  loss: 0.2098  time: 1.4920  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1980/3000]  eta: 0:25:43  lr: 0.000030  loss: 0.4799  time: 1.4917  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1985/3000]  eta: 0:25:36  lr: 0.000030  loss: 0.1938  time: 1.4917  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1985/3000]  eta: 0:25:36  lr: 0.000030  loss: 0.4954  time: 1.4915  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1990/3000]  eta: 0:25:28  lr: 0.000030  loss: 0.3329  time: 1.4995  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1990/3000]  eta: 0:25:28  lr: 0.000030  loss: 0.5159  time: 1.4992  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [1995/3000]  eta: 0:25:21  lr: 0.000030  loss: 0.6634  time: 1.5116  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [1995/3000]  eta: 0:25:21  lr: 0.000030  loss: 0.4154  time: 1.5114  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2000/3000]  eta: 0:25:13  lr: 0.000030  loss: 0.2684  time: 1.5004  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2000/3000]  eta: 0:25:13  lr: 0.000030  loss: 0.2169  time: 1.5003  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2005/3000]  eta: 0:25:06  lr: 0.000030  loss: 0.0423  time: 1.5019  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2005/3000]  eta: 0:25:05  lr: 0.000030  loss: 0.6274  time: 1.5016  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2010/3000]  eta: 0:24:58  lr: 0.000030  loss: 0.7249  time: 1.5007  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2010/3000]  eta: 0:24:58  lr: 0.000030  loss: 0.5548  time: 1.5005  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2015/3000]  eta: 0:24:50  lr: 0.000030  loss: 0.1966  time: 1.4881  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2015/3000]  eta: 0:24:50  lr: 0.000030  loss: 0.2266  time: 1.4878  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2020/3000]  eta: 0:24:43  lr: 0.000030  loss: 0.6352  time: 1.5077  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2020/3000]  eta: 0:24:43  lr: 0.000030  loss: 0.3003  time: 1.5074  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2025/3000]  eta: 0:24:35  lr: 0.000030  loss: 0.4311  time: 1.5241  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2025/3000]  eta: 0:24:35  lr: 0.000030  loss: 0.3456  time: 1.5239  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2030/3000]  eta: 0:24:28  lr: 0.000030  loss: 0.4878  time: 1.5117  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2030/3000]  eta: 0:24:27  lr: 0.000030  loss: 0.4222  time: 1.5115  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2035/3000]  eta: 0:24:20  lr: 0.000030  loss: 0.3580  time: 1.5239  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2035/3000]  eta: 0:24:20  lr: 0.000030  loss: 0.1995  time: 1.5236  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2040/3000]  eta: 0:24:13  lr: 0.000030  loss: 0.3104  time: 1.5275  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2040/3000]  eta: 0:24:13  lr: 0.000030  loss: 0.1700  time: 1.5274  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2045/3000]  eta: 0:24:05  lr: 0.000030  loss: 1.3107  time: 1.5260  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2045/3000]  eta: 0:24:05  lr: 0.000030  loss: 0.2506  time: 1.5258  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2050/3000]  eta: 0:23:58  lr: 0.000030  loss: 0.5627  time: 1.5492  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2050/3000]  eta: 0:23:57  lr: 0.000030  loss: 0.2918  time: 1.5489  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2055/3000]  eta: 0:23:50  lr: 0.000030  loss: 0.4959  time: 1.5499  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2055/3000]  eta: 0:23:50  lr: 0.000030  loss: 0.6830  time: 1.5497  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2060/3000]  eta: 0:23:43  lr: 0.000030  loss: 0.7640  time: 1.5383  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2060/3000]  eta: 0:23:42  lr: 0.000030  loss: 0.5026  time: 1.5380  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2065/3000]  eta: 0:23:35  lr: 0.000030  loss: 0.6214  time: 1.5406  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2065/3000]  eta: 0:23:35  lr: 0.000030  loss: 0.3551  time: 1.5403  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2070/3000]  eta: 0:23:28  lr: 0.000030  loss: 0.7539  time: 1.5497  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2070/3000]  eta: 0:23:28  lr: 0.000030  loss: 0.2844  time: 1.5494  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2075/3000]  eta: 0:23:20  lr: 0.000030  loss: 0.4137  time: 1.5405  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2075/3000]  eta: 0:23:20  lr: 0.000030  loss: 1.4923  time: 1.5402  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2080/3000]  eta: 0:23:13  lr: 0.000030  loss: 1.1523  time: 1.5437  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2080/3000]  eta: 0:23:12  lr: 0.000030  loss: 0.5215  time: 1.5436  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2085/3000]  eta: 0:23:05  lr: 0.000030  loss: 0.1711  time: 1.5298  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2085/3000]  eta: 0:23:05  lr: 0.000030  loss: 0.4652  time: 1.5296  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2090/3000]  eta: 0:22:57  lr: 0.000030  loss: 0.3339  time: 1.5132  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2090/3000]  eta: 0:22:57  lr: 0.000030  loss: 0.6845  time: 1.5130  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2095/3000]  eta: 0:22:50  lr: 0.000030  loss: 0.5836  time: 1.5077  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2095/3000]  eta: 0:22:50  lr: 0.000030  loss: 0.4584  time: 1.5075  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2100/3000]  eta: 0:22:42  lr: 0.000030  loss: 0.2604  time: 1.5051  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2100/3000]  eta: 0:22:42  lr: 0.000030  loss: 0.3520  time: 1.5048  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2105/3000]  eta: 0:22:35  lr: 0.000030  loss: 0.2236  time: 1.5039  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2105/3000]  eta: 0:22:34  lr: 0.000030  loss: 0.4903  time: 1.5037  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2110/3000]  eta: 0:22:27  lr: 0.000030  loss: 0.3256  time: 1.5093  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2110/3000]  eta: 0:22:27  lr: 0.000030  loss: 0.3967  time: 1.5091  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2115/3000]  eta: 0:22:20  lr: 0.000030  loss: 0.3607  time: 1.5138  data: 0.0000  max mem: 18432Train: data epoch: [1]  [2115/3000]  eta: 0:22:19  lr: 0.000030  loss: 0.3886  time: 1.5136  data: 0.0000  max mem: 18151

Train: data epoch: [1]  [2120/3000]  eta: 0:22:12  lr: 0.000030  loss: 0.1316  time: 1.5027  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2120/3000]  eta: 0:22:12  lr: 0.000030  loss: 1.1794  time: 1.5025  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2125/3000]  eta: 0:22:04  lr: 0.000030  loss: 0.2451  time: 1.5108  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2125/3000]  eta: 0:22:04  lr: 0.000030  loss: 0.1807  time: 1.5106  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2130/3000]  eta: 0:21:57  lr: 0.000030  loss: 0.5987  time: 1.5068  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2130/3000]  eta: 0:21:57  lr: 0.000030  loss: 0.2532  time: 1.5065  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2135/3000]  eta: 0:21:49  lr: 0.000030  loss: 0.3472  time: 1.4855  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2135/3000]  eta: 0:21:49  lr: 0.000030  loss: 0.1368  time: 1.4853  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2140/3000]  eta: 0:21:41  lr: 0.000030  loss: 0.7173  time: 1.4815  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2140/3000]  eta: 0:21:41  lr: 0.000030  loss: 0.3787  time: 1.4812  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2145/3000]  eta: 0:21:34  lr: 0.000030  loss: 0.5388  time: 1.4725  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2145/3000]  eta: 0:21:34  lr: 0.000030  loss: 0.1700  time: 1.4722  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2150/3000]  eta: 0:21:26  lr: 0.000030  loss: 0.7307  time: 1.4567  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2150/3000]  eta: 0:21:26  lr: 0.000030  loss: 0.5753  time: 1.4564  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2155/3000]  eta: 0:21:18  lr: 0.000030  loss: 0.5834  time: 1.4632  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2155/3000]  eta: 0:21:18  lr: 0.000030  loss: 1.0452  time: 1.4630  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2160/3000]  eta: 0:21:11  lr: 0.000030  loss: 1.1931  time: 1.4811  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2160/3000]  eta: 0:21:11  lr: 0.000030  loss: 0.5113  time: 1.4809  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2165/3000]  eta: 0:21:03  lr: 0.000030  loss: 0.1724  time: 1.4964  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2165/3000]  eta: 0:21:03  lr: 0.000030  loss: 0.4642  time: 1.4961  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2170/3000]  eta: 0:20:56  lr: 0.000030  loss: 0.3423  time: 1.4894  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2170/3000]  eta: 0:20:55  lr: 0.000030  loss: 0.4069  time: 1.4891  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2175/3000]  eta: 0:20:48  lr: 0.000030  loss: 0.3472  time: 1.5025  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2175/3000]  eta: 0:20:48  lr: 0.000030  loss: 0.3871  time: 1.5022  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2180/3000]  eta: 0:20:40  lr: 0.000030  loss: 0.8124  time: 1.4965  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2180/3000]  eta: 0:20:40  lr: 0.000030  loss: 0.1312  time: 1.4963  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2185/3000]  eta: 0:20:33  lr: 0.000030  loss: 0.1473  time: 1.4781  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2185/3000]  eta: 0:20:33  lr: 0.000030  loss: 0.2702  time: 1.4778  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2190/3000]  eta: 0:20:25  lr: 0.000030  loss: 0.8326  time: 1.5218  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2190/3000]  eta: 0:20:25  lr: 0.000030  loss: 0.3204  time: 1.5216  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2195/3000]  eta: 0:20:18  lr: 0.000030  loss: 0.3144  time: 1.5175  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2195/3000]  eta: 0:20:18  lr: 0.000030  loss: 0.1751  time: 1.5172  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2200/3000]  eta: 0:20:10  lr: 0.000030  loss: 0.3599  time: 1.5039  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2200/3000]  eta: 0:20:10  lr: 0.000030  loss: 0.2243  time: 1.5037  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2205/3000]  eta: 0:20:03  lr: 0.000030  loss: 0.2494  time: 1.5052  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2205/3000]  eta: 0:20:02  lr: 0.000030  loss: 0.0995  time: 1.5050  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2210/3000]  eta: 0:19:55  lr: 0.000030  loss: 0.2676  time: 1.4777  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2210/3000]  eta: 0:19:55  lr: 0.000030  loss: 0.3824  time: 1.4774  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2215/3000]  eta: 0:19:47  lr: 0.000030  loss: 0.2419  time: 1.4916  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2215/3000]  eta: 0:19:47  lr: 0.000030  loss: 0.8094  time: 1.4913  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2220/3000]  eta: 0:19:40  lr: 0.000030  loss: 0.4361  time: 1.5074  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2220/3000]  eta: 0:19:40  lr: 0.000030  loss: 0.5466  time: 1.5072  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2225/3000]  eta: 0:19:32  lr: 0.000030  loss: 0.0764  time: 1.5290  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2225/3000]  eta: 0:19:32  lr: 0.000030  loss: 0.6396  time: 1.5288  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2230/3000]  eta: 0:19:25  lr: 0.000030  loss: 0.0776  time: 1.5223  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2230/3000]  eta: 0:19:24  lr: 0.000030  loss: 0.3007  time: 1.5220  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2235/3000]  eta: 0:19:17  lr: 0.000030  loss: 0.1929  time: 1.4947  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2235/3000]  eta: 0:19:17  lr: 0.000030  loss: 0.5335  time: 1.4944  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2240/3000]  eta: 0:19:09  lr: 0.000030  loss: 0.2882  time: 1.5002  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2240/3000]  eta: 0:19:09  lr: 0.000030  loss: 0.2420  time: 1.4999  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2245/3000]  eta: 0:19:02  lr: 0.000030  loss: 0.2787  time: 1.4929  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2245/3000]  eta: 0:19:02  lr: 0.000030  loss: 1.5007  time: 1.4927  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2250/3000]  eta: 0:18:54  lr: 0.000030  loss: 0.5608  time: 1.4821  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2250/3000]  eta: 0:18:54  lr: 0.000030  loss: 0.2875  time: 1.4818  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2255/3000]  eta: 0:18:47  lr: 0.000030  loss: 0.4156  time: 1.4894  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2255/3000]  eta: 0:18:46  lr: 0.000030  loss: 0.0876  time: 1.4892  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2260/3000]  eta: 0:18:39  lr: 0.000030  loss: 0.4924  time: 1.5051  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2260/3000]  eta: 0:18:39  lr: 0.000030  loss: 1.0876  time: 1.5049  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2265/3000]  eta: 0:18:32  lr: 0.000030  loss: 0.2880  time: 1.5049  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2265/3000]  eta: 0:18:31  lr: 0.000030  loss: 0.3347  time: 1.5047  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2270/3000]  eta: 0:18:24  lr: 0.000030  loss: 0.3905  time: 1.5263  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2270/3000]  eta: 0:18:24  lr: 0.000030  loss: 0.2674  time: 1.5261  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2275/3000]  eta: 0:18:16  lr: 0.000030  loss: 0.1365  time: 1.5220  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2275/3000]  eta: 0:18:16  lr: 0.000030  loss: 0.1684  time: 1.5217  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2280/3000]  eta: 0:18:09  lr: 0.000030  loss: 0.3633  time: 1.5077  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2280/3000]  eta: 0:18:09  lr: 0.000030  loss: 0.2198  time: 1.5073  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2285/3000]  eta: 0:18:01  lr: 0.000030  loss: 0.5808  time: 1.5203  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2285/3000]  eta: 0:18:01  lr: 0.000030  loss: 0.2053  time: 1.5199  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2290/3000]  eta: 0:17:54  lr: 0.000030  loss: 0.3838  time: 1.5266  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2290/3000]  eta: 0:17:54  lr: 0.000030  loss: 0.7092  time: 1.5262  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2295/3000]  eta: 0:17:46  lr: 0.000030  loss: 0.3764  time: 1.5356  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2295/3000]  eta: 0:17:46  lr: 0.000030  loss: 0.2906  time: 1.5352  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2300/3000]  eta: 0:17:39  lr: 0.000030  loss: 0.3070  time: 1.5263  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2300/3000]  eta: 0:17:39  lr: 0.000030  loss: 0.3730  time: 1.5260  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2305/3000]  eta: 0:17:31  lr: 0.000030  loss: 0.8881  time: 1.5114  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2305/3000]  eta: 0:17:31  lr: 0.000030  loss: 0.6383  time: 1.5112  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2310/3000]  eta: 0:17:24  lr: 0.000030  loss: 1.6384  time: 1.5147  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2310/3000]  eta: 0:17:23  lr: 0.000030  loss: 0.6558  time: 1.5145  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2315/3000]  eta: 0:17:16  lr: 0.000030  loss: 0.2097  time: 1.5121  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2315/3000]  eta: 0:17:16  lr: 0.000030  loss: 0.2606  time: 1.5118  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2320/3000]  eta: 0:17:08  lr: 0.000030  loss: 0.1304  time: 1.5144  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2320/3000]  eta: 0:17:08  lr: 0.000030  loss: 0.1796  time: 1.5142  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2325/3000]  eta: 0:17:01  lr: 0.000030  loss: 0.2906  time: 1.5139  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2325/3000]  eta: 0:17:01  lr: 0.000030  loss: 0.2212  time: 1.5136  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2330/3000]  eta: 0:16:53  lr: 0.000030  loss: 0.6660  time: 1.5121  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2330/3000]  eta: 0:16:53  lr: 0.000030  loss: 0.9958  time: 1.5117  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2335/3000]  eta: 0:16:46  lr: 0.000030  loss: 0.2589  time: 1.5131  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2335/3000]  eta: 0:16:46  lr: 0.000030  loss: 0.2013  time: 1.5129  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2340/3000]  eta: 0:16:38  lr: 0.000030  loss: 0.4331  time: 1.5147  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2340/3000]  eta: 0:16:38  lr: 0.000030  loss: 0.5817  time: 1.5144  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2345/3000]  eta: 0:16:31  lr: 0.000030  loss: 0.2017  time: 1.5042  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2345/3000]  eta: 0:16:30  lr: 0.000030  loss: 0.3775  time: 1.5038  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2350/3000]  eta: 0:16:23  lr: 0.000030  loss: 0.2220  time: 1.5063  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2350/3000]  eta: 0:16:23  lr: 0.000030  loss: 0.1633  time: 1.5061  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2355/3000]  eta: 0:16:15  lr: 0.000030  loss: 0.3929  time: 1.5100  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2355/3000]  eta: 0:16:15  lr: 0.000030  loss: 0.7876  time: 1.5097  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2360/3000]  eta: 0:16:08  lr: 0.000030  loss: 0.4828  time: 1.5186  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2360/3000]  eta: 0:16:08  lr: 0.000030  loss: 0.6719  time: 1.5184  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2365/3000]  eta: 0:16:00  lr: 0.000030  loss: 0.4899  time: 1.5059  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2365/3000]  eta: 0:16:00  lr: 0.000030  loss: 0.3527  time: 1.5058  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2370/3000]  eta: 0:15:53  lr: 0.000030  loss: 1.0824  time: 1.4961  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2370/3000]  eta: 0:15:53  lr: 0.000030  loss: 0.1937  time: 1.4958  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2375/3000]  eta: 0:15:45  lr: 0.000030  loss: 0.1815  time: 1.4742  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2375/3000]  eta: 0:15:45  lr: 0.000030  loss: 0.4363  time: 1.4740  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2380/3000]  eta: 0:15:37  lr: 0.000030  loss: 0.7353  time: 1.4738  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2380/3000]  eta: 0:15:37  lr: 0.000030  loss: 0.1486  time: 1.4735  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2385/3000]  eta: 0:15:30  lr: 0.000030  loss: 0.9339  time: 1.5043  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2385/3000]  eta: 0:15:30  lr: 0.000030  loss: 0.4422  time: 1.5040  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2390/3000]  eta: 0:15:22  lr: 0.000030  loss: 0.5158  time: 1.5143  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2390/3000]  eta: 0:15:22  lr: 0.000030  loss: 0.2589  time: 1.5141  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2395/3000]  eta: 0:15:15  lr: 0.000030  loss: 0.1197  time: 1.5390  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2395/3000]  eta: 0:15:15  lr: 0.000030  loss: 0.2433  time: 1.5387  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2400/3000]  eta: 0:15:07  lr: 0.000030  loss: 0.4343  time: 1.5356  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2400/3000]  eta: 0:15:07  lr: 0.000030  loss: 0.6284  time: 1.5353  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2405/3000]  eta: 0:15:00  lr: 0.000030  loss: 0.1100  time: 1.5274  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2405/3000]  eta: 0:15:00  lr: 0.000030  loss: 0.6203  time: 1.5272  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2410/3000]  eta: 0:14:52  lr: 0.000030  loss: 0.1664  time: 1.5200  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2410/3000]  eta: 0:14:52  lr: 0.000030  loss: 0.3491  time: 1.5197  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2415/3000]  eta: 0:14:45  lr: 0.000030  loss: 0.4226  time: 1.5169  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2415/3000]  eta: 0:14:44  lr: 0.000030  loss: 0.2531  time: 1.5167  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2420/3000]  eta: 0:14:37  lr: 0.000030  loss: 0.5738  time: 1.5064  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2420/3000]  eta: 0:14:37  lr: 0.000030  loss: 0.2813  time: 1.5062  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2425/3000]  eta: 0:14:30  lr: 0.000030  loss: 0.2878  time: 1.5133  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2425/3000]  eta: 0:14:29  lr: 0.000030  loss: 0.3301  time: 1.5131  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2430/3000]  eta: 0:14:22  lr: 0.000030  loss: 0.3820  time: 1.5054  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2430/3000]  eta: 0:14:22  lr: 0.000030  loss: 0.1012  time: 1.5052  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2435/3000]  eta: 0:14:14  lr: 0.000030  loss: 0.2715  time: 1.5015  data: 0.0000  max mem: 18151Train: data epoch: [1]  [2435/3000]  eta: 0:14:14  lr: 0.000030  loss: 0.5450  time: 1.5018  data: 0.0000  max mem: 18432

Train: data epoch: [1]  [2440/3000]  eta: 0:14:07  lr: 0.000030  loss: 0.5572  time: 1.4935  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2440/3000]  eta: 0:14:07  lr: 0.000030  loss: 0.3256  time: 1.4932  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2445/3000]  eta: 0:13:59  lr: 0.000030  loss: 0.3324  time: 1.5033  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2445/3000]  eta: 0:13:59  lr: 0.000030  loss: 0.4601  time: 1.5030  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2450/3000]  eta: 0:13:52  lr: 0.000030  loss: 0.1740  time: 1.5132  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2450/3000]  eta: 0:13:52  lr: 0.000030  loss: 0.2389  time: 1.5130  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2455/3000]  eta: 0:13:44  lr: 0.000030  loss: 0.5235  time: 1.5097  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2455/3000]  eta: 0:13:44  lr: 0.000030  loss: 0.2901  time: 1.5096  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2460/3000]  eta: 0:13:37  lr: 0.000030  loss: 0.3515  time: 1.5322  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2460/3000]  eta: 0:13:36  lr: 0.000030  loss: 0.3233  time: 1.5320  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2465/3000]  eta: 0:13:29  lr: 0.000030  loss: 0.9650  time: 1.5087  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2465/3000]  eta: 0:13:29  lr: 0.000030  loss: 0.3959  time: 1.5085  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2470/3000]  eta: 0:13:21  lr: 0.000030  loss: 0.1441  time: 1.5078  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2470/3000]  eta: 0:13:21  lr: 0.000030  loss: 0.4335  time: 1.5075  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2475/3000]  eta: 0:13:14  lr: 0.000030  loss: 0.3835  time: 1.5295  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2475/3000]  eta: 0:13:14  lr: 0.000030  loss: 0.3937  time: 1.5293  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2480/3000]  eta: 0:13:06  lr: 0.000030  loss: 0.2086  time: 1.5243  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2480/3000]  eta: 0:13:06  lr: 0.000030  loss: 0.1196  time: 1.5241  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2485/3000]  eta: 0:12:59  lr: 0.000030  loss: 0.9200  time: 1.5372  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2485/3000]  eta: 0:12:59  lr: 0.000030  loss: 0.1384  time: 1.5369  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2490/3000]  eta: 0:12:51  lr: 0.000030  loss: 0.2359  time: 1.5554  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2490/3000]  eta: 0:12:51  lr: 0.000030  loss: 1.0416  time: 1.5551  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2495/3000]  eta: 0:12:44  lr: 0.000030  loss: 0.4198  time: 1.5342  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2495/3000]  eta: 0:12:44  lr: 0.000030  loss: 0.2610  time: 1.5340  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2500/3000]  eta: 0:12:36  lr: 0.000030  loss: 0.8163  time: 1.5347  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2500/3000]  eta: 0:12:36  lr: 0.000030  loss: 0.1252  time: 1.5344  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2505/3000]  eta: 0:12:29  lr: 0.000030  loss: 0.1936  time: 1.5432  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2505/3000]  eta: 0:12:28  lr: 0.000030  loss: 0.3817  time: 1.5420  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2510/3000]  eta: 0:12:21  lr: 0.000030  loss: 0.4000  time: 1.5469  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2510/3000]  eta: 0:12:21  lr: 0.000030  loss: 0.3246  time: 1.5457  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2515/3000]  eta: 0:12:14  lr: 0.000030  loss: 0.2437  time: 1.5602  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2515/3000]  eta: 0:12:13  lr: 0.000030  loss: 0.2606  time: 1.5590  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2520/3000]  eta: 0:12:06  lr: 0.000030  loss: 0.3386  time: 1.5701  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2520/3000]  eta: 0:12:06  lr: 0.000030  loss: 0.1205  time: 1.5688  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2525/3000]  eta: 0:11:59  lr: 0.000030  loss: 0.1231  time: 1.5525  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2525/3000]  eta: 0:11:58  lr: 0.000030  loss: 0.1918  time: 1.5523  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2530/3000]  eta: 0:11:51  lr: 0.000030  loss: 0.2828  time: 1.5327  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2530/3000]  eta: 0:11:51  lr: 0.000030  loss: 0.7515  time: 1.5324  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2535/3000]  eta: 0:11:43  lr: 0.000030  loss: 0.2786  time: 1.5427  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2535/3000]  eta: 0:11:43  lr: 0.000030  loss: 0.2277  time: 1.5424  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2540/3000]  eta: 0:11:36  lr: 0.000030  loss: 0.6120  time: 1.5165  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2540/3000]  eta: 0:11:36  lr: 0.000030  loss: 0.9710  time: 1.5162  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2545/3000]  eta: 0:11:28  lr: 0.000030  loss: 0.8268  time: 1.5278  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2545/3000]  eta: 0:11:28  lr: 0.000030  loss: 0.5500  time: 1.5276  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2550/3000]  eta: 0:11:21  lr: 0.000030  loss: 0.3211  time: 1.5193  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2550/3000]  eta: 0:11:21  lr: 0.000030  loss: 0.3609  time: 1.5190  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2555/3000]  eta: 0:11:13  lr: 0.000030  loss: 0.3557  time: 1.5022  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2555/3000]  eta: 0:11:13  lr: 0.000030  loss: 0.4811  time: 1.5020  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2560/3000]  eta: 0:11:05  lr: 0.000030  loss: 0.3217  time: 1.4969  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2560/3000]  eta: 0:11:05  lr: 0.000030  loss: 0.2963  time: 1.4966  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2565/3000]  eta: 0:10:58  lr: 0.000030  loss: 0.1850  time: 1.4920  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2565/3000]  eta: 0:10:58  lr: 0.000030  loss: 0.4503  time: 1.4918  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2570/3000]  eta: 0:10:50  lr: 0.000030  loss: 0.9164  time: 1.5096  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2570/3000]  eta: 0:10:50  lr: 0.000030  loss: 0.8285  time: 1.5094  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2575/3000]  eta: 0:10:43  lr: 0.000030  loss: 0.8217  time: 1.5304  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2575/3000]  eta: 0:10:43  lr: 0.000030  loss: 0.3498  time: 1.5301  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2580/3000]  eta: 0:10:35  lr: 0.000030  loss: 0.6831  time: 1.5546  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2580/3000]  eta: 0:10:35  lr: 0.000030  loss: 0.2834  time: 1.5543  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2585/3000]  eta: 0:10:28  lr: 0.000030  loss: 0.3251  time: 1.5627  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2585/3000]  eta: 0:10:28  lr: 0.000030  loss: 0.3267  time: 1.5624  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2590/3000]  eta: 0:10:20  lr: 0.000030  loss: 0.7208  time: 1.5632  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2590/3000]  eta: 0:10:20  lr: 0.000030  loss: 0.3882  time: 1.5629  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2595/3000]  eta: 0:10:13  lr: 0.000030  loss: 0.2797  time: 1.5461  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2595/3000]  eta: 0:10:13  lr: 0.000030  loss: 0.2325  time: 1.5458  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2600/3000]  eta: 0:10:05  lr: 0.000030  loss: 0.5423  time: 1.5327  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2600/3000]  eta: 0:10:05  lr: 0.000030  loss: 0.2883  time: 1.5324  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2605/3000]  eta: 0:09:58  lr: 0.000030  loss: 0.2255  time: 1.5088  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2605/3000]  eta: 0:09:57  lr: 0.000030  loss: 0.5725  time: 1.5085  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2610/3000]  eta: 0:09:50  lr: 0.000030  loss: 0.3826  time: 1.4950  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2610/3000]  eta: 0:09:50  lr: 0.000030  loss: 0.2840  time: 1.4947  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2615/3000]  eta: 0:09:42  lr: 0.000030  loss: 0.3304  time: 1.4828  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2615/3000]  eta: 0:09:42  lr: 0.000030  loss: 0.1149  time: 1.4826  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2620/3000]  eta: 0:09:35  lr: 0.000030  loss: 0.2211  time: 1.4793  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2620/3000]  eta: 0:09:35  lr: 0.000030  loss: 0.2559  time: 1.4790  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2625/3000]  eta: 0:09:27  lr: 0.000030  loss: 0.4336  time: 1.4911  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2625/3000]  eta: 0:09:27  lr: 0.000030  loss: 0.5856  time: 1.4908  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2630/3000]  eta: 0:09:20  lr: 0.000030  loss: 0.3128  time: 1.4755  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2630/3000]  eta: 0:09:19  lr: 0.000030  loss: 0.1041  time: 1.4753  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2635/3000]  eta: 0:09:12  lr: 0.000030  loss: 0.3784  time: 1.4750  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2635/3000]  eta: 0:09:12  lr: 0.000030  loss: 0.4441  time: 1.4748  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2640/3000]  eta: 0:09:04  lr: 0.000030  loss: 0.6460  time: 1.4782  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2640/3000]  eta: 0:09:04  lr: 0.000030  loss: 0.4422  time: 1.4780  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2645/3000]  eta: 0:08:57  lr: 0.000030  loss: 0.2285  time: 1.4769  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2645/3000]  eta: 0:08:57  lr: 0.000030  loss: 0.7244  time: 1.4767  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2650/3000]  eta: 0:08:49  lr: 0.000030  loss: 0.1633  time: 1.4977  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2650/3000]  eta: 0:08:49  lr: 0.000030  loss: 0.5264  time: 1.4975  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2655/3000]  eta: 0:08:42  lr: 0.000030  loss: 0.3189  time: 1.4922  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2655/3000]  eta: 0:08:42  lr: 0.000030  loss: 0.9686  time: 1.4920  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2660/3000]  eta: 0:08:34  lr: 0.000030  loss: 0.4013  time: 1.4908  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2660/3000]  eta: 0:08:34  lr: 0.000030  loss: 0.2837  time: 1.4904  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2665/3000]  eta: 0:08:26  lr: 0.000030  loss: 0.8064  time: 1.4877  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2665/3000]  eta: 0:08:26  lr: 0.000030  loss: 0.5507  time: 1.4872  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2670/3000]  eta: 0:08:19  lr: 0.000030  loss: 0.3743  time: 1.5048  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2670/3000]  eta: 0:08:19  lr: 0.000030  loss: 0.3144  time: 1.5043  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2675/3000]  eta: 0:08:11  lr: 0.000030  loss: 0.0835  time: 1.4967  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2675/3000]  eta: 0:08:11  lr: 0.000030  loss: 0.3543  time: 1.4962  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2680/3000]  eta: 0:08:04  lr: 0.000030  loss: 0.1847  time: 1.4993  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2680/3000]  eta: 0:08:04  lr: 0.000030  loss: 0.2879  time: 1.4991  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2685/3000]  eta: 0:07:56  lr: 0.000030  loss: 0.3579  time: 1.5039  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2685/3000]  eta: 0:07:56  lr: 0.000030  loss: 0.4466  time: 1.5037  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2690/3000]  eta: 0:07:49  lr: 0.000030  loss: 0.2069  time: 1.4993  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2690/3000]  eta: 0:07:49  lr: 0.000030  loss: 0.2076  time: 1.4991  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2695/3000]  eta: 0:07:41  lr: 0.000030  loss: 0.2881  time: 1.5296  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2695/3000]  eta: 0:07:41  lr: 0.000030  loss: 0.9170  time: 1.5294  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2700/3000]  eta: 0:07:34  lr: 0.000030  loss: 0.2095  time: 1.5328  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2700/3000]  eta: 0:07:33  lr: 0.000030  loss: 0.1975  time: 1.5325  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2705/3000]  eta: 0:07:26  lr: 0.000030  loss: 0.3363  time: 1.5497  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2705/3000]  eta: 0:07:26  lr: 0.000030  loss: 0.3084  time: 1.5493  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2710/3000]  eta: 0:07:18  lr: 0.000030  loss: 0.6449  time: 1.5356  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2710/3000]  eta: 0:07:18  lr: 0.000030  loss: 0.7501  time: 1.5352  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2715/3000]  eta: 0:07:11  lr: 0.000030  loss: 0.5220  time: 1.5291  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2715/3000]  eta: 0:07:11  lr: 0.000030  loss: 0.4982  time: 1.5287  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2720/3000]  eta: 0:07:03  lr: 0.000030  loss: 0.5851  time: 1.5365  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2720/3000]  eta: 0:07:03  lr: 0.000030  loss: 0.3115  time: 1.5360  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2725/3000]  eta: 0:06:56  lr: 0.000030  loss: 0.4555  time: 1.4930  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2725/3000]  eta: 0:06:56  lr: 0.000030  loss: 0.4597  time: 1.4928  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2730/3000]  eta: 0:06:48  lr: 0.000030  loss: 0.6119  time: 1.4971  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2730/3000]  eta: 0:06:48  lr: 0.000030  loss: 0.7526  time: 1.4969  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2735/3000]  eta: 0:06:41  lr: 0.000030  loss: 0.1266  time: 1.5028  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2735/3000]  eta: 0:06:40  lr: 0.000030  loss: 0.4460  time: 1.5025  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2740/3000]  eta: 0:06:33  lr: 0.000030  loss: 0.5166  time: 1.4903  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2740/3000]  eta: 0:06:33  lr: 0.000030  loss: 0.8174  time: 1.4901  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2745/3000]  eta: 0:06:25  lr: 0.000030  loss: 0.1670  time: 1.5154  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2745/3000]  eta: 0:06:25  lr: 0.000030  loss: 0.2339  time: 1.5151  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2750/3000]  eta: 0:06:18  lr: 0.000030  loss: 0.3085  time: 1.5119  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2750/3000]  eta: 0:06:18  lr: 0.000030  loss: 0.1795  time: 1.5117  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2755/3000]  eta: 0:06:10  lr: 0.000030  loss: 0.2934  time: 1.5045  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2755/3000]  eta: 0:06:10  lr: 0.000030  loss: 0.4525  time: 1.5042  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2760/3000]  eta: 0:06:03  lr: 0.000030  loss: 0.2103  time: 1.5260  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2760/3000]  eta: 0:06:03  lr: 0.000030  loss: 0.5623  time: 1.5257  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2765/3000]  eta: 0:05:55  lr: 0.000030  loss: 0.1811  time: 1.5228  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2765/3000]  eta: 0:05:55  lr: 0.000030  loss: 0.1747  time: 1.5226  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2770/3000]  eta: 0:05:48  lr: 0.000030  loss: 0.7155  time: 1.5109  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2770/3000]  eta: 0:05:48  lr: 0.000030  loss: 0.4864  time: 1.5106  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2775/3000]  eta: 0:05:40  lr: 0.000030  loss: 0.1122  time: 1.5233  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2775/3000]  eta: 0:05:40  lr: 0.000030  loss: 0.4059  time: 1.5230  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2780/3000]  eta: 0:05:32  lr: 0.000030  loss: 0.2716  time: 1.5097  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2780/3000]  eta: 0:05:32  lr: 0.000030  loss: 0.4785  time: 1.5094  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2785/3000]  eta: 0:05:25  lr: 0.000030  loss: 0.2479  time: 1.5084  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2785/3000]  eta: 0:05:25  lr: 0.000030  loss: 0.3258  time: 1.5082  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2790/3000]  eta: 0:05:17  lr: 0.000030  loss: 0.6312  time: 1.5198  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2790/3000]  eta: 0:05:17  lr: 0.000030  loss: 0.7688  time: 1.5196  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2795/3000]  eta: 0:05:10  lr: 0.000030  loss: 0.2342  time: 1.5051  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2795/3000]  eta: 0:05:10  lr: 0.000030  loss: 0.8936  time: 1.5049  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2800/3000]  eta: 0:05:02  lr: 0.000030  loss: 0.2915  time: 1.5095  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2800/3000]  eta: 0:05:02  lr: 0.000030  loss: 0.8985  time: 1.5092  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2805/3000]  eta: 0:04:55  lr: 0.000030  loss: 0.2238  time: 1.5133  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2805/3000]  eta: 0:04:55  lr: 0.000030  loss: 0.2796  time: 1.5130  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2810/3000]  eta: 0:04:47  lr: 0.000030  loss: 0.3178  time: 1.5234  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2810/3000]  eta: 0:04:47  lr: 0.000030  loss: 0.4164  time: 1.5231  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2815/3000]  eta: 0:04:39  lr: 0.000030  loss: 0.4980  time: 1.5155  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2815/3000]  eta: 0:04:39  lr: 0.000030  loss: 0.7747  time: 1.5152  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2820/3000]  eta: 0:04:32  lr: 0.000030  loss: 0.5136  time: 1.5205  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2820/3000]  eta: 0:04:32  lr: 0.000030  loss: 0.2536  time: 1.5202  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2825/3000]  eta: 0:04:24  lr: 0.000030  loss: 0.4439  time: 1.5190  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2825/3000]  eta: 0:04:24  lr: 0.000030  loss: 0.1318  time: 1.5187  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2830/3000]  eta: 0:04:17  lr: 0.000030  loss: 0.1924  time: 1.5079  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2830/3000]  eta: 0:04:17  lr: 0.000030  loss: 0.6406  time: 1.5076  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2835/3000]  eta: 0:04:09  lr: 0.000030  loss: 0.2485  time: 1.5115  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2835/3000]  eta: 0:04:09  lr: 0.000030  loss: 0.7498  time: 1.5113  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2840/3000]  eta: 0:04:02  lr: 0.000030  loss: 0.4576  time: 1.4866  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2840/3000]  eta: 0:04:02  lr: 0.000030  loss: 0.2543  time: 1.4864  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2845/3000]  eta: 0:03:54  lr: 0.000030  loss: 0.1463  time: 1.4924  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2845/3000]  eta: 0:03:54  lr: 0.000030  loss: 0.4158  time: 1.4921  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2850/3000]  eta: 0:03:47  lr: 0.000030  loss: 0.5212  time: 1.4948  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2850/3000]  eta: 0:03:46  lr: 0.000030  loss: 0.1874  time: 1.4945  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2855/3000]  eta: 0:03:39  lr: 0.000030  loss: 0.4874  time: 1.5033  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2855/3000]  eta: 0:03:39  lr: 0.000030  loss: 0.4112  time: 1.5030  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2860/3000]  eta: 0:03:31  lr: 0.000030  loss: 0.2688  time: 1.5372  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2860/3000]  eta: 0:03:31  lr: 0.000030  loss: 0.2155  time: 1.5370  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2865/3000]  eta: 0:03:24  lr: 0.000030  loss: 0.3962  time: 1.5208  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2865/3000]  eta: 0:03:24  lr: 0.000030  loss: 0.3541  time: 1.5206  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2870/3000]  eta: 0:03:16  lr: 0.000030  loss: 0.1968  time: 1.5092  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2870/3000]  eta: 0:03:16  lr: 0.000030  loss: 0.2995  time: 1.5091  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2875/3000]  eta: 0:03:09  lr: 0.000030  loss: 0.3635  time: 1.5011  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2875/3000]  eta: 0:03:09  lr: 0.000030  loss: 0.3830  time: 1.5008  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2880/3000]  eta: 0:03:01  lr: 0.000030  loss: 0.3517  time: 1.4675  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2880/3000]  eta: 0:03:01  lr: 0.000030  loss: 0.0630  time: 1.4672  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2885/3000]  eta: 0:02:54  lr: 0.000030  loss: 0.3893  time: 1.4935  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2885/3000]  eta: 0:02:53  lr: 0.000030  loss: 0.2076  time: 1.4931  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2890/3000]  eta: 0:02:46  lr: 0.000030  loss: 0.1292  time: 1.4902  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2890/3000]  eta: 0:02:46  lr: 0.000030  loss: 0.5451  time: 1.4898  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2895/3000]  eta: 0:02:38  lr: 0.000030  loss: 0.0712  time: 1.4919  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2895/3000]  eta: 0:02:38  lr: 0.000030  loss: 0.7279  time: 1.4917  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2900/3000]  eta: 0:02:31  lr: 0.000030  loss: 0.5044  time: 1.4998  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2900/3000]  eta: 0:02:31  lr: 0.000030  loss: 0.4491  time: 1.4995  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2905/3000]  eta: 0:02:23  lr: 0.000030  loss: 0.2047  time: 1.4899  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2905/3000]  eta: 0:02:23  lr: 0.000030  loss: 0.1385  time: 1.4896  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2910/3000]  eta: 0:02:16  lr: 0.000030  loss: 1.2746  time: 1.5064  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2910/3000]  eta: 0:02:16  lr: 0.000030  loss: 0.2573  time: 1.5052  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2915/3000]  eta: 0:02:08  lr: 0.000030  loss: 0.4164  time: 1.5255  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2915/3000]  eta: 0:02:08  lr: 0.000030  loss: 0.2899  time: 1.5242  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2920/3000]  eta: 0:02:01  lr: 0.000030  loss: 0.3873  time: 1.5357  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2920/3000]  eta: 0:02:01  lr: 0.000030  loss: 0.4149  time: 1.5345  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2925/3000]  eta: 0:01:53  lr: 0.000030  loss: 0.5002  time: 1.5338  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2925/3000]  eta: 0:01:53  lr: 0.000030  loss: 0.6037  time: 1.5326  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2930/3000]  eta: 0:01:45  lr: 0.000030  loss: 0.2918  time: 1.5440  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2930/3000]  eta: 0:01:45  lr: 0.000030  loss: 0.5330  time: 1.5438  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2935/3000]  eta: 0:01:38  lr: 0.000030  loss: 0.7882  time: 1.5317  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2935/3000]  eta: 0:01:38  lr: 0.000030  loss: 0.2919  time: 1.5314  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2940/3000]  eta: 0:01:30  lr: 0.000030  loss: 0.3440  time: 1.5301  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2940/3000]  eta: 0:01:30  lr: 0.000030  loss: 0.1565  time: 1.5299  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2945/3000]  eta: 0:01:23  lr: 0.000030  loss: 0.5665  time: 1.5128  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2945/3000]  eta: 0:01:23  lr: 0.000030  loss: 0.2279  time: 1.5124  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2950/3000]  eta: 0:01:15  lr: 0.000030  loss: 0.3390  time: 1.4866  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2950/3000]  eta: 0:01:15  lr: 0.000030  loss: 0.2074  time: 1.4864  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2955/3000]  eta: 0:01:08  lr: 0.000030  loss: 0.4302  time: 1.5039  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2955/3000]  eta: 0:01:08  lr: 0.000030  loss: 0.4319  time: 1.5037  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2960/3000]  eta: 0:01:00  lr: 0.000030  loss: 0.1882  time: 1.4995  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2960/3000]  eta: 0:01:00  lr: 0.000030  loss: 0.4587  time: 1.4992  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2965/3000]  eta: 0:00:52  lr: 0.000030  loss: 0.2935  time: 1.5019  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2965/3000]  eta: 0:00:52  lr: 0.000030  loss: 0.1768  time: 1.5016  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2970/3000]  eta: 0:00:45  lr: 0.000030  loss: 0.3997  time: 1.5169  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2970/3000]  eta: 0:00:45  lr: 0.000030  loss: 0.9745  time: 1.5166  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2975/3000]  eta: 0:00:37  lr: 0.000030  loss: 0.5796  time: 1.5062  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2975/3000]  eta: 0:00:37  lr: 0.000030  loss: 0.6593  time: 1.5059  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2980/3000]  eta: 0:00:30  lr: 0.000030  loss: 0.5636  time: 1.4904  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2980/3000]  eta: 0:00:30  lr: 0.000030  loss: 0.7188  time: 1.4902  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2985/3000]  eta: 0:00:22  lr: 0.000030  loss: 0.4219  time: 1.5097  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2985/3000]  eta: 0:00:22  lr: 0.000030  loss: 0.1873  time: 1.5095  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2990/3000]  eta: 0:00:15  lr: 0.000030  loss: 0.3119  time: 1.5215  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2990/3000]  eta: 0:00:15  lr: 0.000030  loss: 0.5062  time: 1.5212  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2995/3000]  eta: 0:00:07  lr: 0.000030  loss: 0.2763  time: 1.5198  data: 0.0000  max mem: 18432
Train: data epoch: [1]  [2995/3000]  eta: 0:00:07  lr: 0.000030  loss: 0.2324  time: 1.5195  data: 0.0000  max mem: 18151
Train: data epoch: [1]  [2999/3000]  eta: 0:00:01  lr: 0.000030  loss: 0.4530  time: 1.5235  data: 0.0000  max mem: 18432
Train: data epoch: [1] Total time: 1:15:40 (1.5134 s / it)
Train: data epoch: [1]  [2999/3000]  eta: 0:00:01  lr: 0.000030  loss: 0.4224  time: 1.5233  data: 0.0000  max mem: 18151
Train: data epoch: [1] Total time: 1:15:40 (1.5134 s / it)
2025-01-19 02:12:06,462 [INFO] Averaged stats: lr: 0.0000  loss: 0.4327
2025-01-19 02:12:06,468 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [1]  [0/1]  eta: 0:00:00    time: 0.7937  data: 0.5074  max mem: 18151
Eval: data epoch: [1] Total time: 0:00:00 (0.9060 s / it)
Eval: data epoch: [1]  [0/1]  eta: 0:00:00    time: 0.9286  data: 0.6439  max mem: 18432
Eval: data epoch: [1] Total time: 0:00:01 (1.0793 s / it)
2025-01-19 02:12:07,572 [INFO] Saving checkpoint at epoch 1 to outputs_stage1_only/202501182338/checkpoint_1.pth.
2025-01-19 02:12:09,953 [INFO] Training Phase
2025-01-19 02:12:09,961 [INFO] Start training epoch 2, 3000 iters per inner epoch.
Train: data epoch: [2]  [   0/3000]  eta: 1:07:20  lr: 0.000030  loss: 0.1691  time: 1.3469  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [   0/3000]  eta: 1:07:20  lr: 0.000030  loss: 0.5775  time: 1.3467  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [   5/3000]  eta: 1:15:36  lr: 0.000030  loss: 0.6579  time: 1.5148  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [   5/3000]  eta: 1:15:35  lr: 0.000030  loss: 0.2546  time: 1.5145  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [  10/3000]  eta: 1:14:43  lr: 0.000030  loss: 0.2066  time: 1.4994  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [  10/3000]  eta: 1:14:42  lr: 0.000030  loss: 0.3826  time: 1.4992  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [  15/3000]  eta: 1:15:05  lr: 0.000030  loss: 0.4835  time: 1.5095  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [  15/3000]  eta: 1:15:04  lr: 0.000030  loss: 0.1882  time: 1.5092  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [  20/3000]  eta: 1:14:13  lr: 0.000030  loss: 0.6330  time: 1.5018  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [  20/3000]  eta: 1:14:12  lr: 0.000030  loss: 0.2372  time: 1.5015  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [  25/3000]  eta: 1:14:02  lr: 0.000030  loss: 0.7959  time: 1.4869  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [  25/3000]  eta: 1:14:01  lr: 0.000030  loss: 0.4111  time: 1.4866  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [  30/3000]  eta: 1:14:17  lr: 0.000030  loss: 0.1906  time: 1.5017  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [  30/3000]  eta: 1:14:16  lr: 0.000030  loss: 0.5054  time: 1.5014  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [  35/3000]  eta: 1:14:13  lr: 0.000030  loss: 0.1546  time: 1.4961  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [  35/3000]  eta: 1:14:12  lr: 0.000030  loss: 1.2216  time: 1.4958  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [  40/3000]  eta: 1:13:34  lr: 0.000030  loss: 0.0676  time: 1.4886  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [  40/3000]  eta: 1:13:34  lr: 0.000030  loss: 0.4084  time: 1.4883  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [  45/3000]  eta: 1:13:36  lr: 0.000030  loss: 0.3291  time: 1.4965  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [  45/3000]  eta: 1:13:36  lr: 0.000030  loss: 1.2025  time: 1.4963  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [  50/3000]  eta: 1:13:19  lr: 0.000030  loss: 0.4694  time: 1.4770  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [  50/3000]  eta: 1:13:19  lr: 0.000030  loss: 0.4440  time: 1.4766  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [  55/3000]  eta: 1:13:09  lr: 0.000030  loss: 0.4096  time: 1.4696  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [  55/3000]  eta: 1:13:08  lr: 0.000030  loss: 0.2832  time: 1.4693  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [  60/3000]  eta: 1:13:02  lr: 0.000030  loss: 0.5374  time: 1.4887  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [  60/3000]  eta: 1:13:01  lr: 0.000030  loss: 0.6706  time: 1.4885  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [  65/3000]  eta: 1:12:56  lr: 0.000030  loss: 0.5527  time: 1.4835  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [  65/3000]  eta: 1:12:56  lr: 0.000030  loss: 0.3066  time: 1.4832  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [  70/3000]  eta: 1:12:58  lr: 0.000030  loss: 0.4702  time: 1.5012  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [  70/3000]  eta: 1:12:57  lr: 0.000030  loss: 0.1170  time: 1.5010  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [  75/3000]  eta: 1:13:04  lr: 0.000030  loss: 0.6056  time: 1.5232  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [  75/3000]  eta: 1:13:04  lr: 0.000030  loss: 0.4180  time: 1.5230  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [  80/3000]  eta: 1:12:57  lr: 0.000030  loss: 0.6267  time: 1.5246  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [  80/3000]  eta: 1:12:56  lr: 0.000030  loss: 0.3368  time: 1.5243  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [  85/3000]  eta: 1:12:35  lr: 0.000030  loss: 0.2582  time: 1.5041  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [  85/3000]  eta: 1:12:35  lr: 0.000030  loss: 0.2954  time: 1.5038  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [  90/3000]  eta: 1:12:02  lr: 0.000030  loss: 0.3821  time: 1.4540  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [  90/3000]  eta: 1:12:01  lr: 0.000030  loss: 0.1561  time: 1.4538  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [  95/3000]  eta: 1:12:01  lr: 0.000030  loss: 0.3594  time: 1.4444  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [  95/3000]  eta: 1:12:00  lr: 0.000030  loss: 0.2364  time: 1.4442  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 100/3000]  eta: 1:12:07  lr: 0.000030  loss: 0.5356  time: 1.4643  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 100/3000]  eta: 1:12:06  lr: 0.000030  loss: 0.2111  time: 1.4641  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 105/3000]  eta: 1:11:47  lr: 0.000030  loss: 0.2325  time: 1.4598  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 105/3000]  eta: 1:11:46  lr: 0.000030  loss: 0.1275  time: 1.4595  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 110/3000]  eta: 1:11:49  lr: 0.000030  loss: 0.6820  time: 1.5182  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 110/3000]  eta: 1:11:49  lr: 0.000030  loss: 0.7417  time: 1.5180  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 115/3000]  eta: 1:11:47  lr: 0.000030  loss: 0.2057  time: 1.5189  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 115/3000]  eta: 1:11:46  lr: 0.000030  loss: 0.4064  time: 1.5187  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 120/3000]  eta: 1:11:41  lr: 0.000030  loss: 0.9248  time: 1.5014  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 120/3000]  eta: 1:11:41  lr: 0.000030  loss: 0.3302  time: 1.5011  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 125/3000]  eta: 1:11:40  lr: 0.000030  loss: 0.8776  time: 1.5395  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 125/3000]  eta: 1:11:40  lr: 0.000030  loss: 0.5798  time: 1.5393  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 130/3000]  eta: 1:11:36  lr: 0.000030  loss: 1.6569  time: 1.5281  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 130/3000]  eta: 1:11:35  lr: 0.000030  loss: 0.4084  time: 1.5279  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 135/3000]  eta: 1:11:27  lr: 0.000030  loss: 0.4242  time: 1.5157  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 135/3000]  eta: 1:11:26  lr: 0.000030  loss: 0.7110  time: 1.5154  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 140/3000]  eta: 1:11:21  lr: 0.000030  loss: 0.8693  time: 1.5178  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 140/3000]  eta: 1:11:20  lr: 0.000030  loss: 0.4916  time: 1.5176  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 145/3000]  eta: 1:11:19  lr: 0.000030  loss: 0.2751  time: 1.5167  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 145/3000]  eta: 1:11:18  lr: 0.000030  loss: 0.6325  time: 1.5164  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 150/3000]  eta: 1:11:06  lr: 0.000030  loss: 0.3217  time: 1.4971  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 150/3000]  eta: 1:11:05  lr: 0.000030  loss: 0.5579  time: 1.4968  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 155/3000]  eta: 1:10:59  lr: 0.000030  loss: 0.7662  time: 1.5034  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 155/3000]  eta: 1:10:59  lr: 0.000030  loss: 0.5569  time: 1.5032  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 160/3000]  eta: 1:10:52  lr: 0.000030  loss: 0.5990  time: 1.4996  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 160/3000]  eta: 1:10:51  lr: 0.000030  loss: 0.4854  time: 1.4994  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 165/3000]  eta: 1:10:53  lr: 0.000030  loss: 0.3688  time: 1.5117  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 165/3000]  eta: 1:10:52  lr: 0.000030  loss: 0.4235  time: 1.5115  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 170/3000]  eta: 1:10:45  lr: 0.000030  loss: 0.9354  time: 1.5247  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 170/3000]  eta: 1:10:44  lr: 0.000030  loss: 0.6332  time: 1.5244  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 175/3000]  eta: 1:10:37  lr: 0.000030  loss: 0.3114  time: 1.5223  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 175/3000]  eta: 1:10:37  lr: 0.000030  loss: 0.7895  time: 1.5221  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 180/3000]  eta: 1:10:31  lr: 0.000030  loss: 0.4469  time: 1.5246  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 180/3000]  eta: 1:10:30  lr: 0.000030  loss: 0.1653  time: 1.5243  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 185/3000]  eta: 1:10:22  lr: 0.000030  loss: 0.6243  time: 1.4977  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 185/3000]  eta: 1:10:22  lr: 0.000030  loss: 0.3676  time: 1.4974  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 190/3000]  eta: 1:10:16  lr: 0.000030  loss: 0.5069  time: 1.5037  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 190/3000]  eta: 1:10:15  lr: 0.000030  loss: 0.4805  time: 1.5035  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 195/3000]  eta: 1:10:11  lr: 0.000030  loss: 0.3950  time: 1.5129  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 195/3000]  eta: 1:10:10  lr: 0.000030  loss: 0.3053  time: 1.5127  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 200/3000]  eta: 1:10:04  lr: 0.000030  loss: 0.1944  time: 1.5135  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 200/3000]  eta: 1:10:04  lr: 0.000030  loss: 0.5093  time: 1.5132  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 205/3000]  eta: 1:09:53  lr: 0.000030  loss: 0.3817  time: 1.5039  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 205/3000]  eta: 1:09:53  lr: 0.000030  loss: 0.1636  time: 1.5037  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 210/3000]  eta: 1:09:50  lr: 0.000030  loss: 0.3287  time: 1.5157  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 210/3000]  eta: 1:09:49  lr: 0.000030  loss: 0.8539  time: 1.5154  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 215/3000]  eta: 1:09:43  lr: 0.000030  loss: 0.5896  time: 1.5090  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 215/3000]  eta: 1:09:42  lr: 0.000030  loss: 0.2312  time: 1.5088  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 220/3000]  eta: 1:09:40  lr: 0.000030  loss: 0.3539  time: 1.5238  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 220/3000]  eta: 1:09:39  lr: 0.000030  loss: 0.5828  time: 1.5236  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 225/3000]  eta: 1:09:31  lr: 0.000030  loss: 0.5972  time: 1.5328  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 225/3000]  eta: 1:09:31  lr: 0.000030  loss: 0.1573  time: 1.5326  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 230/3000]  eta: 1:09:23  lr: 0.000030  loss: 0.4073  time: 1.5157  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 230/3000]  eta: 1:09:23  lr: 0.000030  loss: 0.1731  time: 1.5155  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 235/3000]  eta: 1:09:16  lr: 0.000030  loss: 0.9278  time: 1.5167  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 235/3000]  eta: 1:09:16  lr: 0.000030  loss: 0.1955  time: 1.5165  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 240/3000]  eta: 1:09:07  lr: 0.000030  loss: 0.8461  time: 1.4936  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 240/3000]  eta: 1:09:07  lr: 0.000030  loss: 0.4425  time: 1.4934  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 245/3000]  eta: 1:09:01  lr: 0.000030  loss: 0.4790  time: 1.5036  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 245/3000]  eta: 1:09:00  lr: 0.000030  loss: 0.4766  time: 1.5033  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 250/3000]  eta: 1:08:54  lr: 0.000030  loss: 0.0954  time: 1.5078  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 250/3000]  eta: 1:08:54  lr: 0.000030  loss: 0.2844  time: 1.5076  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 255/3000]  eta: 1:08:46  lr: 0.000030  loss: 0.1184  time: 1.5037  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 255/3000]  eta: 1:08:46  lr: 0.000030  loss: 0.1462  time: 1.5034  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 260/3000]  eta: 1:08:38  lr: 0.000030  loss: 0.2808  time: 1.5053  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 260/3000]  eta: 1:08:37  lr: 0.000030  loss: 0.2372  time: 1.5051  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 265/3000]  eta: 1:08:29  lr: 0.000030  loss: 0.3897  time: 1.4929  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 265/3000]  eta: 1:08:28  lr: 0.000030  loss: 0.4532  time: 1.4927  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 270/3000]  eta: 1:08:23  lr: 0.000030  loss: 0.0865  time: 1.4954  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 270/3000]  eta: 1:08:22  lr: 0.000030  loss: 0.3576  time: 1.4951  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 275/3000]  eta: 1:08:14  lr: 0.000030  loss: 0.7109  time: 1.4929  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 275/3000]  eta: 1:08:14  lr: 0.000030  loss: 0.1750  time: 1.4926  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 280/3000]  eta: 1:08:11  lr: 0.000030  loss: 0.2782  time: 1.5188  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 280/3000]  eta: 1:08:10  lr: 0.000030  loss: 1.3089  time: 1.5185  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 285/3000]  eta: 1:08:05  lr: 0.000030  loss: 0.1051  time: 1.5341  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 285/3000]  eta: 1:08:04  lr: 0.000030  loss: 0.4582  time: 1.5338  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 290/3000]  eta: 1:07:57  lr: 0.000030  loss: 0.5032  time: 1.5291  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 290/3000]  eta: 1:07:57  lr: 0.000030  loss: 0.3143  time: 1.5289  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 295/3000]  eta: 1:07:52  lr: 0.000030  loss: 0.7107  time: 1.5451  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 295/3000]  eta: 1:07:51  lr: 0.000030  loss: 0.3705  time: 1.5448  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 300/3000]  eta: 1:07:43  lr: 0.000030  loss: 0.9408  time: 1.5185  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 300/3000]  eta: 1:07:43  lr: 0.000030  loss: 0.6601  time: 1.5182  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 305/3000]  eta: 1:07:35  lr: 0.000030  loss: 0.3961  time: 1.5058  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 305/3000]  eta: 1:07:34  lr: 0.000030  loss: 0.2278  time: 1.5056  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 310/3000]  eta: 1:07:30  lr: 0.000030  loss: 0.4322  time: 1.5178  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 310/3000]  eta: 1:07:29  lr: 0.000030  loss: 1.3456  time: 1.5175  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 315/3000]  eta: 1:07:23  lr: 0.000030  loss: 0.5500  time: 1.5124  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 315/3000]  eta: 1:07:22  lr: 0.000030  loss: 0.3454  time: 1.5122  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 320/3000]  eta: 1:07:16  lr: 0.000030  loss: 0.1433  time: 1.5194  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 320/3000]  eta: 1:07:15  lr: 0.000030  loss: 0.2109  time: 1.5191  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 325/3000]  eta: 1:07:07  lr: 0.000030  loss: 0.2814  time: 1.5149  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 325/3000]  eta: 1:07:06  lr: 0.000030  loss: 0.4827  time: 1.5146  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 330/3000]  eta: 1:07:01  lr: 0.000030  loss: 0.9684  time: 1.5155  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 330/3000]  eta: 1:07:00  lr: 0.000030  loss: 0.1004  time: 1.5152  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 335/3000]  eta: 1:06:52  lr: 0.000030  loss: 0.3214  time: 1.4982  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 335/3000]  eta: 1:06:51  lr: 0.000030  loss: 0.3733  time: 1.4979  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 340/3000]  eta: 1:06:45  lr: 0.000030  loss: 0.4169  time: 1.5039  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 340/3000]  eta: 1:06:45  lr: 0.000030  loss: 0.5624  time: 1.5037  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 345/3000]  eta: 1:06:37  lr: 0.000030  loss: 0.2037  time: 1.5089  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 345/3000]  eta: 1:06:36  lr: 0.000030  loss: 0.6433  time: 1.5087  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 350/3000]  eta: 1:06:29  lr: 0.000030  loss: 0.1394  time: 1.4943  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 350/3000]  eta: 1:06:28  lr: 0.000030  loss: 0.3745  time: 1.4941  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 355/3000]  eta: 1:06:21  lr: 0.000030  loss: 0.4745  time: 1.5012  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 355/3000]  eta: 1:06:20  lr: 0.000030  loss: 0.1952  time: 1.5009  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 360/3000]  eta: 1:06:15  lr: 0.000030  loss: 0.5449  time: 1.5026  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 360/3000]  eta: 1:06:14  lr: 0.000030  loss: 0.6813  time: 1.5024  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 365/3000]  eta: 1:06:08  lr: 0.000030  loss: 0.6309  time: 1.5121  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 365/3000]  eta: 1:06:07  lr: 0.000030  loss: 0.5170  time: 1.5119  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 370/3000]  eta: 1:06:01  lr: 0.000030  loss: 0.1309  time: 1.5183  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 370/3000]  eta: 1:06:00  lr: 0.000030  loss: 0.7914  time: 1.5181  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 375/3000]  eta: 1:05:51  lr: 0.000030  loss: 0.2275  time: 1.5082  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 375/3000]  eta: 1:05:51  lr: 0.000030  loss: 0.6740  time: 1.5081  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 380/3000]  eta: 1:05:45  lr: 0.000030  loss: 0.5380  time: 1.5120  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 380/3000]  eta: 1:05:45  lr: 0.000030  loss: 0.1761  time: 1.5118  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 385/3000]  eta: 1:05:35  lr: 0.000030  loss: 0.3605  time: 1.4843  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 385/3000]  eta: 1:05:34  lr: 0.000030  loss: 0.3310  time: 1.4839  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 390/3000]  eta: 1:05:25  lr: 0.000030  loss: 0.1261  time: 1.4670  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 390/3000]  eta: 1:05:25  lr: 0.000030  loss: 0.0708  time: 1.4666  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 395/3000]  eta: 1:05:20  lr: 0.000030  loss: 0.2653  time: 1.4969  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 395/3000]  eta: 1:05:19  lr: 0.000030  loss: 0.4792  time: 1.4965  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 400/3000]  eta: 1:05:14  lr: 0.000030  loss: 0.1339  time: 1.4970  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 400/3000]  eta: 1:05:13  lr: 0.000030  loss: 0.9503  time: 1.4967  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 405/3000]  eta: 1:05:07  lr: 0.000030  loss: 0.2902  time: 1.5241  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 405/3000]  eta: 1:05:06  lr: 0.000030  loss: 0.0780  time: 1.5238  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 410/3000]  eta: 1:05:01  lr: 0.000030  loss: 0.2573  time: 1.5506  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 410/3000]  eta: 1:05:01  lr: 0.000030  loss: 0.7582  time: 1.5504  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 415/3000]  eta: 1:04:52  lr: 0.000030  loss: 0.1372  time: 1.5197  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 415/3000]  eta: 1:04:51  lr: 0.000030  loss: 0.2412  time: 1.5195  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 420/3000]  eta: 1:04:45  lr: 0.000030  loss: 1.5066  time: 1.5140  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 420/3000]  eta: 1:04:44  lr: 0.000030  loss: 0.5853  time: 1.5138  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 425/3000]  eta: 1:04:40  lr: 0.000030  loss: 0.6726  time: 1.5271  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 425/3000]  eta: 1:04:39  lr: 0.000030  loss: 0.6316  time: 1.5268  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 430/3000]  eta: 1:04:30  lr: 0.000030  loss: 0.4091  time: 1.4989  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 430/3000]  eta: 1:04:30  lr: 0.000030  loss: 0.1806  time: 1.4986  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 435/3000]  eta: 1:04:23  lr: 0.000030  loss: 0.1405  time: 1.5189  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 435/3000]  eta: 1:04:22  lr: 0.000030  loss: 0.5052  time: 1.5186  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 440/3000]  eta: 1:04:17  lr: 0.000030  loss: 0.1683  time: 1.5206  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 440/3000]  eta: 1:04:16  lr: 0.000030  loss: 0.4162  time: 1.5203  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 445/3000]  eta: 1:04:07  lr: 0.000030  loss: 0.6913  time: 1.4877  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 445/3000]  eta: 1:04:07  lr: 0.000030  loss: 0.2542  time: 1.4874  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 450/3000]  eta: 1:03:59  lr: 0.000030  loss: 0.2623  time: 1.5009  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 450/3000]  eta: 1:03:59  lr: 0.000030  loss: 0.1983  time: 1.5007  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 455/3000]  eta: 1:03:50  lr: 0.000030  loss: 0.1766  time: 1.4760  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 455/3000]  eta: 1:03:49  lr: 0.000030  loss: 0.5000  time: 1.4758  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 460/3000]  eta: 1:03:41  lr: 0.000030  loss: 0.6774  time: 1.4537  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 460/3000]  eta: 1:03:40  lr: 0.000030  loss: 0.2547  time: 1.4536  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 465/3000]  eta: 1:03:35  lr: 0.000030  loss: 0.9220  time: 1.4864  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 465/3000]  eta: 1:03:34  lr: 0.000030  loss: 0.7963  time: 1.4862  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 470/3000]  eta: 1:03:29  lr: 0.000030  loss: 0.3569  time: 1.5007  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 470/3000]  eta: 1:03:28  lr: 0.000030  loss: 0.7933  time: 1.5006  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 475/3000]  eta: 1:03:22  lr: 0.000030  loss: 0.9360  time: 1.5319  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 475/3000]  eta: 1:03:22  lr: 0.000030  loss: 0.1950  time: 1.5317  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 480/3000]  eta: 1:03:16  lr: 0.000030  loss: 0.2047  time: 1.5598  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 480/3000]  eta: 1:03:16  lr: 0.000030  loss: 0.3669  time: 1.5594  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 485/3000]  eta: 1:03:08  lr: 0.000030  loss: 0.3898  time: 1.5305  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 485/3000]  eta: 1:03:07  lr: 0.000030  loss: 0.2092  time: 1.5302  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 490/3000]  eta: 1:02:57  lr: 0.000030  loss: 0.5808  time: 1.4887  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 490/3000]  eta: 1:02:56  lr: 0.000030  loss: 1.0365  time: 1.4885  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 495/3000]  eta: 1:02:49  lr: 0.000030  loss: 0.3389  time: 1.4764  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 495/3000]  eta: 1:02:49  lr: 0.000030  loss: 0.4232  time: 1.4762  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 500/3000]  eta: 1:02:43  lr: 0.000030  loss: 0.3043  time: 1.4725  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 500/3000]  eta: 1:02:42  lr: 0.000030  loss: 0.4630  time: 1.4722  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 505/3000]  eta: 1:02:34  lr: 0.000030  loss: 0.5001  time: 1.4714  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 505/3000]  eta: 1:02:33  lr: 0.000030  loss: 0.5221  time: 1.4711  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 510/3000]  eta: 1:02:28  lr: 0.000030  loss: 0.3411  time: 1.5130  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 510/3000]  eta: 1:02:27  lr: 0.000030  loss: 0.0584  time: 1.5127  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 515/3000]  eta: 1:02:19  lr: 0.000030  loss: 0.3157  time: 1.5076  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 515/3000]  eta: 1:02:19  lr: 0.000030  loss: 0.5208  time: 1.5072  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 520/3000]  eta: 1:02:12  lr: 0.000030  loss: 0.1776  time: 1.4995  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 520/3000]  eta: 1:02:11  lr: 0.000030  loss: 0.3069  time: 1.4991  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 525/3000]  eta: 1:02:05  lr: 0.000030  loss: 0.6336  time: 1.5217  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 525/3000]  eta: 1:02:05  lr: 0.000030  loss: 0.7199  time: 1.5214  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 530/3000]  eta: 1:01:55  lr: 0.000030  loss: 0.2649  time: 1.4812  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 530/3000]  eta: 1:01:55  lr: 0.000030  loss: 0.3510  time: 1.4808  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 535/3000]  eta: 1:01:49  lr: 0.000030  loss: 0.3362  time: 1.5043  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 535/3000]  eta: 1:01:49  lr: 0.000030  loss: 0.4011  time: 1.5040  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 540/3000]  eta: 1:01:40  lr: 0.000030  loss: 0.2440  time: 1.4828  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 540/3000]  eta: 1:01:39  lr: 0.000030  loss: 0.8189  time: 1.4826  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 545/3000]  eta: 1:01:32  lr: 0.000030  loss: 0.2439  time: 1.4731  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 545/3000]  eta: 1:01:32  lr: 0.000030  loss: 0.2305  time: 1.4728  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 550/3000]  eta: 1:01:25  lr: 0.000030  loss: 0.1812  time: 1.5027  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 550/3000]  eta: 1:01:24  lr: 0.000030  loss: 0.3774  time: 1.5025  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 555/3000]  eta: 1:01:16  lr: 0.000030  loss: 0.1372  time: 1.4698  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 555/3000]  eta: 1:01:15  lr: 0.000030  loss: 0.1582  time: 1.4696  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 560/3000]  eta: 1:01:09  lr: 0.000030  loss: 0.2538  time: 1.4898  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 560/3000]  eta: 1:01:08  lr: 0.000030  loss: 0.2475  time: 1.4896  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 565/3000]  eta: 1:01:02  lr: 0.000030  loss: 0.9748  time: 1.4993  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 565/3000]  eta: 1:01:01  lr: 0.000030  loss: 0.1197  time: 1.4991  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 570/3000]  eta: 1:00:55  lr: 0.000030  loss: 0.4130  time: 1.5098  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 570/3000]  eta: 1:00:55  lr: 0.000030  loss: 0.3492  time: 1.5095  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 575/3000]  eta: 1:00:48  lr: 0.000030  loss: 0.6876  time: 1.5216  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 575/3000]  eta: 1:00:47  lr: 0.000030  loss: 0.1822  time: 1.5213  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 580/3000]  eta: 1:00:41  lr: 0.000030  loss: 0.8392  time: 1.5276  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 580/3000]  eta: 1:00:40  lr: 0.000030  loss: 0.4330  time: 1.5274  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 585/3000]  eta: 1:00:34  lr: 0.000030  loss: 0.3782  time: 1.5347  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 585/3000]  eta: 1:00:34  lr: 0.000030  loss: 0.3201  time: 1.5345  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 590/3000]  eta: 1:00:26  lr: 0.000030  loss: 0.7451  time: 1.5128  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 590/3000]  eta: 1:00:25  lr: 0.000030  loss: 0.2804  time: 1.5125  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 595/3000]  eta: 1:00:19  lr: 0.000030  loss: 0.5449  time: 1.5208  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 595/3000]  eta: 1:00:18  lr: 0.000030  loss: 0.6420  time: 1.5206  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 600/3000]  eta: 1:00:12  lr: 0.000030  loss: 0.5248  time: 1.5177  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 600/3000]  eta: 1:00:11  lr: 0.000030  loss: 0.1861  time: 1.5174  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 605/3000]  eta: 1:00:03  lr: 0.000030  loss: 0.2412  time: 1.4837  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 605/3000]  eta: 1:00:02  lr: 0.000030  loss: 0.2364  time: 1.4834  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 610/3000]  eta: 0:59:55  lr: 0.000030  loss: 0.6615  time: 1.4989  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 610/3000]  eta: 0:59:55  lr: 0.000030  loss: 0.2320  time: 1.4987  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 615/3000]  eta: 0:59:47  lr: 0.000030  loss: 0.3052  time: 1.4878  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 615/3000]  eta: 0:59:47  lr: 0.000030  loss: 0.4162  time: 1.4875  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 620/3000]  eta: 0:59:40  lr: 0.000030  loss: 1.5097  time: 1.4803  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 620/3000]  eta: 0:59:39  lr: 0.000030  loss: 0.4901  time: 1.4800  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 625/3000]  eta: 0:59:32  lr: 0.000030  loss: 0.9641  time: 1.5016  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 625/3000]  eta: 0:59:32  lr: 0.000030  loss: 0.8172  time: 1.5014  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 630/3000]  eta: 0:59:25  lr: 0.000030  loss: 0.5218  time: 1.4937  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 630/3000]  eta: 0:59:24  lr: 0.000030  loss: 0.6610  time: 1.4935  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 635/3000]  eta: 0:59:18  lr: 0.000030  loss: 0.6465  time: 1.5191  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 635/3000]  eta: 0:59:18  lr: 0.000030  loss: 0.3581  time: 1.5189  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 640/3000]  eta: 0:59:11  lr: 0.000030  loss: 0.1007  time: 1.5193  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 640/3000]  eta: 0:59:10  lr: 0.000030  loss: 0.0686  time: 1.5191  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 645/3000]  eta: 0:59:03  lr: 0.000030  loss: 0.1626  time: 1.5108  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 645/3000]  eta: 0:59:02  lr: 0.000030  loss: 0.2731  time: 1.5105  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 650/3000]  eta: 0:58:56  lr: 0.000030  loss: 0.4567  time: 1.5223  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 650/3000]  eta: 0:58:55  lr: 0.000030  loss: 0.2930  time: 1.5219  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 655/3000]  eta: 0:58:49  lr: 0.000030  loss: 0.3876  time: 1.5138  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 655/3000]  eta: 0:58:48  lr: 0.000030  loss: 0.6626  time: 1.5135  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 660/3000]  eta: 0:58:40  lr: 0.000030  loss: 0.3490  time: 1.5045  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 660/3000]  eta: 0:58:40  lr: 0.000030  loss: 0.3330  time: 1.5042  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 665/3000]  eta: 0:58:34  lr: 0.000030  loss: 0.1961  time: 1.5312  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 665/3000]  eta: 0:58:34  lr: 0.000030  loss: 0.7010  time: 1.5310  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 670/3000]  eta: 0:58:28  lr: 0.000030  loss: 0.5431  time: 1.5362  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 670/3000]  eta: 0:58:27  lr: 0.000030  loss: 0.6835  time: 1.5360  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 675/3000]  eta: 0:58:21  lr: 0.000030  loss: 0.5908  time: 1.5331  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 675/3000]  eta: 0:58:20  lr: 0.000030  loss: 0.1667  time: 1.5328  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 680/3000]  eta: 0:58:14  lr: 0.000030  loss: 0.4502  time: 1.5599  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 680/3000]  eta: 0:58:14  lr: 0.000030  loss: 0.5228  time: 1.5597  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 685/3000]  eta: 0:58:05  lr: 0.000030  loss: 0.3196  time: 1.5160  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 685/3000]  eta: 0:58:04  lr: 0.000030  loss: 0.3575  time: 1.5158  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 690/3000]  eta: 0:57:58  lr: 0.000030  loss: 0.1619  time: 1.5121  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 690/3000]  eta: 0:57:58  lr: 0.000030  loss: 0.2140  time: 1.5120  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 695/3000]  eta: 0:57:51  lr: 0.000030  loss: 0.2062  time: 1.5055  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 695/3000]  eta: 0:57:50  lr: 0.000030  loss: 0.6818  time: 1.5053  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 700/3000]  eta: 0:57:43  lr: 0.000030  loss: 0.1550  time: 1.4951  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 700/3000]  eta: 0:57:43  lr: 0.000030  loss: 0.6011  time: 1.4949  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 705/3000]  eta: 0:57:36  lr: 0.000030  loss: 0.3851  time: 1.5227  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 705/3000]  eta: 0:57:35  lr: 0.000030  loss: 0.2699  time: 1.5225  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 710/3000]  eta: 0:57:29  lr: 0.000030  loss: 0.1767  time: 1.5209  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 710/3000]  eta: 0:57:28  lr: 0.000030  loss: 1.2857  time: 1.5207  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 715/3000]  eta: 0:57:21  lr: 0.000030  loss: 0.4074  time: 1.5116  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 715/3000]  eta: 0:57:20  lr: 0.000030  loss: 0.5989  time: 1.5114  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 720/3000]  eta: 0:57:14  lr: 0.000030  loss: 1.0575  time: 1.5147  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 720/3000]  eta: 0:57:13  lr: 0.000030  loss: 0.3609  time: 1.5146  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 725/3000]  eta: 0:57:07  lr: 0.000030  loss: 0.1734  time: 1.5243  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 725/3000]  eta: 0:57:06  lr: 0.000030  loss: 0.6611  time: 1.5240  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 730/3000]  eta: 0:56:59  lr: 0.000030  loss: 0.9595  time: 1.5098  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 730/3000]  eta: 0:56:59  lr: 0.000030  loss: 0.4418  time: 1.5096  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 735/3000]  eta: 0:56:52  lr: 0.000030  loss: 0.3661  time: 1.5244  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 735/3000]  eta: 0:56:51  lr: 0.000030  loss: 0.1260  time: 1.5241  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 740/3000]  eta: 0:56:45  lr: 0.000030  loss: 0.5152  time: 1.5351  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 740/3000]  eta: 0:56:45  lr: 0.000030  loss: 0.3847  time: 1.5348  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 745/3000]  eta: 0:56:37  lr: 0.000030  loss: 0.1655  time: 1.5028  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 745/3000]  eta: 0:56:36  lr: 0.000030  loss: 0.1539  time: 1.5026  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 750/3000]  eta: 0:56:30  lr: 0.000030  loss: 0.6662  time: 1.5159  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 750/3000]  eta: 0:56:29  lr: 0.000030  loss: 0.4129  time: 1.5156  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 755/3000]  eta: 0:56:23  lr: 0.000030  loss: 0.2161  time: 1.5254  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 755/3000]  eta: 0:56:22  lr: 0.000030  loss: 0.2531  time: 1.5252  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 760/3000]  eta: 0:56:16  lr: 0.000030  loss: 0.1782  time: 1.5232  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 760/3000]  eta: 0:56:16  lr: 0.000030  loss: 0.8322  time: 1.5230  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 765/3000]  eta: 0:56:09  lr: 0.000030  loss: 0.7457  time: 1.5427  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 765/3000]  eta: 0:56:08  lr: 0.000030  loss: 0.3711  time: 1.5425  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 770/3000]  eta: 0:56:02  lr: 0.000030  loss: 0.3585  time: 1.5450  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 770/3000]  eta: 0:56:01  lr: 0.000030  loss: 0.3415  time: 1.5448  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 775/3000]  eta: 0:55:54  lr: 0.000030  loss: 0.3372  time: 1.5333  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 775/3000]  eta: 0:55:54  lr: 0.000030  loss: 0.2725  time: 1.5330  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 780/3000]  eta: 0:55:47  lr: 0.000030  loss: 0.2507  time: 1.5228  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 780/3000]  eta: 0:55:46  lr: 0.000030  loss: 0.2676  time: 1.5225  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 785/3000]  eta: 0:55:40  lr: 0.000030  loss: 0.1177  time: 1.5367  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 785/3000]  eta: 0:55:40  lr: 0.000030  loss: 0.5425  time: 1.5364  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 790/3000]  eta: 0:55:33  lr: 0.000030  loss: 0.2568  time: 1.5332  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 790/3000]  eta: 0:55:32  lr: 0.000030  loss: 0.1456  time: 1.5330  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 795/3000]  eta: 0:55:25  lr: 0.000030  loss: 0.3647  time: 1.5326  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 795/3000]  eta: 0:55:25  lr: 0.000030  loss: 0.2644  time: 1.5325  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 800/3000]  eta: 0:55:18  lr: 0.000030  loss: 0.2463  time: 1.5231  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 800/3000]  eta: 0:55:17  lr: 0.000030  loss: 0.6069  time: 1.5229  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 805/3000]  eta: 0:55:09  lr: 0.000030  loss: 0.3936  time: 1.4956  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 805/3000]  eta: 0:55:09  lr: 0.000030  loss: 0.6791  time: 1.4953  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 810/3000]  eta: 0:55:01  lr: 0.000030  loss: 0.2526  time: 1.4795  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 810/3000]  eta: 0:55:01  lr: 0.000030  loss: 1.0770  time: 1.4793  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 815/3000]  eta: 0:54:53  lr: 0.000030  loss: 0.5104  time: 1.4722  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 815/3000]  eta: 0:54:53  lr: 0.000030  loss: 0.1908  time: 1.4719  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 820/3000]  eta: 0:54:46  lr: 0.000030  loss: 0.3186  time: 1.4731  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 820/3000]  eta: 0:54:45  lr: 0.000030  loss: 0.2945  time: 1.4728  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 825/3000]  eta: 0:54:38  lr: 0.000030  loss: 0.1469  time: 1.4890  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 825/3000]  eta: 0:54:38  lr: 0.000030  loss: 0.4285  time: 1.4889  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 830/3000]  eta: 0:54:31  lr: 0.000030  loss: 0.3846  time: 1.5061  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 830/3000]  eta: 0:54:30  lr: 0.000030  loss: 0.4705  time: 1.5059  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 835/3000]  eta: 0:54:23  lr: 0.000030  loss: 0.1268  time: 1.5036  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 835/3000]  eta: 0:54:22  lr: 0.000030  loss: 0.3895  time: 1.5034  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 840/3000]  eta: 0:54:16  lr: 0.000030  loss: 0.2238  time: 1.5290  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 840/3000]  eta: 0:54:16  lr: 0.000030  loss: 0.1959  time: 1.5287  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 845/3000]  eta: 0:54:10  lr: 0.000030  loss: 0.5044  time: 1.5488  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 845/3000]  eta: 0:54:10  lr: 0.000030  loss: 0.4209  time: 1.5484  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 850/3000]  eta: 0:54:03  lr: 0.000030  loss: 1.1637  time: 1.5497  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 850/3000]  eta: 0:54:02  lr: 0.000030  loss: 0.3290  time: 1.5494  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 855/3000]  eta: 0:53:56  lr: 0.000030  loss: 0.3097  time: 1.5720  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 855/3000]  eta: 0:53:55  lr: 0.000030  loss: 0.3550  time: 1.5716  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 860/3000]  eta: 0:53:49  lr: 0.000030  loss: 0.4829  time: 1.5591  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 860/3000]  eta: 0:53:48  lr: 0.000030  loss: 0.0774  time: 1.5588  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 865/3000]  eta: 0:53:42  lr: 0.000030  loss: 0.4903  time: 1.5460  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 865/3000]  eta: 0:53:41  lr: 0.000030  loss: 0.4294  time: 1.5457  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 870/3000]  eta: 0:53:33  lr: 0.000030  loss: 0.7699  time: 1.5122  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 870/3000]  eta: 0:53:32  lr: 0.000030  loss: 0.4692  time: 1.5118  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 875/3000]  eta: 0:53:26  lr: 0.000030  loss: 0.5541  time: 1.5171  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 875/3000]  eta: 0:53:26  lr: 0.000030  loss: 0.5226  time: 1.5169  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 880/3000]  eta: 0:53:19  lr: 0.000030  loss: 0.1495  time: 1.5151  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 880/3000]  eta: 0:53:18  lr: 0.000030  loss: 0.3613  time: 1.5149  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 885/3000]  eta: 0:53:12  lr: 0.000030  loss: 0.9862  time: 1.5077  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 885/3000]  eta: 0:53:11  lr: 0.000030  loss: 0.2439  time: 1.5075  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 890/3000]  eta: 0:53:03  lr: 0.000030  loss: 0.6913  time: 1.5236  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 890/3000]  eta: 0:53:03  lr: 0.000030  loss: 0.8471  time: 1.5234  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 895/3000]  eta: 0:52:56  lr: 0.000030  loss: 0.1298  time: 1.5062  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 895/3000]  eta: 0:52:55  lr: 0.000030  loss: 0.4598  time: 1.5059  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 900/3000]  eta: 0:52:49  lr: 0.000030  loss: 0.2349  time: 1.5081  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 900/3000]  eta: 0:52:48  lr: 0.000030  loss: 0.3536  time: 1.5079  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 905/3000]  eta: 0:52:42  lr: 0.000030  loss: 0.9774  time: 1.5137  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 905/3000]  eta: 0:52:41  lr: 0.000030  loss: 0.3750  time: 1.5134  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 910/3000]  eta: 0:52:34  lr: 0.000030  loss: 0.7397  time: 1.5272  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 910/3000]  eta: 0:52:34  lr: 0.000030  loss: 0.6522  time: 1.5270  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 915/3000]  eta: 0:52:27  lr: 0.000030  loss: 0.1400  time: 1.5307  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 915/3000]  eta: 0:52:26  lr: 0.000030  loss: 0.2561  time: 1.5305  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 920/3000]  eta: 0:52:20  lr: 0.000030  loss: 0.7425  time: 1.5342  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 920/3000]  eta: 0:52:19  lr: 0.000030  loss: 0.2909  time: 1.5339  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 925/3000]  eta: 0:52:13  lr: 0.000030  loss: 0.3089  time: 1.5421  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 925/3000]  eta: 0:52:12  lr: 0.000030  loss: 0.3290  time: 1.5419  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 930/3000]  eta: 0:52:06  lr: 0.000030  loss: 0.4276  time: 1.5487  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 930/3000]  eta: 0:52:05  lr: 0.000030  loss: 0.1834  time: 1.5485  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 935/3000]  eta: 0:51:58  lr: 0.000030  loss: 0.9973  time: 1.5417  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 935/3000]  eta: 0:51:57  lr: 0.000030  loss: 0.7958  time: 1.5415  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 940/3000]  eta: 0:51:50  lr: 0.000030  loss: 0.8686  time: 1.5263  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 940/3000]  eta: 0:51:50  lr: 0.000030  loss: 1.3466  time: 1.5261  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 945/3000]  eta: 0:51:43  lr: 0.000030  loss: 0.5481  time: 1.5221  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 945/3000]  eta: 0:51:43  lr: 0.000030  loss: 0.3950  time: 1.5219  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 950/3000]  eta: 0:51:36  lr: 0.000030  loss: 0.7237  time: 1.5245  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 950/3000]  eta: 0:51:36  lr: 0.000030  loss: 0.4517  time: 1.5243  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 955/3000]  eta: 0:51:29  lr: 0.000030  loss: 0.4376  time: 1.5353  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 955/3000]  eta: 0:51:28  lr: 0.000030  loss: 0.2574  time: 1.5351  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 960/3000]  eta: 0:51:21  lr: 0.000030  loss: 0.6744  time: 1.5342  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 960/3000]  eta: 0:51:21  lr: 0.000030  loss: 0.5452  time: 1.5339  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 965/3000]  eta: 0:51:14  lr: 0.000030  loss: 0.2083  time: 1.5278  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 965/3000]  eta: 0:51:13  lr: 0.000030  loss: 0.5751  time: 1.5276  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 970/3000]  eta: 0:51:06  lr: 0.000030  loss: 0.0875  time: 1.5182  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 970/3000]  eta: 0:51:06  lr: 0.000030  loss: 0.8719  time: 1.5180  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 975/3000]  eta: 0:50:59  lr: 0.000030  loss: 0.1887  time: 1.5160  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 975/3000]  eta: 0:50:58  lr: 0.000030  loss: 0.2381  time: 1.5158  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 980/3000]  eta: 0:50:51  lr: 0.000030  loss: 0.2403  time: 1.5164  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 980/3000]  eta: 0:50:51  lr: 0.000030  loss: 0.4119  time: 1.5162  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 985/3000]  eta: 0:50:43  lr: 0.000030  loss: 0.4325  time: 1.5051  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 985/3000]  eta: 0:50:43  lr: 0.000030  loss: 1.0600  time: 1.5048  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 990/3000]  eta: 0:50:36  lr: 0.000030  loss: 0.3630  time: 1.5028  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 990/3000]  eta: 0:50:35  lr: 0.000030  loss: 0.5544  time: 1.5025  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [ 995/3000]  eta: 0:50:28  lr: 0.000030  loss: 0.4173  time: 1.5044  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [ 995/3000]  eta: 0:50:28  lr: 0.000030  loss: 0.7876  time: 1.5041  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1000/3000]  eta: 0:50:21  lr: 0.000030  loss: 0.2617  time: 1.5209  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1000/3000]  eta: 0:50:21  lr: 0.000030  loss: 0.4198  time: 1.5206  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1005/3000]  eta: 0:50:14  lr: 0.000030  loss: 0.3076  time: 1.5292  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1005/3000]  eta: 0:50:13  lr: 0.000030  loss: 0.4923  time: 1.5290  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1010/3000]  eta: 0:50:06  lr: 0.000030  loss: 0.3781  time: 1.5365  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1010/3000]  eta: 0:50:06  lr: 0.000030  loss: 0.4383  time: 1.5363  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1015/3000]  eta: 0:49:59  lr: 0.000030  loss: 0.3005  time: 1.5249  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1015/3000]  eta: 0:49:58  lr: 0.000030  loss: 0.0985  time: 1.5248  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1020/3000]  eta: 0:49:51  lr: 0.000030  loss: 0.4935  time: 1.5135  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1020/3000]  eta: 0:49:51  lr: 0.000030  loss: 0.2890  time: 1.5134  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1025/3000]  eta: 0:49:44  lr: 0.000030  loss: 0.5743  time: 1.5158  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1025/3000]  eta: 0:49:43  lr: 0.000030  loss: 0.3821  time: 1.5157  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1030/3000]  eta: 0:49:36  lr: 0.000030  loss: 0.6422  time: 1.5097  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1030/3000]  eta: 0:49:36  lr: 0.000030  loss: 0.6092  time: 1.5094  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1035/3000]  eta: 0:49:29  lr: 0.000030  loss: 1.1367  time: 1.5123  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1035/3000]  eta: 0:49:28  lr: 0.000030  loss: 0.1962  time: 1.5121  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1040/3000]  eta: 0:49:22  lr: 0.000030  loss: 0.3236  time: 1.5267  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1040/3000]  eta: 0:49:21  lr: 0.000030  loss: 1.2154  time: 1.5264  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1045/3000]  eta: 0:49:14  lr: 0.000030  loss: 0.3458  time: 1.5151  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1045/3000]  eta: 0:49:13  lr: 0.000030  loss: 0.2851  time: 1.5148  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1050/3000]  eta: 0:49:07  lr: 0.000030  loss: 0.5832  time: 1.5259  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1050/3000]  eta: 0:49:06  lr: 0.000030  loss: 0.4047  time: 1.5257  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1055/3000]  eta: 0:48:59  lr: 0.000030  loss: 0.4224  time: 1.5275  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1055/3000]  eta: 0:48:58  lr: 0.000030  loss: 0.7324  time: 1.5273  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1060/3000]  eta: 0:48:51  lr: 0.000030  loss: 0.2736  time: 1.5086  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1060/3000]  eta: 0:48:51  lr: 0.000030  loss: 0.6341  time: 1.5085  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1065/3000]  eta: 0:48:44  lr: 0.000030  loss: 0.7278  time: 1.5148  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1065/3000]  eta: 0:48:43  lr: 0.000030  loss: 0.3718  time: 1.5145  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1070/3000]  eta: 0:48:36  lr: 0.000030  loss: 0.6404  time: 1.5124  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1070/3000]  eta: 0:48:36  lr: 0.000030  loss: 0.8312  time: 1.5122  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1075/3000]  eta: 0:48:28  lr: 0.000030  loss: 0.3077  time: 1.4983  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1075/3000]  eta: 0:48:28  lr: 0.000030  loss: 0.4323  time: 1.4981  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1080/3000]  eta: 0:48:20  lr: 0.000030  loss: 0.6269  time: 1.4966  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1080/3000]  eta: 0:48:20  lr: 0.000030  loss: 0.1858  time: 1.4962  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1085/3000]  eta: 0:48:13  lr: 0.000030  loss: 0.5772  time: 1.4915  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1085/3000]  eta: 0:48:12  lr: 0.000030  loss: 0.3536  time: 1.4912  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1090/3000]  eta: 0:48:05  lr: 0.000030  loss: 0.2590  time: 1.4869  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1090/3000]  eta: 0:48:05  lr: 0.000030  loss: 0.2614  time: 1.4866  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1095/3000]  eta: 0:47:58  lr: 0.000030  loss: 0.2777  time: 1.5101  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1095/3000]  eta: 0:47:58  lr: 0.000030  loss: 0.3095  time: 1.5098  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1100/3000]  eta: 0:47:49  lr: 0.000030  loss: 0.2082  time: 1.5021  data: 0.0000  max mem: 18151Train: data epoch: [2]  [1100/3000]  eta: 0:47:50  lr: 0.000030  loss: 0.3861  time: 1.5023  data: 0.0000  max mem: 18432

Train: data epoch: [2]  [1105/3000]  eta: 0:47:43  lr: 0.000030  loss: 0.5300  time: 1.5260  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1105/3000]  eta: 0:47:43  lr: 0.000030  loss: 0.2182  time: 1.5258  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1110/3000]  eta: 0:47:35  lr: 0.000030  loss: 0.2844  time: 1.5162  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1110/3000]  eta: 0:47:35  lr: 0.000030  loss: 0.7250  time: 1.5160  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1115/3000]  eta: 0:47:27  lr: 0.000030  loss: 0.4447  time: 1.4970  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1115/3000]  eta: 0:47:27  lr: 0.000030  loss: 0.3942  time: 1.4967  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1120/3000]  eta: 0:47:20  lr: 0.000030  loss: 0.0980  time: 1.5104  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1120/3000]  eta: 0:47:19  lr: 0.000030  loss: 1.0680  time: 1.5099  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1125/3000]  eta: 0:47:12  lr: 0.000030  loss: 0.3714  time: 1.5000  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1125/3000]  eta: 0:47:12  lr: 0.000030  loss: 0.0585  time: 1.4987  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1130/3000]  eta: 0:47:04  lr: 0.000030  loss: 0.1699  time: 1.4943  data: 0.0000  max mem: 18151Train: data epoch: [2]  [1130/3000]  eta: 0:47:05  lr: 0.000030  loss: 0.3807  time: 1.4956  data: 0.0000  max mem: 18432

Train: data epoch: [2]  [1135/3000]  eta: 0:46:56  lr: 0.000030  loss: 0.1736  time: 1.4887  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1135/3000]  eta: 0:46:56  lr: 0.000030  loss: 0.0896  time: 1.4876  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1140/3000]  eta: 0:46:48  lr: 0.000030  loss: 0.1266  time: 1.4677  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1140/3000]  eta: 0:46:48  lr: 0.000030  loss: 0.2693  time: 1.4668  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1145/3000]  eta: 0:46:40  lr: 0.000030  loss: 0.5254  time: 1.4491  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1145/3000]  eta: 0:46:40  lr: 0.000030  loss: 0.1812  time: 1.4488  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1150/3000]  eta: 0:46:32  lr: 0.000030  loss: 0.4422  time: 1.4532  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1150/3000]  eta: 0:46:32  lr: 0.000030  loss: 0.9030  time: 1.4531  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1155/3000]  eta: 0:46:25  lr: 0.000030  loss: 0.4175  time: 1.4714  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1155/3000]  eta: 0:46:24  lr: 0.000030  loss: 0.4339  time: 1.4711  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1160/3000]  eta: 0:46:17  lr: 0.000030  loss: 0.2290  time: 1.4967  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1160/3000]  eta: 0:46:17  lr: 0.000030  loss: 0.6133  time: 1.4964  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1165/3000]  eta: 0:46:10  lr: 0.000030  loss: 0.4958  time: 1.5079  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1165/3000]  eta: 0:46:09  lr: 0.000030  loss: 0.3401  time: 1.5077  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1170/3000]  eta: 0:46:03  lr: 0.000030  loss: 0.7246  time: 1.5224  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1170/3000]  eta: 0:46:02  lr: 0.000030  loss: 0.0724  time: 1.5221  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1175/3000]  eta: 0:45:55  lr: 0.000030  loss: 0.1590  time: 1.5227  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1175/3000]  eta: 0:45:55  lr: 0.000030  loss: 0.9329  time: 1.5226  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1180/3000]  eta: 0:45:48  lr: 0.000030  loss: 0.2513  time: 1.5217  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1180/3000]  eta: 0:45:47  lr: 0.000030  loss: 0.5859  time: 1.5214  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1185/3000]  eta: 0:45:40  lr: 0.000030  loss: 0.2542  time: 1.5197  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1185/3000]  eta: 0:45:40  lr: 0.000030  loss: 0.2989  time: 1.5193  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1190/3000]  eta: 0:45:32  lr: 0.000030  loss: 0.1771  time: 1.5081  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1190/3000]  eta: 0:45:32  lr: 0.000030  loss: 0.6531  time: 1.5078  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1195/3000]  eta: 0:45:24  lr: 0.000030  loss: 0.1748  time: 1.4943  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1195/3000]  eta: 0:45:24  lr: 0.000030  loss: 0.2072  time: 1.4941  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1200/3000]  eta: 0:45:17  lr: 0.000030  loss: 0.2144  time: 1.5013  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1200/3000]  eta: 0:45:17  lr: 0.000030  loss: 0.2186  time: 1.5011  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1205/3000]  eta: 0:45:10  lr: 0.000030  loss: 0.2998  time: 1.5186  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1205/3000]  eta: 0:45:10  lr: 0.000030  loss: 0.7016  time: 1.5184  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1210/3000]  eta: 0:45:03  lr: 0.000030  loss: 0.3064  time: 1.5227  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1210/3000]  eta: 0:45:02  lr: 0.000030  loss: 0.5048  time: 1.5224  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1215/3000]  eta: 0:44:55  lr: 0.000030  loss: 0.6119  time: 1.5452  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1215/3000]  eta: 0:44:55  lr: 0.000030  loss: 0.1908  time: 1.5449  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1220/3000]  eta: 0:44:48  lr: 0.000030  loss: 0.3196  time: 1.5458  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1220/3000]  eta: 0:44:48  lr: 0.000030  loss: 0.3623  time: 1.5455  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1225/3000]  eta: 0:44:40  lr: 0.000030  loss: 0.6295  time: 1.5217  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1225/3000]  eta: 0:44:40  lr: 0.000030  loss: 0.9005  time: 1.5214  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1230/3000]  eta: 0:44:33  lr: 0.000030  loss: 0.7944  time: 1.5193  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1230/3000]  eta: 0:44:32  lr: 0.000030  loss: 0.4437  time: 1.5191  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1235/3000]  eta: 0:44:25  lr: 0.000030  loss: 0.1170  time: 1.4968  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1235/3000]  eta: 0:44:24  lr: 0.000030  loss: 0.5475  time: 1.4966  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1240/3000]  eta: 0:44:18  lr: 0.000030  loss: 0.4260  time: 1.4997  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1240/3000]  eta: 0:44:17  lr: 0.000030  loss: 0.8272  time: 1.4994  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1245/3000]  eta: 0:44:10  lr: 0.000030  loss: 0.7712  time: 1.5015  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1245/3000]  eta: 0:44:09  lr: 0.000030  loss: 0.3105  time: 1.5013  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1250/3000]  eta: 0:44:02  lr: 0.000030  loss: 0.2223  time: 1.5102  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1250/3000]  eta: 0:44:02  lr: 0.000030  loss: 0.2675  time: 1.5099  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1255/3000]  eta: 0:43:55  lr: 0.000030  loss: 0.4604  time: 1.5186  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1255/3000]  eta: 0:43:54  lr: 0.000030  loss: 0.5495  time: 1.5183  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1260/3000]  eta: 0:43:47  lr: 0.000030  loss: 0.3453  time: 1.4991  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1260/3000]  eta: 0:43:47  lr: 0.000030  loss: 0.1609  time: 1.4988  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1265/3000]  eta: 0:43:39  lr: 0.000030  loss: 0.2516  time: 1.5016  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1265/3000]  eta: 0:43:39  lr: 0.000030  loss: 0.5255  time: 1.5013  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1270/3000]  eta: 0:43:32  lr: 0.000030  loss: 0.3410  time: 1.5095  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1270/3000]  eta: 0:43:32  lr: 0.000030  loss: 0.9638  time: 1.5093  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1275/3000]  eta: 0:43:25  lr: 0.000030  loss: 0.3811  time: 1.5304  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1275/3000]  eta: 0:43:25  lr: 0.000030  loss: 0.3203  time: 1.5303  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1280/3000]  eta: 0:43:18  lr: 0.000030  loss: 0.1206  time: 1.5524  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1280/3000]  eta: 0:43:17  lr: 0.000030  loss: 0.3202  time: 1.5522  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1285/3000]  eta: 0:43:10  lr: 0.000030  loss: 0.3509  time: 1.5503  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1285/3000]  eta: 0:43:10  lr: 0.000030  loss: 0.8091  time: 1.5500  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1290/3000]  eta: 0:43:03  lr: 0.000030  loss: 0.3205  time: 1.5409  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1290/3000]  eta: 0:43:02  lr: 0.000030  loss: 0.4921  time: 1.5406  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1295/3000]  eta: 0:42:55  lr: 0.000030  loss: 0.0577  time: 1.5390  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1295/3000]  eta: 0:42:56  lr: 0.000030  loss: 1.0527  time: 1.5394  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1300/3000]  eta: 0:42:48  lr: 0.000030  loss: 1.0592  time: 1.5291  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1300/3000]  eta: 0:42:48  lr: 0.000030  loss: 0.2170  time: 1.5289  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1305/3000]  eta: 0:42:41  lr: 0.000030  loss: 1.0140  time: 1.5576  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1305/3000]  eta: 0:42:41  lr: 0.000030  loss: 0.5268  time: 1.5574  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1310/3000]  eta: 0:42:34  lr: 0.000030  loss: 0.4897  time: 1.5514  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1310/3000]  eta: 0:42:33  lr: 0.000030  loss: 0.5361  time: 1.5511  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1315/3000]  eta: 0:42:26  lr: 0.000030  loss: 0.7710  time: 1.5209  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1315/3000]  eta: 0:42:25  lr: 0.000030  loss: 0.3807  time: 1.5207  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1320/3000]  eta: 0:42:18  lr: 0.000030  loss: 0.2517  time: 1.5009  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1320/3000]  eta: 0:42:17  lr: 0.000030  loss: 0.1817  time: 1.5006  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1325/3000]  eta: 0:42:10  lr: 0.000030  loss: 0.4723  time: 1.4806  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1325/3000]  eta: 0:42:10  lr: 0.000030  loss: 1.3550  time: 1.4803  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1330/3000]  eta: 0:42:03  lr: 0.000030  loss: 0.1672  time: 1.4827  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1330/3000]  eta: 0:42:02  lr: 0.000030  loss: 0.4304  time: 1.4825  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1335/3000]  eta: 0:41:55  lr: 0.000030  loss: 0.6853  time: 1.4955  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1335/3000]  eta: 0:41:55  lr: 0.000030  loss: 0.2980  time: 1.4952  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1340/3000]  eta: 0:41:48  lr: 0.000030  loss: 0.1699  time: 1.5111  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1340/3000]  eta: 0:41:47  lr: 0.000030  loss: 0.3352  time: 1.5109  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1345/3000]  eta: 0:41:40  lr: 0.000030  loss: 0.2341  time: 1.5250  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1345/3000]  eta: 0:41:40  lr: 0.000030  loss: 0.2766  time: 1.5247  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1350/3000]  eta: 0:41:33  lr: 0.000030  loss: 0.5747  time: 1.5382  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1350/3000]  eta: 0:41:33  lr: 0.000030  loss: 0.5363  time: 1.5379  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1355/3000]  eta: 0:41:26  lr: 0.000030  loss: 0.7826  time: 1.5424  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1355/3000]  eta: 0:41:25  lr: 0.000030  loss: 0.5392  time: 1.5423  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1360/3000]  eta: 0:41:18  lr: 0.000030  loss: 0.4787  time: 1.5304  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1360/3000]  eta: 0:41:17  lr: 0.000030  loss: 0.2729  time: 1.5301  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1365/3000]  eta: 0:41:10  lr: 0.000030  loss: 0.2123  time: 1.4941  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1365/3000]  eta: 0:41:09  lr: 0.000030  loss: 0.3665  time: 1.4938  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1370/3000]  eta: 0:41:02  lr: 0.000030  loss: 0.2012  time: 1.4772  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1370/3000]  eta: 0:41:02  lr: 0.000030  loss: 1.1664  time: 1.4770  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1375/3000]  eta: 0:40:55  lr: 0.000030  loss: 0.1523  time: 1.4776  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1375/3000]  eta: 0:40:54  lr: 0.000030  loss: 0.1109  time: 1.4773  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1380/3000]  eta: 0:40:47  lr: 0.000030  loss: 0.0907  time: 1.4720  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1380/3000]  eta: 0:40:46  lr: 0.000030  loss: 0.5743  time: 1.4718  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1385/3000]  eta: 0:40:39  lr: 0.000030  loss: 0.2105  time: 1.4890  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1385/3000]  eta: 0:40:39  lr: 0.000030  loss: 0.4585  time: 1.4888  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1390/3000]  eta: 0:40:31  lr: 0.000030  loss: 0.4068  time: 1.4930  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1390/3000]  eta: 0:40:31  lr: 0.000030  loss: 0.4181  time: 1.4927  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1395/3000]  eta: 0:40:24  lr: 0.000030  loss: 0.3350  time: 1.4936  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1395/3000]  eta: 0:40:24  lr: 0.000030  loss: 0.7025  time: 1.4933  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1400/3000]  eta: 0:40:17  lr: 0.000030  loss: 0.3767  time: 1.5297  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1400/3000]  eta: 0:40:16  lr: 0.000030  loss: 0.1502  time: 1.5294  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1405/3000]  eta: 0:40:09  lr: 0.000030  loss: 0.3457  time: 1.5352  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1405/3000]  eta: 0:40:09  lr: 0.000030  loss: 0.8490  time: 1.5350  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1410/3000]  eta: 0:40:02  lr: 0.000030  loss: 0.1541  time: 1.5283  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1410/3000]  eta: 0:40:01  lr: 0.000030  loss: 0.3152  time: 1.5281  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1415/3000]  eta: 0:39:54  lr: 0.000030  loss: 0.3386  time: 1.5300  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1415/3000]  eta: 0:39:54  lr: 0.000030  loss: 0.4795  time: 1.5297  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1420/3000]  eta: 0:39:47  lr: 0.000030  loss: 0.3382  time: 1.5116  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1420/3000]  eta: 0:39:46  lr: 0.000030  loss: 0.6770  time: 1.5114  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1425/3000]  eta: 0:39:39  lr: 0.000030  loss: 0.6275  time: 1.5161  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1425/3000]  eta: 0:39:39  lr: 0.000030  loss: 0.3257  time: 1.5158  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1430/3000]  eta: 0:39:32  lr: 0.000030  loss: 0.5479  time: 1.5191  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1430/3000]  eta: 0:39:31  lr: 0.000030  loss: 0.5918  time: 1.5188  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1435/3000]  eta: 0:39:24  lr: 0.000030  loss: 0.8510  time: 1.5258  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1435/3000]  eta: 0:39:24  lr: 0.000030  loss: 0.1211  time: 1.5255  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1440/3000]  eta: 0:39:16  lr: 0.000030  loss: 0.1763  time: 1.5081  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1440/3000]  eta: 0:39:16  lr: 0.000030  loss: 0.2913  time: 1.5078  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1445/3000]  eta: 0:39:09  lr: 0.000030  loss: 0.4068  time: 1.5129  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1445/3000]  eta: 0:39:09  lr: 0.000030  loss: 0.4544  time: 1.5126  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1450/3000]  eta: 0:39:02  lr: 0.000030  loss: 0.1006  time: 1.5327  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1450/3000]  eta: 0:39:01  lr: 0.000030  loss: 0.0752  time: 1.5324  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1455/3000]  eta: 0:38:54  lr: 0.000030  loss: 0.2736  time: 1.5116  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1455/3000]  eta: 0:38:54  lr: 0.000030  loss: 0.1915  time: 1.5114  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1460/3000]  eta: 0:38:47  lr: 0.000030  loss: 0.3595  time: 1.5363  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1460/3000]  eta: 0:38:46  lr: 0.000030  loss: 0.5085  time: 1.5362  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1465/3000]  eta: 0:38:39  lr: 0.000030  loss: 0.2149  time: 1.5189  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1465/3000]  eta: 0:38:39  lr: 0.000030  loss: 0.6484  time: 1.5187  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1470/3000]  eta: 0:38:31  lr: 0.000030  loss: 0.2059  time: 1.4995  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1470/3000]  eta: 0:38:31  lr: 0.000030  loss: 0.2060  time: 1.4993  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1475/3000]  eta: 0:38:24  lr: 0.000030  loss: 0.4517  time: 1.5105  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1475/3000]  eta: 0:38:23  lr: 0.000030  loss: 0.4220  time: 1.5103  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1480/3000]  eta: 0:38:16  lr: 0.000030  loss: 0.3461  time: 1.4961  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1480/3000]  eta: 0:38:16  lr: 0.000030  loss: 0.3701  time: 1.4959  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1485/3000]  eta: 0:38:09  lr: 0.000030  loss: 1.0939  time: 1.5186  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1485/3000]  eta: 0:38:09  lr: 0.000030  loss: 0.2616  time: 1.5183  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1490/3000]  eta: 0:38:02  lr: 0.000030  loss: 0.4335  time: 1.5387  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1490/3000]  eta: 0:38:01  lr: 0.000030  loss: 0.1279  time: 1.5384  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1495/3000]  eta: 0:37:54  lr: 0.000030  loss: 0.1136  time: 1.5135  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1495/3000]  eta: 0:37:53  lr: 0.000030  loss: 0.2414  time: 1.5133  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1500/3000]  eta: 0:37:46  lr: 0.000030  loss: 1.4867  time: 1.5083  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1500/3000]  eta: 0:37:46  lr: 0.000030  loss: 0.7487  time: 1.5081  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1505/3000]  eta: 0:37:38  lr: 0.000030  loss: 0.2809  time: 1.4922  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1505/3000]  eta: 0:37:38  lr: 0.000030  loss: 0.2890  time: 1.4920  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1510/3000]  eta: 0:37:31  lr: 0.000030  loss: 0.8524  time: 1.4849  data: 0.0000  max mem: 18151Train: data epoch: [2]  [1510/3000]  eta: 0:37:31  lr: 0.000030  loss: 0.5934  time: 1.4852  data: 0.0000  max mem: 18432

Train: data epoch: [2]  [1515/3000]  eta: 0:37:23  lr: 0.000030  loss: 0.4761  time: 1.5075  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1515/3000]  eta: 0:37:23  lr: 0.000030  loss: 0.5654  time: 1.5073  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1520/3000]  eta: 0:37:16  lr: 0.000030  loss: 0.3349  time: 1.5304  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1520/3000]  eta: 0:37:16  lr: 0.000030  loss: 0.3661  time: 1.5302  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1525/3000]  eta: 0:37:08  lr: 0.000030  loss: 1.7881  time: 1.5202  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1525/3000]  eta: 0:37:08  lr: 0.000030  loss: 0.2525  time: 1.5199  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1530/3000]  eta: 0:37:00  lr: 0.000030  loss: 0.3937  time: 1.4916  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1530/3000]  eta: 0:37:00  lr: 0.000030  loss: 0.1968  time: 1.4915  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1535/3000]  eta: 0:36:53  lr: 0.000030  loss: 0.0978  time: 1.4902  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1535/3000]  eta: 0:36:52  lr: 0.000030  loss: 0.1151  time: 1.4900  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1540/3000]  eta: 0:36:45  lr: 0.000030  loss: 0.5357  time: 1.4877  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1540/3000]  eta: 0:36:45  lr: 0.000030  loss: 0.4438  time: 1.4875  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1545/3000]  eta: 0:36:38  lr: 0.000030  loss: 0.4085  time: 1.4867  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1545/3000]  eta: 0:36:37  lr: 0.000030  loss: 0.8170  time: 1.4866  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1550/3000]  eta: 0:36:30  lr: 0.000030  loss: 1.0744  time: 1.5108  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1550/3000]  eta: 0:36:30  lr: 0.000030  loss: 0.2394  time: 1.5107  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1555/3000]  eta: 0:36:22  lr: 0.000030  loss: 0.6614  time: 1.5043  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1555/3000]  eta: 0:36:22  lr: 0.000030  loss: 0.2809  time: 1.5041  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1560/3000]  eta: 0:36:15  lr: 0.000030  loss: 0.4664  time: 1.4905  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1560/3000]  eta: 0:36:14  lr: 0.000030  loss: 0.4584  time: 1.4903  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1565/3000]  eta: 0:36:07  lr: 0.000030  loss: 0.5288  time: 1.4853  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1565/3000]  eta: 0:36:07  lr: 0.000030  loss: 0.7632  time: 1.4850  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1570/3000]  eta: 0:35:59  lr: 0.000030  loss: 0.4446  time: 1.4829  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1570/3000]  eta: 0:35:59  lr: 0.000030  loss: 0.3777  time: 1.4826  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1575/3000]  eta: 0:35:52  lr: 0.000030  loss: 0.5196  time: 1.5052  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1575/3000]  eta: 0:35:52  lr: 0.000030  loss: 0.3341  time: 1.5049  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1580/3000]  eta: 0:35:45  lr: 0.000030  loss: 0.0193  time: 1.5152  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1580/3000]  eta: 0:35:44  lr: 0.000030  loss: 0.6798  time: 1.5149  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1585/3000]  eta: 0:35:37  lr: 0.000030  loss: 0.5354  time: 1.5313  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1585/3000]  eta: 0:35:37  lr: 0.000030  loss: 0.3085  time: 1.5310  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1590/3000]  eta: 0:35:29  lr: 0.000030  loss: 0.3367  time: 1.5186  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1590/3000]  eta: 0:35:29  lr: 0.000030  loss: 0.3968  time: 1.5183  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1595/3000]  eta: 0:35:22  lr: 0.000030  loss: 0.3240  time: 1.4866  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1595/3000]  eta: 0:35:21  lr: 0.000030  loss: 0.2223  time: 1.4864  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1600/3000]  eta: 0:35:14  lr: 0.000030  loss: 0.4728  time: 1.4811  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1600/3000]  eta: 0:35:14  lr: 0.000030  loss: 0.4200  time: 1.4808  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1605/3000]  eta: 0:35:06  lr: 0.000030  loss: 0.7712  time: 1.4851  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1605/3000]  eta: 0:35:06  lr: 0.000030  loss: 0.2040  time: 1.4848  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1610/3000]  eta: 0:34:59  lr: 0.000030  loss: 0.2155  time: 1.5070  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1610/3000]  eta: 0:34:59  lr: 0.000030  loss: 0.2475  time: 1.5068  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1615/3000]  eta: 0:34:51  lr: 0.000030  loss: 0.1889  time: 1.5183  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1615/3000]  eta: 0:34:51  lr: 0.000030  loss: 0.2651  time: 1.5179  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1620/3000]  eta: 0:34:44  lr: 0.000030  loss: 0.1787  time: 1.5090  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1620/3000]  eta: 0:34:43  lr: 0.000030  loss: 0.9246  time: 1.5088  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1625/3000]  eta: 0:34:36  lr: 0.000030  loss: 0.1212  time: 1.4876  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1625/3000]  eta: 0:34:35  lr: 0.000030  loss: 0.1324  time: 1.4874  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1630/3000]  eta: 0:34:29  lr: 0.000030  loss: 0.7303  time: 1.4927  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1630/3000]  eta: 0:34:28  lr: 0.000030  loss: 0.6093  time: 1.4924  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1635/3000]  eta: 0:34:21  lr: 0.000030  loss: 0.2892  time: 1.5068  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1635/3000]  eta: 0:34:21  lr: 0.000030  loss: 0.5944  time: 1.5066  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1640/3000]  eta: 0:34:14  lr: 0.000030  loss: 0.3539  time: 1.5190  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1640/3000]  eta: 0:34:13  lr: 0.000030  loss: 0.4588  time: 1.5188  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1645/3000]  eta: 0:34:06  lr: 0.000030  loss: 0.2892  time: 1.5393  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1645/3000]  eta: 0:34:06  lr: 0.000030  loss: 0.3707  time: 1.5391  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1650/3000]  eta: 0:33:59  lr: 0.000030  loss: 1.0526  time: 1.5318  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1650/3000]  eta: 0:33:58  lr: 0.000030  loss: 0.5043  time: 1.5316  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1655/3000]  eta: 0:33:51  lr: 0.000030  loss: 0.4008  time: 1.5171  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1655/3000]  eta: 0:33:51  lr: 0.000030  loss: 0.5760  time: 1.5168  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1660/3000]  eta: 0:33:44  lr: 0.000030  loss: 0.1963  time: 1.5237  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1660/3000]  eta: 0:33:43  lr: 0.000030  loss: 0.5899  time: 1.5234  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1665/3000]  eta: 0:33:36  lr: 0.000030  loss: 0.3105  time: 1.5142  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1665/3000]  eta: 0:33:36  lr: 0.000030  loss: 0.3370  time: 1.5138  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1670/3000]  eta: 0:33:28  lr: 0.000030  loss: 0.4720  time: 1.4957  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1670/3000]  eta: 0:33:28  lr: 0.000030  loss: 0.3069  time: 1.4953  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1675/3000]  eta: 0:33:21  lr: 0.000030  loss: 0.6431  time: 1.5058  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1675/3000]  eta: 0:33:20  lr: 0.000030  loss: 0.5927  time: 1.5055  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1680/3000]  eta: 0:33:13  lr: 0.000030  loss: 0.5311  time: 1.5095  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1680/3000]  eta: 0:33:13  lr: 0.000030  loss: 0.1405  time: 1.5093  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1685/3000]  eta: 0:33:06  lr: 0.000030  loss: 0.4889  time: 1.5294  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1685/3000]  eta: 0:33:06  lr: 0.000030  loss: 0.4318  time: 1.5293  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1690/3000]  eta: 0:32:58  lr: 0.000030  loss: 0.2897  time: 1.5149  data: 0.0000  max mem: 18432Train: data epoch: [2]  [1690/3000]  eta: 0:32:58  lr: 0.000030  loss: 0.6051  time: 1.5147  data: 0.0000  max mem: 18151

Train: data epoch: [2]  [1695/3000]  eta: 0:32:51  lr: 0.000030  loss: 0.2669  time: 1.5199  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1695/3000]  eta: 0:32:50  lr: 0.000030  loss: 0.7767  time: 1.5197  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1700/3000]  eta: 0:32:43  lr: 0.000030  loss: 0.1059  time: 1.5171  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1700/3000]  eta: 0:32:43  lr: 0.000030  loss: 0.8912  time: 1.5169  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1705/3000]  eta: 0:32:36  lr: 0.000030  loss: 0.3386  time: 1.5045  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1705/3000]  eta: 0:32:35  lr: 0.000030  loss: 0.1650  time: 1.5043  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1710/3000]  eta: 0:32:28  lr: 0.000030  loss: 0.6713  time: 1.5262  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1710/3000]  eta: 0:32:28  lr: 0.000030  loss: 0.3933  time: 1.5260  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1715/3000]  eta: 0:32:21  lr: 0.000030  loss: 0.2851  time: 1.5208  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1715/3000]  eta: 0:32:20  lr: 0.000030  loss: 0.4527  time: 1.5207  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1720/3000]  eta: 0:32:13  lr: 0.000030  loss: 0.3725  time: 1.5070  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1720/3000]  eta: 0:32:13  lr: 0.000030  loss: 0.4203  time: 1.5068  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1725/3000]  eta: 0:32:05  lr: 0.000030  loss: 0.1687  time: 1.4926  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1725/3000]  eta: 0:32:05  lr: 0.000030  loss: 0.2688  time: 1.4923  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1730/3000]  eta: 0:31:58  lr: 0.000030  loss: 0.1907  time: 1.5034  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1730/3000]  eta: 0:31:58  lr: 0.000030  loss: 0.2242  time: 1.5031  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1735/3000]  eta: 0:31:50  lr: 0.000030  loss: 1.1628  time: 1.4997  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1735/3000]  eta: 0:31:50  lr: 0.000030  loss: 0.1043  time: 1.4995  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1740/3000]  eta: 0:31:43  lr: 0.000030  loss: 0.3166  time: 1.4918  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1740/3000]  eta: 0:31:42  lr: 0.000030  loss: 0.2113  time: 1.4915  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1745/3000]  eta: 0:31:35  lr: 0.000030  loss: 0.2096  time: 1.5080  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1745/3000]  eta: 0:31:35  lr: 0.000030  loss: 0.1244  time: 1.5077  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1750/3000]  eta: 0:31:27  lr: 0.000030  loss: 0.0399  time: 1.4893  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1750/3000]  eta: 0:31:27  lr: 0.000030  loss: 0.1712  time: 1.4890  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1755/3000]  eta: 0:31:20  lr: 0.000030  loss: 1.0646  time: 1.4735  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1755/3000]  eta: 0:31:19  lr: 0.000030  loss: 0.1359  time: 1.4732  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1760/3000]  eta: 0:31:12  lr: 0.000030  loss: 0.1168  time: 1.4955  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1760/3000]  eta: 0:31:12  lr: 0.000030  loss: 0.4705  time: 1.4953  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1765/3000]  eta: 0:31:05  lr: 0.000030  loss: 0.7229  time: 1.4890  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1765/3000]  eta: 0:31:04  lr: 0.000030  loss: 0.5498  time: 1.4888  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1770/3000]  eta: 0:30:57  lr: 0.000030  loss: 0.2289  time: 1.4931  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1770/3000]  eta: 0:30:57  lr: 0.000030  loss: 0.0936  time: 1.4928  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1775/3000]  eta: 0:30:49  lr: 0.000030  loss: 0.6739  time: 1.5130  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1775/3000]  eta: 0:30:49  lr: 0.000030  loss: 0.8364  time: 1.5127  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1780/3000]  eta: 0:30:42  lr: 0.000030  loss: 0.4409  time: 1.5038  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1780/3000]  eta: 0:30:42  lr: 0.000030  loss: 0.3955  time: 1.5035  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1785/3000]  eta: 0:30:34  lr: 0.000030  loss: 0.5885  time: 1.4992  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1785/3000]  eta: 0:30:34  lr: 0.000030  loss: 0.2111  time: 1.4989  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1790/3000]  eta: 0:30:27  lr: 0.000030  loss: 0.3398  time: 1.5130  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1790/3000]  eta: 0:30:26  lr: 0.000030  loss: 0.5481  time: 1.5128  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1795/3000]  eta: 0:30:19  lr: 0.000030  loss: 0.5956  time: 1.4927  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1795/3000]  eta: 0:30:19  lr: 0.000030  loss: 0.4914  time: 1.4925  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1800/3000]  eta: 0:30:12  lr: 0.000030  loss: 0.6721  time: 1.5026  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1800/3000]  eta: 0:30:11  lr: 0.000030  loss: 1.0695  time: 1.5023  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1805/3000]  eta: 0:30:04  lr: 0.000030  loss: 0.3873  time: 1.5305  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1805/3000]  eta: 0:30:04  lr: 0.000030  loss: 0.5151  time: 1.5303  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1810/3000]  eta: 0:29:57  lr: 0.000030  loss: 0.0886  time: 1.5245  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1810/3000]  eta: 0:29:56  lr: 0.000030  loss: 0.2860  time: 1.5242  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1815/3000]  eta: 0:29:49  lr: 0.000030  loss: 0.1883  time: 1.5335  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1815/3000]  eta: 0:29:49  lr: 0.000030  loss: 0.2766  time: 1.5332  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1820/3000]  eta: 0:29:42  lr: 0.000030  loss: 0.2336  time: 1.5389  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1820/3000]  eta: 0:29:41  lr: 0.000030  loss: 0.6112  time: 1.5387  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1825/3000]  eta: 0:29:34  lr: 0.000030  loss: 0.7558  time: 1.5137  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1825/3000]  eta: 0:29:34  lr: 0.000030  loss: 0.6150  time: 1.5135  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1830/3000]  eta: 0:29:26  lr: 0.000030  loss: 0.3778  time: 1.4953  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1830/3000]  eta: 0:29:26  lr: 0.000030  loss: 0.1640  time: 1.4950  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1835/3000]  eta: 0:29:19  lr: 0.000030  loss: 0.3021  time: 1.5089  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1835/3000]  eta: 0:29:19  lr: 0.000030  loss: 0.3669  time: 1.5086  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1840/3000]  eta: 0:29:11  lr: 0.000030  loss: 0.2358  time: 1.4891  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1840/3000]  eta: 0:29:11  lr: 0.000030  loss: 0.2998  time: 1.4889  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1845/3000]  eta: 0:29:04  lr: 0.000030  loss: 0.2017  time: 1.4972  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1845/3000]  eta: 0:29:03  lr: 0.000030  loss: 0.2001  time: 1.4969  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1850/3000]  eta: 0:28:56  lr: 0.000030  loss: 0.4894  time: 1.5078  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1850/3000]  eta: 0:28:56  lr: 0.000030  loss: 0.5855  time: 1.5075  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1855/3000]  eta: 0:28:48  lr: 0.000030  loss: 0.5631  time: 1.4972  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1855/3000]  eta: 0:28:48  lr: 0.000030  loss: 0.3110  time: 1.4970  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1860/3000]  eta: 0:28:41  lr: 0.000030  loss: 0.4627  time: 1.5141  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1860/3000]  eta: 0:28:41  lr: 0.000030  loss: 0.2348  time: 1.5137  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1865/3000]  eta: 0:28:33  lr: 0.000030  loss: 0.3968  time: 1.4861  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1865/3000]  eta: 0:28:33  lr: 0.000030  loss: 0.2545  time: 1.4858  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1870/3000]  eta: 0:28:26  lr: 0.000030  loss: 0.1247  time: 1.4795  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1870/3000]  eta: 0:28:25  lr: 0.000030  loss: 0.3238  time: 1.4792  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1875/3000]  eta: 0:28:18  lr: 0.000030  loss: 0.3445  time: 1.4776  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1875/3000]  eta: 0:28:18  lr: 0.000030  loss: 0.5781  time: 1.4773  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1880/3000]  eta: 0:28:10  lr: 0.000030  loss: 0.1013  time: 1.4722  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1880/3000]  eta: 0:28:10  lr: 0.000030  loss: 0.3513  time: 1.4720  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1885/3000]  eta: 0:28:03  lr: 0.000030  loss: 0.4768  time: 1.5180  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1885/3000]  eta: 0:28:03  lr: 0.000030  loss: 0.4502  time: 1.5178  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1890/3000]  eta: 0:27:55  lr: 0.000030  loss: 0.5909  time: 1.5149  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1890/3000]  eta: 0:27:55  lr: 0.000030  loss: 0.6912  time: 1.5146  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1895/3000]  eta: 0:27:48  lr: 0.000030  loss: 0.1266  time: 1.5200  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1895/3000]  eta: 0:27:48  lr: 0.000030  loss: 0.5514  time: 1.5197  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1900/3000]  eta: 0:27:40  lr: 0.000030  loss: 0.4554  time: 1.5079  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1900/3000]  eta: 0:27:40  lr: 0.000030  loss: 0.2333  time: 1.5076  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1905/3000]  eta: 0:27:33  lr: 0.000030  loss: 0.4069  time: 1.4828  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1905/3000]  eta: 0:27:32  lr: 0.000030  loss: 0.7179  time: 1.4825  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1910/3000]  eta: 0:27:25  lr: 0.000030  loss: 0.7676  time: 1.5117  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1910/3000]  eta: 0:27:25  lr: 0.000030  loss: 0.4068  time: 1.5114  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1915/3000]  eta: 0:27:17  lr: 0.000030  loss: 0.5210  time: 1.4944  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1915/3000]  eta: 0:27:17  lr: 0.000030  loss: 0.1832  time: 1.4942  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1920/3000]  eta: 0:27:10  lr: 0.000030  loss: 0.8169  time: 1.5074  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1920/3000]  eta: 0:27:10  lr: 0.000030  loss: 0.7133  time: 1.5071  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1925/3000]  eta: 0:27:02  lr: 0.000030  loss: 0.6770  time: 1.5058  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1925/3000]  eta: 0:27:02  lr: 0.000030  loss: 0.3316  time: 1.5056  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1930/3000]  eta: 0:26:55  lr: 0.000030  loss: 0.2227  time: 1.4930  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1930/3000]  eta: 0:26:55  lr: 0.000030  loss: 0.1898  time: 1.4929  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1935/3000]  eta: 0:26:47  lr: 0.000030  loss: 0.2331  time: 1.5273  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1935/3000]  eta: 0:26:47  lr: 0.000030  loss: 0.5802  time: 1.5270  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1940/3000]  eta: 0:26:40  lr: 0.000030  loss: 0.3436  time: 1.4974  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1940/3000]  eta: 0:26:39  lr: 0.000030  loss: 0.4141  time: 1.4971  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1945/3000]  eta: 0:26:32  lr: 0.000030  loss: 0.3289  time: 1.5128  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1945/3000]  eta: 0:26:32  lr: 0.000030  loss: 0.1737  time: 1.5125  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1950/3000]  eta: 0:26:24  lr: 0.000030  loss: 0.2915  time: 1.4934  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1950/3000]  eta: 0:26:24  lr: 0.000030  loss: 0.2597  time: 1.4931  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1955/3000]  eta: 0:26:17  lr: 0.000030  loss: 0.2133  time: 1.4741  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1955/3000]  eta: 0:26:17  lr: 0.000030  loss: 0.4108  time: 1.4740  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1960/3000]  eta: 0:26:09  lr: 0.000030  loss: 0.2296  time: 1.5031  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1960/3000]  eta: 0:26:09  lr: 0.000030  loss: 0.5724  time: 1.5028  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1965/3000]  eta: 0:26:02  lr: 0.000030  loss: 0.2808  time: 1.4822  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1965/3000]  eta: 0:26:01  lr: 0.000030  loss: 0.2262  time: 1.4819  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1970/3000]  eta: 0:25:54  lr: 0.000030  loss: 0.1709  time: 1.5002  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1970/3000]  eta: 0:25:54  lr: 0.000030  loss: 0.5811  time: 1.4999  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1975/3000]  eta: 0:25:46  lr: 0.000030  loss: 0.3341  time: 1.4769  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1975/3000]  eta: 0:25:46  lr: 0.000030  loss: 0.4328  time: 1.4765  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1980/3000]  eta: 0:25:39  lr: 0.000030  loss: 0.2887  time: 1.4607  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1980/3000]  eta: 0:25:38  lr: 0.000030  loss: 0.1868  time: 1.4604  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1985/3000]  eta: 0:25:31  lr: 0.000030  loss: 0.7397  time: 1.4825  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1985/3000]  eta: 0:25:31  lr: 0.000030  loss: 0.8305  time: 1.4821  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1990/3000]  eta: 0:25:24  lr: 0.000030  loss: 0.2990  time: 1.4858  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1990/3000]  eta: 0:25:24  lr: 0.000030  loss: 0.4845  time: 1.4856  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [1995/3000]  eta: 0:25:16  lr: 0.000030  loss: 0.4035  time: 1.5302  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [1995/3000]  eta: 0:25:16  lr: 0.000030  loss: 0.1698  time: 1.5300  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2000/3000]  eta: 0:25:09  lr: 0.000030  loss: 0.9566  time: 1.5475  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2000/3000]  eta: 0:25:09  lr: 0.000030  loss: 0.3212  time: 1.5472  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2005/3000]  eta: 0:25:02  lr: 0.000030  loss: 0.3093  time: 1.5568  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2005/3000]  eta: 0:25:01  lr: 0.000030  loss: 0.4034  time: 1.5566  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2010/3000]  eta: 0:24:54  lr: 0.000030  loss: 0.4369  time: 1.5634  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2010/3000]  eta: 0:24:54  lr: 0.000030  loss: 0.4041  time: 1.5632  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2015/3000]  eta: 0:24:47  lr: 0.000030  loss: 0.4658  time: 1.5463  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2015/3000]  eta: 0:24:46  lr: 0.000030  loss: 0.4314  time: 1.5462  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2020/3000]  eta: 0:24:39  lr: 0.000030  loss: 0.3398  time: 1.5318  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2020/3000]  eta: 0:24:39  lr: 0.000030  loss: 0.7013  time: 1.5317  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2025/3000]  eta: 0:24:31  lr: 0.000030  loss: 0.4630  time: 1.5188  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2025/3000]  eta: 0:24:31  lr: 0.000030  loss: 0.3321  time: 1.5185  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2030/3000]  eta: 0:24:24  lr: 0.000030  loss: 0.2788  time: 1.5151  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2030/3000]  eta: 0:24:24  lr: 0.000030  loss: 1.2961  time: 1.5148  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2035/3000]  eta: 0:24:17  lr: 0.000030  loss: 0.1822  time: 1.5271  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2035/3000]  eta: 0:24:16  lr: 0.000030  loss: 1.0381  time: 1.5268  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2040/3000]  eta: 0:24:09  lr: 0.000030  loss: 0.4954  time: 1.5517  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2040/3000]  eta: 0:24:09  lr: 0.000030  loss: 0.3619  time: 1.5515  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2045/3000]  eta: 0:24:02  lr: 0.000030  loss: 0.3666  time: 1.5659  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2045/3000]  eta: 0:24:02  lr: 0.000030  loss: 0.5061  time: 1.5657  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2050/3000]  eta: 0:23:54  lr: 0.000030  loss: 0.1177  time: 1.5384  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2050/3000]  eta: 0:23:54  lr: 0.000030  loss: 0.4147  time: 1.5382  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2055/3000]  eta: 0:23:47  lr: 0.000030  loss: 0.4516  time: 1.5368  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2055/3000]  eta: 0:23:46  lr: 0.000030  loss: 0.3479  time: 1.5365  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2060/3000]  eta: 0:23:39  lr: 0.000030  loss: 0.3681  time: 1.5270  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2060/3000]  eta: 0:23:39  lr: 0.000030  loss: 0.6092  time: 1.5267  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2065/3000]  eta: 0:23:32  lr: 0.000030  loss: 0.2845  time: 1.5152  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2065/3000]  eta: 0:23:31  lr: 0.000030  loss: 1.0220  time: 1.5149  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2070/3000]  eta: 0:23:24  lr: 0.000030  loss: 0.5878  time: 1.5284  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2070/3000]  eta: 0:23:24  lr: 0.000030  loss: 0.1317  time: 1.5281  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2075/3000]  eta: 0:23:16  lr: 0.000030  loss: 0.4391  time: 1.5108  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2075/3000]  eta: 0:23:16  lr: 0.000030  loss: 0.4135  time: 1.5106  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2080/3000]  eta: 0:23:09  lr: 0.000030  loss: 0.7422  time: 1.5211  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2080/3000]  eta: 0:23:09  lr: 0.000030  loss: 0.4630  time: 1.5208  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2085/3000]  eta: 0:23:01  lr: 0.000030  loss: 0.3714  time: 1.5049  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2085/3000]  eta: 0:23:01  lr: 0.000030  loss: 0.2780  time: 1.5046  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2090/3000]  eta: 0:22:54  lr: 0.000030  loss: 0.1685  time: 1.5173  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2090/3000]  eta: 0:22:54  lr: 0.000030  loss: 0.2550  time: 1.5170  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2095/3000]  eta: 0:22:46  lr: 0.000030  loss: 0.9581  time: 1.5393  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2095/3000]  eta: 0:22:46  lr: 0.000030  loss: 0.6717  time: 1.5389  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2100/3000]  eta: 0:22:39  lr: 0.000030  loss: 0.3287  time: 1.5073  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2100/3000]  eta: 0:22:39  lr: 0.000030  loss: 0.4440  time: 1.5071  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2105/3000]  eta: 0:22:31  lr: 0.000030  loss: 1.6880  time: 1.5126  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2105/3000]  eta: 0:22:31  lr: 0.000030  loss: 0.2627  time: 1.5124  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2110/3000]  eta: 0:22:24  lr: 0.000030  loss: 0.1919  time: 1.4989  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2110/3000]  eta: 0:22:23  lr: 0.000030  loss: 0.7088  time: 1.4987  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2115/3000]  eta: 0:22:16  lr: 0.000030  loss: 0.5696  time: 1.5031  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2115/3000]  eta: 0:22:16  lr: 0.000030  loss: 0.4654  time: 1.5029  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2120/3000]  eta: 0:22:09  lr: 0.000030  loss: 0.2424  time: 1.5267  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2120/3000]  eta: 0:22:08  lr: 0.000030  loss: 0.2204  time: 1.5264  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2125/3000]  eta: 0:22:01  lr: 0.000030  loss: 0.0610  time: 1.5344  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2125/3000]  eta: 0:22:01  lr: 0.000030  loss: 0.4053  time: 1.5342  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2130/3000]  eta: 0:21:54  lr: 0.000030  loss: 0.8217  time: 1.5597  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2130/3000]  eta: 0:21:54  lr: 0.000030  loss: 0.3621  time: 1.5595  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2135/3000]  eta: 0:21:46  lr: 0.000030  loss: 0.1656  time: 1.5451  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2135/3000]  eta: 0:21:46  lr: 0.000030  loss: 0.1624  time: 1.5449  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2140/3000]  eta: 0:21:39  lr: 0.000030  loss: 1.1966  time: 1.5432  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2140/3000]  eta: 0:21:39  lr: 0.000030  loss: 0.3124  time: 1.5430  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2145/3000]  eta: 0:21:31  lr: 0.000030  loss: 0.8684  time: 1.5329  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2145/3000]  eta: 0:21:31  lr: 0.000030  loss: 0.2508  time: 1.5326  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2150/3000]  eta: 0:21:24  lr: 0.000030  loss: 0.6129  time: 1.5281  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2150/3000]  eta: 0:21:23  lr: 0.000030  loss: 0.2291  time: 1.5278  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2155/3000]  eta: 0:21:16  lr: 0.000030  loss: 0.4297  time: 1.5301  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2155/3000]  eta: 0:21:16  lr: 0.000030  loss: 0.1622  time: 1.5298  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2160/3000]  eta: 0:21:09  lr: 0.000030  loss: 0.5521  time: 1.5221  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2160/3000]  eta: 0:21:08  lr: 0.000030  loss: 0.2858  time: 1.5219  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2165/3000]  eta: 0:21:01  lr: 0.000030  loss: 0.0880  time: 1.5220  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2165/3000]  eta: 0:21:01  lr: 0.000030  loss: 0.1712  time: 1.5218  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2170/3000]  eta: 0:20:53  lr: 0.000030  loss: 0.2480  time: 1.5039  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2170/3000]  eta: 0:20:53  lr: 0.000030  loss: 0.4868  time: 1.5037  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2175/3000]  eta: 0:20:46  lr: 0.000030  loss: 1.2481  time: 1.5173  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2175/3000]  eta: 0:20:46  lr: 0.000030  loss: 0.1566  time: 1.5169  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2180/3000]  eta: 0:20:39  lr: 0.000030  loss: 0.5483  time: 1.5315  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2180/3000]  eta: 0:20:38  lr: 0.000030  loss: 0.2270  time: 1.5311  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2185/3000]  eta: 0:20:31  lr: 0.000030  loss: 0.3072  time: 1.5424  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2185/3000]  eta: 0:20:31  lr: 0.000030  loss: 0.2095  time: 1.5420  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2190/3000]  eta: 0:20:23  lr: 0.000030  loss: 0.1597  time: 1.5401  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2190/3000]  eta: 0:20:23  lr: 0.000030  loss: 0.2648  time: 1.5396  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2195/3000]  eta: 0:20:16  lr: 0.000030  loss: 0.2377  time: 1.5348  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2195/3000]  eta: 0:20:16  lr: 0.000030  loss: 0.5347  time: 1.5346  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2200/3000]  eta: 0:20:08  lr: 0.000030  loss: 0.9106  time: 1.5313  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2200/3000]  eta: 0:20:08  lr: 0.000030  loss: 0.2544  time: 1.5311  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2205/3000]  eta: 0:20:01  lr: 0.000030  loss: 1.4221  time: 1.5132  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2205/3000]  eta: 0:20:01  lr: 0.000030  loss: 0.3481  time: 1.5130  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2210/3000]  eta: 0:19:53  lr: 0.000030  loss: 0.6503  time: 1.5279  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2210/3000]  eta: 0:19:53  lr: 0.000030  loss: 0.2469  time: 1.5277  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2215/3000]  eta: 0:19:46  lr: 0.000030  loss: 0.3840  time: 1.5145  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2215/3000]  eta: 0:19:46  lr: 0.000030  loss: 0.3214  time: 1.5144  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2220/3000]  eta: 0:19:38  lr: 0.000030  loss: 0.4209  time: 1.5070  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2220/3000]  eta: 0:19:38  lr: 0.000030  loss: 0.3061  time: 1.5067  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2225/3000]  eta: 0:19:31  lr: 0.000030  loss: 0.2132  time: 1.5151  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2225/3000]  eta: 0:19:30  lr: 0.000030  loss: 0.6069  time: 1.5149  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2230/3000]  eta: 0:19:23  lr: 0.000030  loss: 0.4913  time: 1.5171  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2230/3000]  eta: 0:19:23  lr: 0.000030  loss: 0.4254  time: 1.5168  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2235/3000]  eta: 0:19:16  lr: 0.000030  loss: 0.2700  time: 1.5106  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2235/3000]  eta: 0:19:15  lr: 0.000030  loss: 0.3729  time: 1.5103  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2240/3000]  eta: 0:19:08  lr: 0.000030  loss: 0.4697  time: 1.5235  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2240/3000]  eta: 0:19:08  lr: 0.000030  loss: 1.0883  time: 1.5232  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2245/3000]  eta: 0:19:01  lr: 0.000030  loss: 0.3812  time: 1.5440  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2245/3000]  eta: 0:19:00  lr: 0.000030  loss: 0.2563  time: 1.5437  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2250/3000]  eta: 0:18:53  lr: 0.000030  loss: 0.1998  time: 1.5259  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2250/3000]  eta: 0:18:53  lr: 0.000030  loss: 0.4415  time: 1.5257  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2255/3000]  eta: 0:18:45  lr: 0.000030  loss: 0.7942  time: 1.5296  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2255/3000]  eta: 0:18:45  lr: 0.000030  loss: 0.1962  time: 1.5295  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2260/3000]  eta: 0:18:38  lr: 0.000030  loss: 0.1277  time: 1.5195  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2260/3000]  eta: 0:18:38  lr: 0.000030  loss: 0.2433  time: 1.5193  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2265/3000]  eta: 0:18:30  lr: 0.000030  loss: 0.4002  time: 1.4986  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2265/3000]  eta: 0:18:30  lr: 0.000030  loss: 0.1946  time: 1.4984  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2270/3000]  eta: 0:18:23  lr: 0.000030  loss: 0.4348  time: 1.5154  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2270/3000]  eta: 0:18:23  lr: 0.000030  loss: 0.2276  time: 1.5150  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2275/3000]  eta: 0:18:15  lr: 0.000030  loss: 0.1907  time: 1.5065  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2275/3000]  eta: 0:18:15  lr: 0.000030  loss: 0.1056  time: 1.5062  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2280/3000]  eta: 0:18:08  lr: 0.000030  loss: 0.6186  time: 1.5063  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2280/3000]  eta: 0:18:07  lr: 0.000030  loss: 0.1329  time: 1.5059  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2285/3000]  eta: 0:18:00  lr: 0.000030  loss: 0.6085  time: 1.5174  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2285/3000]  eta: 0:18:00  lr: 0.000030  loss: 0.5143  time: 1.5170  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2290/3000]  eta: 0:17:53  lr: 0.000030  loss: 0.2891  time: 1.5132  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2290/3000]  eta: 0:17:52  lr: 0.000030  loss: 0.4337  time: 1.5130  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2295/3000]  eta: 0:17:45  lr: 0.000030  loss: 0.2054  time: 1.5228  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2295/3000]  eta: 0:17:45  lr: 0.000030  loss: 0.5569  time: 1.5225  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2300/3000]  eta: 0:17:37  lr: 0.000030  loss: 0.4989  time: 1.5175  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2300/3000]  eta: 0:17:37  lr: 0.000030  loss: 0.3497  time: 1.5173  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2305/3000]  eta: 0:17:30  lr: 0.000030  loss: 0.6545  time: 1.5134  data: 0.0000  max mem: 18432Train: data epoch: [2]  [2305/3000]  eta: 0:17:30  lr: 0.000030  loss: 1.2750  time: 1.5131  data: 0.0000  max mem: 18151

Train: data epoch: [2]  [2310/3000]  eta: 0:17:22  lr: 0.000030  loss: 0.4437  time: 1.5108  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2310/3000]  eta: 0:17:22  lr: 0.000030  loss: 0.4191  time: 1.5106  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2315/3000]  eta: 0:17:15  lr: 0.000030  loss: 0.1939  time: 1.5187  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2315/3000]  eta: 0:17:15  lr: 0.000030  loss: 1.0654  time: 1.5185  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2320/3000]  eta: 0:17:07  lr: 0.000030  loss: 0.4752  time: 1.5161  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2320/3000]  eta: 0:17:07  lr: 0.000030  loss: 0.4033  time: 1.5159  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2325/3000]  eta: 0:17:00  lr: 0.000030  loss: 0.5549  time: 1.5120  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2325/3000]  eta: 0:17:00  lr: 0.000030  loss: 0.4839  time: 1.5118  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2330/3000]  eta: 0:16:52  lr: 0.000030  loss: 0.2275  time: 1.5106  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2330/3000]  eta: 0:16:52  lr: 0.000030  loss: 0.2624  time: 1.5104  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2335/3000]  eta: 0:16:45  lr: 0.000030  loss: 0.2906  time: 1.5007  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2335/3000]  eta: 0:16:44  lr: 0.000030  loss: 0.2810  time: 1.5005  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2340/3000]  eta: 0:16:37  lr: 0.000030  loss: 0.4654  time: 1.4993  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2340/3000]  eta: 0:16:37  lr: 0.000030  loss: 0.1002  time: 1.4989  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2345/3000]  eta: 0:16:29  lr: 0.000030  loss: 0.3560  time: 1.5108  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2345/3000]  eta: 0:16:29  lr: 0.000030  loss: 0.3312  time: 1.5105  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2350/3000]  eta: 0:16:22  lr: 0.000030  loss: 0.1404  time: 1.4958  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2350/3000]  eta: 0:16:22  lr: 0.000030  loss: 0.6735  time: 1.4956  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2355/3000]  eta: 0:16:14  lr: 0.000030  loss: 0.5916  time: 1.5021  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2355/3000]  eta: 0:16:14  lr: 0.000030  loss: 0.2296  time: 1.5018  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2360/3000]  eta: 0:16:07  lr: 0.000030  loss: 0.2444  time: 1.5043  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2360/3000]  eta: 0:16:07  lr: 0.000030  loss: 0.1401  time: 1.5041  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2365/3000]  eta: 0:15:59  lr: 0.000030  loss: 0.2009  time: 1.4865  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2365/3000]  eta: 0:15:59  lr: 0.000030  loss: 0.5825  time: 1.4862  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2370/3000]  eta: 0:15:51  lr: 0.000030  loss: 0.4915  time: 1.4830  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2370/3000]  eta: 0:15:51  lr: 0.000030  loss: 0.1748  time: 1.4826  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2375/3000]  eta: 0:15:44  lr: 0.000030  loss: 0.3870  time: 1.4656  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2375/3000]  eta: 0:15:44  lr: 0.000030  loss: 0.4672  time: 1.4653  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2380/3000]  eta: 0:15:36  lr: 0.000030  loss: 0.2992  time: 1.4553  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2380/3000]  eta: 0:15:36  lr: 0.000030  loss: 0.9529  time: 1.4549  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2385/3000]  eta: 0:15:29  lr: 0.000030  loss: 0.2353  time: 1.4852  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2385/3000]  eta: 0:15:29  lr: 0.000030  loss: 0.3691  time: 1.4849  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2390/3000]  eta: 0:15:21  lr: 0.000030  loss: 0.2847  time: 1.5128  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2390/3000]  eta: 0:15:21  lr: 0.000030  loss: 0.3883  time: 1.5125  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2395/3000]  eta: 0:15:14  lr: 0.000030  loss: 0.4695  time: 1.5303  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2395/3000]  eta: 0:15:14  lr: 0.000030  loss: 1.2449  time: 1.5301  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2400/3000]  eta: 0:15:06  lr: 0.000030  loss: 0.6021  time: 1.5529  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2400/3000]  eta: 0:15:06  lr: 0.000030  loss: 0.4083  time: 1.5526  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2405/3000]  eta: 0:14:59  lr: 0.000030  loss: 0.1300  time: 1.5310  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2405/3000]  eta: 0:14:58  lr: 0.000030  loss: 0.3277  time: 1.5308  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2410/3000]  eta: 0:14:51  lr: 0.000030  loss: 0.3603  time: 1.5200  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2410/3000]  eta: 0:14:51  lr: 0.000030  loss: 0.2845  time: 1.5198  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2415/3000]  eta: 0:14:43  lr: 0.000030  loss: 0.4537  time: 1.5142  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2415/3000]  eta: 0:14:43  lr: 0.000030  loss: 0.1001  time: 1.5139  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2420/3000]  eta: 0:14:36  lr: 0.000030  loss: 0.2230  time: 1.5232  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2420/3000]  eta: 0:14:36  lr: 0.000030  loss: 0.0917  time: 1.5230  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2425/3000]  eta: 0:14:28  lr: 0.000030  loss: 0.4804  time: 1.5059  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2425/3000]  eta: 0:14:28  lr: 0.000030  loss: 0.2236  time: 1.5057  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2430/3000]  eta: 0:14:21  lr: 0.000030  loss: 1.0662  time: 1.5166  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2430/3000]  eta: 0:14:21  lr: 0.000030  loss: 0.2926  time: 1.5163  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2435/3000]  eta: 0:14:13  lr: 0.000030  loss: 0.2458  time: 1.5240  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2435/3000]  eta: 0:14:13  lr: 0.000030  loss: 0.2762  time: 1.5237  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2440/3000]  eta: 0:14:06  lr: 0.000030  loss: 0.3849  time: 1.5214  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2440/3000]  eta: 0:14:06  lr: 0.000030  loss: 0.2948  time: 1.5211  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2445/3000]  eta: 0:13:58  lr: 0.000030  loss: 0.2946  time: 1.5494  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2445/3000]  eta: 0:13:58  lr: 0.000030  loss: 0.1212  time: 1.5491  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2450/3000]  eta: 0:13:51  lr: 0.000030  loss: 0.3395  time: 1.5457  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2450/3000]  eta: 0:13:51  lr: 0.000030  loss: 0.1426  time: 1.5455  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2455/3000]  eta: 0:13:43  lr: 0.000030  loss: 0.8448  time: 1.5362  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2455/3000]  eta: 0:13:43  lr: 0.000030  loss: 0.7215  time: 1.5359  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2460/3000]  eta: 0:13:36  lr: 0.000030  loss: 0.1947  time: 1.5242  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2460/3000]  eta: 0:13:36  lr: 0.000030  loss: 0.8082  time: 1.5240  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2465/3000]  eta: 0:13:28  lr: 0.000030  loss: 0.9459  time: 1.5110  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2465/3000]  eta: 0:13:28  lr: 0.000030  loss: 0.1896  time: 1.5107  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2470/3000]  eta: 0:13:20  lr: 0.000030  loss: 0.8102  time: 1.4948  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2470/3000]  eta: 0:13:20  lr: 0.000030  loss: 0.1376  time: 1.4946  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2475/3000]  eta: 0:13:13  lr: 0.000030  loss: 0.4326  time: 1.5057  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2475/3000]  eta: 0:13:13  lr: 0.000030  loss: 0.3094  time: 1.5055  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2480/3000]  eta: 0:13:05  lr: 0.000030  loss: 0.5860  time: 1.4991  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2480/3000]  eta: 0:13:05  lr: 0.000030  loss: 0.4088  time: 1.4989  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2485/3000]  eta: 0:12:58  lr: 0.000030  loss: 0.6055  time: 1.5166  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2485/3000]  eta: 0:12:58  lr: 0.000030  loss: 0.6431  time: 1.5163  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2490/3000]  eta: 0:12:50  lr: 0.000030  loss: 0.3206  time: 1.5120  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2490/3000]  eta: 0:12:50  lr: 0.000030  loss: 0.2597  time: 1.5117  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2495/3000]  eta: 0:12:43  lr: 0.000030  loss: 0.5400  time: 1.5128  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2495/3000]  eta: 0:12:43  lr: 0.000030  loss: 0.2948  time: 1.5126  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2500/3000]  eta: 0:12:35  lr: 0.000030  loss: 0.2985  time: 1.5101  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2500/3000]  eta: 0:12:35  lr: 0.000030  loss: 0.5321  time: 1.5099  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2505/3000]  eta: 0:12:28  lr: 0.000030  loss: 0.3283  time: 1.4941  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2505/3000]  eta: 0:12:27  lr: 0.000030  loss: 0.5561  time: 1.4939  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2510/3000]  eta: 0:12:20  lr: 0.000030  loss: 0.6280  time: 1.5174  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2510/3000]  eta: 0:12:20  lr: 0.000030  loss: 0.2666  time: 1.5171  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2515/3000]  eta: 0:12:13  lr: 0.000030  loss: 0.2288  time: 1.5178  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2515/3000]  eta: 0:12:12  lr: 0.000030  loss: 0.2472  time: 1.5175  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2520/3000]  eta: 0:12:05  lr: 0.000030  loss: 0.6719  time: 1.5347  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2520/3000]  eta: 0:12:05  lr: 0.000030  loss: 0.0937  time: 1.5344  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2525/3000]  eta: 0:11:58  lr: 0.000030  loss: 0.5875  time: 1.5496  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2525/3000]  eta: 0:11:57  lr: 0.000030  loss: 0.7634  time: 1.5493  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2530/3000]  eta: 0:11:50  lr: 0.000030  loss: 0.1500  time: 1.5312  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2530/3000]  eta: 0:11:50  lr: 0.000030  loss: 0.4213  time: 1.5311  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2535/3000]  eta: 0:11:42  lr: 0.000030  loss: 0.1595  time: 1.5328  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2535/3000]  eta: 0:11:42  lr: 0.000030  loss: 0.1006  time: 1.5327  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2540/3000]  eta: 0:11:35  lr: 0.000030  loss: 1.0642  time: 1.5216  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2540/3000]  eta: 0:11:35  lr: 0.000030  loss: 0.2101  time: 1.5213  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2545/3000]  eta: 0:11:27  lr: 0.000030  loss: 0.7054  time: 1.5231  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2545/3000]  eta: 0:11:27  lr: 0.000030  loss: 0.6289  time: 1.5227  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2550/3000]  eta: 0:11:20  lr: 0.000030  loss: 0.3363  time: 1.5315  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2550/3000]  eta: 0:11:20  lr: 0.000030  loss: 0.5715  time: 1.5309  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2555/3000]  eta: 0:11:12  lr: 0.000030  loss: 0.6829  time: 1.5297  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2555/3000]  eta: 0:11:12  lr: 0.000030  loss: 0.7831  time: 1.5290  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2560/3000]  eta: 0:11:05  lr: 0.000030  loss: 0.4492  time: 1.5050  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2560/3000]  eta: 0:11:04  lr: 0.000030  loss: 0.6845  time: 1.5046  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2565/3000]  eta: 0:10:57  lr: 0.000030  loss: 0.4131  time: 1.5004  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2565/3000]  eta: 0:10:57  lr: 0.000030  loss: 0.1107  time: 1.5001  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2570/3000]  eta: 0:10:50  lr: 0.000030  loss: 0.4921  time: 1.5101  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2570/3000]  eta: 0:10:49  lr: 0.000030  loss: 0.2421  time: 1.5098  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2575/3000]  eta: 0:10:42  lr: 0.000030  loss: 0.6876  time: 1.5141  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2575/3000]  eta: 0:10:42  lr: 0.000030  loss: 0.2750  time: 1.5138  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2580/3000]  eta: 0:10:34  lr: 0.000030  loss: 0.1823  time: 1.5439  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2580/3000]  eta: 0:10:34  lr: 0.000030  loss: 0.2305  time: 1.5436  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2585/3000]  eta: 0:10:27  lr: 0.000030  loss: 0.2635  time: 1.5196  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2585/3000]  eta: 0:10:27  lr: 0.000030  loss: 0.2218  time: 1.5193  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2590/3000]  eta: 0:10:19  lr: 0.000030  loss: 0.3165  time: 1.5316  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2590/3000]  eta: 0:10:19  lr: 0.000030  loss: 0.2132  time: 1.5314  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2595/3000]  eta: 0:10:12  lr: 0.000030  loss: 0.6411  time: 1.4962  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2595/3000]  eta: 0:10:12  lr: 0.000030  loss: 0.2776  time: 1.4959  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2600/3000]  eta: 0:10:04  lr: 0.000030  loss: 0.1343  time: 1.5010  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2600/3000]  eta: 0:10:04  lr: 0.000030  loss: 0.2276  time: 1.5007  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2605/3000]  eta: 0:09:57  lr: 0.000030  loss: 1.0388  time: 1.4961  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2605/3000]  eta: 0:09:56  lr: 0.000030  loss: 0.4756  time: 1.4959  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2610/3000]  eta: 0:09:49  lr: 0.000030  loss: 0.8819  time: 1.4940  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2610/3000]  eta: 0:09:49  lr: 0.000030  loss: 0.6817  time: 1.4938  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2615/3000]  eta: 0:09:42  lr: 0.000030  loss: 0.8315  time: 1.5223  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2615/3000]  eta: 0:09:41  lr: 0.000030  loss: 0.1894  time: 1.5222  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2620/3000]  eta: 0:09:34  lr: 0.000030  loss: 0.5837  time: 1.5037  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2620/3000]  eta: 0:09:34  lr: 0.000030  loss: 0.2582  time: 1.5035  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2625/3000]  eta: 0:09:26  lr: 0.000030  loss: 0.6056  time: 1.5072  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2625/3000]  eta: 0:09:26  lr: 0.000030  loss: 0.3474  time: 1.5070  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2630/3000]  eta: 0:09:19  lr: 0.000030  loss: 0.6486  time: 1.4693  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2630/3000]  eta: 0:09:19  lr: 0.000030  loss: 1.2611  time: 1.4691  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2635/3000]  eta: 0:09:11  lr: 0.000030  loss: 0.2134  time: 1.4584  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2635/3000]  eta: 0:09:11  lr: 0.000030  loss: 0.7046  time: 1.4582  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2640/3000]  eta: 0:09:04  lr: 0.000030  loss: 0.2032  time: 1.4524  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2640/3000]  eta: 0:09:03  lr: 0.000030  loss: 0.4327  time: 1.4522  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2645/3000]  eta: 0:08:56  lr: 0.000030  loss: 0.4056  time: 1.4647  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2645/3000]  eta: 0:08:56  lr: 0.000030  loss: 0.1127  time: 1.4644  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2650/3000]  eta: 0:08:48  lr: 0.000030  loss: 0.6826  time: 1.5003  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2650/3000]  eta: 0:08:48  lr: 0.000030  loss: 0.1616  time: 1.5000  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2655/3000]  eta: 0:08:41  lr: 0.000030  loss: 0.5510  time: 1.5082  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2655/3000]  eta: 0:08:41  lr: 0.000030  loss: 0.4300  time: 1.5079  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2660/3000]  eta: 0:08:33  lr: 0.000030  loss: 0.1890  time: 1.5018  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2660/3000]  eta: 0:08:33  lr: 0.000030  loss: 0.4143  time: 1.5016  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2665/3000]  eta: 0:08:26  lr: 0.000030  loss: 0.1383  time: 1.4874  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2665/3000]  eta: 0:08:26  lr: 0.000030  loss: 0.1963  time: 1.4872  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2670/3000]  eta: 0:08:18  lr: 0.000030  loss: 0.4376  time: 1.4858  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2670/3000]  eta: 0:08:18  lr: 0.000030  loss: 0.3025  time: 1.4856  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2675/3000]  eta: 0:08:11  lr: 0.000030  loss: 0.3514  time: 1.4845  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2675/3000]  eta: 0:08:11  lr: 0.000030  loss: 0.4417  time: 1.4842  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2680/3000]  eta: 0:08:03  lr: 0.000030  loss: 0.7366  time: 1.4875  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2680/3000]  eta: 0:08:03  lr: 0.000030  loss: 0.3966  time: 1.4872  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2685/3000]  eta: 0:07:55  lr: 0.000030  loss: 0.6666  time: 1.5136  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2685/3000]  eta: 0:07:55  lr: 0.000030  loss: 0.2776  time: 1.5133  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2690/3000]  eta: 0:07:48  lr: 0.000030  loss: 0.2530  time: 1.5015  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2690/3000]  eta: 0:07:48  lr: 0.000030  loss: 0.4141  time: 1.5013  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2695/3000]  eta: 0:07:40  lr: 0.000030  loss: 0.2730  time: 1.4900  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2695/3000]  eta: 0:07:40  lr: 0.000030  loss: 0.8641  time: 1.4897  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2700/3000]  eta: 0:07:33  lr: 0.000030  loss: 0.1641  time: 1.4978  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2700/3000]  eta: 0:07:33  lr: 0.000030  loss: 0.5304  time: 1.4975  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2705/3000]  eta: 0:07:25  lr: 0.000030  loss: 0.5565  time: 1.4736  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2705/3000]  eta: 0:07:25  lr: 0.000030  loss: 0.3526  time: 1.4733  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2710/3000]  eta: 0:07:18  lr: 0.000030  loss: 0.1871  time: 1.4569  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2710/3000]  eta: 0:07:18  lr: 0.000030  loss: 0.6264  time: 1.4567  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2715/3000]  eta: 0:07:10  lr: 0.000030  loss: 0.2346  time: 1.4641  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2715/3000]  eta: 0:07:10  lr: 0.000030  loss: 0.1793  time: 1.4638  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2720/3000]  eta: 0:07:02  lr: 0.000030  loss: 0.6014  time: 1.4778  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2720/3000]  eta: 0:07:02  lr: 0.000030  loss: 0.4797  time: 1.4776  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2725/3000]  eta: 0:06:55  lr: 0.000030  loss: 0.2606  time: 1.4910  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2725/3000]  eta: 0:06:55  lr: 0.000030  loss: 0.2484  time: 1.4908  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2730/3000]  eta: 0:06:47  lr: 0.000030  loss: 0.4559  time: 1.5263  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2730/3000]  eta: 0:06:47  lr: 0.000030  loss: 0.5899  time: 1.5261  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2735/3000]  eta: 0:06:40  lr: 0.000030  loss: 0.5493  time: 1.5395  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2735/3000]  eta: 0:06:40  lr: 0.000030  loss: 0.2855  time: 1.5392  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2740/3000]  eta: 0:06:32  lr: 0.000030  loss: 0.2314  time: 1.5118  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2740/3000]  eta: 0:06:32  lr: 0.000030  loss: 0.4151  time: 1.5116  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2745/3000]  eta: 0:06:25  lr: 0.000030  loss: 0.1647  time: 1.5324  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2745/3000]  eta: 0:06:25  lr: 0.000030  loss: 0.1809  time: 1.5321  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2750/3000]  eta: 0:06:17  lr: 0.000030  loss: 0.2723  time: 1.5008  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2750/3000]  eta: 0:06:17  lr: 0.000030  loss: 0.9739  time: 1.5005  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2755/3000]  eta: 0:06:10  lr: 0.000030  loss: 0.2082  time: 1.5031  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2755/3000]  eta: 0:06:10  lr: 0.000030  loss: 0.4188  time: 1.5029  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2760/3000]  eta: 0:06:02  lr: 0.000030  loss: 0.3754  time: 1.5156  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2760/3000]  eta: 0:06:02  lr: 0.000030  loss: 0.3226  time: 1.5153  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2765/3000]  eta: 0:05:54  lr: 0.000030  loss: 0.3772  time: 1.4893  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2765/3000]  eta: 0:05:54  lr: 0.000030  loss: 1.0409  time: 1.4891  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2770/3000]  eta: 0:05:47  lr: 0.000030  loss: 0.4995  time: 1.4943  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2770/3000]  eta: 0:05:47  lr: 0.000030  loss: 0.4353  time: 1.4941  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2775/3000]  eta: 0:05:39  lr: 0.000030  loss: 0.3059  time: 1.4792  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2775/3000]  eta: 0:05:39  lr: 0.000030  loss: 0.4839  time: 1.4790  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2780/3000]  eta: 0:05:32  lr: 0.000030  loss: 0.1255  time: 1.4845  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2780/3000]  eta: 0:05:32  lr: 0.000030  loss: 0.2163  time: 1.4843  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2785/3000]  eta: 0:05:24  lr: 0.000030  loss: 0.4157  time: 1.5009  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2785/3000]  eta: 0:05:24  lr: 0.000030  loss: 0.8279  time: 1.5007  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2790/3000]  eta: 0:05:17  lr: 0.000030  loss: 0.3308  time: 1.5123  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2790/3000]  eta: 0:05:17  lr: 0.000030  loss: 0.5217  time: 1.5121  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2795/3000]  eta: 0:05:09  lr: 0.000030  loss: 0.2780  time: 1.5095  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2795/3000]  eta: 0:05:09  lr: 0.000030  loss: 1.0266  time: 1.5092  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2800/3000]  eta: 0:05:02  lr: 0.000030  loss: 0.1212  time: 1.5174  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2800/3000]  eta: 0:05:02  lr: 0.000030  loss: 0.5953  time: 1.5173  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2805/3000]  eta: 0:04:54  lr: 0.000030  loss: 0.2254  time: 1.5188  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2805/3000]  eta: 0:04:54  lr: 0.000030  loss: 1.1189  time: 1.5186  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2810/3000]  eta: 0:04:47  lr: 0.000030  loss: 0.3597  time: 1.5342  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2810/3000]  eta: 0:04:46  lr: 0.000030  loss: 0.3819  time: 1.5339  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2815/3000]  eta: 0:04:39  lr: 0.000030  loss: 0.3417  time: 1.5603  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2815/3000]  eta: 0:04:39  lr: 0.000030  loss: 0.1280  time: 1.5600  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2820/3000]  eta: 0:04:31  lr: 0.000030  loss: 0.1720  time: 1.5654  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2820/3000]  eta: 0:04:31  lr: 0.000030  loss: 0.0442  time: 1.5651  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2825/3000]  eta: 0:04:24  lr: 0.000030  loss: 0.3513  time: 1.5570  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2825/3000]  eta: 0:04:24  lr: 0.000030  loss: 0.1433  time: 1.5566  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2830/3000]  eta: 0:04:16  lr: 0.000030  loss: 0.4205  time: 1.5560  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2830/3000]  eta: 0:04:16  lr: 0.000030  loss: 0.7719  time: 1.5557  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2835/3000]  eta: 0:04:09  lr: 0.000030  loss: 0.1843  time: 1.5397  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2835/3000]  eta: 0:04:09  lr: 0.000030  loss: 0.9521  time: 1.5395  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2840/3000]  eta: 0:04:01  lr: 0.000030  loss: 0.6069  time: 1.5268  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2840/3000]  eta: 0:04:01  lr: 0.000030  loss: 0.2757  time: 1.5266  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2845/3000]  eta: 0:03:54  lr: 0.000030  loss: 0.4932  time: 1.5295  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2845/3000]  eta: 0:03:54  lr: 0.000030  loss: 0.4710  time: 1.5293  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2850/3000]  eta: 0:03:46  lr: 0.000030  loss: 0.1351  time: 1.5080  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2850/3000]  eta: 0:03:46  lr: 0.000030  loss: 0.5418  time: 1.5077  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2855/3000]  eta: 0:03:39  lr: 0.000030  loss: 0.5749  time: 1.5194  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2855/3000]  eta: 0:03:39  lr: 0.000030  loss: 0.2398  time: 1.5190  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2860/3000]  eta: 0:03:31  lr: 0.000030  loss: 0.2149  time: 1.5313  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2860/3000]  eta: 0:03:31  lr: 0.000030  loss: 0.6867  time: 1.5308  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2865/3000]  eta: 0:03:24  lr: 0.000030  loss: 0.5443  time: 1.5418  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2865/3000]  eta: 0:03:23  lr: 0.000030  loss: 0.1784  time: 1.5415  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2870/3000]  eta: 0:03:16  lr: 0.000030  loss: 0.9272  time: 1.5413  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2870/3000]  eta: 0:03:16  lr: 0.000030  loss: 0.1997  time: 1.5411  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2875/3000]  eta: 0:03:08  lr: 0.000030  loss: 0.2945  time: 1.5303  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2875/3000]  eta: 0:03:08  lr: 0.000030  loss: 0.7880  time: 1.5301  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2880/3000]  eta: 0:03:01  lr: 0.000030  loss: 0.2012  time: 1.5118  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2880/3000]  eta: 0:03:01  lr: 0.000030  loss: 0.5392  time: 1.5116  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2885/3000]  eta: 0:02:53  lr: 0.000030  loss: 0.3947  time: 1.4727  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2885/3000]  eta: 0:02:53  lr: 0.000030  loss: 0.4076  time: 1.4724  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2890/3000]  eta: 0:02:46  lr: 0.000030  loss: 0.5622  time: 1.4692  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2890/3000]  eta: 0:02:46  lr: 0.000030  loss: 0.4736  time: 1.4690  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2895/3000]  eta: 0:02:38  lr: 0.000030  loss: 0.0438  time: 1.4600  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2895/3000]  eta: 0:02:38  lr: 0.000030  loss: 0.3631  time: 1.4598  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2900/3000]  eta: 0:02:31  lr: 0.000030  loss: 0.4728  time: 1.4527  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2900/3000]  eta: 0:02:31  lr: 0.000030  loss: 0.0879  time: 1.4524  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2905/3000]  eta: 0:02:23  lr: 0.000030  loss: 0.2076  time: 1.4751  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2905/3000]  eta: 0:02:23  lr: 0.000030  loss: 0.1291  time: 1.4748  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2910/3000]  eta: 0:02:15  lr: 0.000030  loss: 0.4287  time: 1.4864  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2910/3000]  eta: 0:02:15  lr: 0.000030  loss: 0.1955  time: 1.4860  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2915/3000]  eta: 0:02:08  lr: 0.000030  loss: 0.4218  time: 1.5135  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2915/3000]  eta: 0:02:08  lr: 0.000030  loss: 0.3101  time: 1.5132  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2920/3000]  eta: 0:02:00  lr: 0.000030  loss: 0.4457  time: 1.5478  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2920/3000]  eta: 0:02:00  lr: 0.000030  loss: 0.3026  time: 1.5475  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2925/3000]  eta: 0:01:53  lr: 0.000030  loss: 0.5029  time: 1.5534  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2925/3000]  eta: 0:01:53  lr: 0.000030  loss: 0.2280  time: 1.5532  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2930/3000]  eta: 0:01:45  lr: 0.000030  loss: 0.5181  time: 1.5542  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2930/3000]  eta: 0:01:45  lr: 0.000030  loss: 0.1266  time: 1.5539  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2935/3000]  eta: 0:01:38  lr: 0.000030  loss: 0.3206  time: 1.5414  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2935/3000]  eta: 0:01:38  lr: 0.000030  loss: 0.5100  time: 1.5412  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2940/3000]  eta: 0:01:30  lr: 0.000030  loss: 0.1690  time: 1.5331  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2940/3000]  eta: 0:01:30  lr: 0.000030  loss: 0.1584  time: 1.5329  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2945/3000]  eta: 0:01:23  lr: 0.000030  loss: 0.2044  time: 1.5257  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2945/3000]  eta: 0:01:23  lr: 0.000030  loss: 0.3298  time: 1.5255  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2950/3000]  eta: 0:01:15  lr: 0.000030  loss: 0.7555  time: 1.5296  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2950/3000]  eta: 0:01:15  lr: 0.000030  loss: 0.3401  time: 1.5294  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2955/3000]  eta: 0:01:08  lr: 0.000030  loss: 0.4025  time: 1.5174  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2955/3000]  eta: 0:01:07  lr: 0.000030  loss: 0.6950  time: 1.5171  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2960/3000]  eta: 0:01:00  lr: 0.000030  loss: 0.3311  time: 1.5186  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2960/3000]  eta: 0:01:00  lr: 0.000030  loss: 0.5186  time: 1.5183  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2965/3000]  eta: 0:00:52  lr: 0.000030  loss: 0.7504  time: 1.5181  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2965/3000]  eta: 0:00:52  lr: 0.000030  loss: 0.3325  time: 1.5178  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2970/3000]  eta: 0:00:45  lr: 0.000030  loss: 0.5024  time: 1.5087  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2970/3000]  eta: 0:00:45  lr: 0.000030  loss: 0.9033  time: 1.5084  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2975/3000]  eta: 0:00:37  lr: 0.000030  loss: 0.1785  time: 1.4981  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2975/3000]  eta: 0:00:37  lr: 0.000030  loss: 0.2961  time: 1.4979  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2980/3000]  eta: 0:00:30  lr: 0.000030  loss: 0.2593  time: 1.4863  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2980/3000]  eta: 0:00:30  lr: 0.000030  loss: 0.3065  time: 1.4861  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2985/3000]  eta: 0:00:22  lr: 0.000030  loss: 0.0886  time: 1.4897  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2985/3000]  eta: 0:00:22  lr: 0.000030  loss: 0.4515  time: 1.4895  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2990/3000]  eta: 0:00:15  lr: 0.000030  loss: 0.2506  time: 1.4916  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2990/3000]  eta: 0:00:15  lr: 0.000030  loss: 0.4337  time: 1.4913  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2995/3000]  eta: 0:00:07  lr: 0.000030  loss: 0.5043  time: 1.4949  data: 0.0000  max mem: 18432
Train: data epoch: [2]  [2995/3000]  eta: 0:00:07  lr: 0.000030  loss: 0.2954  time: 1.4947  data: 0.0000  max mem: 18151
Train: data epoch: [2]  [2999/3000]  eta: 0:00:01  lr: 0.000030  loss: 0.1705  time: 1.4923  data: 0.0000  max mem: 18432
Train: data epoch: [2] Total time: 1:15:33 (1.5111 s / it)
Train: data epoch: [2]  [2999/3000]  eta: 0:00:01  lr: 0.000030  loss: 0.3849  time: 1.4920  data: 0.0000  max mem: 18151
Train: data epoch: [2] Total time: 1:15:33 (1.5111 s / it)
2025-01-19 03:27:43,139 [INFO] Averaged stats: lr: 0.0000  loss: 0.4307
2025-01-19 03:27:43,145 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [2]  [0/1]  eta: 0:00:00    time: 0.7955  data: 0.4989  max mem: 18151
Eval: data epoch: [2] Total time: 0:00:00 (0.9074 s / it)
Eval: data epoch: [2]  [0/1]  eta: 0:00:00    time: 0.9266  data: 0.6395  max mem: 18432
Eval: data epoch: [2] Total time: 0:00:01 (1.0712 s / it)
2025-01-19 03:27:44,242 [INFO] Saving checkpoint at epoch 2 to outputs_stage1_only/202501182338/checkpoint_2.pth.
2025-01-19 03:27:46,621 [INFO] Training Phase
2025-01-19 03:27:46,630 [INFO] Start training epoch 3, 3000 iters per inner epoch.
Train: data epoch: [3]  [   0/3000]  eta: 1:08:35  lr: 0.000030  loss: 0.7022  time: 1.3719  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [   0/3000]  eta: 1:08:34  lr: 0.000030  loss: 0.1318  time: 1.3715  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [   5/3000]  eta: 1:17:58  lr: 0.000030  loss: 0.4844  time: 1.5621  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [   5/3000]  eta: 1:17:57  lr: 0.000030  loss: 0.6168  time: 1.5616  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [  10/3000]  eta: 1:16:28  lr: 0.000030  loss: 0.2929  time: 1.5346  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [  10/3000]  eta: 1:16:27  lr: 0.000030  loss: 0.4716  time: 1.5343  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [  15/3000]  eta: 1:16:01  lr: 0.000030  loss: 0.2137  time: 1.5282  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [  15/3000]  eta: 1:16:00  lr: 0.000030  loss: 0.5810  time: 1.5279  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [  20/3000]  eta: 1:14:41  lr: 0.000030  loss: 0.3921  time: 1.5104  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [  20/3000]  eta: 1:14:40  lr: 0.000030  loss: 0.2931  time: 1.5102  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [  25/3000]  eta: 1:14:25  lr: 0.000030  loss: 0.0901  time: 1.4825  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [  25/3000]  eta: 1:14:24  lr: 0.000030  loss: 0.6289  time: 1.4823  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [  30/3000]  eta: 1:13:51  lr: 0.000030  loss: 0.4826  time: 1.4687  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [  30/3000]  eta: 1:13:50  lr: 0.000030  loss: 0.2831  time: 1.4684  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [  35/3000]  eta: 1:13:08  lr: 0.000030  loss: 0.3324  time: 1.4413  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [  35/3000]  eta: 1:13:07  lr: 0.000030  loss: 0.3031  time: 1.4410  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [  40/3000]  eta: 1:13:08  lr: 0.000030  loss: 0.2179  time: 1.4600  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [  40/3000]  eta: 1:13:07  lr: 0.000030  loss: 0.8076  time: 1.4598  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [  45/3000]  eta: 1:12:51  lr: 0.000030  loss: 0.1738  time: 1.4517  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [  45/3000]  eta: 1:12:51  lr: 0.000030  loss: 0.2203  time: 1.4515  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [  50/3000]  eta: 1:12:31  lr: 0.000030  loss: 0.1372  time: 1.4490  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [  50/3000]  eta: 1:12:30  lr: 0.000030  loss: 0.3224  time: 1.4487  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [  55/3000]  eta: 1:12:16  lr: 0.000030  loss: 0.1271  time: 1.4591  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [  55/3000]  eta: 1:12:15  lr: 0.000030  loss: 0.2843  time: 1.4588  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [  60/3000]  eta: 1:12:15  lr: 0.000030  loss: 0.3089  time: 1.4585  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [  60/3000]  eta: 1:12:14  lr: 0.000030  loss: 0.2412  time: 1.4582  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [  65/3000]  eta: 1:12:20  lr: 0.000030  loss: 0.4299  time: 1.4776  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [  65/3000]  eta: 1:12:19  lr: 0.000030  loss: 0.7646  time: 1.4772  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [  70/3000]  eta: 1:12:06  lr: 0.000030  loss: 0.0758  time: 1.4800  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [  70/3000]  eta: 1:12:05  lr: 0.000030  loss: 0.1723  time: 1.4799  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [  75/3000]  eta: 1:12:08  lr: 0.000030  loss: 0.3069  time: 1.5004  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [  75/3000]  eta: 1:12:07  lr: 0.000030  loss: 0.7647  time: 1.5002  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [  80/3000]  eta: 1:12:09  lr: 0.000030  loss: 0.5036  time: 1.5076  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [  80/3000]  eta: 1:12:08  lr: 0.000030  loss: 0.2306  time: 1.5073  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [  85/3000]  eta: 1:12:07  lr: 0.000030  loss: 0.2593  time: 1.5028  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [  85/3000]  eta: 1:12:06  lr: 0.000030  loss: 0.8278  time: 1.5026  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [  90/3000]  eta: 1:11:57  lr: 0.000030  loss: 0.4492  time: 1.5088  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [  90/3000]  eta: 1:11:56  lr: 0.000030  loss: 0.3420  time: 1.5086  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [  95/3000]  eta: 1:12:03  lr: 0.000030  loss: 0.2260  time: 1.5207  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [  95/3000]  eta: 1:12:02  lr: 0.000030  loss: 0.5669  time: 1.5204  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 100/3000]  eta: 1:12:04  lr: 0.000029  loss: 0.2755  time: 1.5260  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 100/3000]  eta: 1:12:04  lr: 0.000029  loss: 0.7000  time: 1.5258  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 105/3000]  eta: 1:11:56  lr: 0.000029  loss: 0.4762  time: 1.5185  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 105/3000]  eta: 1:11:55  lr: 0.000029  loss: 0.5981  time: 1.5182  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 110/3000]  eta: 1:11:51  lr: 0.000029  loss: 0.2245  time: 1.5288  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 110/3000]  eta: 1:11:50  lr: 0.000029  loss: 0.6205  time: 1.5286  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 115/3000]  eta: 1:11:38  lr: 0.000029  loss: 0.6346  time: 1.4971  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 115/3000]  eta: 1:11:37  lr: 0.000029  loss: 0.5425  time: 1.4968  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 120/3000]  eta: 1:11:37  lr: 0.000029  loss: 0.0933  time: 1.4963  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 120/3000]  eta: 1:11:36  lr: 0.000029  loss: 0.2404  time: 1.4961  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 125/3000]  eta: 1:11:35  lr: 0.000029  loss: 0.5483  time: 1.5111  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 125/3000]  eta: 1:11:34  lr: 0.000029  loss: 0.8040  time: 1.5109  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 130/3000]  eta: 1:11:28  lr: 0.000029  loss: 0.1997  time: 1.5072  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 130/3000]  eta: 1:11:27  lr: 0.000029  loss: 0.1667  time: 1.5069  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 135/3000]  eta: 1:11:19  lr: 0.000029  loss: 0.4208  time: 1.5170  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 135/3000]  eta: 1:11:19  lr: 0.000029  loss: 0.0646  time: 1.5168  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 140/3000]  eta: 1:11:09  lr: 0.000029  loss: 0.3786  time: 1.4963  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 140/3000]  eta: 1:11:08  lr: 0.000029  loss: 0.1309  time: 1.4961  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 145/3000]  eta: 1:11:10  lr: 0.000029  loss: 0.4280  time: 1.5075  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 145/3000]  eta: 1:11:10  lr: 0.000029  loss: 0.5265  time: 1.5072  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 150/3000]  eta: 1:11:05  lr: 0.000029  loss: 0.3097  time: 1.5134  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 150/3000]  eta: 1:11:04  lr: 0.000029  loss: 0.1630  time: 1.5132  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 155/3000]  eta: 1:11:02  lr: 0.000029  loss: 0.0888  time: 1.5286  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 155/3000]  eta: 1:11:01  lr: 0.000029  loss: 0.3559  time: 1.5283  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 160/3000]  eta: 1:10:58  lr: 0.000029  loss: 0.7384  time: 1.5464  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 160/3000]  eta: 1:10:57  lr: 0.000029  loss: 0.2370  time: 1.5461  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 165/3000]  eta: 1:10:55  lr: 0.000029  loss: 0.5305  time: 1.5391  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 165/3000]  eta: 1:10:54  lr: 0.000029  loss: 0.6642  time: 1.5389  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 170/3000]  eta: 1:10:45  lr: 0.000029  loss: 0.3219  time: 1.5270  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 170/3000]  eta: 1:10:44  lr: 0.000029  loss: 0.2315  time: 1.5268  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 175/3000]  eta: 1:10:32  lr: 0.000029  loss: 0.6607  time: 1.4979  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 175/3000]  eta: 1:10:31  lr: 0.000029  loss: 0.1700  time: 1.4978  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 180/3000]  eta: 1:10:27  lr: 0.000029  loss: 0.9008  time: 1.4966  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 180/3000]  eta: 1:10:26  lr: 0.000029  loss: 0.3254  time: 1.4963  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 185/3000]  eta: 1:10:26  lr: 0.000029  loss: 0.1433  time: 1.5048  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 185/3000]  eta: 1:10:26  lr: 0.000029  loss: 0.6524  time: 1.5046  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 190/3000]  eta: 1:10:14  lr: 0.000029  loss: 0.1629  time: 1.4945  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 190/3000]  eta: 1:10:13  lr: 0.000029  loss: 0.2011  time: 1.4943  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 195/3000]  eta: 1:10:07  lr: 0.000029  loss: 0.4703  time: 1.5164  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 195/3000]  eta: 1:10:07  lr: 0.000029  loss: 0.8738  time: 1.5162  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 200/3000]  eta: 1:10:02  lr: 0.000029  loss: 0.7608  time: 1.5177  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 200/3000]  eta: 1:10:01  lr: 0.000029  loss: 0.6983  time: 1.5175  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 205/3000]  eta: 1:09:52  lr: 0.000029  loss: 0.3456  time: 1.4853  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 205/3000]  eta: 1:09:51  lr: 0.000029  loss: 0.4064  time: 1.4850  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 210/3000]  eta: 1:09:49  lr: 0.000029  loss: 0.2915  time: 1.5211  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 210/3000]  eta: 1:09:48  lr: 0.000029  loss: 0.4046  time: 1.5208  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 215/3000]  eta: 1:09:44  lr: 0.000029  loss: 0.1876  time: 1.5263  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 215/3000]  eta: 1:09:43  lr: 0.000029  loss: 0.0811  time: 1.5261  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 220/3000]  eta: 1:09:35  lr: 0.000029  loss: 0.8104  time: 1.5131  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 220/3000]  eta: 1:09:34  lr: 0.000029  loss: 0.1660  time: 1.5128  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 225/3000]  eta: 1:09:29  lr: 0.000029  loss: 0.2327  time: 1.5300  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 225/3000]  eta: 1:09:29  lr: 0.000029  loss: 0.3096  time: 1.5298  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 230/3000]  eta: 1:09:26  lr: 0.000029  loss: 0.3429  time: 1.5286  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 230/3000]  eta: 1:09:25  lr: 0.000029  loss: 0.2678  time: 1.5284  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 235/3000]  eta: 1:09:19  lr: 0.000029  loss: 0.0755  time: 1.5243  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 235/3000]  eta: 1:09:18  lr: 0.000029  loss: 0.5105  time: 1.5241  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 240/3000]  eta: 1:09:12  lr: 0.000029  loss: 0.2759  time: 1.5309  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 240/3000]  eta: 1:09:11  lr: 0.000029  loss: 0.9670  time: 1.5307  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 245/3000]  eta: 1:09:04  lr: 0.000029  loss: 0.2548  time: 1.5225  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 245/3000]  eta: 1:09:03  lr: 0.000029  loss: 0.4902  time: 1.5223  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 250/3000]  eta: 1:08:56  lr: 0.000029  loss: 0.1606  time: 1.5049  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 250/3000]  eta: 1:08:55  lr: 0.000029  loss: 0.4869  time: 1.5047  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 255/3000]  eta: 1:08:52  lr: 0.000029  loss: 0.6432  time: 1.5196  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 255/3000]  eta: 1:08:52  lr: 0.000029  loss: 0.3940  time: 1.5194  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 260/3000]  eta: 1:08:45  lr: 0.000029  loss: 0.4265  time: 1.5213  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 260/3000]  eta: 1:08:45  lr: 0.000029  loss: 0.3624  time: 1.5211  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 265/3000]  eta: 1:08:37  lr: 0.000029  loss: 0.7013  time: 1.5189  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 265/3000]  eta: 1:08:36  lr: 0.000029  loss: 0.2878  time: 1.5187  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 270/3000]  eta: 1:08:30  lr: 0.000029  loss: 0.1789  time: 1.5255  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 270/3000]  eta: 1:08:29  lr: 0.000029  loss: 0.3121  time: 1.5253  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 275/3000]  eta: 1:08:22  lr: 0.000029  loss: 0.9718  time: 1.5066  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 275/3000]  eta: 1:08:22  lr: 0.000029  loss: 1.3192  time: 1.5064  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 280/3000]  eta: 1:08:13  lr: 0.000029  loss: 0.2090  time: 1.4962  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 280/3000]  eta: 1:08:13  lr: 0.000029  loss: 0.3938  time: 1.4960  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 285/3000]  eta: 1:08:08  lr: 0.000029  loss: 0.4946  time: 1.5130  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 285/3000]  eta: 1:08:07  lr: 0.000029  loss: 0.2524  time: 1.5127  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 290/3000]  eta: 1:08:02  lr: 0.000029  loss: 0.3310  time: 1.5162  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 290/3000]  eta: 1:08:01  lr: 0.000029  loss: 1.0625  time: 1.5160  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 295/3000]  eta: 1:07:52  lr: 0.000029  loss: 0.4894  time: 1.5043  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 295/3000]  eta: 1:07:51  lr: 0.000029  loss: 0.7465  time: 1.5041  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 300/3000]  eta: 1:07:41  lr: 0.000029  loss: 0.7467  time: 1.4936  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 300/3000]  eta: 1:07:40  lr: 0.000029  loss: 0.5955  time: 1.4933  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 305/3000]  eta: 1:07:35  lr: 0.000029  loss: 0.6126  time: 1.4899  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 305/3000]  eta: 1:07:34  lr: 0.000029  loss: 0.3910  time: 1.4897  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 310/3000]  eta: 1:07:26  lr: 0.000029  loss: 1.3612  time: 1.4756  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 310/3000]  eta: 1:07:26  lr: 0.000029  loss: 0.6618  time: 1.4753  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 315/3000]  eta: 1:07:20  lr: 0.000029  loss: 0.9064  time: 1.4915  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 315/3000]  eta: 1:07:19  lr: 0.000029  loss: 0.2163  time: 1.4912  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 320/3000]  eta: 1:07:15  lr: 0.000029  loss: 0.3566  time: 1.5264  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 320/3000]  eta: 1:07:14  lr: 0.000029  loss: 0.1336  time: 1.5262  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 325/3000]  eta: 1:07:07  lr: 0.000029  loss: 0.6638  time: 1.5179  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 325/3000]  eta: 1:07:06  lr: 0.000029  loss: 0.1708  time: 1.5176  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 330/3000]  eta: 1:06:59  lr: 0.000029  loss: 0.7846  time: 1.5187  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 330/3000]  eta: 1:06:58  lr: 0.000029  loss: 0.4185  time: 1.5185  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 335/3000]  eta: 1:06:54  lr: 0.000029  loss: 0.3250  time: 1.5353  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 335/3000]  eta: 1:06:54  lr: 0.000029  loss: 0.4931  time: 1.5350  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 340/3000]  eta: 1:06:47  lr: 0.000029  loss: 0.6123  time: 1.5212  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 340/3000]  eta: 1:06:46  lr: 0.000029  loss: 0.2351  time: 1.5209  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 345/3000]  eta: 1:06:39  lr: 0.000029  loss: 0.3925  time: 1.5211  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 345/3000]  eta: 1:06:39  lr: 0.000029  loss: 0.0881  time: 1.5208  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 350/3000]  eta: 1:06:35  lr: 0.000029  loss: 0.6715  time: 1.5476  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 350/3000]  eta: 1:06:34  lr: 0.000029  loss: 1.1177  time: 1.5473  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 355/3000]  eta: 1:06:23  lr: 0.000029  loss: 0.1578  time: 1.5013  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 355/3000]  eta: 1:06:23  lr: 0.000029  loss: 0.0982  time: 1.5010  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 360/3000]  eta: 1:06:18  lr: 0.000029  loss: 0.3988  time: 1.5130  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 360/3000]  eta: 1:06:17  lr: 0.000029  loss: 0.4313  time: 1.5127  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 365/3000]  eta: 1:06:11  lr: 0.000029  loss: 0.5270  time: 1.5190  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 365/3000]  eta: 1:06:10  lr: 0.000029  loss: 0.6972  time: 1.5188  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 370/3000]  eta: 1:06:05  lr: 0.000029  loss: 0.3130  time: 1.5116  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 370/3000]  eta: 1:06:05  lr: 0.000029  loss: 0.5160  time: 1.5114  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 375/3000]  eta: 1:05:57  lr: 0.000029  loss: 0.4439  time: 1.5334  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 375/3000]  eta: 1:05:56  lr: 0.000029  loss: 0.1547  time: 1.5331  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 380/3000]  eta: 1:05:49  lr: 0.000029  loss: 0.3794  time: 1.5175  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 380/3000]  eta: 1:05:48  lr: 0.000029  loss: 0.2738  time: 1.5173  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 385/3000]  eta: 1:05:42  lr: 0.000029  loss: 0.0934  time: 1.5136  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 385/3000]  eta: 1:05:41  lr: 0.000029  loss: 0.6320  time: 1.5133  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 390/3000]  eta: 1:05:36  lr: 0.000029  loss: 0.7040  time: 1.5134  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 390/3000]  eta: 1:05:35  lr: 0.000029  loss: 0.6113  time: 1.5130  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 395/3000]  eta: 1:05:28  lr: 0.000029  loss: 0.4436  time: 1.5180  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 395/3000]  eta: 1:05:28  lr: 0.000029  loss: 0.2190  time: 1.5178  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 400/3000]  eta: 1:05:21  lr: 0.000029  loss: 0.5068  time: 1.5222  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 400/3000]  eta: 1:05:20  lr: 0.000029  loss: 0.8334  time: 1.5220  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 405/3000]  eta: 1:05:14  lr: 0.000029  loss: 0.9303  time: 1.5288  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 405/3000]  eta: 1:05:14  lr: 0.000029  loss: 0.4731  time: 1.5286  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 410/3000]  eta: 1:05:04  lr: 0.000029  loss: 0.4450  time: 1.4978  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 410/3000]  eta: 1:05:04  lr: 0.000029  loss: 0.1914  time: 1.4976  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 415/3000]  eta: 1:04:56  lr: 0.000029  loss: 0.1310  time: 1.4912  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 415/3000]  eta: 1:04:55  lr: 0.000029  loss: 0.2301  time: 1.4910  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 420/3000]  eta: 1:04:48  lr: 0.000029  loss: 0.9365  time: 1.4883  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 420/3000]  eta: 1:04:48  lr: 0.000029  loss: 0.5696  time: 1.4881  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 425/3000]  eta: 1:04:41  lr: 0.000029  loss: 0.2945  time: 1.4851  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 425/3000]  eta: 1:04:41  lr: 0.000029  loss: 0.8655  time: 1.4848  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 430/3000]  eta: 1:04:34  lr: 0.000029  loss: 0.1493  time: 1.5092  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 430/3000]  eta: 1:04:34  lr: 0.000029  loss: 0.6323  time: 1.5089  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 435/3000]  eta: 1:04:25  lr: 0.000029  loss: 0.6062  time: 1.4964  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 435/3000]  eta: 1:04:24  lr: 0.000029  loss: 0.3789  time: 1.4962  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 440/3000]  eta: 1:04:18  lr: 0.000029  loss: 0.3244  time: 1.5079  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 440/3000]  eta: 1:04:18  lr: 0.000029  loss: 0.5059  time: 1.5076  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 445/3000]  eta: 1:04:10  lr: 0.000029  loss: 0.6509  time: 1.5006  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 445/3000]  eta: 1:04:10  lr: 0.000029  loss: 0.1506  time: 1.5003  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 450/3000]  eta: 1:04:00  lr: 0.000029  loss: 0.9780  time: 1.4715  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 450/3000]  eta: 1:04:00  lr: 0.000029  loss: 0.5647  time: 1.4712  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 455/3000]  eta: 1:03:54  lr: 0.000029  loss: 0.4511  time: 1.5041  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 455/3000]  eta: 1:03:54  lr: 0.000029  loss: 0.7369  time: 1.5038  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 460/3000]  eta: 1:03:46  lr: 0.000029  loss: 0.1630  time: 1.4882  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 460/3000]  eta: 1:03:45  lr: 0.000029  loss: 0.5077  time: 1.4879  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 465/3000]  eta: 1:03:38  lr: 0.000029  loss: 0.4223  time: 1.4861  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 465/3000]  eta: 1:03:37  lr: 0.000029  loss: 0.5814  time: 1.4858  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 470/3000]  eta: 1:03:29  lr: 0.000029  loss: 0.3395  time: 1.4931  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 470/3000]  eta: 1:03:28  lr: 0.000029  loss: 0.0719  time: 1.4929  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 475/3000]  eta: 1:03:21  lr: 0.000029  loss: 0.1811  time: 1.4832  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 475/3000]  eta: 1:03:21  lr: 0.000029  loss: 0.4747  time: 1.4829  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 480/3000]  eta: 1:03:14  lr: 0.000029  loss: 0.2986  time: 1.4883  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 480/3000]  eta: 1:03:13  lr: 0.000029  loss: 0.2339  time: 1.4880  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 485/3000]  eta: 1:03:07  lr: 0.000029  loss: 0.8290  time: 1.4984  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 485/3000]  eta: 1:03:06  lr: 0.000029  loss: 0.1270  time: 1.4980  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 490/3000]  eta: 1:02:59  lr: 0.000029  loss: 0.5887  time: 1.5090  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 490/3000]  eta: 1:02:58  lr: 0.000029  loss: 0.1446  time: 1.5086  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 495/3000]  eta: 1:02:52  lr: 0.000029  loss: 0.8545  time: 1.5145  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 495/3000]  eta: 1:02:52  lr: 0.000029  loss: 0.6306  time: 1.5142  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 500/3000]  eta: 1:02:46  lr: 0.000029  loss: 0.2394  time: 1.5270  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 500/3000]  eta: 1:02:45  lr: 0.000029  loss: 0.1181  time: 1.5267  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 505/3000]  eta: 1:02:40  lr: 0.000029  loss: 0.3632  time: 1.5367  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 505/3000]  eta: 1:02:39  lr: 0.000029  loss: 0.9566  time: 1.5365  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 510/3000]  eta: 1:02:31  lr: 0.000029  loss: 0.2911  time: 1.5310  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 510/3000]  eta: 1:02:31  lr: 0.000029  loss: 0.2313  time: 1.5308  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 515/3000]  eta: 1:02:23  lr: 0.000029  loss: 0.2874  time: 1.5186  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 515/3000]  eta: 1:02:23  lr: 0.000029  loss: 0.3263  time: 1.5184  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 520/3000]  eta: 1:02:17  lr: 0.000029  loss: 0.6335  time: 1.5176  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 520/3000]  eta: 1:02:16  lr: 0.000029  loss: 0.4135  time: 1.5174  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 525/3000]  eta: 1:02:08  lr: 0.000029  loss: 0.2385  time: 1.4927  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 525/3000]  eta: 1:02:08  lr: 0.000029  loss: 0.6037  time: 1.4925  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 530/3000]  eta: 1:01:59  lr: 0.000029  loss: 0.2864  time: 1.4809  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 530/3000]  eta: 1:01:58  lr: 0.000029  loss: 0.6915  time: 1.4807  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 535/3000]  eta: 1:01:52  lr: 0.000029  loss: 0.4531  time: 1.4967  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 535/3000]  eta: 1:01:52  lr: 0.000029  loss: 0.3099  time: 1.4965  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 540/3000]  eta: 1:01:45  lr: 0.000029  loss: 0.4230  time: 1.4880  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 540/3000]  eta: 1:01:44  lr: 0.000029  loss: 0.3265  time: 1.4878  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 545/3000]  eta: 1:01:39  lr: 0.000029  loss: 0.2349  time: 1.5151  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 545/3000]  eta: 1:01:38  lr: 0.000029  loss: 0.3958  time: 1.5148  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 550/3000]  eta: 1:01:32  lr: 0.000029  loss: 0.4195  time: 1.5467  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 550/3000]  eta: 1:01:32  lr: 0.000029  loss: 1.2350  time: 1.5475  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 555/3000]  eta: 1:01:22  lr: 0.000029  loss: 0.0692  time: 1.5082  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 555/3000]  eta: 1:01:22  lr: 0.000029  loss: 0.2888  time: 1.5079  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 560/3000]  eta: 1:01:16  lr: 0.000029  loss: 0.8467  time: 1.5185  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 560/3000]  eta: 1:01:15  lr: 0.000029  loss: 0.5917  time: 1.5183  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 565/3000]  eta: 1:01:09  lr: 0.000029  loss: 0.5897  time: 1.5071  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 565/3000]  eta: 1:01:08  lr: 0.000029  loss: 0.2416  time: 1.5068  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 570/3000]  eta: 1:01:02  lr: 0.000029  loss: 1.2329  time: 1.5058  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 570/3000]  eta: 1:01:01  lr: 0.000029  loss: 0.2727  time: 1.5046  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 575/3000]  eta: 1:00:54  lr: 0.000029  loss: 0.2275  time: 1.5277  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 575/3000]  eta: 1:00:53  lr: 0.000029  loss: 0.5564  time: 1.5275  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 580/3000]  eta: 1:00:48  lr: 0.000029  loss: 0.6010  time: 1.5327  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 580/3000]  eta: 1:00:47  lr: 0.000029  loss: 1.0130  time: 1.5324  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 585/3000]  eta: 1:00:41  lr: 0.000029  loss: 0.2672  time: 1.5297  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 585/3000]  eta: 1:00:40  lr: 0.000029  loss: 0.3169  time: 1.5295  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 590/3000]  eta: 1:00:33  lr: 0.000029  loss: 0.5981  time: 1.5239  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 590/3000]  eta: 1:00:33  lr: 0.000029  loss: 0.4298  time: 1.5236  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 595/3000]  eta: 1:00:27  lr: 0.000029  loss: 0.2384  time: 1.5444  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 595/3000]  eta: 1:00:26  lr: 0.000029  loss: 0.6070  time: 1.5443  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 600/3000]  eta: 1:00:20  lr: 0.000029  loss: 0.2287  time: 1.5336  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 600/3000]  eta: 1:00:19  lr: 0.000029  loss: 0.1849  time: 1.5334  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 605/3000]  eta: 1:00:11  lr: 0.000029  loss: 0.4166  time: 1.5192  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 605/3000]  eta: 1:00:11  lr: 0.000029  loss: 0.2168  time: 1.5189  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 610/3000]  eta: 1:00:03  lr: 0.000029  loss: 0.6170  time: 1.5040  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 610/3000]  eta: 1:00:02  lr: 0.000029  loss: 0.2694  time: 1.5037  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 615/3000]  eta: 0:59:55  lr: 0.000029  loss: 0.1100  time: 1.4883  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 615/3000]  eta: 0:59:55  lr: 0.000029  loss: 0.1676  time: 1.4880  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 620/3000]  eta: 0:59:49  lr: 0.000029  loss: 1.0415  time: 1.5030  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 620/3000]  eta: 0:59:49  lr: 0.000029  loss: 0.4943  time: 1.5028  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 625/3000]  eta: 0:59:42  lr: 0.000029  loss: 0.1226  time: 1.5158  data: 0.0000  max mem: 18432Train: data epoch: [3]  [ 625/3000]  eta: 0:59:41  lr: 0.000029  loss: 0.6777  time: 1.5156  data: 0.0000  max mem: 18151

Train: data epoch: [3]  [ 630/3000]  eta: 0:59:34  lr: 0.000029  loss: 0.5241  time: 1.5319  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 630/3000]  eta: 0:59:34  lr: 0.000029  loss: 0.5149  time: 1.5318  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 635/3000]  eta: 0:59:26  lr: 0.000029  loss: 0.2601  time: 1.5267  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 635/3000]  eta: 0:59:26  lr: 0.000029  loss: 0.4837  time: 1.5264  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 640/3000]  eta: 0:59:19  lr: 0.000029  loss: 0.2382  time: 1.5050  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 640/3000]  eta: 0:59:18  lr: 0.000029  loss: 0.3152  time: 1.5047  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 645/3000]  eta: 0:59:11  lr: 0.000029  loss: 0.3887  time: 1.5008  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 645/3000]  eta: 0:59:10  lr: 0.000029  loss: 0.3906  time: 1.5006  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 650/3000]  eta: 0:59:04  lr: 0.000029  loss: 0.2978  time: 1.5026  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 650/3000]  eta: 0:59:03  lr: 0.000029  loss: 0.2939  time: 1.5024  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 655/3000]  eta: 0:58:57  lr: 0.000029  loss: 0.1322  time: 1.5119  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 655/3000]  eta: 0:58:56  lr: 0.000029  loss: 0.4186  time: 1.5117  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 660/3000]  eta: 0:58:48  lr: 0.000029  loss: 0.3101  time: 1.4948  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 660/3000]  eta: 0:58:47  lr: 0.000029  loss: 0.4549  time: 1.4946  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 665/3000]  eta: 0:58:41  lr: 0.000029  loss: 0.3055  time: 1.5144  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 665/3000]  eta: 0:58:41  lr: 0.000029  loss: 1.1333  time: 1.5143  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 670/3000]  eta: 0:58:34  lr: 0.000029  loss: 0.3190  time: 1.5150  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 670/3000]  eta: 0:58:34  lr: 0.000029  loss: 0.3006  time: 1.5147  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 675/3000]  eta: 0:58:27  lr: 0.000029  loss: 1.0775  time: 1.5236  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 675/3000]  eta: 0:58:27  lr: 0.000029  loss: 0.2472  time: 1.5233  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 680/3000]  eta: 0:58:19  lr: 0.000029  loss: 0.0548  time: 1.5289  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 680/3000]  eta: 0:58:18  lr: 0.000029  loss: 0.3189  time: 1.5286  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 685/3000]  eta: 0:58:11  lr: 0.000029  loss: 0.5343  time: 1.5100  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 685/3000]  eta: 0:58:11  lr: 0.000029  loss: 0.4063  time: 1.5097  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 690/3000]  eta: 0:58:04  lr: 0.000029  loss: 1.0192  time: 1.5090  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 690/3000]  eta: 0:58:03  lr: 0.000029  loss: 0.3057  time: 1.5088  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 695/3000]  eta: 0:57:56  lr: 0.000029  loss: 0.7885  time: 1.4960  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 695/3000]  eta: 0:57:56  lr: 0.000029  loss: 0.8102  time: 1.4958  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 700/3000]  eta: 0:57:48  lr: 0.000029  loss: 0.5788  time: 1.4997  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 700/3000]  eta: 0:57:48  lr: 0.000029  loss: 0.3826  time: 1.4995  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 705/3000]  eta: 0:57:40  lr: 0.000029  loss: 0.3346  time: 1.4860  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 705/3000]  eta: 0:57:39  lr: 0.000029  loss: 0.1910  time: 1.4857  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 710/3000]  eta: 0:57:32  lr: 0.000029  loss: 0.3411  time: 1.4735  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 710/3000]  eta: 0:57:31  lr: 0.000029  loss: 0.5101  time: 1.4733  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 715/3000]  eta: 0:57:23  lr: 0.000029  loss: 0.2864  time: 1.4640  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 715/3000]  eta: 0:57:23  lr: 0.000029  loss: 0.3210  time: 1.4637  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 720/3000]  eta: 0:57:16  lr: 0.000029  loss: 0.3767  time: 1.4817  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 720/3000]  eta: 0:57:16  lr: 0.000029  loss: 0.8293  time: 1.4815  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 725/3000]  eta: 0:57:07  lr: 0.000029  loss: 0.3555  time: 1.4814  data: 0.0000  max mem: 18151Train: data epoch: [3]  [ 725/3000]  eta: 0:57:08  lr: 0.000029  loss: 0.5650  time: 1.4817  data: 0.0000  max mem: 18432

Train: data epoch: [3]  [ 730/3000]  eta: 0:57:02  lr: 0.000029  loss: 0.2954  time: 1.5120  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 730/3000]  eta: 0:57:01  lr: 0.000029  loss: 0.2276  time: 1.5118  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 735/3000]  eta: 0:56:54  lr: 0.000029  loss: 0.6831  time: 1.5240  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 735/3000]  eta: 0:56:54  lr: 0.000029  loss: 0.3563  time: 1.5238  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 740/3000]  eta: 0:56:46  lr: 0.000029  loss: 0.3347  time: 1.5033  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 740/3000]  eta: 0:56:45  lr: 0.000029  loss: 0.3399  time: 1.5031  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 745/3000]  eta: 0:56:39  lr: 0.000029  loss: 0.1426  time: 1.5198  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 745/3000]  eta: 0:56:38  lr: 0.000029  loss: 0.6163  time: 1.5196  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 750/3000]  eta: 0:56:31  lr: 0.000029  loss: 0.2941  time: 1.4991  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 750/3000]  eta: 0:56:30  lr: 0.000029  loss: 0.2848  time: 1.4989  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 755/3000]  eta: 0:56:24  lr: 0.000029  loss: 0.4290  time: 1.5064  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 755/3000]  eta: 0:56:24  lr: 0.000029  loss: 0.2458  time: 1.5062  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 760/3000]  eta: 0:56:16  lr: 0.000029  loss: 0.6574  time: 1.5101  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 760/3000]  eta: 0:56:16  lr: 0.000029  loss: 0.1519  time: 1.5099  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 765/3000]  eta: 0:56:08  lr: 0.000029  loss: 0.7258  time: 1.5080  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 765/3000]  eta: 0:56:08  lr: 0.000029  loss: 0.3794  time: 1.5079  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 770/3000]  eta: 0:56:01  lr: 0.000029  loss: 0.5381  time: 1.5126  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 770/3000]  eta: 0:56:01  lr: 0.000029  loss: 0.3910  time: 1.5124  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 775/3000]  eta: 0:55:54  lr: 0.000029  loss: 0.4936  time: 1.5140  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 775/3000]  eta: 0:55:54  lr: 0.000029  loss: 1.0937  time: 1.5138  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 780/3000]  eta: 0:55:47  lr: 0.000029  loss: 0.3308  time: 1.5243  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 780/3000]  eta: 0:55:46  lr: 0.000029  loss: 0.3418  time: 1.5241  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 785/3000]  eta: 0:55:40  lr: 0.000029  loss: 0.4692  time: 1.5314  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 785/3000]  eta: 0:55:39  lr: 0.000029  loss: 0.7544  time: 1.5310  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 790/3000]  eta: 0:55:31  lr: 0.000029  loss: 0.4012  time: 1.5144  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 790/3000]  eta: 0:55:31  lr: 0.000029  loss: 0.1483  time: 1.5142  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 795/3000]  eta: 0:55:25  lr: 0.000029  loss: 0.9158  time: 1.5235  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 795/3000]  eta: 0:55:24  lr: 0.000029  loss: 0.1641  time: 1.5232  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 800/3000]  eta: 0:55:18  lr: 0.000029  loss: 0.8894  time: 1.5230  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 800/3000]  eta: 0:55:17  lr: 0.000029  loss: 0.3445  time: 1.5227  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 805/3000]  eta: 0:55:09  lr: 0.000029  loss: 0.1192  time: 1.4986  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 805/3000]  eta: 0:55:08  lr: 0.000029  loss: 0.3118  time: 1.4984  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 810/3000]  eta: 0:55:01  lr: 0.000029  loss: 0.4553  time: 1.4987  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 810/3000]  eta: 0:55:00  lr: 0.000029  loss: 0.2395  time: 1.4985  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 815/3000]  eta: 0:54:53  lr: 0.000029  loss: 0.2939  time: 1.4800  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 815/3000]  eta: 0:54:53  lr: 0.000029  loss: 0.8999  time: 1.4798  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 820/3000]  eta: 0:54:45  lr: 0.000029  loss: 0.4630  time: 1.4689  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 820/3000]  eta: 0:54:45  lr: 0.000029  loss: 0.4027  time: 1.4686  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 825/3000]  eta: 0:54:38  lr: 0.000029  loss: 0.6320  time: 1.4936  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 825/3000]  eta: 0:54:38  lr: 0.000029  loss: 0.6725  time: 1.4933  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 830/3000]  eta: 0:54:29  lr: 0.000029  loss: 0.4092  time: 1.4820  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 830/3000]  eta: 0:54:29  lr: 0.000029  loss: 0.3902  time: 1.4817  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 835/3000]  eta: 0:54:22  lr: 0.000029  loss: 0.6375  time: 1.4776  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 835/3000]  eta: 0:54:21  lr: 0.000029  loss: 0.8092  time: 1.4773  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 840/3000]  eta: 0:54:14  lr: 0.000029  loss: 0.8937  time: 1.4881  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 840/3000]  eta: 0:54:14  lr: 0.000029  loss: 0.3575  time: 1.4878  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 845/3000]  eta: 0:54:06  lr: 0.000029  loss: 0.1708  time: 1.4785  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 845/3000]  eta: 0:54:06  lr: 0.000029  loss: 0.2267  time: 1.4783  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 850/3000]  eta: 0:53:59  lr: 0.000029  loss: 0.4850  time: 1.5085  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 850/3000]  eta: 0:53:59  lr: 0.000029  loss: 0.2400  time: 1.5083  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 855/3000]  eta: 0:53:52  lr: 0.000029  loss: 0.3766  time: 1.5078  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 855/3000]  eta: 0:53:51  lr: 0.000029  loss: 0.1348  time: 1.5075  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 860/3000]  eta: 0:53:44  lr: 0.000029  loss: 0.4691  time: 1.4995  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 860/3000]  eta: 0:53:43  lr: 0.000029  loss: 0.3990  time: 1.4992  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 865/3000]  eta: 0:53:36  lr: 0.000029  loss: 0.6854  time: 1.4948  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 865/3000]  eta: 0:53:35  lr: 0.000029  loss: 0.1763  time: 1.4945  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 870/3000]  eta: 0:53:29  lr: 0.000029  loss: 0.4561  time: 1.5050  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 870/3000]  eta: 0:53:29  lr: 0.000029  loss: 0.3807  time: 1.5047  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 875/3000]  eta: 0:53:22  lr: 0.000029  loss: 1.1629  time: 1.5281  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 875/3000]  eta: 0:53:22  lr: 0.000029  loss: 0.1909  time: 1.5280  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 880/3000]  eta: 0:53:15  lr: 0.000029  loss: 1.1631  time: 1.5393  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 880/3000]  eta: 0:53:15  lr: 0.000029  loss: 0.5845  time: 1.5396  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 885/3000]  eta: 0:53:07  lr: 0.000029  loss: 0.1506  time: 1.5368  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 885/3000]  eta: 0:53:07  lr: 0.000029  loss: 0.2991  time: 1.5366  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 890/3000]  eta: 0:53:00  lr: 0.000029  loss: 0.1468  time: 1.5341  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 890/3000]  eta: 0:53:00  lr: 0.000029  loss: 0.4388  time: 1.5339  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 895/3000]  eta: 0:52:53  lr: 0.000029  loss: 0.7232  time: 1.5208  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 895/3000]  eta: 0:52:52  lr: 0.000029  loss: 0.5005  time: 1.5206  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 900/3000]  eta: 0:52:45  lr: 0.000029  loss: 0.1635  time: 1.5046  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 900/3000]  eta: 0:52:44  lr: 0.000029  loss: 0.2657  time: 1.5045  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 905/3000]  eta: 0:52:38  lr: 0.000029  loss: 0.7137  time: 1.5319  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 905/3000]  eta: 0:52:38  lr: 0.000029  loss: 0.1771  time: 1.5316  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 910/3000]  eta: 0:52:30  lr: 0.000029  loss: 0.2636  time: 1.5152  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 910/3000]  eta: 0:52:30  lr: 0.000029  loss: 0.3130  time: 1.5149  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 915/3000]  eta: 0:52:23  lr: 0.000029  loss: 0.1980  time: 1.5164  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 915/3000]  eta: 0:52:23  lr: 0.000029  loss: 0.5301  time: 1.5162  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 920/3000]  eta: 0:52:15  lr: 0.000029  loss: 0.1716  time: 1.5211  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 920/3000]  eta: 0:52:15  lr: 0.000029  loss: 0.2510  time: 1.5208  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 925/3000]  eta: 0:52:08  lr: 0.000029  loss: 0.7622  time: 1.5133  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 925/3000]  eta: 0:52:08  lr: 0.000029  loss: 0.3429  time: 1.5131  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 930/3000]  eta: 0:52:01  lr: 0.000029  loss: 0.5339  time: 1.5283  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 930/3000]  eta: 0:52:01  lr: 0.000029  loss: 0.4891  time: 1.5281  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 935/3000]  eta: 0:51:54  lr: 0.000029  loss: 0.6512  time: 1.5405  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 935/3000]  eta: 0:51:54  lr: 0.000029  loss: 0.5749  time: 1.5402  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 940/3000]  eta: 0:51:47  lr: 0.000029  loss: 0.5058  time: 1.5399  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 940/3000]  eta: 0:51:46  lr: 0.000029  loss: 0.7169  time: 1.5396  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 945/3000]  eta: 0:51:40  lr: 0.000029  loss: 0.3214  time: 1.5475  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 945/3000]  eta: 0:51:39  lr: 0.000029  loss: 0.3095  time: 1.5472  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 950/3000]  eta: 0:51:32  lr: 0.000029  loss: 0.7460  time: 1.5368  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 950/3000]  eta: 0:51:32  lr: 0.000029  loss: 0.2166  time: 1.5366  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 955/3000]  eta: 0:51:24  lr: 0.000029  loss: 0.6953  time: 1.4961  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 955/3000]  eta: 0:51:23  lr: 0.000029  loss: 0.3725  time: 1.4959  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 960/3000]  eta: 0:51:16  lr: 0.000029  loss: 0.5331  time: 1.4933  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 960/3000]  eta: 0:51:15  lr: 0.000029  loss: 0.5229  time: 1.4921  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 965/3000]  eta: 0:51:07  lr: 0.000029  loss: 0.4781  time: 1.4541  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 965/3000]  eta: 0:51:07  lr: 0.000029  loss: 0.2352  time: 1.4530  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 970/3000]  eta: 0:51:00  lr: 0.000029  loss: 0.1618  time: 1.4703  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 970/3000]  eta: 0:51:00  lr: 0.000029  loss: 0.5668  time: 1.4692  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 975/3000]  eta: 0:50:54  lr: 0.000029  loss: 0.3302  time: 1.5106  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 975/3000]  eta: 0:50:53  lr: 0.000029  loss: 0.5192  time: 1.5094  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 980/3000]  eta: 0:50:46  lr: 0.000029  loss: 0.2909  time: 1.5180  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 980/3000]  eta: 0:50:46  lr: 0.000029  loss: 0.6873  time: 1.5177  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 985/3000]  eta: 0:50:38  lr: 0.000029  loss: 0.1025  time: 1.5263  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 985/3000]  eta: 0:50:37  lr: 0.000029  loss: 0.1837  time: 1.5261  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 990/3000]  eta: 0:50:31  lr: 0.000029  loss: 0.3363  time: 1.5177  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 990/3000]  eta: 0:50:30  lr: 0.000029  loss: 0.4521  time: 1.5175  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [ 995/3000]  eta: 0:50:23  lr: 0.000029  loss: 0.3096  time: 1.5039  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [ 995/3000]  eta: 0:50:23  lr: 0.000029  loss: 0.0908  time: 1.5036  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1000/3000]  eta: 0:50:16  lr: 0.000029  loss: 0.4673  time: 1.5185  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1000/3000]  eta: 0:50:16  lr: 0.000029  loss: 0.6047  time: 1.5183  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1005/3000]  eta: 0:50:09  lr: 0.000029  loss: 0.4770  time: 1.5528  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1005/3000]  eta: 0:50:09  lr: 0.000029  loss: 0.0973  time: 1.5526  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1010/3000]  eta: 0:50:02  lr: 0.000029  loss: 0.6779  time: 1.5496  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1010/3000]  eta: 0:50:02  lr: 0.000029  loss: 0.2704  time: 1.5494  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1015/3000]  eta: 0:49:55  lr: 0.000029  loss: 0.2840  time: 1.5593  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1015/3000]  eta: 0:49:55  lr: 0.000029  loss: 0.5564  time: 1.5591  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1020/3000]  eta: 0:49:48  lr: 0.000029  loss: 0.4811  time: 1.5569  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1020/3000]  eta: 0:49:47  lr: 0.000029  loss: 0.3723  time: 1.5566  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1025/3000]  eta: 0:49:41  lr: 0.000029  loss: 0.2551  time: 1.5473  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1025/3000]  eta: 0:49:40  lr: 0.000029  loss: 0.4434  time: 1.5471  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1030/3000]  eta: 0:49:33  lr: 0.000029  loss: 0.1670  time: 1.5279  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1030/3000]  eta: 0:49:32  lr: 0.000029  loss: 0.5492  time: 1.5277  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1035/3000]  eta: 0:49:25  lr: 0.000029  loss: 0.1784  time: 1.5172  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1035/3000]  eta: 0:49:25  lr: 0.000029  loss: 0.1169  time: 1.5169  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1040/3000]  eta: 0:49:18  lr: 0.000029  loss: 0.2809  time: 1.5176  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1040/3000]  eta: 0:49:18  lr: 0.000029  loss: 0.7534  time: 1.5174  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1045/3000]  eta: 0:49:11  lr: 0.000029  loss: 0.4193  time: 1.5171  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1045/3000]  eta: 0:49:10  lr: 0.000029  loss: 1.0078  time: 1.5169  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1050/3000]  eta: 0:49:04  lr: 0.000029  loss: 0.1845  time: 1.5435  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1050/3000]  eta: 0:49:03  lr: 0.000029  loss: 0.1522  time: 1.5432  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1055/3000]  eta: 0:48:56  lr: 0.000029  loss: 0.3508  time: 1.5403  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1055/3000]  eta: 0:48:56  lr: 0.000029  loss: 0.4484  time: 1.5401  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1060/3000]  eta: 0:48:48  lr: 0.000029  loss: 0.5729  time: 1.5240  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1060/3000]  eta: 0:48:48  lr: 0.000029  loss: 0.3613  time: 1.5237  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1065/3000]  eta: 0:48:40  lr: 0.000029  loss: 0.2874  time: 1.4911  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1065/3000]  eta: 0:48:40  lr: 0.000029  loss: 0.2523  time: 1.4910  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1070/3000]  eta: 0:48:32  lr: 0.000029  loss: 0.2240  time: 1.4737  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1070/3000]  eta: 0:48:32  lr: 0.000029  loss: 0.3372  time: 1.4735  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1075/3000]  eta: 0:48:25  lr: 0.000029  loss: 0.6164  time: 1.4794  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1075/3000]  eta: 0:48:25  lr: 0.000029  loss: 0.1746  time: 1.4792  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1080/3000]  eta: 0:48:18  lr: 0.000029  loss: 0.8682  time: 1.4957  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1080/3000]  eta: 0:48:17  lr: 0.000029  loss: 0.3324  time: 1.4954  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1085/3000]  eta: 0:48:10  lr: 0.000029  loss: 0.2054  time: 1.5121  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1085/3000]  eta: 0:48:09  lr: 0.000029  loss: 0.3788  time: 1.5119  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1090/3000]  eta: 0:48:03  lr: 0.000029  loss: 0.3276  time: 1.5298  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1090/3000]  eta: 0:48:02  lr: 0.000029  loss: 0.4009  time: 1.5297  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1095/3000]  eta: 0:47:55  lr: 0.000029  loss: 0.3159  time: 1.5096  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1095/3000]  eta: 0:47:54  lr: 0.000029  loss: 0.1620  time: 1.5094  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1100/3000]  eta: 0:47:48  lr: 0.000029  loss: 0.5079  time: 1.5218  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1100/3000]  eta: 0:47:48  lr: 0.000029  loss: 1.0160  time: 1.5216  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1105/3000]  eta: 0:47:40  lr: 0.000029  loss: 0.6027  time: 1.5310  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1105/3000]  eta: 0:47:40  lr: 0.000029  loss: 0.4259  time: 1.5307  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1110/3000]  eta: 0:47:34  lr: 0.000029  loss: 0.1311  time: 1.5351  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1110/3000]  eta: 0:47:33  lr: 0.000029  loss: 0.3955  time: 1.5349  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1115/3000]  eta: 0:47:26  lr: 0.000029  loss: 0.7943  time: 1.5408  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1115/3000]  eta: 0:47:25  lr: 0.000029  loss: 0.4662  time: 1.5405  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1120/3000]  eta: 0:47:18  lr: 0.000029  loss: 0.6620  time: 1.5224  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1120/3000]  eta: 0:47:18  lr: 0.000029  loss: 0.6128  time: 1.5222  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1125/3000]  eta: 0:47:11  lr: 0.000029  loss: 0.6559  time: 1.5245  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1125/3000]  eta: 0:47:10  lr: 0.000029  loss: 1.1098  time: 1.5242  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1130/3000]  eta: 0:47:03  lr: 0.000029  loss: 0.4252  time: 1.5058  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1130/3000]  eta: 0:47:03  lr: 0.000029  loss: 0.2047  time: 1.5056  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1135/3000]  eta: 0:46:56  lr: 0.000029  loss: 0.2765  time: 1.5273  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1135/3000]  eta: 0:46:56  lr: 0.000029  loss: 0.3583  time: 1.5271  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1140/3000]  eta: 0:46:49  lr: 0.000029  loss: 0.8657  time: 1.5311  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1140/3000]  eta: 0:46:48  lr: 0.000029  loss: 0.2566  time: 1.5309  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1145/3000]  eta: 0:46:41  lr: 0.000029  loss: 0.4548  time: 1.5379  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1145/3000]  eta: 0:46:41  lr: 0.000029  loss: 0.2203  time: 1.5376  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1150/3000]  eta: 0:46:34  lr: 0.000029  loss: 0.2200  time: 1.5471  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1150/3000]  eta: 0:46:34  lr: 0.000029  loss: 0.3768  time: 1.5468  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1155/3000]  eta: 0:46:26  lr: 0.000029  loss: 0.3963  time: 1.5181  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1155/3000]  eta: 0:46:26  lr: 0.000029  loss: 0.3194  time: 1.5179  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1160/3000]  eta: 0:46:19  lr: 0.000029  loss: 0.2729  time: 1.5215  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1160/3000]  eta: 0:46:18  lr: 0.000029  loss: 0.5301  time: 1.5213  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1165/3000]  eta: 0:46:11  lr: 0.000029  loss: 0.2178  time: 1.5089  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1165/3000]  eta: 0:46:11  lr: 0.000029  loss: 0.4255  time: 1.5086  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1170/3000]  eta: 0:46:04  lr: 0.000029  loss: 0.3170  time: 1.4971  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1170/3000]  eta: 0:46:03  lr: 0.000029  loss: 0.1900  time: 1.4968  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1175/3000]  eta: 0:45:56  lr: 0.000029  loss: 0.9140  time: 1.5128  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1175/3000]  eta: 0:45:56  lr: 0.000029  loss: 0.6164  time: 1.5126  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1180/3000]  eta: 0:45:48  lr: 0.000029  loss: 0.4090  time: 1.5014  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1180/3000]  eta: 0:45:48  lr: 0.000029  loss: 0.4770  time: 1.5011  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1185/3000]  eta: 0:45:41  lr: 0.000029  loss: 0.1073  time: 1.5071  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1185/3000]  eta: 0:45:40  lr: 0.000029  loss: 0.7171  time: 1.5069  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1190/3000]  eta: 0:45:34  lr: 0.000029  loss: 0.4047  time: 1.5162  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1190/3000]  eta: 0:45:33  lr: 0.000029  loss: 0.8662  time: 1.5160  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1195/3000]  eta: 0:45:26  lr: 0.000029  loss: 0.3235  time: 1.5171  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1195/3000]  eta: 0:45:26  lr: 0.000029  loss: 0.6077  time: 1.5167  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1200/3000]  eta: 0:45:18  lr: 0.000029  loss: 0.8443  time: 1.5196  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1200/3000]  eta: 0:45:18  lr: 0.000029  loss: 0.3819  time: 1.5193  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1205/3000]  eta: 0:45:11  lr: 0.000029  loss: 0.3286  time: 1.5062  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1205/3000]  eta: 0:45:10  lr: 0.000029  loss: 1.0134  time: 1.5060  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1210/3000]  eta: 0:45:03  lr: 0.000029  loss: 0.5698  time: 1.5076  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1210/3000]  eta: 0:45:03  lr: 0.000029  loss: 0.2705  time: 1.5073  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1215/3000]  eta: 0:44:56  lr: 0.000029  loss: 0.3034  time: 1.5112  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1215/3000]  eta: 0:44:55  lr: 0.000029  loss: 0.2458  time: 1.5110  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1220/3000]  eta: 0:44:48  lr: 0.000029  loss: 0.4234  time: 1.5047  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1220/3000]  eta: 0:44:48  lr: 0.000029  loss: 0.3711  time: 1.5045  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1225/3000]  eta: 0:44:40  lr: 0.000029  loss: 0.3792  time: 1.5141  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1225/3000]  eta: 0:44:40  lr: 0.000029  loss: 0.7832  time: 1.5139  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1230/3000]  eta: 0:44:33  lr: 0.000029  loss: 0.2673  time: 1.4941  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1230/3000]  eta: 0:44:32  lr: 0.000029  loss: 0.2392  time: 1.4938  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1235/3000]  eta: 0:44:25  lr: 0.000029  loss: 0.3228  time: 1.4729  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1235/3000]  eta: 0:44:24  lr: 0.000029  loss: 0.2758  time: 1.4727  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1240/3000]  eta: 0:44:17  lr: 0.000029  loss: 0.4265  time: 1.4922  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1240/3000]  eta: 0:44:17  lr: 0.000029  loss: 0.2369  time: 1.4920  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1245/3000]  eta: 0:44:10  lr: 0.000029  loss: 0.2481  time: 1.4969  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1245/3000]  eta: 0:44:09  lr: 0.000029  loss: 0.6469  time: 1.4966  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1250/3000]  eta: 0:44:02  lr: 0.000029  loss: 0.2645  time: 1.4852  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1250/3000]  eta: 0:44:01  lr: 0.000029  loss: 0.7738  time: 1.4850  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1255/3000]  eta: 0:43:54  lr: 0.000029  loss: 0.1847  time: 1.5018  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1255/3000]  eta: 0:43:54  lr: 0.000029  loss: 0.3264  time: 1.5016  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1260/3000]  eta: 0:43:46  lr: 0.000029  loss: 0.5330  time: 1.4729  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1260/3000]  eta: 0:43:46  lr: 0.000029  loss: 0.3718  time: 1.4726  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1265/3000]  eta: 0:43:39  lr: 0.000029  loss: 0.4250  time: 1.4868  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1265/3000]  eta: 0:43:39  lr: 0.000029  loss: 0.2166  time: 1.4865  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1270/3000]  eta: 0:43:32  lr: 0.000029  loss: 0.5256  time: 1.5221  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1270/3000]  eta: 0:43:31  lr: 0.000029  loss: 0.1225  time: 1.5218  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1275/3000]  eta: 0:43:24  lr: 0.000029  loss: 0.3070  time: 1.5204  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1275/3000]  eta: 0:43:24  lr: 0.000029  loss: 0.2669  time: 1.5201  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1280/3000]  eta: 0:43:16  lr: 0.000029  loss: 0.5808  time: 1.5254  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1280/3000]  eta: 0:43:16  lr: 0.000029  loss: 0.5232  time: 1.5251  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1285/3000]  eta: 0:43:09  lr: 0.000029  loss: 0.1251  time: 1.5088  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1285/3000]  eta: 0:43:08  lr: 0.000029  loss: 0.7924  time: 1.5085  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1290/3000]  eta: 0:43:02  lr: 0.000029  loss: 0.2380  time: 1.5095  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1290/3000]  eta: 0:43:01  lr: 0.000029  loss: 0.5345  time: 1.5092  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1295/3000]  eta: 0:42:54  lr: 0.000029  loss: 0.2576  time: 1.5111  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1295/3000]  eta: 0:42:54  lr: 0.000029  loss: 0.5743  time: 1.5108  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1300/3000]  eta: 0:42:47  lr: 0.000029  loss: 0.3346  time: 1.5303  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1300/3000]  eta: 0:42:46  lr: 0.000029  loss: 0.6965  time: 1.5301  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1305/3000]  eta: 0:42:39  lr: 0.000029  loss: 0.6787  time: 1.5421  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1305/3000]  eta: 0:42:39  lr: 0.000029  loss: 0.3456  time: 1.5418  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1310/3000]  eta: 0:42:32  lr: 0.000029  loss: 0.4481  time: 1.5278  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1310/3000]  eta: 0:42:31  lr: 0.000029  loss: 0.9909  time: 1.5276  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1315/3000]  eta: 0:42:24  lr: 0.000029  loss: 1.0366  time: 1.5226  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1315/3000]  eta: 0:42:24  lr: 0.000029  loss: 0.1219  time: 1.5224  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1320/3000]  eta: 0:42:16  lr: 0.000029  loss: 0.4961  time: 1.4846  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1320/3000]  eta: 0:42:15  lr: 0.000029  loss: 0.2350  time: 1.4844  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1325/3000]  eta: 0:42:08  lr: 0.000029  loss: 0.8227  time: 1.4646  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1325/3000]  eta: 0:42:08  lr: 0.000029  loss: 0.3936  time: 1.4645  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1330/3000]  eta: 0:42:00  lr: 0.000029  loss: 0.2894  time: 1.4589  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1330/3000]  eta: 0:42:00  lr: 0.000029  loss: 0.6381  time: 1.4587  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1335/3000]  eta: 0:41:53  lr: 0.000029  loss: 0.8897  time: 1.4841  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1335/3000]  eta: 0:41:53  lr: 0.000029  loss: 0.8951  time: 1.4839  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1340/3000]  eta: 0:41:46  lr: 0.000029  loss: 0.5812  time: 1.5058  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1340/3000]  eta: 0:41:45  lr: 0.000029  loss: 0.3698  time: 1.5055  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1345/3000]  eta: 0:41:38  lr: 0.000029  loss: 0.2873  time: 1.5005  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1345/3000]  eta: 0:41:37  lr: 0.000029  loss: 0.3789  time: 1.5003  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1350/3000]  eta: 0:41:30  lr: 0.000029  loss: 0.8488  time: 1.5116  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1350/3000]  eta: 0:41:30  lr: 0.000029  loss: 0.6902  time: 1.5114  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1355/3000]  eta: 0:41:23  lr: 0.000029  loss: 0.4427  time: 1.4873  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1355/3000]  eta: 0:41:22  lr: 0.000029  loss: 0.3793  time: 1.4871  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1360/3000]  eta: 0:41:15  lr: 0.000029  loss: 0.6277  time: 1.5003  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1360/3000]  eta: 0:41:15  lr: 0.000029  loss: 0.3126  time: 1.5001  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1365/3000]  eta: 0:41:08  lr: 0.000029  loss: 0.5470  time: 1.5232  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1365/3000]  eta: 0:41:07  lr: 0.000029  loss: 0.2762  time: 1.5229  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1370/3000]  eta: 0:41:00  lr: 0.000029  loss: 0.5387  time: 1.5271  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1370/3000]  eta: 0:41:00  lr: 0.000029  loss: 0.9706  time: 1.5268  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1375/3000]  eta: 0:40:53  lr: 0.000029  loss: 0.4486  time: 1.5402  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1375/3000]  eta: 0:40:53  lr: 0.000029  loss: 0.5293  time: 1.5399  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1380/3000]  eta: 0:40:45  lr: 0.000029  loss: 0.4183  time: 1.5139  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1380/3000]  eta: 0:40:45  lr: 0.000029  loss: 0.1356  time: 1.5137  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1385/3000]  eta: 0:40:38  lr: 0.000029  loss: 1.0671  time: 1.5097  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1385/3000]  eta: 0:40:37  lr: 0.000029  loss: 0.2099  time: 1.5095  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1390/3000]  eta: 0:40:30  lr: 0.000029  loss: 0.5025  time: 1.4984  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1390/3000]  eta: 0:40:30  lr: 0.000029  loss: 0.1858  time: 1.4981  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1395/3000]  eta: 0:40:22  lr: 0.000029  loss: 0.9120  time: 1.4849  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1395/3000]  eta: 0:40:22  lr: 0.000029  loss: 0.2534  time: 1.4847  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1400/3000]  eta: 0:40:15  lr: 0.000029  loss: 0.3111  time: 1.5139  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1400/3000]  eta: 0:40:15  lr: 0.000029  loss: 0.4135  time: 1.5136  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1405/3000]  eta: 0:40:07  lr: 0.000029  loss: 0.2492  time: 1.5038  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1405/3000]  eta: 0:40:07  lr: 0.000029  loss: 0.1829  time: 1.5036  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1410/3000]  eta: 0:40:00  lr: 0.000029  loss: 1.0471  time: 1.5124  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1410/3000]  eta: 0:39:59  lr: 0.000029  loss: 0.1456  time: 1.5122  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1415/3000]  eta: 0:39:53  lr: 0.000029  loss: 0.5751  time: 1.5277  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1415/3000]  eta: 0:39:52  lr: 0.000029  loss: 0.3334  time: 1.5275  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1420/3000]  eta: 0:39:45  lr: 0.000029  loss: 0.1429  time: 1.5367  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1420/3000]  eta: 0:39:45  lr: 0.000029  loss: 0.3634  time: 1.5365  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1425/3000]  eta: 0:39:38  lr: 0.000029  loss: 0.7159  time: 1.5583  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1425/3000]  eta: 0:39:38  lr: 0.000029  loss: 0.3506  time: 1.5580  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1430/3000]  eta: 0:39:30  lr: 0.000029  loss: 0.2622  time: 1.5443  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1430/3000]  eta: 0:39:30  lr: 0.000029  loss: 0.3297  time: 1.5440  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1435/3000]  eta: 0:39:23  lr: 0.000029  loss: 0.2817  time: 1.5411  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1435/3000]  eta: 0:39:23  lr: 0.000029  loss: 0.3486  time: 1.5409  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1440/3000]  eta: 0:39:16  lr: 0.000029  loss: 0.1893  time: 1.5396  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1440/3000]  eta: 0:39:15  lr: 0.000029  loss: 0.4877  time: 1.5395  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1445/3000]  eta: 0:39:08  lr: 0.000029  loss: 1.4752  time: 1.5249  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1445/3000]  eta: 0:39:08  lr: 0.000029  loss: 0.5046  time: 1.5247  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1450/3000]  eta: 0:39:01  lr: 0.000029  loss: 0.2561  time: 1.5353  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1450/3000]  eta: 0:39:00  lr: 0.000029  loss: 0.1290  time: 1.5351  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1455/3000]  eta: 0:38:53  lr: 0.000029  loss: 0.5996  time: 1.5228  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1455/3000]  eta: 0:38:53  lr: 0.000029  loss: 0.6132  time: 1.5225  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1460/3000]  eta: 0:38:45  lr: 0.000029  loss: 0.1265  time: 1.4996  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1460/3000]  eta: 0:38:45  lr: 0.000029  loss: 0.4270  time: 1.4993  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1465/3000]  eta: 0:38:38  lr: 0.000029  loss: 0.1462  time: 1.5078  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1465/3000]  eta: 0:38:38  lr: 0.000029  loss: 0.4578  time: 1.5075  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1470/3000]  eta: 0:38:30  lr: 0.000029  loss: 0.1208  time: 1.5076  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1470/3000]  eta: 0:38:30  lr: 0.000029  loss: 0.9163  time: 1.5073  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1475/3000]  eta: 0:38:23  lr: 0.000029  loss: 0.1836  time: 1.5152  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1475/3000]  eta: 0:38:23  lr: 0.000029  loss: 0.2432  time: 1.5149  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1480/3000]  eta: 0:38:15  lr: 0.000029  loss: 0.4328  time: 1.5007  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1480/3000]  eta: 0:38:15  lr: 0.000029  loss: 0.6006  time: 1.5004  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1485/3000]  eta: 0:38:07  lr: 0.000029  loss: 0.1446  time: 1.4766  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1485/3000]  eta: 0:38:07  lr: 0.000029  loss: 0.2087  time: 1.4763  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1490/3000]  eta: 0:38:00  lr: 0.000029  loss: 0.2345  time: 1.4817  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1490/3000]  eta: 0:37:59  lr: 0.000029  loss: 0.2066  time: 1.4814  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1495/3000]  eta: 0:37:52  lr: 0.000029  loss: 0.5747  time: 1.4814  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1495/3000]  eta: 0:37:52  lr: 0.000029  loss: 0.8700  time: 1.4812  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1500/3000]  eta: 0:37:44  lr: 0.000029  loss: 0.2506  time: 1.4786  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1500/3000]  eta: 0:37:44  lr: 0.000029  loss: 0.3457  time: 1.4783  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1505/3000]  eta: 0:37:37  lr: 0.000029  loss: 0.5317  time: 1.4968  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1505/3000]  eta: 0:37:36  lr: 0.000029  loss: 0.2467  time: 1.4965  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1510/3000]  eta: 0:37:29  lr: 0.000029  loss: 0.1192  time: 1.4867  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1510/3000]  eta: 0:37:29  lr: 0.000029  loss: 0.4557  time: 1.4865  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1515/3000]  eta: 0:37:21  lr: 0.000029  loss: 0.2570  time: 1.4788  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1515/3000]  eta: 0:37:21  lr: 0.000029  loss: 0.6061  time: 1.4785  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1520/3000]  eta: 0:37:14  lr: 0.000029  loss: 0.2303  time: 1.5006  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1520/3000]  eta: 0:37:13  lr: 0.000029  loss: 0.3160  time: 1.5004  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1525/3000]  eta: 0:37:07  lr: 0.000029  loss: 0.4903  time: 1.5113  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1525/3000]  eta: 0:37:06  lr: 0.000029  loss: 0.3314  time: 1.5110  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1530/3000]  eta: 0:36:59  lr: 0.000029  loss: 0.4297  time: 1.5211  data: 0.0000  max mem: 18151Train: data epoch: [3]  [1530/3000]  eta: 0:36:59  lr: 0.000029  loss: 0.2709  time: 1.5215  data: 0.0000  max mem: 18432

Train: data epoch: [3]  [1535/3000]  eta: 0:36:52  lr: 0.000029  loss: 0.1151  time: 1.5305  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1535/3000]  eta: 0:36:51  lr: 0.000029  loss: 0.2449  time: 1.5303  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1540/3000]  eta: 0:36:44  lr: 0.000029  loss: 0.4837  time: 1.5314  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1540/3000]  eta: 0:36:44  lr: 0.000029  loss: 0.4678  time: 1.5311  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1545/3000]  eta: 0:36:37  lr: 0.000029  loss: 0.6534  time: 1.5306  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1545/3000]  eta: 0:36:36  lr: 0.000029  loss: 0.2232  time: 1.5304  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1550/3000]  eta: 0:36:29  lr: 0.000029  loss: 0.2574  time: 1.5290  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1550/3000]  eta: 0:36:29  lr: 0.000029  loss: 0.7174  time: 1.5287  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1555/3000]  eta: 0:36:21  lr: 0.000029  loss: 0.8519  time: 1.5163  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1555/3000]  eta: 0:36:21  lr: 0.000029  loss: 0.8285  time: 1.5160  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1560/3000]  eta: 0:36:14  lr: 0.000029  loss: 0.1620  time: 1.5230  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1560/3000]  eta: 0:36:14  lr: 0.000029  loss: 0.3749  time: 1.5228  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1565/3000]  eta: 0:36:07  lr: 0.000029  loss: 0.3193  time: 1.5150  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1565/3000]  eta: 0:36:06  lr: 0.000029  loss: 0.5377  time: 1.5148  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1570/3000]  eta: 0:35:59  lr: 0.000029  loss: 0.5133  time: 1.5089  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1570/3000]  eta: 0:35:59  lr: 0.000029  loss: 0.5566  time: 1.5086  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1575/3000]  eta: 0:35:51  lr: 0.000029  loss: 0.2220  time: 1.5118  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1575/3000]  eta: 0:35:51  lr: 0.000029  loss: 0.2195  time: 1.5115  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1580/3000]  eta: 0:35:44  lr: 0.000029  loss: 0.2910  time: 1.5228  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1580/3000]  eta: 0:35:44  lr: 0.000029  loss: 0.4046  time: 1.5225  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1585/3000]  eta: 0:35:36  lr: 0.000029  loss: 0.1692  time: 1.5105  data: 0.0000  max mem: 18432Train: data epoch: [3]  [1585/3000]  eta: 0:35:36  lr: 0.000029  loss: 0.3885  time: 1.5101  data: 0.0000  max mem: 18151

Train: data epoch: [3]  [1590/3000]  eta: 0:35:29  lr: 0.000029  loss: 0.3277  time: 1.5312  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1590/3000]  eta: 0:35:29  lr: 0.000029  loss: 0.9158  time: 1.5309  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1595/3000]  eta: 0:35:22  lr: 0.000029  loss: 0.4396  time: 1.5417  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1595/3000]  eta: 0:35:21  lr: 0.000029  loss: 0.3304  time: 1.5415  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1600/3000]  eta: 0:35:14  lr: 0.000029  loss: 0.5892  time: 1.5339  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1600/3000]  eta: 0:35:14  lr: 0.000029  loss: 0.2798  time: 1.5337  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1605/3000]  eta: 0:35:07  lr: 0.000029  loss: 0.1870  time: 1.5598  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1605/3000]  eta: 0:35:07  lr: 0.000029  loss: 0.2850  time: 1.5596  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1610/3000]  eta: 0:35:00  lr: 0.000029  loss: 0.4349  time: 1.5499  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1610/3000]  eta: 0:34:59  lr: 0.000029  loss: 0.4200  time: 1.5496  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1615/3000]  eta: 0:34:52  lr: 0.000029  loss: 0.2820  time: 1.5386  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1615/3000]  eta: 0:34:52  lr: 0.000029  loss: 0.5416  time: 1.5384  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1620/3000]  eta: 0:34:45  lr: 0.000029  loss: 0.4996  time: 1.5386  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1620/3000]  eta: 0:34:44  lr: 0.000029  loss: 0.5433  time: 1.5383  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1625/3000]  eta: 0:34:37  lr: 0.000029  loss: 0.7461  time: 1.5191  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1625/3000]  eta: 0:34:37  lr: 0.000029  loss: 0.3803  time: 1.5188  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1630/3000]  eta: 0:34:30  lr: 0.000029  loss: 0.4408  time: 1.5253  data: 0.0000  max mem: 18432Train: data epoch: [3]  [1630/3000]  eta: 0:34:29  lr: 0.000029  loss: 0.5527  time: 1.5251  data: 0.0000  max mem: 18151

Train: data epoch: [3]  [1635/3000]  eta: 0:34:22  lr: 0.000029  loss: 0.4520  time: 1.5444  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1635/3000]  eta: 0:34:22  lr: 0.000029  loss: 0.1419  time: 1.5441  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1640/3000]  eta: 0:34:15  lr: 0.000029  loss: 0.2765  time: 1.5242  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1640/3000]  eta: 0:34:14  lr: 0.000029  loss: 0.4916  time: 1.5241  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1645/3000]  eta: 0:34:07  lr: 0.000029  loss: 0.2757  time: 1.5364  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1645/3000]  eta: 0:34:07  lr: 0.000029  loss: 0.3302  time: 1.5362  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1650/3000]  eta: 0:34:00  lr: 0.000029  loss: 0.4494  time: 1.5299  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1650/3000]  eta: 0:33:59  lr: 0.000029  loss: 0.4418  time: 1.5297  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1655/3000]  eta: 0:33:52  lr: 0.000029  loss: 0.1001  time: 1.5165  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1655/3000]  eta: 0:33:52  lr: 0.000029  loss: 0.8601  time: 1.5163  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1660/3000]  eta: 0:33:44  lr: 0.000029  loss: 0.3057  time: 1.5067  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1660/3000]  eta: 0:33:44  lr: 0.000029  loss: 0.4867  time: 1.5064  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1665/3000]  eta: 0:33:37  lr: 0.000029  loss: 0.7105  time: 1.5002  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1665/3000]  eta: 0:33:36  lr: 0.000029  loss: 0.5151  time: 1.4999  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1670/3000]  eta: 0:33:29  lr: 0.000029  loss: 0.4854  time: 1.4794  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1670/3000]  eta: 0:33:29  lr: 0.000029  loss: 0.5463  time: 1.4792  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1675/3000]  eta: 0:33:21  lr: 0.000029  loss: 0.7958  time: 1.4700  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1675/3000]  eta: 0:33:21  lr: 0.000029  loss: 1.1441  time: 1.4698  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1680/3000]  eta: 0:33:13  lr: 0.000029  loss: 0.4277  time: 1.4709  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1680/3000]  eta: 0:33:13  lr: 0.000029  loss: 0.1234  time: 1.4707  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1685/3000]  eta: 0:33:06  lr: 0.000029  loss: 0.5572  time: 1.4801  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1685/3000]  eta: 0:33:06  lr: 0.000029  loss: 0.7281  time: 1.4800  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1690/3000]  eta: 0:32:58  lr: 0.000029  loss: 0.7205  time: 1.4880  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1690/3000]  eta: 0:32:58  lr: 0.000029  loss: 0.2306  time: 1.4877  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1695/3000]  eta: 0:32:51  lr: 0.000029  loss: 0.2551  time: 1.5085  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1695/3000]  eta: 0:32:51  lr: 0.000029  loss: 0.3891  time: 1.5082  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1700/3000]  eta: 0:32:43  lr: 0.000029  loss: 0.5689  time: 1.5272  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1700/3000]  eta: 0:32:43  lr: 0.000029  loss: 0.7229  time: 1.5269  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1705/3000]  eta: 0:32:36  lr: 0.000029  loss: 0.2224  time: 1.5177  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1705/3000]  eta: 0:32:36  lr: 0.000029  loss: 0.0675  time: 1.5174  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1710/3000]  eta: 0:32:28  lr: 0.000029  loss: 0.1360  time: 1.5211  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1710/3000]  eta: 0:32:28  lr: 0.000029  loss: 0.4003  time: 1.5210  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1715/3000]  eta: 0:32:20  lr: 0.000029  loss: 0.7671  time: 1.4862  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1715/3000]  eta: 0:32:20  lr: 0.000029  loss: 0.3011  time: 1.4860  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1720/3000]  eta: 0:32:13  lr: 0.000029  loss: 0.5822  time: 1.4911  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1720/3000]  eta: 0:32:13  lr: 0.000029  loss: 0.3181  time: 1.4908  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1725/3000]  eta: 0:32:05  lr: 0.000029  loss: 0.3598  time: 1.4894  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1725/3000]  eta: 0:32:05  lr: 0.000029  loss: 0.2945  time: 1.4892  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1730/3000]  eta: 0:31:58  lr: 0.000029  loss: 0.6309  time: 1.4728  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1730/3000]  eta: 0:31:57  lr: 0.000029  loss: 0.8569  time: 1.4724  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1735/3000]  eta: 0:31:50  lr: 0.000029  loss: 0.9130  time: 1.4877  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1735/3000]  eta: 0:31:50  lr: 0.000029  loss: 0.3766  time: 1.4874  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1740/3000]  eta: 0:31:42  lr: 0.000029  loss: 0.2911  time: 1.4806  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1740/3000]  eta: 0:31:42  lr: 0.000029  loss: 0.4609  time: 1.4803  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1745/3000]  eta: 0:31:35  lr: 0.000029  loss: 0.2975  time: 1.4760  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1745/3000]  eta: 0:31:34  lr: 0.000029  loss: 0.3995  time: 1.4757  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1750/3000]  eta: 0:31:27  lr: 0.000029  loss: 0.6502  time: 1.4734  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1750/3000]  eta: 0:31:27  lr: 0.000029  loss: 0.2606  time: 1.4732  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1755/3000]  eta: 0:31:19  lr: 0.000029  loss: 1.1449  time: 1.4734  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1755/3000]  eta: 0:31:19  lr: 0.000029  loss: 0.1490  time: 1.4731  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1760/3000]  eta: 0:31:12  lr: 0.000029  loss: 0.2467  time: 1.4751  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1760/3000]  eta: 0:31:11  lr: 0.000029  loss: 0.5196  time: 1.4748  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1765/3000]  eta: 0:31:04  lr: 0.000029  loss: 0.1870  time: 1.4674  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1765/3000]  eta: 0:31:04  lr: 0.000029  loss: 0.1514  time: 1.4672  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1770/3000]  eta: 0:30:57  lr: 0.000029  loss: 0.4905  time: 1.5026  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1770/3000]  eta: 0:30:56  lr: 0.000029  loss: 1.3944  time: 1.5023  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1775/3000]  eta: 0:30:49  lr: 0.000029  loss: 1.0262  time: 1.5300  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1775/3000]  eta: 0:30:49  lr: 0.000029  loss: 0.1981  time: 1.5288  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1780/3000]  eta: 0:30:42  lr: 0.000029  loss: 0.6467  time: 1.5413  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1780/3000]  eta: 0:30:41  lr: 0.000029  loss: 0.1972  time: 1.5401  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1785/3000]  eta: 0:30:34  lr: 0.000029  loss: 0.2581  time: 1.5630  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1785/3000]  eta: 0:30:34  lr: 0.000029  loss: 0.3182  time: 1.5617  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1790/3000]  eta: 0:30:27  lr: 0.000029  loss: 0.4336  time: 1.5647  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1790/3000]  eta: 0:30:27  lr: 0.000029  loss: 0.2629  time: 1.5635  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1795/3000]  eta: 0:30:20  lr: 0.000029  loss: 0.4371  time: 1.5672  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1795/3000]  eta: 0:30:19  lr: 0.000029  loss: 0.1123  time: 1.5670  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1800/3000]  eta: 0:30:12  lr: 0.000029  loss: 0.5277  time: 1.5644  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1800/3000]  eta: 0:30:12  lr: 0.000029  loss: 0.3614  time: 1.5642  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1805/3000]  eta: 0:30:05  lr: 0.000029  loss: 0.5035  time: 1.5638  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1805/3000]  eta: 0:30:05  lr: 0.000029  loss: 0.5968  time: 1.5636  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1810/3000]  eta: 0:29:57  lr: 0.000029  loss: 0.7571  time: 1.5501  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1810/3000]  eta: 0:29:57  lr: 0.000029  loss: 0.1198  time: 1.5498  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1815/3000]  eta: 0:29:50  lr: 0.000029  loss: 0.4787  time: 1.5147  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1815/3000]  eta: 0:29:49  lr: 0.000029  loss: 0.2402  time: 1.5144  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1820/3000]  eta: 0:29:42  lr: 0.000029  loss: 0.3259  time: 1.5180  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1820/3000]  eta: 0:29:42  lr: 0.000029  loss: 0.3707  time: 1.5175  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1825/3000]  eta: 0:29:35  lr: 0.000029  loss: 0.3404  time: 1.5208  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1825/3000]  eta: 0:29:35  lr: 0.000029  loss: 0.2120  time: 1.5204  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1830/3000]  eta: 0:29:27  lr: 0.000029  loss: 0.4513  time: 1.5221  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1830/3000]  eta: 0:29:27  lr: 0.000029  loss: 0.7277  time: 1.5218  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1835/3000]  eta: 0:29:20  lr: 0.000029  loss: 0.5792  time: 1.5481  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1835/3000]  eta: 0:29:20  lr: 0.000029  loss: 0.5917  time: 1.5478  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1840/3000]  eta: 0:29:13  lr: 0.000029  loss: 0.4336  time: 1.5512  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1840/3000]  eta: 0:29:12  lr: 0.000029  loss: 0.4112  time: 1.5509  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1845/3000]  eta: 0:29:05  lr: 0.000029  loss: 0.1284  time: 1.5387  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1845/3000]  eta: 0:29:05  lr: 0.000029  loss: 0.8304  time: 1.5384  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1850/3000]  eta: 0:28:57  lr: 0.000029  loss: 0.1968  time: 1.5328  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1850/3000]  eta: 0:28:57  lr: 0.000029  loss: 0.3197  time: 1.5325  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1855/3000]  eta: 0:28:50  lr: 0.000029  loss: 0.4419  time: 1.5312  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1855/3000]  eta: 0:28:50  lr: 0.000029  loss: 0.7772  time: 1.5309  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1860/3000]  eta: 0:28:42  lr: 0.000029  loss: 0.3930  time: 1.5228  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1860/3000]  eta: 0:28:42  lr: 0.000029  loss: 0.3910  time: 1.5225  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1865/3000]  eta: 0:28:35  lr: 0.000029  loss: 0.5517  time: 1.5289  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1865/3000]  eta: 0:28:35  lr: 0.000029  loss: 0.5214  time: 1.5286  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1870/3000]  eta: 0:28:27  lr: 0.000029  loss: 0.5015  time: 1.5228  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1870/3000]  eta: 0:28:27  lr: 0.000029  loss: 0.2297  time: 1.5226  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1875/3000]  eta: 0:28:20  lr: 0.000029  loss: 0.5746  time: 1.5112  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1875/3000]  eta: 0:28:19  lr: 0.000029  loss: 0.4707  time: 1.5110  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1880/3000]  eta: 0:28:12  lr: 0.000029  loss: 0.7420  time: 1.5031  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1880/3000]  eta: 0:28:12  lr: 0.000029  loss: 0.1328  time: 1.5029  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1885/3000]  eta: 0:28:05  lr: 0.000029  loss: 0.5621  time: 1.4973  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1885/3000]  eta: 0:28:04  lr: 0.000029  loss: 0.1560  time: 1.4971  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1890/3000]  eta: 0:27:57  lr: 0.000029  loss: 0.2259  time: 1.5141  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1890/3000]  eta: 0:27:57  lr: 0.000029  loss: 0.6237  time: 1.5139  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1895/3000]  eta: 0:27:49  lr: 0.000029  loss: 0.1716  time: 1.5116  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1895/3000]  eta: 0:27:50  lr: 0.000029  loss: 0.3231  time: 1.5120  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1900/3000]  eta: 0:27:42  lr: 0.000029  loss: 0.2782  time: 1.5231  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1900/3000]  eta: 0:27:42  lr: 0.000029  loss: 0.3922  time: 1.5230  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1905/3000]  eta: 0:27:34  lr: 0.000029  loss: 0.5639  time: 1.5117  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1905/3000]  eta: 0:27:34  lr: 0.000029  loss: 0.4509  time: 1.5116  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1910/3000]  eta: 0:27:27  lr: 0.000029  loss: 0.7754  time: 1.5074  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1910/3000]  eta: 0:27:27  lr: 0.000029  loss: 0.3419  time: 1.5073  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1915/3000]  eta: 0:27:19  lr: 0.000029  loss: 0.2991  time: 1.5258  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1915/3000]  eta: 0:27:19  lr: 0.000029  loss: 0.5831  time: 1.5258  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1920/3000]  eta: 0:27:12  lr: 0.000029  loss: 0.2830  time: 1.4992  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1920/3000]  eta: 0:27:11  lr: 0.000029  loss: 0.1520  time: 1.4988  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1925/3000]  eta: 0:27:04  lr: 0.000029  loss: 0.3068  time: 1.4914  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1925/3000]  eta: 0:27:04  lr: 0.000029  loss: 0.3507  time: 1.4911  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1930/3000]  eta: 0:26:57  lr: 0.000029  loss: 0.6126  time: 1.5052  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1930/3000]  eta: 0:26:56  lr: 0.000029  loss: 0.7187  time: 1.5049  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1935/3000]  eta: 0:26:49  lr: 0.000029  loss: 0.0933  time: 1.4950  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1935/3000]  eta: 0:26:49  lr: 0.000029  loss: 0.2322  time: 1.4947  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1940/3000]  eta: 0:26:42  lr: 0.000029  loss: 0.5762  time: 1.5185  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1940/3000]  eta: 0:26:41  lr: 0.000029  loss: 0.4147  time: 1.5184  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1945/3000]  eta: 0:26:34  lr: 0.000029  loss: 0.2640  time: 1.5398  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1945/3000]  eta: 0:26:34  lr: 0.000029  loss: 0.1155  time: 1.5396  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1950/3000]  eta: 0:26:26  lr: 0.000029  loss: 0.5422  time: 1.5138  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1950/3000]  eta: 0:26:26  lr: 0.000029  loss: 0.3930  time: 1.5135  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1955/3000]  eta: 0:26:19  lr: 0.000029  loss: 0.1726  time: 1.5238  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1955/3000]  eta: 0:26:19  lr: 0.000029  loss: 0.2712  time: 1.5236  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1960/3000]  eta: 0:26:11  lr: 0.000029  loss: 0.1899  time: 1.5254  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1960/3000]  eta: 0:26:11  lr: 0.000029  loss: 0.4150  time: 1.5252  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1965/3000]  eta: 0:26:04  lr: 0.000029  loss: 0.2876  time: 1.5224  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1965/3000]  eta: 0:26:04  lr: 0.000029  loss: 0.3696  time: 1.5222  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1970/3000]  eta: 0:25:56  lr: 0.000029  loss: 0.3880  time: 1.5443  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1970/3000]  eta: 0:25:56  lr: 0.000029  loss: 0.8678  time: 1.5440  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1975/3000]  eta: 0:25:49  lr: 0.000029  loss: 0.7928  time: 1.5283  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1975/3000]  eta: 0:25:49  lr: 0.000029  loss: 0.8754  time: 1.5280  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1980/3000]  eta: 0:25:41  lr: 0.000029  loss: 0.4228  time: 1.5085  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1980/3000]  eta: 0:25:41  lr: 0.000029  loss: 0.1786  time: 1.5082  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1985/3000]  eta: 0:25:34  lr: 0.000029  loss: 0.4039  time: 1.5094  data: 0.0000  max mem: 18432Train: data epoch: [3]  [1985/3000]  eta: 0:25:33  lr: 0.000029  loss: 0.9539  time: 1.5091  data: 0.0000  max mem: 18151

Train: data epoch: [3]  [1990/3000]  eta: 0:25:26  lr: 0.000029  loss: 0.8205  time: 1.5181  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1990/3000]  eta: 0:25:26  lr: 0.000029  loss: 0.2390  time: 1.5179  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [1995/3000]  eta: 0:25:19  lr: 0.000029  loss: 0.9598  time: 1.5238  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [1995/3000]  eta: 0:25:19  lr: 0.000029  loss: 0.7005  time: 1.5235  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2000/3000]  eta: 0:25:11  lr: 0.000029  loss: 0.4310  time: 1.5577  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2000/3000]  eta: 0:25:11  lr: 0.000029  loss: 0.1147  time: 1.5575  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2005/3000]  eta: 0:25:04  lr: 0.000029  loss: 0.6137  time: 1.5486  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2005/3000]  eta: 0:25:04  lr: 0.000029  loss: 0.2221  time: 1.5484  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2010/3000]  eta: 0:24:56  lr: 0.000029  loss: 0.3332  time: 1.5260  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2010/3000]  eta: 0:24:56  lr: 0.000029  loss: 0.2867  time: 1.5258  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2015/3000]  eta: 0:24:49  lr: 0.000029  loss: 0.1503  time: 1.5195  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2015/3000]  eta: 0:24:48  lr: 0.000029  loss: 0.2101  time: 1.5193  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2020/3000]  eta: 0:24:41  lr: 0.000029  loss: 0.3198  time: 1.4942  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2020/3000]  eta: 0:24:41  lr: 0.000029  loss: 0.1233  time: 1.4940  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2025/3000]  eta: 0:24:34  lr: 0.000029  loss: 0.2455  time: 1.5161  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2025/3000]  eta: 0:24:33  lr: 0.000029  loss: 0.1429  time: 1.5159  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2030/3000]  eta: 0:24:26  lr: 0.000029  loss: 0.4625  time: 1.5230  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2030/3000]  eta: 0:24:26  lr: 0.000029  loss: 0.4832  time: 1.5228  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2035/3000]  eta: 0:24:19  lr: 0.000029  loss: 0.1776  time: 1.5317  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2035/3000]  eta: 0:24:18  lr: 0.000029  loss: 0.4591  time: 1.5315  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2040/3000]  eta: 0:24:11  lr: 0.000029  loss: 0.4552  time: 1.5169  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2040/3000]  eta: 0:24:11  lr: 0.000029  loss: 0.1417  time: 1.5166  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2045/3000]  eta: 0:24:03  lr: 0.000029  loss: 0.4647  time: 1.4889  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2045/3000]  eta: 0:24:03  lr: 0.000029  loss: 0.3782  time: 1.4886  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2050/3000]  eta: 0:23:56  lr: 0.000029  loss: 0.8257  time: 1.4962  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2050/3000]  eta: 0:23:55  lr: 0.000029  loss: 0.3714  time: 1.4958  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2055/3000]  eta: 0:23:48  lr: 0.000029  loss: 0.4930  time: 1.5078  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2055/3000]  eta: 0:23:48  lr: 0.000029  loss: 0.2122  time: 1.5075  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2060/3000]  eta: 0:23:41  lr: 0.000029  loss: 0.5668  time: 1.5375  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2060/3000]  eta: 0:23:41  lr: 0.000029  loss: 0.2363  time: 1.5372  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2065/3000]  eta: 0:23:33  lr: 0.000029  loss: 0.2728  time: 1.5654  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2065/3000]  eta: 0:23:33  lr: 0.000029  loss: 0.2294  time: 1.5651  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2070/3000]  eta: 0:23:26  lr: 0.000029  loss: 0.3005  time: 1.5578  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2070/3000]  eta: 0:23:26  lr: 0.000029  loss: 0.6470  time: 1.5575  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2075/3000]  eta: 0:23:18  lr: 0.000029  loss: 0.5381  time: 1.5456  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2075/3000]  eta: 0:23:18  lr: 0.000029  loss: 0.1920  time: 1.5453  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2080/3000]  eta: 0:23:11  lr: 0.000029  loss: 0.1489  time: 1.5460  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2080/3000]  eta: 0:23:11  lr: 0.000029  loss: 0.6179  time: 1.5457  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2085/3000]  eta: 0:23:03  lr: 0.000029  loss: 0.2218  time: 1.5394  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2085/3000]  eta: 0:23:03  lr: 0.000029  loss: 0.9625  time: 1.5392  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2090/3000]  eta: 0:22:56  lr: 0.000029  loss: 0.3192  time: 1.5373  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2090/3000]  eta: 0:22:56  lr: 0.000029  loss: 0.4159  time: 1.5371  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2095/3000]  eta: 0:22:48  lr: 0.000029  loss: 0.3128  time: 1.5233  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2095/3000]  eta: 0:22:48  lr: 0.000029  loss: 0.3307  time: 1.5231  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2100/3000]  eta: 0:22:41  lr: 0.000029  loss: 0.8411  time: 1.5341  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2100/3000]  eta: 0:22:41  lr: 0.000029  loss: 0.1065  time: 1.5339  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2105/3000]  eta: 0:22:33  lr: 0.000029  loss: 0.3167  time: 1.5293  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2105/3000]  eta: 0:22:33  lr: 0.000029  loss: 0.3759  time: 1.5291  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2110/3000]  eta: 0:22:26  lr: 0.000029  loss: 0.4825  time: 1.5303  data: 0.0000  max mem: 18151Train: data epoch: [3]  [2110/3000]  eta: 0:22:26  lr: 0.000029  loss: 0.5583  time: 1.5305  data: 0.0000  max mem: 18432

Train: data epoch: [3]  [2115/3000]  eta: 0:22:18  lr: 0.000029  loss: 0.1261  time: 1.5470  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2115/3000]  eta: 0:22:18  lr: 0.000029  loss: 0.2034  time: 1.5468  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2120/3000]  eta: 0:22:11  lr: 0.000029  loss: 0.3977  time: 1.5268  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2120/3000]  eta: 0:22:10  lr: 0.000029  loss: 0.5503  time: 1.5266  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2125/3000]  eta: 0:22:03  lr: 0.000029  loss: 0.2910  time: 1.5060  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2125/3000]  eta: 0:22:03  lr: 0.000029  loss: 0.3291  time: 1.5058  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2130/3000]  eta: 0:21:55  lr: 0.000029  loss: 0.5163  time: 1.4940  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2130/3000]  eta: 0:21:55  lr: 0.000029  loss: 0.1310  time: 1.4938  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2135/3000]  eta: 0:21:48  lr: 0.000029  loss: 0.3293  time: 1.4964  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2135/3000]  eta: 0:21:48  lr: 0.000029  loss: 0.7170  time: 1.4962  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2140/3000]  eta: 0:21:40  lr: 0.000029  loss: 0.3078  time: 1.4811  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2140/3000]  eta: 0:21:40  lr: 0.000029  loss: 0.1984  time: 1.4809  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2145/3000]  eta: 0:21:32  lr: 0.000029  loss: 0.1442  time: 1.4794  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2145/3000]  eta: 0:21:32  lr: 0.000029  loss: 0.4119  time: 1.4791  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2150/3000]  eta: 0:21:25  lr: 0.000029  loss: 0.2547  time: 1.4895  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2150/3000]  eta: 0:21:25  lr: 0.000029  loss: 0.6517  time: 1.4892  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2155/3000]  eta: 0:21:17  lr: 0.000029  loss: 0.8889  time: 1.4746  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2155/3000]  eta: 0:21:17  lr: 0.000029  loss: 0.6278  time: 1.4743  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2160/3000]  eta: 0:21:10  lr: 0.000029  loss: 0.3705  time: 1.4939  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2160/3000]  eta: 0:21:10  lr: 0.000029  loss: 0.3452  time: 1.4936  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2165/3000]  eta: 0:21:02  lr: 0.000029  loss: 0.4786  time: 1.4871  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2165/3000]  eta: 0:21:02  lr: 0.000029  loss: 0.2545  time: 1.4868  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2170/3000]  eta: 0:20:54  lr: 0.000029  loss: 0.3790  time: 1.4791  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2170/3000]  eta: 0:20:54  lr: 0.000029  loss: 0.0910  time: 1.4788  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2175/3000]  eta: 0:20:47  lr: 0.000029  loss: 0.3474  time: 1.4526  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2175/3000]  eta: 0:20:46  lr: 0.000029  loss: 0.3049  time: 1.4523  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2180/3000]  eta: 0:20:39  lr: 0.000029  loss: 0.2672  time: 1.4575  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2180/3000]  eta: 0:20:39  lr: 0.000029  loss: 0.7627  time: 1.4573  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2185/3000]  eta: 0:20:32  lr: 0.000029  loss: 0.7176  time: 1.4954  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2185/3000]  eta: 0:20:31  lr: 0.000029  loss: 0.7625  time: 1.4951  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2190/3000]  eta: 0:20:24  lr: 0.000029  loss: 0.1268  time: 1.5019  data: 0.0000  max mem: 18432Train: data epoch: [3]  [2190/3000]  eta: 0:20:24  lr: 0.000029  loss: 0.4536  time: 1.5015  data: 0.0000  max mem: 18151

Train: data epoch: [3]  [2195/3000]  eta: 0:20:16  lr: 0.000029  loss: 0.2711  time: 1.5095  data: 0.0000  max mem: 18151Train: data epoch: [3]  [2195/3000]  eta: 0:20:16  lr: 0.000029  loss: 0.5502  time: 1.5098  data: 0.0000  max mem: 18432

Train: data epoch: [3]  [2200/3000]  eta: 0:20:09  lr: 0.000029  loss: 1.1242  time: 1.5081  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2200/3000]  eta: 0:20:09  lr: 0.000029  loss: 0.2339  time: 1.5078  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2205/3000]  eta: 0:20:01  lr: 0.000029  loss: 0.4147  time: 1.5059  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2205/3000]  eta: 0:20:01  lr: 0.000029  loss: 0.3680  time: 1.5057  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2210/3000]  eta: 0:19:54  lr: 0.000029  loss: 0.1814  time: 1.4938  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2210/3000]  eta: 0:19:54  lr: 0.000029  loss: 0.2796  time: 1.4935  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2215/3000]  eta: 0:19:46  lr: 0.000029  loss: 0.4888  time: 1.5094  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2215/3000]  eta: 0:19:46  lr: 0.000029  loss: 0.5571  time: 1.5091  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2220/3000]  eta: 0:19:38  lr: 0.000029  loss: 0.1581  time: 1.4890  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2220/3000]  eta: 0:19:38  lr: 0.000029  loss: 0.1932  time: 1.4887  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2225/3000]  eta: 0:19:31  lr: 0.000029  loss: 0.3009  time: 1.4839  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2225/3000]  eta: 0:19:31  lr: 0.000029  loss: 0.5408  time: 1.4836  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2230/3000]  eta: 0:19:23  lr: 0.000029  loss: 0.7171  time: 1.5098  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2230/3000]  eta: 0:19:23  lr: 0.000029  loss: 0.3730  time: 1.5096  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2235/3000]  eta: 0:19:16  lr: 0.000029  loss: 0.2659  time: 1.5255  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2235/3000]  eta: 0:19:16  lr: 0.000029  loss: 0.4816  time: 1.5253  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2240/3000]  eta: 0:19:08  lr: 0.000029  loss: 0.1644  time: 1.5271  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2240/3000]  eta: 0:19:08  lr: 0.000029  loss: 1.1528  time: 1.5268  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2245/3000]  eta: 0:19:01  lr: 0.000029  loss: 0.5513  time: 1.5296  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2245/3000]  eta: 0:19:01  lr: 0.000029  loss: 0.3971  time: 1.5293  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2250/3000]  eta: 0:18:53  lr: 0.000029  loss: 0.3559  time: 1.5249  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2250/3000]  eta: 0:18:53  lr: 0.000029  loss: 0.3804  time: 1.5247  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2255/3000]  eta: 0:18:46  lr: 0.000029  loss: 0.5022  time: 1.5079  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2255/3000]  eta: 0:18:46  lr: 0.000029  loss: 1.0582  time: 1.5077  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2260/3000]  eta: 0:18:38  lr: 0.000029  loss: 0.3682  time: 1.5186  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2260/3000]  eta: 0:18:38  lr: 0.000029  loss: 0.4413  time: 1.5184  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2265/3000]  eta: 0:18:31  lr: 0.000029  loss: 0.1617  time: 1.5044  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2265/3000]  eta: 0:18:30  lr: 0.000029  loss: 0.7580  time: 1.5042  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2270/3000]  eta: 0:18:23  lr: 0.000029  loss: 0.3214  time: 1.4943  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2270/3000]  eta: 0:18:23  lr: 0.000029  loss: 0.4865  time: 1.4940  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2275/3000]  eta: 0:18:15  lr: 0.000029  loss: 0.2788  time: 1.4975  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2275/3000]  eta: 0:18:15  lr: 0.000029  loss: 0.3282  time: 1.4974  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2280/3000]  eta: 0:18:08  lr: 0.000029  loss: 0.2633  time: 1.4950  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2280/3000]  eta: 0:18:08  lr: 0.000029  loss: 0.3794  time: 1.4948  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2285/3000]  eta: 0:18:00  lr: 0.000029  loss: 0.4886  time: 1.4843  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2285/3000]  eta: 0:18:00  lr: 0.000029  loss: 0.6695  time: 1.4840  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2290/3000]  eta: 0:17:53  lr: 0.000029  loss: 0.5870  time: 1.4694  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2290/3000]  eta: 0:17:52  lr: 0.000029  loss: 0.3982  time: 1.4692  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2295/3000]  eta: 0:17:45  lr: 0.000029  loss: 0.2999  time: 1.4604  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2295/3000]  eta: 0:17:45  lr: 0.000029  loss: 0.1359  time: 1.4601  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2300/3000]  eta: 0:17:37  lr: 0.000029  loss: 0.2843  time: 1.4563  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2300/3000]  eta: 0:17:37  lr: 0.000029  loss: 0.1220  time: 1.4559  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2305/3000]  eta: 0:17:30  lr: 0.000029  loss: 0.4789  time: 1.4437  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2305/3000]  eta: 0:17:29  lr: 0.000029  loss: 0.4230  time: 1.4435  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2310/3000]  eta: 0:17:22  lr: 0.000029  loss: 0.1609  time: 1.4507  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2310/3000]  eta: 0:17:22  lr: 0.000029  loss: 0.1636  time: 1.4504  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2315/3000]  eta: 0:17:14  lr: 0.000029  loss: 0.1918  time: 1.4723  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2315/3000]  eta: 0:17:14  lr: 0.000029  loss: 0.2825  time: 1.4721  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2320/3000]  eta: 0:17:07  lr: 0.000029  loss: 0.3700  time: 1.4992  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2320/3000]  eta: 0:17:07  lr: 0.000029  loss: 0.1027  time: 1.4991  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2325/3000]  eta: 0:17:00  lr: 0.000029  loss: 0.7348  time: 1.5457  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2325/3000]  eta: 0:16:59  lr: 0.000029  loss: 0.2675  time: 1.5455  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2330/3000]  eta: 0:16:52  lr: 0.000029  loss: 0.5101  time: 1.5360  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2330/3000]  eta: 0:16:52  lr: 0.000029  loss: 0.8514  time: 1.5357  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2335/3000]  eta: 0:16:44  lr: 0.000029  loss: 0.3696  time: 1.4968  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2335/3000]  eta: 0:16:44  lr: 0.000029  loss: 0.2316  time: 1.4966  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2340/3000]  eta: 0:16:37  lr: 0.000029  loss: 0.2490  time: 1.4816  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2340/3000]  eta: 0:16:36  lr: 0.000029  loss: 0.2932  time: 1.4813  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2345/3000]  eta: 0:16:29  lr: 0.000029  loss: 0.1273  time: 1.4437  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2345/3000]  eta: 0:16:29  lr: 0.000029  loss: 0.8127  time: 1.4435  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2350/3000]  eta: 0:16:21  lr: 0.000029  loss: 0.5572  time: 1.4682  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2350/3000]  eta: 0:16:21  lr: 0.000029  loss: 0.5146  time: 1.4679  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2355/3000]  eta: 0:16:14  lr: 0.000029  loss: 0.3536  time: 1.5193  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2355/3000]  eta: 0:16:14  lr: 0.000029  loss: 0.5063  time: 1.5189  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2360/3000]  eta: 0:16:06  lr: 0.000029  loss: 0.7719  time: 1.5271  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2360/3000]  eta: 0:16:06  lr: 0.000029  loss: 0.3677  time: 1.5268  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2365/3000]  eta: 0:15:59  lr: 0.000029  loss: 0.4594  time: 1.5567  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2365/3000]  eta: 0:15:59  lr: 0.000029  loss: 0.3630  time: 1.5564  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2370/3000]  eta: 0:15:51  lr: 0.000029  loss: 0.7887  time: 1.5565  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2370/3000]  eta: 0:15:51  lr: 0.000029  loss: 0.5656  time: 1.5562  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2375/3000]  eta: 0:15:44  lr: 0.000029  loss: 0.6980  time: 1.5596  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2375/3000]  eta: 0:15:44  lr: 0.000029  loss: 0.3079  time: 1.5594  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2380/3000]  eta: 0:15:36  lr: 0.000029  loss: 0.2422  time: 1.5453  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2380/3000]  eta: 0:15:36  lr: 0.000029  loss: 0.1507  time: 1.5450  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2385/3000]  eta: 0:15:29  lr: 0.000029  loss: 0.1479  time: 1.5189  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2385/3000]  eta: 0:15:29  lr: 0.000029  loss: 0.5316  time: 1.5186  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2390/3000]  eta: 0:15:21  lr: 0.000029  loss: 0.5650  time: 1.4928  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2390/3000]  eta: 0:15:21  lr: 0.000029  loss: 0.3764  time: 1.4926  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2395/3000]  eta: 0:15:14  lr: 0.000029  loss: 0.3587  time: 1.4648  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2395/3000]  eta: 0:15:13  lr: 0.000029  loss: 0.2198  time: 1.4644  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2400/3000]  eta: 0:15:06  lr: 0.000029  loss: 0.6622  time: 1.4363  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2400/3000]  eta: 0:15:06  lr: 0.000029  loss: 0.1125  time: 1.4361  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2405/3000]  eta: 0:14:58  lr: 0.000029  loss: 0.7897  time: 1.4601  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2405/3000]  eta: 0:14:58  lr: 0.000029  loss: 0.3382  time: 1.4599  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2410/3000]  eta: 0:14:51  lr: 0.000029  loss: 0.3866  time: 1.4843  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2410/3000]  eta: 0:14:51  lr: 0.000029  loss: 0.2831  time: 1.4841  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2415/3000]  eta: 0:14:43  lr: 0.000029  loss: 0.5886  time: 1.4721  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2415/3000]  eta: 0:14:43  lr: 0.000029  loss: 0.5564  time: 1.4719  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2420/3000]  eta: 0:14:35  lr: 0.000029  loss: 0.3314  time: 1.4873  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2420/3000]  eta: 0:14:35  lr: 0.000029  loss: 0.2228  time: 1.4870  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2425/3000]  eta: 0:14:28  lr: 0.000029  loss: 0.1621  time: 1.4821  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2425/3000]  eta: 0:14:28  lr: 0.000029  loss: 0.7087  time: 1.4818  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2430/3000]  eta: 0:14:20  lr: 0.000029  loss: 0.4403  time: 1.4786  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2430/3000]  eta: 0:14:20  lr: 0.000029  loss: 0.1229  time: 1.4784  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2435/3000]  eta: 0:14:13  lr: 0.000029  loss: 0.3171  time: 1.5050  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2435/3000]  eta: 0:14:13  lr: 0.000029  loss: 0.7436  time: 1.5048  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2440/3000]  eta: 0:14:05  lr: 0.000029  loss: 0.2543  time: 1.5112  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2440/3000]  eta: 0:14:05  lr: 0.000029  loss: 0.4906  time: 1.5110  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2445/3000]  eta: 0:13:58  lr: 0.000029  loss: 0.4930  time: 1.5053  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2445/3000]  eta: 0:13:58  lr: 0.000029  loss: 0.2972  time: 1.5051  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2450/3000]  eta: 0:13:50  lr: 0.000029  loss: 0.3586  time: 1.4945  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2450/3000]  eta: 0:13:50  lr: 0.000029  loss: 0.8317  time: 1.4944  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2455/3000]  eta: 0:13:43  lr: 0.000029  loss: 0.1254  time: 1.4806  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2455/3000]  eta: 0:13:42  lr: 0.000029  loss: 0.3524  time: 1.4804  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2460/3000]  eta: 0:13:35  lr: 0.000029  loss: 0.3176  time: 1.4787  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2460/3000]  eta: 0:13:35  lr: 0.000029  loss: 0.5607  time: 1.4785  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2465/3000]  eta: 0:13:27  lr: 0.000029  loss: 0.2883  time: 1.4846  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2465/3000]  eta: 0:13:27  lr: 0.000029  loss: 0.4310  time: 1.4843  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2470/3000]  eta: 0:13:20  lr: 0.000029  loss: 0.4122  time: 1.5161  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2470/3000]  eta: 0:13:20  lr: 0.000029  loss: 0.3635  time: 1.5158  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2475/3000]  eta: 0:13:12  lr: 0.000029  loss: 0.3165  time: 1.5339  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2475/3000]  eta: 0:13:12  lr: 0.000029  loss: 0.2481  time: 1.5336  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2480/3000]  eta: 0:13:05  lr: 0.000029  loss: 0.1512  time: 1.5540  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2480/3000]  eta: 0:13:05  lr: 0.000029  loss: 0.6513  time: 1.5537  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2485/3000]  eta: 0:12:57  lr: 0.000029  loss: 0.1525  time: 1.5675  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2485/3000]  eta: 0:12:57  lr: 0.000029  loss: 0.6789  time: 1.5672  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2490/3000]  eta: 0:12:50  lr: 0.000029  loss: 0.5991  time: 1.5521  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2490/3000]  eta: 0:12:50  lr: 0.000029  loss: 0.8621  time: 1.5518  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2495/3000]  eta: 0:12:42  lr: 0.000029  loss: 0.4436  time: 1.5314  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2495/3000]  eta: 0:12:42  lr: 0.000029  loss: 0.6173  time: 1.5312  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2500/3000]  eta: 0:12:35  lr: 0.000029  loss: 0.4481  time: 1.5278  data: 0.0000  max mem: 18151Train: data epoch: [3]  [2500/3000]  eta: 0:12:35  lr: 0.000029  loss: 0.2817  time: 1.5282  data: 0.0000  max mem: 18432

Train: data epoch: [3]  [2505/3000]  eta: 0:12:27  lr: 0.000029  loss: 0.6152  time: 1.4994  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2505/3000]  eta: 0:12:27  lr: 0.000029  loss: 0.3001  time: 1.4992  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2510/3000]  eta: 0:12:20  lr: 0.000029  loss: 0.7076  time: 1.4845  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2510/3000]  eta: 0:12:19  lr: 0.000029  loss: 0.2227  time: 1.4843  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2515/3000]  eta: 0:12:12  lr: 0.000029  loss: 0.3937  time: 1.4918  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2515/3000]  eta: 0:12:12  lr: 0.000029  loss: 1.1334  time: 1.4915  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2520/3000]  eta: 0:12:05  lr: 0.000029  loss: 0.1171  time: 1.4974  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2520/3000]  eta: 0:12:04  lr: 0.000029  loss: 1.6327  time: 1.4973  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2525/3000]  eta: 0:11:57  lr: 0.000029  loss: 0.1693  time: 1.4937  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2525/3000]  eta: 0:11:57  lr: 0.000029  loss: 0.4709  time: 1.4935  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2530/3000]  eta: 0:11:49  lr: 0.000029  loss: 0.3493  time: 1.4852  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2530/3000]  eta: 0:11:49  lr: 0.000029  loss: 0.2428  time: 1.4849  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2535/3000]  eta: 0:11:42  lr: 0.000029  loss: 0.2643  time: 1.4639  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2535/3000]  eta: 0:11:42  lr: 0.000029  loss: 0.4673  time: 1.4637  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2540/3000]  eta: 0:11:34  lr: 0.000029  loss: 0.3763  time: 1.4485  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2540/3000]  eta: 0:11:34  lr: 0.000029  loss: 0.8432  time: 1.4482  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2545/3000]  eta: 0:11:27  lr: 0.000029  loss: 0.1949  time: 1.4802  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2545/3000]  eta: 0:11:26  lr: 0.000029  loss: 0.3617  time: 1.4800  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2550/3000]  eta: 0:11:19  lr: 0.000029  loss: 0.4077  time: 1.4861  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2550/3000]  eta: 0:11:19  lr: 0.000029  loss: 0.1821  time: 1.4858  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2555/3000]  eta: 0:11:11  lr: 0.000029  loss: 0.5663  time: 1.4981  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2555/3000]  eta: 0:11:11  lr: 0.000029  loss: 0.2891  time: 1.4978  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2560/3000]  eta: 0:11:04  lr: 0.000029  loss: 0.4780  time: 1.5105  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2560/3000]  eta: 0:11:04  lr: 0.000029  loss: 0.2837  time: 1.5102  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2565/3000]  eta: 0:10:56  lr: 0.000029  loss: 0.1341  time: 1.4945  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2565/3000]  eta: 0:10:56  lr: 0.000029  loss: 0.5373  time: 1.4941  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2570/3000]  eta: 0:10:49  lr: 0.000029  loss: 0.4675  time: 1.4876  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2570/3000]  eta: 0:10:49  lr: 0.000029  loss: 0.5505  time: 1.4873  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2575/3000]  eta: 0:10:41  lr: 0.000029  loss: 0.5945  time: 1.4784  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2575/3000]  eta: 0:10:41  lr: 0.000029  loss: 0.5667  time: 1.4782  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2580/3000]  eta: 0:10:34  lr: 0.000029  loss: 0.1627  time: 1.4845  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2580/3000]  eta: 0:10:33  lr: 0.000029  loss: 0.4951  time: 1.4842  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2585/3000]  eta: 0:10:26  lr: 0.000029  loss: 0.2094  time: 1.4966  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2585/3000]  eta: 0:10:26  lr: 0.000029  loss: 0.3474  time: 1.4964  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2590/3000]  eta: 0:10:19  lr: 0.000029  loss: 0.5206  time: 1.5011  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2590/3000]  eta: 0:10:18  lr: 0.000029  loss: 0.1864  time: 1.5009  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2595/3000]  eta: 0:10:11  lr: 0.000029  loss: 0.8365  time: 1.5130  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2595/3000]  eta: 0:10:11  lr: 0.000029  loss: 0.5586  time: 1.5127  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2600/3000]  eta: 0:10:03  lr: 0.000029  loss: 0.3211  time: 1.4948  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2600/3000]  eta: 0:10:03  lr: 0.000029  loss: 0.4481  time: 1.4946  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2605/3000]  eta: 0:09:56  lr: 0.000029  loss: 0.5950  time: 1.5041  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2605/3000]  eta: 0:09:56  lr: 0.000029  loss: 0.1398  time: 1.5038  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2610/3000]  eta: 0:09:48  lr: 0.000029  loss: 0.7668  time: 1.5258  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2610/3000]  eta: 0:09:48  lr: 0.000029  loss: 0.6148  time: 1.5254  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2615/3000]  eta: 0:09:41  lr: 0.000029  loss: 0.3276  time: 1.5436  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2615/3000]  eta: 0:09:41  lr: 0.000029  loss: 0.2867  time: 1.5434  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2620/3000]  eta: 0:09:33  lr: 0.000029  loss: 0.2910  time: 1.5338  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2620/3000]  eta: 0:09:33  lr: 0.000029  loss: 0.2474  time: 1.5335  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2625/3000]  eta: 0:09:26  lr: 0.000029  loss: 0.1012  time: 1.4840  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2625/3000]  eta: 0:09:26  lr: 0.000029  loss: 0.5742  time: 1.4838  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2630/3000]  eta: 0:09:18  lr: 0.000029  loss: 0.1072  time: 1.4901  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2630/3000]  eta: 0:09:18  lr: 0.000029  loss: 0.3985  time: 1.4899  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2635/3000]  eta: 0:09:11  lr: 0.000029  loss: 0.6129  time: 1.4788  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2635/3000]  eta: 0:09:10  lr: 0.000029  loss: 0.2873  time: 1.4785  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2640/3000]  eta: 0:09:03  lr: 0.000029  loss: 0.4055  time: 1.5021  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2640/3000]  eta: 0:09:03  lr: 0.000029  loss: 0.6236  time: 1.5018  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2645/3000]  eta: 0:08:55  lr: 0.000029  loss: 0.2157  time: 1.5350  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2645/3000]  eta: 0:08:55  lr: 0.000029  loss: 0.1305  time: 1.5346  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2650/3000]  eta: 0:08:48  lr: 0.000029  loss: 0.4058  time: 1.5348  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2650/3000]  eta: 0:08:48  lr: 0.000029  loss: 0.2378  time: 1.5344  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2655/3000]  eta: 0:08:40  lr: 0.000029  loss: 0.4037  time: 1.5350  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2655/3000]  eta: 0:08:40  lr: 0.000029  loss: 0.1026  time: 1.5348  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2660/3000]  eta: 0:08:33  lr: 0.000029  loss: 0.2883  time: 1.5070  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2660/3000]  eta: 0:08:33  lr: 0.000029  loss: 0.2342  time: 1.5067  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2665/3000]  eta: 0:08:25  lr: 0.000029  loss: 0.7165  time: 1.4998  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2665/3000]  eta: 0:08:25  lr: 0.000029  loss: 0.7230  time: 1.4995  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2670/3000]  eta: 0:08:18  lr: 0.000029  loss: 0.2352  time: 1.4997  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2670/3000]  eta: 0:08:18  lr: 0.000029  loss: 0.6872  time: 1.4995  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2675/3000]  eta: 0:08:10  lr: 0.000029  loss: 0.2691  time: 1.4972  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2675/3000]  eta: 0:08:10  lr: 0.000029  loss: 0.6700  time: 1.4969  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2680/3000]  eta: 0:08:03  lr: 0.000029  loss: 0.5788  time: 1.5072  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2680/3000]  eta: 0:08:03  lr: 0.000029  loss: 0.5139  time: 1.5068  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2685/3000]  eta: 0:07:55  lr: 0.000029  loss: 0.4778  time: 1.5150  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2685/3000]  eta: 0:07:55  lr: 0.000029  loss: 0.2686  time: 1.5147  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2690/3000]  eta: 0:07:48  lr: 0.000029  loss: 0.5709  time: 1.4946  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2690/3000]  eta: 0:07:47  lr: 0.000029  loss: 0.4331  time: 1.4943  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2695/3000]  eta: 0:07:40  lr: 0.000029  loss: 0.5370  time: 1.4834  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2695/3000]  eta: 0:07:40  lr: 0.000029  loss: 0.8371  time: 1.4832  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2700/3000]  eta: 0:07:32  lr: 0.000029  loss: 0.3162  time: 1.4974  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2700/3000]  eta: 0:07:32  lr: 0.000029  loss: 0.2496  time: 1.4972  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2705/3000]  eta: 0:07:25  lr: 0.000029  loss: 0.2765  time: 1.4918  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2705/3000]  eta: 0:07:25  lr: 0.000029  loss: 1.2351  time: 1.4916  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2710/3000]  eta: 0:07:17  lr: 0.000029  loss: 0.1096  time: 1.5132  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2710/3000]  eta: 0:07:17  lr: 0.000029  loss: 0.2321  time: 1.5131  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2715/3000]  eta: 0:07:10  lr: 0.000029  loss: 0.7537  time: 1.5349  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2715/3000]  eta: 0:07:10  lr: 0.000029  loss: 0.5648  time: 1.5347  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2720/3000]  eta: 0:07:02  lr: 0.000029  loss: 0.5829  time: 1.5400  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2720/3000]  eta: 0:07:02  lr: 0.000029  loss: 0.5159  time: 1.5398  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2725/3000]  eta: 0:06:55  lr: 0.000029  loss: 0.7762  time: 1.5569  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2725/3000]  eta: 0:06:55  lr: 0.000029  loss: 1.3226  time: 1.5567  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2730/3000]  eta: 0:06:47  lr: 0.000029  loss: 1.0181  time: 1.5490  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2730/3000]  eta: 0:06:47  lr: 0.000029  loss: 0.9088  time: 1.5487  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2735/3000]  eta: 0:06:40  lr: 0.000029  loss: 0.1772  time: 1.5406  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2735/3000]  eta: 0:06:40  lr: 0.000029  loss: 0.9786  time: 1.5404  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2740/3000]  eta: 0:06:32  lr: 0.000029  loss: 0.5213  time: 1.5360  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2740/3000]  eta: 0:06:32  lr: 0.000029  loss: 0.2249  time: 1.5357  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2745/3000]  eta: 0:06:25  lr: 0.000029  loss: 1.1429  time: 1.5228  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2745/3000]  eta: 0:06:25  lr: 0.000029  loss: 0.1997  time: 1.5225  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2750/3000]  eta: 0:06:17  lr: 0.000029  loss: 0.3639  time: 1.5272  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2750/3000]  eta: 0:06:17  lr: 0.000029  loss: 0.3732  time: 1.5269  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2755/3000]  eta: 0:06:10  lr: 0.000029  loss: 0.3652  time: 1.5369  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2755/3000]  eta: 0:06:09  lr: 0.000029  loss: 0.3749  time: 1.5366  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2760/3000]  eta: 0:06:02  lr: 0.000029  loss: 0.6073  time: 1.5379  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2760/3000]  eta: 0:06:02  lr: 0.000029  loss: 0.9066  time: 1.5376  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2765/3000]  eta: 0:05:54  lr: 0.000029  loss: 0.6476  time: 1.5587  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2765/3000]  eta: 0:05:54  lr: 0.000029  loss: 0.6007  time: 1.5584  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2770/3000]  eta: 0:05:47  lr: 0.000029  loss: 0.1234  time: 1.5574  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2770/3000]  eta: 0:05:47  lr: 0.000029  loss: 0.1716  time: 1.5571  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2775/3000]  eta: 0:05:39  lr: 0.000029  loss: 0.6393  time: 1.5595  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2775/3000]  eta: 0:05:39  lr: 0.000029  loss: 1.0729  time: 1.5593  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2780/3000]  eta: 0:05:32  lr: 0.000029  loss: 0.9034  time: 1.5417  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2780/3000]  eta: 0:05:32  lr: 0.000029  loss: 0.5913  time: 1.5415  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2785/3000]  eta: 0:05:24  lr: 0.000029  loss: 0.3675  time: 1.5259  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2785/3000]  eta: 0:05:24  lr: 0.000029  loss: 1.0569  time: 1.5256  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2790/3000]  eta: 0:05:17  lr: 0.000029  loss: 0.3081  time: 1.5125  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2790/3000]  eta: 0:05:17  lr: 0.000029  loss: 0.3287  time: 1.5123  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2795/3000]  eta: 0:05:09  lr: 0.000029  loss: 0.2357  time: 1.4685  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2795/3000]  eta: 0:05:09  lr: 0.000029  loss: 0.2422  time: 1.4682  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2800/3000]  eta: 0:05:02  lr: 0.000029  loss: 0.1389  time: 1.4742  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2800/3000]  eta: 0:05:01  lr: 0.000029  loss: 0.4859  time: 1.4740  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2805/3000]  eta: 0:04:54  lr: 0.000029  loss: 0.3520  time: 1.4487  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2805/3000]  eta: 0:04:54  lr: 0.000029  loss: 0.8685  time: 1.4484  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2810/3000]  eta: 0:04:46  lr: 0.000029  loss: 0.1289  time: 1.4667  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2810/3000]  eta: 0:04:46  lr: 0.000029  loss: 0.6762  time: 1.4665  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2815/3000]  eta: 0:04:39  lr: 0.000029  loss: 0.7522  time: 1.5029  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2815/3000]  eta: 0:04:39  lr: 0.000029  loss: 0.6920  time: 1.5027  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2820/3000]  eta: 0:04:31  lr: 0.000029  loss: 0.1316  time: 1.4830  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2820/3000]  eta: 0:04:31  lr: 0.000029  loss: 0.5192  time: 1.4828  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2825/3000]  eta: 0:04:24  lr: 0.000029  loss: 0.2608  time: 1.4975  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2825/3000]  eta: 0:04:24  lr: 0.000029  loss: 0.5171  time: 1.4973  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2830/3000]  eta: 0:04:16  lr: 0.000029  loss: 0.7181  time: 1.4973  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2830/3000]  eta: 0:04:16  lr: 0.000029  loss: 0.3562  time: 1.4970  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2835/3000]  eta: 0:04:09  lr: 0.000029  loss: 0.1835  time: 1.4743  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2835/3000]  eta: 0:04:09  lr: 0.000029  loss: 0.1955  time: 1.4740  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2840/3000]  eta: 0:04:01  lr: 0.000029  loss: 1.3787  time: 1.5103  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2840/3000]  eta: 0:04:01  lr: 0.000029  loss: 0.2415  time: 1.5102  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2845/3000]  eta: 0:03:54  lr: 0.000029  loss: 0.9134  time: 1.5120  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2845/3000]  eta: 0:03:54  lr: 0.000029  loss: 0.3757  time: 1.5117  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2850/3000]  eta: 0:03:46  lr: 0.000029  loss: 0.2804  time: 1.5157  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2850/3000]  eta: 0:03:46  lr: 0.000029  loss: 0.3926  time: 1.5154  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2855/3000]  eta: 0:03:38  lr: 0.000029  loss: 0.3126  time: 1.5442  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2855/3000]  eta: 0:03:38  lr: 0.000029  loss: 0.5551  time: 1.5439  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2860/3000]  eta: 0:03:31  lr: 0.000029  loss: 0.1590  time: 1.5476  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2860/3000]  eta: 0:03:31  lr: 0.000029  loss: 0.6640  time: 1.5473  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2865/3000]  eta: 0:03:23  lr: 0.000029  loss: 0.5074  time: 1.5535  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2865/3000]  eta: 0:03:23  lr: 0.000029  loss: 1.2162  time: 1.5533  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2870/3000]  eta: 0:03:16  lr: 0.000029  loss: 0.4224  time: 1.5164  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2870/3000]  eta: 0:03:16  lr: 0.000029  loss: 0.5097  time: 1.5162  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2875/3000]  eta: 0:03:08  lr: 0.000029  loss: 0.5943  time: 1.5216  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2875/3000]  eta: 0:03:08  lr: 0.000029  loss: 0.8388  time: 1.5214  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2880/3000]  eta: 0:03:01  lr: 0.000029  loss: 0.4523  time: 1.5287  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2880/3000]  eta: 0:03:01  lr: 0.000029  loss: 0.7251  time: 1.5284  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2885/3000]  eta: 0:02:53  lr: 0.000029  loss: 0.2926  time: 1.5384  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2885/3000]  eta: 0:02:53  lr: 0.000029  loss: 0.4171  time: 1.5382  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2890/3000]  eta: 0:02:46  lr: 0.000029  loss: 0.5238  time: 1.5397  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2890/3000]  eta: 0:02:46  lr: 0.000029  loss: 0.1329  time: 1.5394  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2895/3000]  eta: 0:02:38  lr: 0.000029  loss: 0.3736  time: 1.5320  data: 0.0000  max mem: 18432Train: data epoch: [3]  [2895/3000]  eta: 0:02:38  lr: 0.000029  loss: 0.3645  time: 1.5317  data: 0.0000  max mem: 18151

Train: data epoch: [3]  [2900/3000]  eta: 0:02:31  lr: 0.000029  loss: 0.4531  time: 1.5072  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2900/3000]  eta: 0:02:31  lr: 0.000029  loss: 0.3719  time: 1.5070  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2905/3000]  eta: 0:02:23  lr: 0.000029  loss: 0.5007  time: 1.4865  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2905/3000]  eta: 0:02:23  lr: 0.000029  loss: 0.1167  time: 1.4862  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2910/3000]  eta: 0:02:15  lr: 0.000029  loss: 0.0751  time: 1.5080  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2910/3000]  eta: 0:02:15  lr: 0.000029  loss: 0.7697  time: 1.5078  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2915/3000]  eta: 0:02:08  lr: 0.000029  loss: 0.5355  time: 1.5120  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2915/3000]  eta: 0:02:08  lr: 0.000029  loss: 0.7133  time: 1.5118  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2920/3000]  eta: 0:02:00  lr: 0.000029  loss: 0.6827  time: 1.5128  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2920/3000]  eta: 0:02:00  lr: 0.000029  loss: 1.0281  time: 1.5126  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2925/3000]  eta: 0:01:53  lr: 0.000029  loss: 0.6035  time: 1.5249  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2925/3000]  eta: 0:01:53  lr: 0.000029  loss: 0.1479  time: 1.5247  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2930/3000]  eta: 0:01:45  lr: 0.000029  loss: 0.3780  time: 1.5397  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2930/3000]  eta: 0:01:45  lr: 0.000029  loss: 1.5048  time: 1.5395  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2935/3000]  eta: 0:01:38  lr: 0.000029  loss: 0.2879  time: 1.5484  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2935/3000]  eta: 0:01:38  lr: 0.000029  loss: 0.7698  time: 1.5482  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2940/3000]  eta: 0:01:30  lr: 0.000029  loss: 0.3483  time: 1.5314  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2940/3000]  eta: 0:01:30  lr: 0.000029  loss: 0.2331  time: 1.5311  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2945/3000]  eta: 0:01:23  lr: 0.000029  loss: 0.7843  time: 1.5296  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2945/3000]  eta: 0:01:23  lr: 0.000029  loss: 0.2554  time: 1.5293  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2950/3000]  eta: 0:01:15  lr: 0.000029  loss: 0.4374  time: 1.5186  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2950/3000]  eta: 0:01:15  lr: 0.000029  loss: 0.8925  time: 1.5182  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2955/3000]  eta: 0:01:07  lr: 0.000029  loss: 0.5856  time: 1.5029  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2955/3000]  eta: 0:01:07  lr: 0.000029  loss: 0.3059  time: 1.5026  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2960/3000]  eta: 0:01:00  lr: 0.000029  loss: 0.3936  time: 1.5291  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2960/3000]  eta: 0:01:00  lr: 0.000029  loss: 0.4063  time: 1.5288  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2965/3000]  eta: 0:00:52  lr: 0.000029  loss: 0.6156  time: 1.5374  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2965/3000]  eta: 0:00:52  lr: 0.000029  loss: 0.5654  time: 1.5370  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2970/3000]  eta: 0:00:45  lr: 0.000029  loss: 0.3570  time: 1.5358  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2970/3000]  eta: 0:00:45  lr: 0.000029  loss: 0.1314  time: 1.5355  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2975/3000]  eta: 0:00:37  lr: 0.000029  loss: 0.3141  time: 1.5197  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2975/3000]  eta: 0:00:37  lr: 0.000029  loss: 0.1507  time: 1.5194  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2980/3000]  eta: 0:00:30  lr: 0.000029  loss: 0.3528  time: 1.5193  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2980/3000]  eta: 0:00:30  lr: 0.000029  loss: 0.3017  time: 1.5191  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2985/3000]  eta: 0:00:22  lr: 0.000029  loss: 0.3045  time: 1.5194  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2985/3000]  eta: 0:00:22  lr: 0.000029  loss: 0.4623  time: 1.5192  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2990/3000]  eta: 0:00:15  lr: 0.000029  loss: 0.5905  time: 1.5063  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2990/3000]  eta: 0:00:15  lr: 0.000029  loss: 0.5911  time: 1.5061  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2995/3000]  eta: 0:00:07  lr: 0.000029  loss: 0.2288  time: 1.5351  data: 0.0000  max mem: 18432
Train: data epoch: [3]  [2995/3000]  eta: 0:00:07  lr: 0.000029  loss: 0.3921  time: 1.5340  data: 0.0000  max mem: 18151
Train: data epoch: [3]  [2999/3000]  eta: 0:00:01  lr: 0.000029  loss: 0.2115  time: 1.5240  data: 0.0000  max mem: 18432
Train: data epoch: [3] Total time: 1:15:32 (1.5110 s / it)
Train: data epoch: [3]  [2999/3000]  eta: 0:00:01  lr: 0.000029  loss: 0.2815  time: 1.5230  data: 0.0000  max mem: 18151
Train: data epoch: [3] Total time: 1:15:32 (1.5110 s / it)
2025-01-19 04:43:19,562 [INFO] Averaged stats: lr: 0.0000  loss: 0.4342
2025-01-19 04:43:19,569 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [3]  [0/1]  eta: 0:00:00    time: 0.7551  data: 0.4725  max mem: 18151
Eval: data epoch: [3] Total time: 0:00:00 (0.8679 s / it)
Eval: data epoch: [3]  [0/1]  eta: 0:00:00    time: 0.9358  data: 0.6423  max mem: 18432
Eval: data epoch: [3] Total time: 0:00:01 (1.0767 s / it)
2025-01-19 04:43:20,669 [INFO] Saving checkpoint at epoch 3 to outputs_stage1_only/202501182338/checkpoint_3.pth.
2025-01-19 04:43:23,052 [INFO] Training Phase
2025-01-19 04:43:23,060 [INFO] Start training epoch 4, 3000 iters per inner epoch.
Train: data epoch: [4]  [   0/3000]  eta: 1:18:44  lr: 0.000029  loss: 0.4414  time: 1.5748  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [   0/3000]  eta: 1:18:43  lr: 0.000029  loss: 0.6636  time: 1.5745  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [   5/3000]  eta: 1:16:46  lr: 0.000029  loss: 0.2048  time: 1.5381  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [   5/3000]  eta: 1:16:46  lr: 0.000029  loss: 0.4471  time: 1.5380  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [  10/3000]  eta: 1:15:39  lr: 0.000029  loss: 0.4867  time: 1.5182  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [  10/3000]  eta: 1:15:38  lr: 0.000029  loss: 1.1708  time: 1.5179  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [  15/3000]  eta: 1:15:49  lr: 0.000029  loss: 0.7882  time: 1.5240  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [  15/3000]  eta: 1:15:48  lr: 0.000029  loss: 0.4256  time: 1.5237  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [  20/3000]  eta: 1:14:10  lr: 0.000029  loss: 0.5167  time: 1.4895  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [  20/3000]  eta: 1:14:10  lr: 0.000029  loss: 0.6087  time: 1.4893  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [  25/3000]  eta: 1:14:38  lr: 0.000029  loss: 0.5551  time: 1.4955  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [  25/3000]  eta: 1:14:37  lr: 0.000029  loss: 0.4484  time: 1.4953  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [  30/3000]  eta: 1:14:38  lr: 0.000029  loss: 0.5135  time: 1.5025  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [  30/3000]  eta: 1:14:38  lr: 0.000029  loss: 1.0028  time: 1.5022  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [  35/3000]  eta: 1:14:37  lr: 0.000029  loss: 1.2757  time: 1.4992  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [  35/3000]  eta: 1:14:37  lr: 0.000029  loss: 0.3466  time: 1.4990  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [  40/3000]  eta: 1:14:36  lr: 0.000029  loss: 0.6215  time: 1.5317  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [  40/3000]  eta: 1:14:35  lr: 0.000029  loss: 0.3600  time: 1.5315  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [  45/3000]  eta: 1:14:23  lr: 0.000029  loss: 0.6824  time: 1.5168  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [  45/3000]  eta: 1:14:22  lr: 0.000029  loss: 0.1049  time: 1.5166  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [  50/3000]  eta: 1:14:12  lr: 0.000029  loss: 0.3048  time: 1.5112  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [  50/3000]  eta: 1:14:11  lr: 0.000029  loss: 0.7667  time: 1.5110  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [  55/3000]  eta: 1:14:03  lr: 0.000029  loss: 0.4436  time: 1.5066  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [  55/3000]  eta: 1:14:03  lr: 0.000029  loss: 0.4512  time: 1.5063  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [  60/3000]  eta: 1:13:56  lr: 0.000029  loss: 0.4090  time: 1.5030  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [  60/3000]  eta: 1:13:56  lr: 0.000029  loss: 0.3102  time: 1.5027  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [  65/3000]  eta: 1:14:05  lr: 0.000029  loss: 0.3732  time: 1.5246  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [  65/3000]  eta: 1:14:04  lr: 0.000029  loss: 0.5556  time: 1.5244  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [  70/3000]  eta: 1:13:41  lr: 0.000029  loss: 0.8076  time: 1.5087  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [  70/3000]  eta: 1:13:41  lr: 0.000029  loss: 0.5419  time: 1.5085  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [  75/3000]  eta: 1:13:37  lr: 0.000029  loss: 0.7690  time: 1.5134  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [  75/3000]  eta: 1:13:36  lr: 0.000029  loss: 0.8946  time: 1.5132  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [  80/3000]  eta: 1:13:27  lr: 0.000029  loss: 0.2576  time: 1.5099  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [  80/3000]  eta: 1:13:26  lr: 0.000029  loss: 0.2710  time: 1.5096  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [  85/3000]  eta: 1:13:31  lr: 0.000029  loss: 0.5588  time: 1.5096  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [  85/3000]  eta: 1:13:31  lr: 0.000029  loss: 0.3753  time: 1.5093  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [  90/3000]  eta: 1:13:25  lr: 0.000029  loss: 0.4880  time: 1.5311  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [  90/3000]  eta: 1:13:24  lr: 0.000029  loss: 0.3745  time: 1.5308  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [  95/3000]  eta: 1:13:05  lr: 0.000029  loss: 0.5258  time: 1.5086  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [  95/3000]  eta: 1:13:05  lr: 0.000029  loss: 0.5707  time: 1.5084  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 100/3000]  eta: 1:13:05  lr: 0.000029  loss: 0.6662  time: 1.5233  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 100/3000]  eta: 1:13:04  lr: 0.000029  loss: 0.4140  time: 1.5231  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 105/3000]  eta: 1:13:03  lr: 0.000029  loss: 0.2368  time: 1.5162  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 105/3000]  eta: 1:13:02  lr: 0.000029  loss: 0.4541  time: 1.5159  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 110/3000]  eta: 1:13:03  lr: 0.000029  loss: 0.3301  time: 1.5289  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 110/3000]  eta: 1:13:02  lr: 0.000029  loss: 0.7540  time: 1.5285  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 115/3000]  eta: 1:13:02  lr: 0.000029  loss: 0.2690  time: 1.5645  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 115/3000]  eta: 1:13:02  lr: 0.000029  loss: 0.2710  time: 1.5641  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 120/3000]  eta: 1:12:44  lr: 0.000029  loss: 0.2780  time: 1.5314  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 120/3000]  eta: 1:12:43  lr: 0.000029  loss: 0.2953  time: 1.5311  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 125/3000]  eta: 1:12:32  lr: 0.000029  loss: 0.4114  time: 1.5127  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 125/3000]  eta: 1:12:31  lr: 0.000029  loss: 0.6513  time: 1.5125  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 130/3000]  eta: 1:12:28  lr: 0.000029  loss: 0.4243  time: 1.5072  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 130/3000]  eta: 1:12:27  lr: 0.000029  loss: 0.1885  time: 1.5070  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 135/3000]  eta: 1:12:18  lr: 0.000029  loss: 0.2185  time: 1.4849  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 135/3000]  eta: 1:12:17  lr: 0.000029  loss: 0.6453  time: 1.4846  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 140/3000]  eta: 1:12:05  lr: 0.000029  loss: 0.2185  time: 1.4942  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 140/3000]  eta: 1:12:04  lr: 0.000029  loss: 0.3155  time: 1.4938  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 145/3000]  eta: 1:11:53  lr: 0.000029  loss: 0.4077  time: 1.4948  data: 0.0000  max mem: 18151Train: data epoch: [4]  [ 145/3000]  eta: 1:11:54  lr: 0.000029  loss: 0.3124  time: 1.4952  data: 0.0000  max mem: 18432

Train: data epoch: [4]  [ 150/3000]  eta: 1:11:43  lr: 0.000029  loss: 0.4869  time: 1.4765  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 150/3000]  eta: 1:11:42  lr: 0.000029  loss: 0.3845  time: 1.4762  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 155/3000]  eta: 1:11:43  lr: 0.000029  loss: 0.4028  time: 1.5028  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 155/3000]  eta: 1:11:42  lr: 0.000029  loss: 0.4794  time: 1.5027  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 160/3000]  eta: 1:11:39  lr: 0.000029  loss: 0.8846  time: 1.5244  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 160/3000]  eta: 1:11:38  lr: 0.000029  loss: 0.6948  time: 1.5242  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 165/3000]  eta: 1:11:38  lr: 0.000029  loss: 0.2951  time: 1.5525  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 165/3000]  eta: 1:11:37  lr: 0.000029  loss: 0.4994  time: 1.5524  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 170/3000]  eta: 1:11:27  lr: 0.000029  loss: 0.1363  time: 1.5517  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 170/3000]  eta: 1:11:26  lr: 0.000029  loss: 0.0488  time: 1.5514  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 175/3000]  eta: 1:11:10  lr: 0.000029  loss: 0.2823  time: 1.5066  data: 0.0000  max mem: 18151Train: data epoch: [4]  [ 175/3000]  eta: 1:11:11  lr: 0.000029  loss: 0.6219  time: 1.5070  data: 0.0000  max mem: 18432

Train: data epoch: [4]  [ 180/3000]  eta: 1:11:05  lr: 0.000029  loss: 0.5567  time: 1.5039  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 180/3000]  eta: 1:11:05  lr: 0.000029  loss: 0.2663  time: 1.5036  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 185/3000]  eta: 1:10:53  lr: 0.000029  loss: 0.9515  time: 1.4688  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 185/3000]  eta: 1:10:53  lr: 0.000029  loss: 1.0338  time: 1.4685  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 190/3000]  eta: 1:10:44  lr: 0.000029  loss: 0.4615  time: 1.4715  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 190/3000]  eta: 1:10:43  lr: 0.000029  loss: 0.2570  time: 1.4712  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 195/3000]  eta: 1:10:37  lr: 0.000029  loss: 0.1396  time: 1.4995  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 195/3000]  eta: 1:10:37  lr: 0.000029  loss: 0.3489  time: 1.4993  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 200/3000]  eta: 1:10:24  lr: 0.000029  loss: 0.8154  time: 1.4740  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 200/3000]  eta: 1:10:24  lr: 0.000029  loss: 0.1826  time: 1.4738  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 205/3000]  eta: 1:10:19  lr: 0.000029  loss: 0.4087  time: 1.4980  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 205/3000]  eta: 1:10:19  lr: 0.000029  loss: 0.5104  time: 1.4977  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 210/3000]  eta: 1:10:15  lr: 0.000029  loss: 0.8207  time: 1.5150  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 210/3000]  eta: 1:10:14  lr: 0.000029  loss: 0.4273  time: 1.5148  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 215/3000]  eta: 1:10:02  lr: 0.000029  loss: 0.3122  time: 1.4916  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 215/3000]  eta: 1:10:01  lr: 0.000029  loss: 0.6188  time: 1.4913  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 220/3000]  eta: 1:09:56  lr: 0.000029  loss: 0.2342  time: 1.5162  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 220/3000]  eta: 1:09:55  lr: 0.000029  loss: 0.4665  time: 1.5160  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 225/3000]  eta: 1:09:47  lr: 0.000029  loss: 0.1894  time: 1.5015  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 225/3000]  eta: 1:09:47  lr: 0.000029  loss: 0.2902  time: 1.5012  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 230/3000]  eta: 1:09:37  lr: 0.000029  loss: 0.4136  time: 1.4808  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 230/3000]  eta: 1:09:37  lr: 0.000029  loss: 0.6335  time: 1.4806  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 235/3000]  eta: 1:09:31  lr: 0.000029  loss: 0.8796  time: 1.5053  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 235/3000]  eta: 1:09:30  lr: 0.000029  loss: 0.4577  time: 1.5051  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 240/3000]  eta: 1:09:21  lr: 0.000029  loss: 0.1693  time: 1.4875  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 240/3000]  eta: 1:09:20  lr: 0.000029  loss: 0.2446  time: 1.4872  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 245/3000]  eta: 1:09:14  lr: 0.000029  loss: 0.4294  time: 1.4964  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 245/3000]  eta: 1:09:14  lr: 0.000029  loss: 0.5078  time: 1.4962  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 250/3000]  eta: 1:09:07  lr: 0.000029  loss: 0.1757  time: 1.5084  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 250/3000]  eta: 1:09:06  lr: 0.000029  loss: 0.3020  time: 1.5081  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 255/3000]  eta: 1:09:00  lr: 0.000029  loss: 0.6256  time: 1.5038  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 255/3000]  eta: 1:08:59  lr: 0.000029  loss: 0.3394  time: 1.5034  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 260/3000]  eta: 1:08:51  lr: 0.000029  loss: 0.8090  time: 1.5081  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 260/3000]  eta: 1:08:50  lr: 0.000029  loss: 0.5005  time: 1.5078  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 265/3000]  eta: 1:08:43  lr: 0.000029  loss: 0.2289  time: 1.5039  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 265/3000]  eta: 1:08:43  lr: 0.000029  loss: 0.4667  time: 1.5037  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 270/3000]  eta: 1:08:33  lr: 0.000029  loss: 0.5208  time: 1.4907  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 270/3000]  eta: 1:08:33  lr: 0.000029  loss: 0.1388  time: 1.4904  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 275/3000]  eta: 1:08:27  lr: 0.000029  loss: 0.9490  time: 1.4930  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 275/3000]  eta: 1:08:26  lr: 0.000029  loss: 0.2145  time: 1.4928  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 280/3000]  eta: 1:08:22  lr: 0.000029  loss: 0.3601  time: 1.5154  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 280/3000]  eta: 1:08:21  lr: 0.000029  loss: 0.6676  time: 1.5151  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 285/3000]  eta: 1:08:14  lr: 0.000029  loss: 0.4994  time: 1.5107  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 285/3000]  eta: 1:08:13  lr: 0.000029  loss: 0.5302  time: 1.5105  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 290/3000]  eta: 1:08:08  lr: 0.000029  loss: 0.3299  time: 1.5300  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 290/3000]  eta: 1:08:07  lr: 0.000029  loss: 0.5278  time: 1.5298  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 295/3000]  eta: 1:08:00  lr: 0.000029  loss: 0.5940  time: 1.5241  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 295/3000]  eta: 1:07:59  lr: 0.000029  loss: 0.3700  time: 1.5238  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 300/3000]  eta: 1:07:54  lr: 0.000029  loss: 0.4091  time: 1.5218  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 300/3000]  eta: 1:07:54  lr: 0.000029  loss: 0.1454  time: 1.5215  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 305/3000]  eta: 1:07:49  lr: 0.000029  loss: 0.2087  time: 1.5388  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 305/3000]  eta: 1:07:48  lr: 0.000029  loss: 0.4359  time: 1.5385  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 310/3000]  eta: 1:07:38  lr: 0.000029  loss: 0.3785  time: 1.5146  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 310/3000]  eta: 1:07:38  lr: 0.000029  loss: 0.2654  time: 1.5143  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 315/3000]  eta: 1:07:29  lr: 0.000029  loss: 0.1797  time: 1.5055  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 315/3000]  eta: 1:07:28  lr: 0.000029  loss: 0.6510  time: 1.5053  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 320/3000]  eta: 1:07:17  lr: 0.000029  loss: 0.5433  time: 1.4676  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 320/3000]  eta: 1:07:16  lr: 0.000029  loss: 0.3474  time: 1.4672  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 325/3000]  eta: 1:07:07  lr: 0.000029  loss: 0.5343  time: 1.4400  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 325/3000]  eta: 1:07:07  lr: 0.000029  loss: 0.1101  time: 1.4397  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 330/3000]  eta: 1:07:02  lr: 0.000029  loss: 0.8990  time: 1.4708  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 330/3000]  eta: 1:07:01  lr: 0.000029  loss: 0.3317  time: 1.4706  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 335/3000]  eta: 1:06:57  lr: 0.000029  loss: 0.3570  time: 1.4957  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 335/3000]  eta: 1:06:56  lr: 0.000029  loss: 0.1206  time: 1.4954  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 340/3000]  eta: 1:06:51  lr: 0.000029  loss: 0.2910  time: 1.5299  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 340/3000]  eta: 1:06:50  lr: 0.000029  loss: 0.1192  time: 1.5296  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 345/3000]  eta: 1:06:43  lr: 0.000029  loss: 0.8651  time: 1.5428  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 345/3000]  eta: 1:06:42  lr: 0.000029  loss: 0.4607  time: 1.5426  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 350/3000]  eta: 1:06:34  lr: 0.000029  loss: 0.4084  time: 1.5188  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 350/3000]  eta: 1:06:33  lr: 0.000029  loss: 0.3799  time: 1.5186  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 355/3000]  eta: 1:06:23  lr: 0.000029  loss: 0.9370  time: 1.4815  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 355/3000]  eta: 1:06:22  lr: 0.000029  loss: 1.0327  time: 1.4812  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 360/3000]  eta: 1:06:16  lr: 0.000029  loss: 0.3837  time: 1.4792  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 360/3000]  eta: 1:06:16  lr: 0.000029  loss: 0.2010  time: 1.4790  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 365/3000]  eta: 1:06:09  lr: 0.000029  loss: 0.2753  time: 1.4834  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 365/3000]  eta: 1:06:08  lr: 0.000029  loss: 0.5502  time: 1.4830  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 370/3000]  eta: 1:05:59  lr: 0.000029  loss: 0.2697  time: 1.4719  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 370/3000]  eta: 1:05:58  lr: 0.000029  loss: 0.3156  time: 1.4716  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 375/3000]  eta: 1:05:49  lr: 0.000029  loss: 0.3309  time: 1.4829  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 375/3000]  eta: 1:05:49  lr: 0.000029  loss: 0.4627  time: 1.4827  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 380/3000]  eta: 1:05:40  lr: 0.000029  loss: 0.1247  time: 1.4616  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 380/3000]  eta: 1:05:39  lr: 0.000029  loss: 0.2043  time: 1.4613  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 385/3000]  eta: 1:05:33  lr: 0.000029  loss: 0.5910  time: 1.4638  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 385/3000]  eta: 1:05:33  lr: 0.000029  loss: 0.3447  time: 1.4636  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 390/3000]  eta: 1:05:24  lr: 0.000029  loss: 0.2982  time: 1.4744  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 390/3000]  eta: 1:05:24  lr: 0.000029  loss: 0.3632  time: 1.4742  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 395/3000]  eta: 1:05:18  lr: 0.000029  loss: 0.4308  time: 1.4938  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 395/3000]  eta: 1:05:17  lr: 0.000029  loss: 0.8980  time: 1.4926  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 400/3000]  eta: 1:05:11  lr: 0.000029  loss: 0.6441  time: 1.5145  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 400/3000]  eta: 1:05:10  lr: 0.000029  loss: 0.2754  time: 1.5134  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 405/3000]  eta: 1:05:02  lr: 0.000029  loss: 0.3531  time: 1.4948  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 405/3000]  eta: 1:05:01  lr: 0.000029  loss: 0.5734  time: 1.4936  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 410/3000]  eta: 1:04:54  lr: 0.000029  loss: 0.3677  time: 1.4980  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 410/3000]  eta: 1:04:53  lr: 0.000029  loss: 0.2920  time: 1.4968  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 415/3000]  eta: 1:04:49  lr: 0.000029  loss: 0.7730  time: 1.5123  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 415/3000]  eta: 1:04:48  lr: 0.000029  loss: 0.3778  time: 1.5121  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 420/3000]  eta: 1:04:42  lr: 0.000029  loss: 1.0835  time: 1.5115  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 420/3000]  eta: 1:04:41  lr: 0.000029  loss: 0.7204  time: 1.5112  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 425/3000]  eta: 1:04:36  lr: 0.000029  loss: 0.3003  time: 1.5357  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 425/3000]  eta: 1:04:35  lr: 0.000029  loss: 0.3075  time: 1.5355  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 430/3000]  eta: 1:04:27  lr: 0.000029  loss: 0.4009  time: 1.5331  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 430/3000]  eta: 1:04:26  lr: 0.000029  loss: 0.1409  time: 1.5328  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 435/3000]  eta: 1:04:20  lr: 0.000029  loss: 0.4994  time: 1.5147  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 435/3000]  eta: 1:04:19  lr: 0.000029  loss: 0.2831  time: 1.5144  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 440/3000]  eta: 1:04:14  lr: 0.000029  loss: 0.2253  time: 1.5186  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 440/3000]  eta: 1:04:13  lr: 0.000029  loss: 0.8764  time: 1.5184  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 445/3000]  eta: 1:04:07  lr: 0.000029  loss: 0.2409  time: 1.5142  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 445/3000]  eta: 1:04:06  lr: 0.000029  loss: 0.6638  time: 1.5140  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 450/3000]  eta: 1:03:59  lr: 0.000029  loss: 0.3313  time: 1.5214  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 450/3000]  eta: 1:03:58  lr: 0.000029  loss: 0.5206  time: 1.5212  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 455/3000]  eta: 1:03:51  lr: 0.000029  loss: 0.3994  time: 1.5166  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 455/3000]  eta: 1:03:50  lr: 0.000029  loss: 0.8971  time: 1.5164  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 460/3000]  eta: 1:03:43  lr: 0.000029  loss: 0.3405  time: 1.5026  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 460/3000]  eta: 1:03:42  lr: 0.000029  loss: 0.3186  time: 1.5023  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 465/3000]  eta: 1:03:37  lr: 0.000029  loss: 0.2059  time: 1.5138  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 465/3000]  eta: 1:03:37  lr: 0.000029  loss: 0.1746  time: 1.5135  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 470/3000]  eta: 1:03:29  lr: 0.000029  loss: 0.1999  time: 1.5107  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 470/3000]  eta: 1:03:29  lr: 0.000029  loss: 0.9857  time: 1.5104  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 475/3000]  eta: 1:03:23  lr: 0.000029  loss: 0.4184  time: 1.5258  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 475/3000]  eta: 1:03:22  lr: 0.000029  loss: 0.2164  time: 1.5256  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 480/3000]  eta: 1:03:17  lr: 0.000029  loss: 0.2094  time: 1.5428  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 480/3000]  eta: 1:03:16  lr: 0.000029  loss: 0.7494  time: 1.5426  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 485/3000]  eta: 1:03:10  lr: 0.000029  loss: 0.2966  time: 1.5299  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 485/3000]  eta: 1:03:09  lr: 0.000029  loss: 0.3849  time: 1.5297  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 490/3000]  eta: 1:03:03  lr: 0.000029  loss: 0.2764  time: 1.5455  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 490/3000]  eta: 1:03:03  lr: 0.000029  loss: 0.2100  time: 1.5452  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 495/3000]  eta: 1:02:56  lr: 0.000029  loss: 0.3293  time: 1.5404  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 495/3000]  eta: 1:02:56  lr: 0.000029  loss: 1.1789  time: 1.5402  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 500/3000]  eta: 1:02:50  lr: 0.000029  loss: 0.3538  time: 1.5386  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 500/3000]  eta: 1:02:49  lr: 0.000029  loss: 0.9101  time: 1.5383  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 505/3000]  eta: 1:02:42  lr: 0.000029  loss: 0.1131  time: 1.5286  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 505/3000]  eta: 1:02:41  lr: 0.000029  loss: 0.3910  time: 1.5284  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 510/3000]  eta: 1:02:36  lr: 0.000029  loss: 0.3391  time: 1.5322  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 510/3000]  eta: 1:02:35  lr: 0.000029  loss: 0.2016  time: 1.5319  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 515/3000]  eta: 1:02:30  lr: 0.000029  loss: 0.4280  time: 1.5438  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 515/3000]  eta: 1:02:29  lr: 0.000029  loss: 0.2656  time: 1.5436  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 520/3000]  eta: 1:02:24  lr: 0.000029  loss: 0.2354  time: 1.5524  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 520/3000]  eta: 1:02:23  lr: 0.000029  loss: 0.6683  time: 1.5522  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 525/3000]  eta: 1:02:17  lr: 0.000029  loss: 0.2000  time: 1.5620  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 525/3000]  eta: 1:02:16  lr: 0.000029  loss: 0.3424  time: 1.5617  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 530/3000]  eta: 1:02:09  lr: 0.000029  loss: 0.3535  time: 1.5513  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 530/3000]  eta: 1:02:09  lr: 0.000029  loss: 0.1158  time: 1.5511  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 535/3000]  eta: 1:02:01  lr: 0.000029  loss: 0.6819  time: 1.5288  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 535/3000]  eta: 1:02:01  lr: 0.000029  loss: 0.2012  time: 1.5286  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 540/3000]  eta: 1:01:56  lr: 0.000029  loss: 0.3933  time: 1.5306  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 540/3000]  eta: 1:01:55  lr: 0.000029  loss: 0.3365  time: 1.5304  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 545/3000]  eta: 1:01:49  lr: 0.000029  loss: 0.2630  time: 1.5365  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 545/3000]  eta: 1:01:48  lr: 0.000029  loss: 0.2010  time: 1.5363  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 550/3000]  eta: 1:01:42  lr: 0.000029  loss: 0.2982  time: 1.5444  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 550/3000]  eta: 1:01:41  lr: 0.000029  loss: 0.5540  time: 1.5441  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 555/3000]  eta: 1:01:35  lr: 0.000029  loss: 0.1522  time: 1.5566  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 555/3000]  eta: 1:01:35  lr: 0.000029  loss: 0.7056  time: 1.5562  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 560/3000]  eta: 1:01:29  lr: 0.000029  loss: 0.3875  time: 1.5569  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 560/3000]  eta: 1:01:29  lr: 0.000029  loss: 0.4670  time: 1.5566  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 565/3000]  eta: 1:01:22  lr: 0.000029  loss: 0.4885  time: 1.5494  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 565/3000]  eta: 1:01:21  lr: 0.000029  loss: 0.7913  time: 1.5491  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 570/3000]  eta: 1:01:14  lr: 0.000029  loss: 0.2104  time: 1.5325  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 570/3000]  eta: 1:01:13  lr: 0.000029  loss: 0.0338  time: 1.5323  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 575/3000]  eta: 1:01:07  lr: 0.000029  loss: 0.3420  time: 1.5325  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 575/3000]  eta: 1:01:06  lr: 0.000029  loss: 1.0476  time: 1.5323  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 580/3000]  eta: 1:00:58  lr: 0.000029  loss: 0.3465  time: 1.4975  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 580/3000]  eta: 1:00:57  lr: 0.000029  loss: 0.2037  time: 1.4973  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 585/3000]  eta: 1:00:51  lr: 0.000029  loss: 0.4321  time: 1.5020  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 585/3000]  eta: 1:00:50  lr: 0.000029  loss: 0.2927  time: 1.5018  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 590/3000]  eta: 1:00:43  lr: 0.000029  loss: 0.1199  time: 1.5068  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 590/3000]  eta: 1:00:42  lr: 0.000029  loss: 0.2758  time: 1.5065  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 595/3000]  eta: 1:00:36  lr: 0.000029  loss: 1.0125  time: 1.5042  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 595/3000]  eta: 1:00:35  lr: 0.000029  loss: 0.6195  time: 1.5039  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 600/3000]  eta: 1:00:26  lr: 0.000029  loss: 0.3101  time: 1.4919  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 600/3000]  eta: 1:00:26  lr: 0.000029  loss: 0.1775  time: 1.4916  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 605/3000]  eta: 1:00:20  lr: 0.000029  loss: 0.2156  time: 1.4988  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 605/3000]  eta: 1:00:19  lr: 0.000029  loss: 0.2959  time: 1.4986  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 610/3000]  eta: 1:00:12  lr: 0.000029  loss: 0.6362  time: 1.4969  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 610/3000]  eta: 1:00:11  lr: 0.000029  loss: 0.3748  time: 1.4967  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 615/3000]  eta: 1:00:03  lr: 0.000029  loss: 0.1408  time: 1.4757  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 615/3000]  eta: 1:00:02  lr: 0.000029  loss: 0.4358  time: 1.4755  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 620/3000]  eta: 0:59:55  lr: 0.000029  loss: 0.4819  time: 1.5035  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 620/3000]  eta: 0:59:55  lr: 0.000029  loss: 0.3020  time: 1.5032  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 625/3000]  eta: 0:59:48  lr: 0.000029  loss: 0.1903  time: 1.4879  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 625/3000]  eta: 0:59:47  lr: 0.000029  loss: 0.6857  time: 1.4876  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 630/3000]  eta: 0:59:40  lr: 0.000029  loss: 0.6809  time: 1.4910  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 630/3000]  eta: 0:59:39  lr: 0.000029  loss: 0.5294  time: 1.4907  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 635/3000]  eta: 0:59:33  lr: 0.000029  loss: 0.4252  time: 1.5124  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 635/3000]  eta: 0:59:32  lr: 0.000029  loss: 0.2560  time: 1.5122  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 640/3000]  eta: 0:59:27  lr: 0.000029  loss: 0.1860  time: 1.5312  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 640/3000]  eta: 0:59:26  lr: 0.000029  loss: 0.5565  time: 1.5309  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 645/3000]  eta: 0:59:20  lr: 0.000029  loss: 0.0715  time: 1.5442  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 645/3000]  eta: 0:59:19  lr: 0.000029  loss: 0.4003  time: 1.5440  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 650/3000]  eta: 0:59:12  lr: 0.000029  loss: 0.4513  time: 1.5460  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 650/3000]  eta: 0:59:12  lr: 0.000029  loss: 0.1293  time: 1.5458  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 655/3000]  eta: 0:59:05  lr: 0.000029  loss: 0.4282  time: 1.5423  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 655/3000]  eta: 0:59:04  lr: 0.000029  loss: 0.3682  time: 1.5421  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 660/3000]  eta: 0:58:58  lr: 0.000029  loss: 0.3121  time: 1.5275  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 660/3000]  eta: 0:58:57  lr: 0.000029  loss: 0.8291  time: 1.5272  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 665/3000]  eta: 0:58:50  lr: 0.000029  loss: 0.2211  time: 1.5128  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 665/3000]  eta: 0:58:49  lr: 0.000029  loss: 0.2055  time: 1.5125  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 670/3000]  eta: 0:58:41  lr: 0.000029  loss: 0.7531  time: 1.4960  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 670/3000]  eta: 0:58:40  lr: 0.000029  loss: 0.5629  time: 1.4958  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 675/3000]  eta: 0:58:33  lr: 0.000029  loss: 0.7197  time: 1.4852  data: 0.0000  max mem: 18432Train: data epoch: [4]  [ 675/3000]  eta: 0:58:32  lr: 0.000029  loss: 0.2731  time: 1.4848  data: 0.0000  max mem: 18151

Train: data epoch: [4]  [ 680/3000]  eta: 0:58:26  lr: 0.000029  loss: 0.1451  time: 1.4924  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 680/3000]  eta: 0:58:25  lr: 0.000029  loss: 0.2218  time: 1.4922  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 685/3000]  eta: 0:58:19  lr: 0.000029  loss: 0.2448  time: 1.5120  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 685/3000]  eta: 0:58:19  lr: 0.000029  loss: 0.4836  time: 1.5118  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 690/3000]  eta: 0:58:11  lr: 0.000029  loss: 0.4384  time: 1.5163  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 690/3000]  eta: 0:58:10  lr: 0.000029  loss: 1.1004  time: 1.5159  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 695/3000]  eta: 0:58:05  lr: 0.000029  loss: 0.1243  time: 1.5439  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 695/3000]  eta: 0:58:04  lr: 0.000029  loss: 0.3738  time: 1.5438  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 700/3000]  eta: 0:57:57  lr: 0.000029  loss: 0.2192  time: 1.5241  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 700/3000]  eta: 0:57:56  lr: 0.000029  loss: 0.1622  time: 1.5238  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 705/3000]  eta: 0:57:50  lr: 0.000029  loss: 0.2579  time: 1.5298  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 705/3000]  eta: 0:57:50  lr: 0.000029  loss: 0.7111  time: 1.5295  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 710/3000]  eta: 0:57:41  lr: 0.000029  loss: 0.4408  time: 1.5189  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 710/3000]  eta: 0:57:41  lr: 0.000029  loss: 0.1619  time: 1.5187  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 715/3000]  eta: 0:57:34  lr: 0.000029  loss: 0.7500  time: 1.4978  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 715/3000]  eta: 0:57:33  lr: 0.000029  loss: 0.1701  time: 1.4975  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 720/3000]  eta: 0:57:27  lr: 0.000029  loss: 0.1176  time: 1.5146  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 720/3000]  eta: 0:57:26  lr: 0.000029  loss: 0.2564  time: 1.5143  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 725/3000]  eta: 0:57:20  lr: 0.000029  loss: 0.2637  time: 1.5110  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 725/3000]  eta: 0:57:19  lr: 0.000029  loss: 0.3986  time: 1.5108  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 730/3000]  eta: 0:57:13  lr: 0.000029  loss: 0.2906  time: 1.5412  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 730/3000]  eta: 0:57:12  lr: 0.000029  loss: 0.6453  time: 1.5410  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 735/3000]  eta: 0:57:05  lr: 0.000029  loss: 0.5529  time: 1.5424  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 735/3000]  eta: 0:57:05  lr: 0.000029  loss: 0.1194  time: 1.5422  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 740/3000]  eta: 0:56:58  lr: 0.000029  loss: 0.4340  time: 1.5387  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 740/3000]  eta: 0:56:57  lr: 0.000029  loss: 0.3003  time: 1.5385  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 745/3000]  eta: 0:56:51  lr: 0.000029  loss: 0.6390  time: 1.5399  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 745/3000]  eta: 0:56:51  lr: 0.000029  loss: 1.1254  time: 1.5397  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 750/3000]  eta: 0:56:44  lr: 0.000029  loss: 0.3562  time: 1.5301  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 750/3000]  eta: 0:56:43  lr: 0.000029  loss: 0.3661  time: 1.5298  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 755/3000]  eta: 0:56:36  lr: 0.000029  loss: 0.1710  time: 1.5302  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 755/3000]  eta: 0:56:35  lr: 0.000029  loss: 0.1701  time: 1.5299  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 760/3000]  eta: 0:56:29  lr: 0.000029  loss: 0.4961  time: 1.5309  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 760/3000]  eta: 0:56:28  lr: 0.000029  loss: 0.6294  time: 1.5307  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 765/3000]  eta: 0:56:21  lr: 0.000029  loss: 0.4684  time: 1.5125  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 765/3000]  eta: 0:56:20  lr: 0.000029  loss: 0.6871  time: 1.5123  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 770/3000]  eta: 0:56:13  lr: 0.000029  loss: 0.0803  time: 1.5001  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 770/3000]  eta: 0:56:12  lr: 0.000029  loss: 0.3297  time: 1.4999  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 775/3000]  eta: 0:56:06  lr: 0.000029  loss: 0.9359  time: 1.5167  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 775/3000]  eta: 0:56:05  lr: 0.000029  loss: 0.5859  time: 1.5164  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 780/3000]  eta: 0:55:58  lr: 0.000029  loss: 0.1676  time: 1.5120  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 780/3000]  eta: 0:55:58  lr: 0.000029  loss: 0.6161  time: 1.5117  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 785/3000]  eta: 0:55:51  lr: 0.000029  loss: 0.1363  time: 1.5084  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 785/3000]  eta: 0:55:50  lr: 0.000029  loss: 0.4825  time: 1.5081  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 790/3000]  eta: 0:55:43  lr: 0.000029  loss: 0.4118  time: 1.5320  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 790/3000]  eta: 0:55:43  lr: 0.000029  loss: 0.3036  time: 1.5317  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 795/3000]  eta: 0:55:36  lr: 0.000029  loss: 0.8577  time: 1.5162  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 795/3000]  eta: 0:55:35  lr: 0.000029  loss: 0.7440  time: 1.5160  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 800/3000]  eta: 0:55:28  lr: 0.000029  loss: 0.4819  time: 1.5122  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 800/3000]  eta: 0:55:28  lr: 0.000029  loss: 0.4095  time: 1.5119  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 805/3000]  eta: 0:55:20  lr: 0.000029  loss: 0.1102  time: 1.5106  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 805/3000]  eta: 0:55:20  lr: 0.000029  loss: 0.3776  time: 1.5103  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 810/3000]  eta: 0:55:12  lr: 0.000029  loss: 0.7716  time: 1.4915  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 810/3000]  eta: 0:55:11  lr: 0.000029  loss: 0.4947  time: 1.4913  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 815/3000]  eta: 0:55:05  lr: 0.000029  loss: 0.1986  time: 1.4970  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 815/3000]  eta: 0:55:04  lr: 0.000029  loss: 0.0543  time: 1.4968  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 820/3000]  eta: 0:54:57  lr: 0.000029  loss: 0.9329  time: 1.4971  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 820/3000]  eta: 0:54:56  lr: 0.000029  loss: 0.5273  time: 1.4968  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 825/3000]  eta: 0:54:48  lr: 0.000029  loss: 0.6155  time: 1.4812  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 825/3000]  eta: 0:54:48  lr: 0.000029  loss: 0.8083  time: 1.4809  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 830/3000]  eta: 0:54:40  lr: 0.000029  loss: 0.5265  time: 1.4803  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 830/3000]  eta: 0:54:40  lr: 0.000029  loss: 0.7335  time: 1.4800  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 835/3000]  eta: 0:54:33  lr: 0.000029  loss: 0.2809  time: 1.4827  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 835/3000]  eta: 0:54:32  lr: 0.000029  loss: 0.6085  time: 1.4825  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 840/3000]  eta: 0:54:26  lr: 0.000029  loss: 0.1303  time: 1.4943  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 840/3000]  eta: 0:54:25  lr: 0.000029  loss: 0.8565  time: 1.4941  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 845/3000]  eta: 0:54:18  lr: 0.000029  loss: 0.2935  time: 1.5151  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 845/3000]  eta: 0:54:18  lr: 0.000029  loss: 0.3476  time: 1.5148  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 850/3000]  eta: 0:54:10  lr: 0.000029  loss: 0.9333  time: 1.5209  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 850/3000]  eta: 0:54:10  lr: 0.000029  loss: 0.1116  time: 1.5206  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 855/3000]  eta: 0:54:02  lr: 0.000029  loss: 0.5012  time: 1.4994  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 855/3000]  eta: 0:54:01  lr: 0.000029  loss: 0.8521  time: 1.4990  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 860/3000]  eta: 0:53:55  lr: 0.000029  loss: 0.5099  time: 1.4927  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 860/3000]  eta: 0:53:54  lr: 0.000029  loss: 0.2760  time: 1.4924  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 865/3000]  eta: 0:53:48  lr: 0.000029  loss: 0.4487  time: 1.5099  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 865/3000]  eta: 0:53:47  lr: 0.000029  loss: 0.9652  time: 1.5096  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 870/3000]  eta: 0:53:41  lr: 0.000029  loss: 0.5545  time: 1.5262  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 870/3000]  eta: 0:53:40  lr: 0.000029  loss: 0.6090  time: 1.5260  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 875/3000]  eta: 0:53:33  lr: 0.000029  loss: 0.3899  time: 1.5378  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 875/3000]  eta: 0:53:33  lr: 0.000029  loss: 0.3285  time: 1.5376  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 880/3000]  eta: 0:53:26  lr: 0.000029  loss: 0.2078  time: 1.5439  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 880/3000]  eta: 0:53:25  lr: 0.000029  loss: 0.9121  time: 1.5437  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 885/3000]  eta: 0:53:19  lr: 0.000029  loss: 0.4894  time: 1.5361  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 885/3000]  eta: 0:53:18  lr: 0.000029  loss: 0.6410  time: 1.5358  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 890/3000]  eta: 0:53:11  lr: 0.000029  loss: 0.5891  time: 1.5239  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 890/3000]  eta: 0:53:11  lr: 0.000029  loss: 0.5579  time: 1.5237  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 895/3000]  eta: 0:53:03  lr: 0.000029  loss: 1.1012  time: 1.5057  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 895/3000]  eta: 0:53:02  lr: 0.000029  loss: 0.4873  time: 1.5054  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 900/3000]  eta: 0:52:54  lr: 0.000029  loss: 0.3485  time: 1.4724  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 900/3000]  eta: 0:52:53  lr: 0.000029  loss: 0.0966  time: 1.4722  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 905/3000]  eta: 0:52:46  lr: 0.000029  loss: 0.1240  time: 1.4597  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 905/3000]  eta: 0:52:45  lr: 0.000029  loss: 0.2642  time: 1.4595  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 910/3000]  eta: 0:52:39  lr: 0.000029  loss: 0.4576  time: 1.4736  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 910/3000]  eta: 0:52:38  lr: 0.000029  loss: 1.1015  time: 1.4732  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 915/3000]  eta: 0:52:31  lr: 0.000029  loss: 0.8041  time: 1.4857  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 915/3000]  eta: 0:52:31  lr: 0.000029  loss: 0.1764  time: 1.4855  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 920/3000]  eta: 0:52:22  lr: 0.000029  loss: 0.6308  time: 1.4850  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 920/3000]  eta: 0:52:22  lr: 0.000029  loss: 0.1218  time: 1.4848  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 925/3000]  eta: 0:52:14  lr: 0.000029  loss: 0.5478  time: 1.4752  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 925/3000]  eta: 0:52:14  lr: 0.000029  loss: 0.4299  time: 1.4749  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 930/3000]  eta: 0:52:07  lr: 0.000029  loss: 0.4989  time: 1.4700  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 930/3000]  eta: 0:52:06  lr: 0.000029  loss: 0.3712  time: 1.4698  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 935/3000]  eta: 0:52:00  lr: 0.000029  loss: 0.9030  time: 1.4997  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 935/3000]  eta: 0:52:00  lr: 0.000029  loss: 0.7465  time: 1.4995  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 940/3000]  eta: 0:51:53  lr: 0.000029  loss: 0.3553  time: 1.5347  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 940/3000]  eta: 0:51:53  lr: 0.000029  loss: 0.9106  time: 1.5343  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 945/3000]  eta: 0:51:46  lr: 0.000029  loss: 0.2500  time: 1.5479  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 945/3000]  eta: 0:51:45  lr: 0.000029  loss: 0.4674  time: 1.5477  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 950/3000]  eta: 0:51:38  lr: 0.000029  loss: 0.1193  time: 1.5473  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 950/3000]  eta: 0:51:38  lr: 0.000029  loss: 0.4479  time: 1.5471  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 955/3000]  eta: 0:51:32  lr: 0.000029  loss: 0.3264  time: 1.5458  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 955/3000]  eta: 0:51:31  lr: 0.000029  loss: 0.3561  time: 1.5456  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 960/3000]  eta: 0:51:25  lr: 0.000029  loss: 0.5463  time: 1.5575  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 960/3000]  eta: 0:51:24  lr: 0.000029  loss: 0.4270  time: 1.5573  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 965/3000]  eta: 0:51:17  lr: 0.000029  loss: 0.4365  time: 1.5531  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 965/3000]  eta: 0:51:17  lr: 0.000029  loss: 0.9376  time: 1.5529  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 970/3000]  eta: 0:51:10  lr: 0.000029  loss: 0.5686  time: 1.5528  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 970/3000]  eta: 0:51:09  lr: 0.000029  loss: 0.6411  time: 1.5526  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 975/3000]  eta: 0:51:02  lr: 0.000029  loss: 0.1415  time: 1.5319  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 975/3000]  eta: 0:51:02  lr: 0.000029  loss: 0.2041  time: 1.5316  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 980/3000]  eta: 0:50:55  lr: 0.000029  loss: 0.8966  time: 1.5171  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 980/3000]  eta: 0:50:54  lr: 0.000029  loss: 0.4422  time: 1.5169  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 985/3000]  eta: 0:50:47  lr: 0.000029  loss: 0.4223  time: 1.5163  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 985/3000]  eta: 0:50:46  lr: 0.000029  loss: 0.1804  time: 1.5160  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 990/3000]  eta: 0:50:39  lr: 0.000029  loss: 0.3990  time: 1.5013  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 990/3000]  eta: 0:50:39  lr: 0.000029  loss: 0.1494  time: 1.5010  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [ 995/3000]  eta: 0:50:32  lr: 0.000029  loss: 0.2275  time: 1.5100  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [ 995/3000]  eta: 0:50:31  lr: 0.000029  loss: 0.7354  time: 1.5098  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1000/3000]  eta: 0:50:25  lr: 0.000029  loss: 1.0632  time: 1.5188  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1000/3000]  eta: 0:50:24  lr: 0.000029  loss: 0.3483  time: 1.5186  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1005/3000]  eta: 0:50:17  lr: 0.000029  loss: 0.2551  time: 1.5233  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1005/3000]  eta: 0:50:17  lr: 0.000029  loss: 0.2872  time: 1.5231  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1010/3000]  eta: 0:50:10  lr: 0.000029  loss: 0.3842  time: 1.5323  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1010/3000]  eta: 0:50:09  lr: 0.000029  loss: 0.5797  time: 1.5321  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1015/3000]  eta: 0:50:02  lr: 0.000029  loss: 0.4222  time: 1.5316  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1015/3000]  eta: 0:50:02  lr: 0.000029  loss: 0.4094  time: 1.5313  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1020/3000]  eta: 0:49:55  lr: 0.000029  loss: 0.3091  time: 1.5138  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1020/3000]  eta: 0:49:54  lr: 0.000029  loss: 0.2012  time: 1.5136  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1025/3000]  eta: 0:49:47  lr: 0.000029  loss: 1.0424  time: 1.5093  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1025/3000]  eta: 0:49:46  lr: 0.000029  loss: 0.3615  time: 1.5090  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1030/3000]  eta: 0:49:40  lr: 0.000029  loss: 0.6223  time: 1.5273  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1030/3000]  eta: 0:49:39  lr: 0.000029  loss: 0.2098  time: 1.5271  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1035/3000]  eta: 0:49:32  lr: 0.000029  loss: 0.7203  time: 1.5153  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1035/3000]  eta: 0:49:32  lr: 0.000029  loss: 0.9119  time: 1.5151  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1040/3000]  eta: 0:49:24  lr: 0.000029  loss: 0.4307  time: 1.5034  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1040/3000]  eta: 0:49:24  lr: 0.000029  loss: 0.3487  time: 1.5031  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1045/3000]  eta: 0:49:16  lr: 0.000029  loss: 0.4481  time: 1.4928  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1045/3000]  eta: 0:49:15  lr: 0.000029  loss: 0.6004  time: 1.4926  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1050/3000]  eta: 0:49:09  lr: 0.000029  loss: 0.3683  time: 1.4887  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1050/3000]  eta: 0:49:08  lr: 0.000029  loss: 0.5308  time: 1.4884  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1055/3000]  eta: 0:49:02  lr: 0.000029  loss: 0.5659  time: 1.5157  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1055/3000]  eta: 0:49:02  lr: 0.000029  loss: 0.5277  time: 1.5154  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1060/3000]  eta: 0:48:54  lr: 0.000029  loss: 0.5140  time: 1.5308  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1060/3000]  eta: 0:48:54  lr: 0.000029  loss: 0.2976  time: 1.5306  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1065/3000]  eta: 0:48:46  lr: 0.000029  loss: 0.2418  time: 1.5212  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1065/3000]  eta: 0:48:45  lr: 0.000029  loss: 0.5897  time: 1.5211  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1070/3000]  eta: 0:48:38  lr: 0.000029  loss: 0.5331  time: 1.5039  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1070/3000]  eta: 0:48:38  lr: 0.000029  loss: 0.3377  time: 1.5037  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1075/3000]  eta: 0:48:31  lr: 0.000029  loss: 0.8419  time: 1.4868  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1075/3000]  eta: 0:48:30  lr: 0.000029  loss: 0.2832  time: 1.4866  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1080/3000]  eta: 0:48:24  lr: 0.000029  loss: 0.3014  time: 1.5037  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1080/3000]  eta: 0:48:23  lr: 0.000029  loss: 0.1156  time: 1.5035  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1085/3000]  eta: 0:48:16  lr: 0.000029  loss: 0.6178  time: 1.5352  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1085/3000]  eta: 0:48:16  lr: 0.000029  loss: 1.5154  time: 1.5349  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1090/3000]  eta: 0:48:09  lr: 0.000029  loss: 0.5103  time: 1.5422  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1090/3000]  eta: 0:48:09  lr: 0.000029  loss: 0.4412  time: 1.5419  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1095/3000]  eta: 0:48:01  lr: 0.000029  loss: 0.4675  time: 1.5241  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1095/3000]  eta: 0:48:01  lr: 0.000029  loss: 0.1736  time: 1.5240  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1100/3000]  eta: 0:47:53  lr: 0.000029  loss: 0.5129  time: 1.5007  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1100/3000]  eta: 0:47:53  lr: 0.000029  loss: 0.8149  time: 1.5004  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1105/3000]  eta: 0:47:46  lr: 0.000029  loss: 0.2238  time: 1.5121  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1105/3000]  eta: 0:47:46  lr: 0.000029  loss: 0.3763  time: 1.5120  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1110/3000]  eta: 0:47:38  lr: 0.000029  loss: 0.3716  time: 1.5026  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1110/3000]  eta: 0:47:38  lr: 0.000029  loss: 0.2018  time: 1.5025  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1115/3000]  eta: 0:47:31  lr: 0.000029  loss: 0.2172  time: 1.5326  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1115/3000]  eta: 0:47:31  lr: 0.000029  loss: 0.3623  time: 1.5323  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1120/3000]  eta: 0:47:24  lr: 0.000029  loss: 0.3746  time: 1.5523  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1120/3000]  eta: 0:47:24  lr: 0.000029  loss: 0.1686  time: 1.5521  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1125/3000]  eta: 0:47:17  lr: 0.000029  loss: 0.4056  time: 1.5367  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1125/3000]  eta: 0:47:16  lr: 0.000029  loss: 0.5239  time: 1.5364  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1130/3000]  eta: 0:47:09  lr: 0.000029  loss: 0.1286  time: 1.5341  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1130/3000]  eta: 0:47:08  lr: 0.000029  loss: 0.1694  time: 1.5338  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1135/3000]  eta: 0:47:01  lr: 0.000029  loss: 0.1828  time: 1.5196  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1135/3000]  eta: 0:47:01  lr: 0.000029  loss: 0.1070  time: 1.5193  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1140/3000]  eta: 0:46:54  lr: 0.000029  loss: 0.3862  time: 1.5215  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1140/3000]  eta: 0:46:54  lr: 0.000029  loss: 0.2476  time: 1.5212  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1145/3000]  eta: 0:46:47  lr: 0.000029  loss: 0.4536  time: 1.5263  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1145/3000]  eta: 0:46:46  lr: 0.000029  loss: 0.4089  time: 1.5259  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1150/3000]  eta: 0:46:39  lr: 0.000029  loss: 0.4115  time: 1.5137  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1150/3000]  eta: 0:46:38  lr: 0.000029  loss: 0.2355  time: 1.5134  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1155/3000]  eta: 0:46:31  lr: 0.000029  loss: 0.1286  time: 1.5014  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1155/3000]  eta: 0:46:30  lr: 0.000029  loss: 0.4981  time: 1.5012  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1160/3000]  eta: 0:46:23  lr: 0.000029  loss: 0.6950  time: 1.4863  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1160/3000]  eta: 0:46:23  lr: 0.000029  loss: 0.7277  time: 1.4861  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1165/3000]  eta: 0:46:16  lr: 0.000029  loss: 0.4624  time: 1.4945  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1165/3000]  eta: 0:46:16  lr: 0.000029  loss: 0.2975  time: 1.4943  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1170/3000]  eta: 0:46:08  lr: 0.000029  loss: 0.2117  time: 1.5146  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1170/3000]  eta: 0:46:08  lr: 0.000029  loss: 0.5388  time: 1.5144  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1175/3000]  eta: 0:46:01  lr: 0.000029  loss: 0.2305  time: 1.5188  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1175/3000]  eta: 0:46:00  lr: 0.000029  loss: 0.2502  time: 1.5186  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1180/3000]  eta: 0:45:53  lr: 0.000029  loss: 0.2437  time: 1.5107  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1180/3000]  eta: 0:45:52  lr: 0.000029  loss: 0.4287  time: 1.5104  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1185/3000]  eta: 0:45:46  lr: 0.000029  loss: 0.4418  time: 1.5098  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1185/3000]  eta: 0:45:45  lr: 0.000029  loss: 0.7171  time: 1.5095  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1190/3000]  eta: 0:45:38  lr: 0.000029  loss: 0.9165  time: 1.4989  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1190/3000]  eta: 0:45:37  lr: 0.000029  loss: 0.2010  time: 1.4987  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1195/3000]  eta: 0:45:31  lr: 0.000029  loss: 0.8992  time: 1.5212  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1195/3000]  eta: 0:45:30  lr: 0.000029  loss: 0.4931  time: 1.5210  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1200/3000]  eta: 0:45:23  lr: 0.000029  loss: 0.0710  time: 1.5333  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1200/3000]  eta: 0:45:23  lr: 0.000029  loss: 0.4756  time: 1.5330  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1205/3000]  eta: 0:45:15  lr: 0.000029  loss: 0.4948  time: 1.5017  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1205/3000]  eta: 0:45:15  lr: 0.000029  loss: 0.1295  time: 1.5014  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1210/3000]  eta: 0:45:07  lr: 0.000029  loss: 0.1482  time: 1.4986  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1210/3000]  eta: 0:45:07  lr: 0.000029  loss: 0.4572  time: 1.4984  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1215/3000]  eta: 0:44:59  lr: 0.000029  loss: 0.7735  time: 1.4725  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1215/3000]  eta: 0:44:59  lr: 0.000029  loss: 0.5681  time: 1.4722  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1220/3000]  eta: 0:44:52  lr: 0.000029  loss: 0.6751  time: 1.4736  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1220/3000]  eta: 0:44:51  lr: 0.000029  loss: 0.1361  time: 1.4734  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1225/3000]  eta: 0:44:45  lr: 0.000029  loss: 0.5188  time: 1.5109  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1225/3000]  eta: 0:44:44  lr: 0.000029  loss: 0.3530  time: 1.5106  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1230/3000]  eta: 0:44:37  lr: 0.000029  loss: 0.6632  time: 1.5251  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1230/3000]  eta: 0:44:37  lr: 0.000029  loss: 0.6401  time: 1.5248  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1235/3000]  eta: 0:44:30  lr: 0.000029  loss: 0.5411  time: 1.5447  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1235/3000]  eta: 0:44:29  lr: 0.000029  loss: 0.0974  time: 1.5445  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1240/3000]  eta: 0:44:22  lr: 0.000029  loss: 0.2967  time: 1.5341  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1240/3000]  eta: 0:44:22  lr: 0.000029  loss: 0.3422  time: 1.5339  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1245/3000]  eta: 0:44:15  lr: 0.000029  loss: 0.5246  time: 1.5163  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1245/3000]  eta: 0:44:14  lr: 0.000029  loss: 0.6278  time: 1.5160  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1250/3000]  eta: 0:44:07  lr: 0.000029  loss: 0.1987  time: 1.5207  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1250/3000]  eta: 0:44:07  lr: 0.000029  loss: 0.8238  time: 1.5205  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1255/3000]  eta: 0:44:00  lr: 0.000029  loss: 0.9404  time: 1.5325  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1255/3000]  eta: 0:44:00  lr: 0.000029  loss: 0.2028  time: 1.5323  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1260/3000]  eta: 0:43:53  lr: 0.000029  loss: 0.6083  time: 1.5348  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1260/3000]  eta: 0:43:52  lr: 0.000029  loss: 0.9226  time: 1.5345  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1265/3000]  eta: 0:43:45  lr: 0.000029  loss: 0.5312  time: 1.5419  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1265/3000]  eta: 0:43:45  lr: 0.000029  loss: 0.3504  time: 1.5416  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1270/3000]  eta: 0:43:37  lr: 0.000029  loss: 1.1533  time: 1.5347  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1270/3000]  eta: 0:43:37  lr: 0.000029  loss: 0.2361  time: 1.5344  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1275/3000]  eta: 0:43:30  lr: 0.000029  loss: 0.7653  time: 1.5293  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1275/3000]  eta: 0:43:30  lr: 0.000029  loss: 0.4227  time: 1.5290  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1280/3000]  eta: 0:43:22  lr: 0.000029  loss: 0.4832  time: 1.5155  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1280/3000]  eta: 0:43:22  lr: 0.000029  loss: 0.1728  time: 1.5153  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1285/3000]  eta: 0:43:15  lr: 0.000029  loss: 0.5750  time: 1.5066  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1285/3000]  eta: 0:43:14  lr: 0.000029  loss: 0.5292  time: 1.5063  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1290/3000]  eta: 0:43:08  lr: 0.000029  loss: 0.3755  time: 1.5265  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1290/3000]  eta: 0:43:07  lr: 0.000029  loss: 0.3994  time: 1.5262  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1295/3000]  eta: 0:43:00  lr: 0.000029  loss: 0.0762  time: 1.5165  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1295/3000]  eta: 0:43:00  lr: 0.000029  loss: 0.1897  time: 1.5163  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1300/3000]  eta: 0:42:53  lr: 0.000029  loss: 0.2874  time: 1.5325  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1300/3000]  eta: 0:42:52  lr: 0.000029  loss: 0.3093  time: 1.5322  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1305/3000]  eta: 0:42:45  lr: 0.000029  loss: 0.1317  time: 1.5249  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1305/3000]  eta: 0:42:44  lr: 0.000029  loss: 0.4710  time: 1.5247  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1310/3000]  eta: 0:42:37  lr: 0.000029  loss: 0.5335  time: 1.5014  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1310/3000]  eta: 0:42:37  lr: 0.000029  loss: 0.1530  time: 1.5011  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1315/3000]  eta: 0:42:29  lr: 0.000029  loss: 0.4871  time: 1.4954  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1315/3000]  eta: 0:42:29  lr: 0.000029  loss: 0.5178  time: 1.4951  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1320/3000]  eta: 0:42:22  lr: 0.000029  loss: 0.2244  time: 1.4859  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1320/3000]  eta: 0:42:21  lr: 0.000029  loss: 0.1961  time: 1.4857  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1325/3000]  eta: 0:42:14  lr: 0.000029  loss: 0.6458  time: 1.4847  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1325/3000]  eta: 0:42:13  lr: 0.000029  loss: 0.4179  time: 1.4844  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1330/3000]  eta: 0:42:06  lr: 0.000029  loss: 0.1799  time: 1.4711  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1330/3000]  eta: 0:42:05  lr: 0.000029  loss: 0.2517  time: 1.4707  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1335/3000]  eta: 0:41:58  lr: 0.000029  loss: 0.8043  time: 1.4804  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1335/3000]  eta: 0:41:58  lr: 0.000029  loss: 0.5664  time: 1.4801  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1340/3000]  eta: 0:41:50  lr: 0.000029  loss: 0.1668  time: 1.4750  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1340/3000]  eta: 0:41:50  lr: 0.000029  loss: 0.3388  time: 1.4746  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1345/3000]  eta: 0:41:43  lr: 0.000029  loss: 0.0954  time: 1.4794  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1345/3000]  eta: 0:41:42  lr: 0.000029  loss: 0.2013  time: 1.4791  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1350/3000]  eta: 0:41:35  lr: 0.000029  loss: 0.6343  time: 1.5002  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1350/3000]  eta: 0:41:35  lr: 0.000029  loss: 0.4595  time: 1.5000  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1355/3000]  eta: 0:41:28  lr: 0.000029  loss: 0.1913  time: 1.4935  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1355/3000]  eta: 0:41:27  lr: 0.000029  loss: 0.8582  time: 1.4932  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1360/3000]  eta: 0:41:20  lr: 0.000029  loss: 0.3991  time: 1.5238  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1360/3000]  eta: 0:41:20  lr: 0.000029  loss: 0.3842  time: 1.5236  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1365/3000]  eta: 0:41:13  lr: 0.000029  loss: 0.4270  time: 1.5227  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1365/3000]  eta: 0:41:12  lr: 0.000029  loss: 0.1974  time: 1.5224  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1370/3000]  eta: 0:41:05  lr: 0.000029  loss: 0.4462  time: 1.5006  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1370/3000]  eta: 0:41:04  lr: 0.000029  loss: 0.4917  time: 1.5003  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1375/3000]  eta: 0:40:57  lr: 0.000029  loss: 0.1863  time: 1.4890  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1375/3000]  eta: 0:40:56  lr: 0.000029  loss: 0.2843  time: 1.4887  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1380/3000]  eta: 0:40:49  lr: 0.000029  loss: 0.3096  time: 1.4549  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1380/3000]  eta: 0:40:48  lr: 0.000029  loss: 0.1629  time: 1.4546  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1385/3000]  eta: 0:40:42  lr: 0.000029  loss: 0.2777  time: 1.4766  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1385/3000]  eta: 0:40:41  lr: 0.000029  loss: 0.3036  time: 1.4764  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1390/3000]  eta: 0:40:34  lr: 0.000029  loss: 0.4575  time: 1.5030  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1390/3000]  eta: 0:40:34  lr: 0.000029  loss: 0.9321  time: 1.5028  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1395/3000]  eta: 0:40:26  lr: 0.000029  loss: 0.1851  time: 1.5030  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1395/3000]  eta: 0:40:26  lr: 0.000029  loss: 0.2928  time: 1.5027  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1400/3000]  eta: 0:40:19  lr: 0.000029  loss: 0.4540  time: 1.5351  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1400/3000]  eta: 0:40:19  lr: 0.000029  loss: 0.2971  time: 1.5348  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1405/3000]  eta: 0:40:12  lr: 0.000029  loss: 0.3009  time: 1.5377  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1405/3000]  eta: 0:40:11  lr: 0.000029  loss: 0.7423  time: 1.5375  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1410/3000]  eta: 0:40:04  lr: 0.000029  loss: 0.5752  time: 1.5295  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1410/3000]  eta: 0:40:04  lr: 0.000029  loss: 0.5119  time: 1.5292  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1415/3000]  eta: 0:39:56  lr: 0.000029  loss: 0.4145  time: 1.5282  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1415/3000]  eta: 0:39:56  lr: 0.000029  loss: 0.0639  time: 1.5280  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1420/3000]  eta: 0:39:49  lr: 0.000029  loss: 0.4230  time: 1.5237  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1420/3000]  eta: 0:39:49  lr: 0.000029  loss: 0.2201  time: 1.5234  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1425/3000]  eta: 0:39:41  lr: 0.000029  loss: 0.0762  time: 1.4935  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1425/3000]  eta: 0:39:41  lr: 0.000029  loss: 0.1280  time: 1.4932  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1430/3000]  eta: 0:39:33  lr: 0.000029  loss: 0.4639  time: 1.4863  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1430/3000]  eta: 0:39:33  lr: 0.000029  loss: 0.1522  time: 1.4861  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1435/3000]  eta: 0:39:26  lr: 0.000029  loss: 0.9746  time: 1.4896  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1435/3000]  eta: 0:39:25  lr: 0.000029  loss: 0.3458  time: 1.4894  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1440/3000]  eta: 0:39:18  lr: 0.000029  loss: 0.5231  time: 1.4758  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1440/3000]  eta: 0:39:18  lr: 0.000029  loss: 0.4004  time: 1.4756  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1445/3000]  eta: 0:39:10  lr: 0.000029  loss: 0.6626  time: 1.4880  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1445/3000]  eta: 0:39:10  lr: 0.000029  loss: 0.5476  time: 1.4877  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1450/3000]  eta: 0:39:03  lr: 0.000029  loss: 0.5687  time: 1.5130  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1450/3000]  eta: 0:39:03  lr: 0.000029  loss: 0.6908  time: 1.5128  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1455/3000]  eta: 0:38:55  lr: 0.000029  loss: 0.2520  time: 1.5050  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1455/3000]  eta: 0:38:55  lr: 0.000029  loss: 0.1091  time: 1.5047  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1460/3000]  eta: 0:38:48  lr: 0.000029  loss: 0.4902  time: 1.5127  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1460/3000]  eta: 0:38:47  lr: 0.000029  loss: 0.4969  time: 1.5124  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1465/3000]  eta: 0:38:41  lr: 0.000029  loss: 0.4199  time: 1.5259  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1465/3000]  eta: 0:38:40  lr: 0.000029  loss: 0.2917  time: 1.5257  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1470/3000]  eta: 0:38:33  lr: 0.000029  loss: 0.4262  time: 1.5220  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1470/3000]  eta: 0:38:33  lr: 0.000029  loss: 0.7103  time: 1.5217  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1475/3000]  eta: 0:38:26  lr: 0.000029  loss: 0.4059  time: 1.5448  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1475/3000]  eta: 0:38:25  lr: 0.000029  loss: 0.8929  time: 1.5446  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1480/3000]  eta: 0:38:19  lr: 0.000029  loss: 0.2089  time: 1.5610  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1480/3000]  eta: 0:38:18  lr: 0.000029  loss: 0.2493  time: 1.5608  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1485/3000]  eta: 0:38:11  lr: 0.000029  loss: 0.5278  time: 1.5628  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1485/3000]  eta: 0:38:11  lr: 0.000029  loss: 0.2151  time: 1.5625  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1490/3000]  eta: 0:38:04  lr: 0.000029  loss: 0.2762  time: 1.5471  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1490/3000]  eta: 0:38:03  lr: 0.000029  loss: 0.2219  time: 1.5468  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1495/3000]  eta: 0:37:56  lr: 0.000029  loss: 0.3718  time: 1.5350  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1495/3000]  eta: 0:37:56  lr: 0.000029  loss: 0.3560  time: 1.5347  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1500/3000]  eta: 0:37:48  lr: 0.000029  loss: 0.3557  time: 1.5138  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1500/3000]  eta: 0:37:48  lr: 0.000029  loss: 0.2426  time: 1.5135  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1505/3000]  eta: 0:37:41  lr: 0.000029  loss: 0.3282  time: 1.5167  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1505/3000]  eta: 0:37:41  lr: 0.000029  loss: 0.1364  time: 1.5164  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1510/3000]  eta: 0:37:33  lr: 0.000029  loss: 0.1280  time: 1.5070  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1510/3000]  eta: 0:37:33  lr: 0.000029  loss: 0.2280  time: 1.5068  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1515/3000]  eta: 0:37:25  lr: 0.000029  loss: 0.1617  time: 1.4982  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1515/3000]  eta: 0:37:25  lr: 0.000029  loss: 0.0790  time: 1.4979  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1520/3000]  eta: 0:37:18  lr: 0.000029  loss: 0.5315  time: 1.4964  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1520/3000]  eta: 0:37:17  lr: 0.000029  loss: 0.2910  time: 1.4961  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1525/3000]  eta: 0:37:10  lr: 0.000029  loss: 0.2427  time: 1.4857  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1525/3000]  eta: 0:37:10  lr: 0.000029  loss: 0.9495  time: 1.4854  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1530/3000]  eta: 0:37:02  lr: 0.000029  loss: 0.0742  time: 1.4847  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1530/3000]  eta: 0:37:02  lr: 0.000029  loss: 0.3430  time: 1.4844  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1535/3000]  eta: 0:36:55  lr: 0.000029  loss: 0.4764  time: 1.5176  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1535/3000]  eta: 0:36:55  lr: 0.000029  loss: 0.3736  time: 1.5174  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1540/3000]  eta: 0:36:48  lr: 0.000029  loss: 0.1174  time: 1.5237  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1540/3000]  eta: 0:36:47  lr: 0.000029  loss: 0.2239  time: 1.5235  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1545/3000]  eta: 0:36:40  lr: 0.000029  loss: 0.4726  time: 1.5069  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1545/3000]  eta: 0:36:40  lr: 0.000029  loss: 0.5838  time: 1.5067  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1550/3000]  eta: 0:36:32  lr: 0.000029  loss: 0.1084  time: 1.5143  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1550/3000]  eta: 0:36:32  lr: 0.000029  loss: 0.2000  time: 1.5141  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1555/3000]  eta: 0:36:25  lr: 0.000029  loss: 0.1516  time: 1.5017  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1555/3000]  eta: 0:36:24  lr: 0.000029  loss: 0.3461  time: 1.5015  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1560/3000]  eta: 0:36:17  lr: 0.000029  loss: 0.3664  time: 1.4804  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1560/3000]  eta: 0:36:17  lr: 0.000029  loss: 0.4353  time: 1.4802  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1565/3000]  eta: 0:36:09  lr: 0.000029  loss: 0.2969  time: 1.4813  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1565/3000]  eta: 0:36:09  lr: 0.000029  loss: 0.4429  time: 1.4810  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1570/3000]  eta: 0:36:02  lr: 0.000029  loss: 0.2309  time: 1.5051  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1570/3000]  eta: 0:36:02  lr: 0.000029  loss: 0.2662  time: 1.5048  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1575/3000]  eta: 0:35:54  lr: 0.000029  loss: 0.5210  time: 1.5064  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1575/3000]  eta: 0:35:54  lr: 0.000029  loss: 0.1912  time: 1.5061  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1580/3000]  eta: 0:35:47  lr: 0.000029  loss: 0.1777  time: 1.5375  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1580/3000]  eta: 0:35:47  lr: 0.000029  loss: 0.5332  time: 1.5373  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1585/3000]  eta: 0:35:40  lr: 0.000029  loss: 0.3435  time: 1.5590  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1585/3000]  eta: 0:35:39  lr: 0.000029  loss: 0.1873  time: 1.5588  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1590/3000]  eta: 0:35:32  lr: 0.000029  loss: 0.6833  time: 1.5478  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1590/3000]  eta: 0:35:32  lr: 0.000029  loss: 0.5248  time: 1.5475  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1595/3000]  eta: 0:35:25  lr: 0.000029  loss: 0.3147  time: 1.5279  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1595/3000]  eta: 0:35:24  lr: 0.000029  loss: 0.5341  time: 1.5277  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1600/3000]  eta: 0:35:17  lr: 0.000029  loss: 0.2278  time: 1.5211  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1600/3000]  eta: 0:35:17  lr: 0.000029  loss: 0.2489  time: 1.5208  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1605/3000]  eta: 0:35:09  lr: 0.000029  loss: 0.5961  time: 1.5099  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1605/3000]  eta: 0:35:09  lr: 0.000029  loss: 0.6144  time: 1.5096  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1610/3000]  eta: 0:35:02  lr: 0.000029  loss: 0.4132  time: 1.5044  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1610/3000]  eta: 0:35:02  lr: 0.000029  loss: 0.2539  time: 1.5041  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1615/3000]  eta: 0:34:55  lr: 0.000029  loss: 0.6281  time: 1.5399  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1615/3000]  eta: 0:34:54  lr: 0.000029  loss: 0.4153  time: 1.5386  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1620/3000]  eta: 0:34:47  lr: 0.000029  loss: 0.7425  time: 1.5209  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1620/3000]  eta: 0:34:47  lr: 0.000029  loss: 0.1480  time: 1.5196  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1625/3000]  eta: 0:34:40  lr: 0.000029  loss: 0.5870  time: 1.5389  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1625/3000]  eta: 0:34:39  lr: 0.000029  loss: 1.0408  time: 1.5376  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1630/3000]  eta: 0:34:32  lr: 0.000029  loss: 0.7016  time: 1.5298  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1630/3000]  eta: 0:34:32  lr: 0.000029  loss: 0.3764  time: 1.5285  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1635/3000]  eta: 0:34:25  lr: 0.000029  loss: 0.6290  time: 1.5220  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1635/3000]  eta: 0:34:24  lr: 0.000029  loss: 0.4868  time: 1.5218  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1640/3000]  eta: 0:34:17  lr: 0.000029  loss: 0.3385  time: 1.5414  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1640/3000]  eta: 0:34:17  lr: 0.000029  loss: 0.4592  time: 1.5412  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1645/3000]  eta: 0:34:10  lr: 0.000029  loss: 0.3916  time: 1.5242  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1645/3000]  eta: 0:34:09  lr: 0.000029  loss: 0.5194  time: 1.5239  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1650/3000]  eta: 0:34:02  lr: 0.000029  loss: 0.2873  time: 1.5478  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1650/3000]  eta: 0:34:02  lr: 0.000029  loss: 0.3098  time: 1.5476  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1655/3000]  eta: 0:33:55  lr: 0.000029  loss: 0.5100  time: 1.5393  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1655/3000]  eta: 0:33:54  lr: 0.000029  loss: 0.1200  time: 1.5391  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1660/3000]  eta: 0:33:47  lr: 0.000029  loss: 0.6616  time: 1.5146  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1660/3000]  eta: 0:33:47  lr: 0.000029  loss: 0.7061  time: 1.5144  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1665/3000]  eta: 0:33:39  lr: 0.000029  loss: 0.5522  time: 1.5102  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1665/3000]  eta: 0:33:39  lr: 0.000029  loss: 0.8207  time: 1.5100  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1670/3000]  eta: 0:33:32  lr: 0.000029  loss: 0.2375  time: 1.4918  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1670/3000]  eta: 0:33:31  lr: 0.000029  loss: 0.4622  time: 1.4915  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1675/3000]  eta: 0:33:24  lr: 0.000029  loss: 0.1433  time: 1.4889  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1675/3000]  eta: 0:33:24  lr: 0.000029  loss: 0.2558  time: 1.4886  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1680/3000]  eta: 0:33:17  lr: 0.000029  loss: 0.3360  time: 1.5032  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1680/3000]  eta: 0:33:16  lr: 0.000029  loss: 0.2413  time: 1.5030  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1685/3000]  eta: 0:33:09  lr: 0.000029  loss: 0.2096  time: 1.5068  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1685/3000]  eta: 0:33:09  lr: 0.000029  loss: 0.3732  time: 1.5066  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1690/3000]  eta: 0:33:01  lr: 0.000029  loss: 0.6633  time: 1.4984  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1690/3000]  eta: 0:33:01  lr: 0.000029  loss: 0.3667  time: 1.4983  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1695/3000]  eta: 0:32:54  lr: 0.000029  loss: 1.1626  time: 1.5088  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1695/3000]  eta: 0:32:53  lr: 0.000029  loss: 0.3086  time: 1.5086  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1700/3000]  eta: 0:32:46  lr: 0.000029  loss: 0.3672  time: 1.5162  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1700/3000]  eta: 0:32:46  lr: 0.000029  loss: 0.7289  time: 1.5160  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1705/3000]  eta: 0:32:39  lr: 0.000029  loss: 0.1974  time: 1.5139  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1705/3000]  eta: 0:32:38  lr: 0.000029  loss: 0.8266  time: 1.5136  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1710/3000]  eta: 0:32:31  lr: 0.000029  loss: 0.9086  time: 1.5432  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1710/3000]  eta: 0:32:31  lr: 0.000029  loss: 0.1929  time: 1.5429  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1715/3000]  eta: 0:32:24  lr: 0.000029  loss: 0.4301  time: 1.5419  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1715/3000]  eta: 0:32:24  lr: 0.000029  loss: 0.3690  time: 1.5416  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1720/3000]  eta: 0:32:16  lr: 0.000029  loss: 0.3409  time: 1.5328  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1720/3000]  eta: 0:32:16  lr: 0.000029  loss: 0.2259  time: 1.5325  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1725/3000]  eta: 0:32:09  lr: 0.000029  loss: 0.6790  time: 1.5337  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1725/3000]  eta: 0:32:08  lr: 0.000029  loss: 0.3658  time: 1.5335  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1730/3000]  eta: 0:32:01  lr: 0.000029  loss: 0.9242  time: 1.5145  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1730/3000]  eta: 0:32:01  lr: 0.000029  loss: 0.7391  time: 1.5142  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1735/3000]  eta: 0:31:54  lr: 0.000029  loss: 0.3296  time: 1.4983  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1735/3000]  eta: 0:31:53  lr: 0.000029  loss: 0.2300  time: 1.4981  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1740/3000]  eta: 0:31:46  lr: 0.000029  loss: 0.7380  time: 1.5037  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1740/3000]  eta: 0:31:46  lr: 0.000029  loss: 1.3685  time: 1.5035  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1745/3000]  eta: 0:31:38  lr: 0.000029  loss: 0.0978  time: 1.5022  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1745/3000]  eta: 0:31:38  lr: 0.000029  loss: 0.1579  time: 1.5019  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1750/3000]  eta: 0:31:31  lr: 0.000029  loss: 0.3701  time: 1.4894  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1750/3000]  eta: 0:31:30  lr: 0.000029  loss: 0.2660  time: 1.4892  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1755/3000]  eta: 0:31:23  lr: 0.000029  loss: 0.2233  time: 1.4935  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1755/3000]  eta: 0:31:23  lr: 0.000029  loss: 0.4036  time: 1.4933  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1760/3000]  eta: 0:31:15  lr: 0.000029  loss: 0.1107  time: 1.4880  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1760/3000]  eta: 0:31:15  lr: 0.000029  loss: 0.3459  time: 1.4878  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1765/3000]  eta: 0:31:08  lr: 0.000029  loss: 0.5946  time: 1.5032  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1765/3000]  eta: 0:31:08  lr: 0.000029  loss: 0.4970  time: 1.5031  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1770/3000]  eta: 0:31:00  lr: 0.000029  loss: 0.2152  time: 1.5102  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1770/3000]  eta: 0:31:00  lr: 0.000029  loss: 0.0584  time: 1.5100  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1775/3000]  eta: 0:30:53  lr: 0.000029  loss: 0.3493  time: 1.5208  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1775/3000]  eta: 0:30:53  lr: 0.000029  loss: 0.3137  time: 1.5206  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1780/3000]  eta: 0:30:45  lr: 0.000029  loss: 0.5580  time: 1.5376  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1780/3000]  eta: 0:30:45  lr: 0.000029  loss: 0.6933  time: 1.5374  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1785/3000]  eta: 0:30:38  lr: 0.000029  loss: 0.1391  time: 1.5315  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1785/3000]  eta: 0:30:38  lr: 0.000029  loss: 0.4142  time: 1.5312  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1790/3000]  eta: 0:30:30  lr: 0.000029  loss: 0.1199  time: 1.5407  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1790/3000]  eta: 0:30:30  lr: 0.000029  loss: 0.2491  time: 1.5404  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1795/3000]  eta: 0:30:23  lr: 0.000029  loss: 0.6082  time: 1.5134  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1795/3000]  eta: 0:30:22  lr: 0.000029  loss: 0.5647  time: 1.5132  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1800/3000]  eta: 0:30:15  lr: 0.000029  loss: 0.0787  time: 1.5089  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1800/3000]  eta: 0:30:15  lr: 0.000029  loss: 0.3946  time: 1.5086  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1805/3000]  eta: 0:30:07  lr: 0.000029  loss: 0.5707  time: 1.4914  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1805/3000]  eta: 0:30:07  lr: 0.000029  loss: 0.5850  time: 1.4911  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1810/3000]  eta: 0:30:00  lr: 0.000029  loss: 0.3533  time: 1.4999  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1810/3000]  eta: 0:30:00  lr: 0.000029  loss: 0.3931  time: 1.4997  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1815/3000]  eta: 0:29:52  lr: 0.000029  loss: 0.3783  time: 1.5188  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1815/3000]  eta: 0:29:52  lr: 0.000029  loss: 0.4416  time: 1.5185  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1820/3000]  eta: 0:29:45  lr: 0.000029  loss: 0.3142  time: 1.5153  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1820/3000]  eta: 0:29:45  lr: 0.000029  loss: 0.2976  time: 1.5151  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1825/3000]  eta: 0:29:37  lr: 0.000029  loss: 0.4461  time: 1.5410  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1825/3000]  eta: 0:29:37  lr: 0.000029  loss: 0.4309  time: 1.5408  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1830/3000]  eta: 0:29:30  lr: 0.000029  loss: 0.4889  time: 1.5326  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1830/3000]  eta: 0:29:30  lr: 0.000029  loss: 0.3093  time: 1.5323  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1835/3000]  eta: 0:29:22  lr: 0.000029  loss: 0.2111  time: 1.5321  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1835/3000]  eta: 0:29:22  lr: 0.000029  loss: 0.2694  time: 1.5319  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1840/3000]  eta: 0:29:15  lr: 0.000029  loss: 0.3992  time: 1.5488  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1840/3000]  eta: 0:29:15  lr: 0.000029  loss: 0.2163  time: 1.5486  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1845/3000]  eta: 0:29:07  lr: 0.000029  loss: 0.5909  time: 1.5246  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1845/3000]  eta: 0:29:07  lr: 0.000029  loss: 0.2033  time: 1.5244  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1850/3000]  eta: 0:29:00  lr: 0.000029  loss: 0.4785  time: 1.5248  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1850/3000]  eta: 0:28:59  lr: 0.000029  loss: 0.7933  time: 1.5247  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1855/3000]  eta: 0:28:52  lr: 0.000029  loss: 0.6161  time: 1.5295  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1855/3000]  eta: 0:28:52  lr: 0.000029  loss: 0.7261  time: 1.5293  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1860/3000]  eta: 0:28:45  lr: 0.000029  loss: 0.3202  time: 1.5170  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1860/3000]  eta: 0:28:45  lr: 0.000029  loss: 0.7041  time: 1.5168  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1865/3000]  eta: 0:28:37  lr: 0.000029  loss: 0.9916  time: 1.5348  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1865/3000]  eta: 0:28:37  lr: 0.000029  loss: 0.3536  time: 1.5346  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1870/3000]  eta: 0:28:30  lr: 0.000029  loss: 0.5464  time: 1.5414  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1870/3000]  eta: 0:28:30  lr: 0.000029  loss: 0.1196  time: 1.5410  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1875/3000]  eta: 0:28:22  lr: 0.000029  loss: 0.3996  time: 1.5315  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1875/3000]  eta: 0:28:22  lr: 0.000029  loss: 0.0727  time: 1.5312  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1880/3000]  eta: 0:28:15  lr: 0.000029  loss: 0.1322  time: 1.5183  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1880/3000]  eta: 0:28:14  lr: 0.000029  loss: 0.1608  time: 1.5180  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1885/3000]  eta: 0:28:07  lr: 0.000029  loss: 0.3746  time: 1.5220  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1885/3000]  eta: 0:28:07  lr: 0.000029  loss: 0.3459  time: 1.5217  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1890/3000]  eta: 0:27:59  lr: 0.000029  loss: 0.1197  time: 1.4945  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1890/3000]  eta: 0:27:59  lr: 0.000029  loss: 1.0227  time: 1.4943  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1895/3000]  eta: 0:27:52  lr: 0.000029  loss: 0.2009  time: 1.4927  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1895/3000]  eta: 0:27:51  lr: 0.000029  loss: 0.6070  time: 1.4925  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1900/3000]  eta: 0:27:44  lr: 0.000029  loss: 0.3977  time: 1.4915  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1900/3000]  eta: 0:27:44  lr: 0.000029  loss: 0.7230  time: 1.4913  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1905/3000]  eta: 0:27:37  lr: 0.000029  loss: 0.1689  time: 1.4847  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1905/3000]  eta: 0:27:36  lr: 0.000029  loss: 0.1698  time: 1.4844  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1910/3000]  eta: 0:27:29  lr: 0.000029  loss: 0.2089  time: 1.4854  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1910/3000]  eta: 0:27:28  lr: 0.000029  loss: 0.2236  time: 1.4850  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1915/3000]  eta: 0:27:21  lr: 0.000029  loss: 0.3221  time: 1.4891  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1915/3000]  eta: 0:27:21  lr: 0.000029  loss: 0.5971  time: 1.4887  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1920/3000]  eta: 0:27:14  lr: 0.000029  loss: 0.3856  time: 1.4970  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1920/3000]  eta: 0:27:13  lr: 0.000029  loss: 0.0481  time: 1.4967  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1925/3000]  eta: 0:27:06  lr: 0.000029  loss: 0.5638  time: 1.4856  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1925/3000]  eta: 0:27:06  lr: 0.000029  loss: 0.3183  time: 1.4852  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1930/3000]  eta: 0:26:58  lr: 0.000029  loss: 0.1653  time: 1.4951  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1930/3000]  eta: 0:26:58  lr: 0.000029  loss: 0.5940  time: 1.4949  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1935/3000]  eta: 0:26:51  lr: 0.000029  loss: 0.2252  time: 1.4902  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1935/3000]  eta: 0:26:50  lr: 0.000029  loss: 0.2689  time: 1.4900  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1940/3000]  eta: 0:26:43  lr: 0.000029  loss: 0.3738  time: 1.4981  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1940/3000]  eta: 0:26:43  lr: 0.000029  loss: 0.4201  time: 1.4979  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1945/3000]  eta: 0:26:36  lr: 0.000029  loss: 0.2887  time: 1.5118  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1945/3000]  eta: 0:26:35  lr: 0.000029  loss: 0.2714  time: 1.5117  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1950/3000]  eta: 0:26:28  lr: 0.000029  loss: 0.7928  time: 1.5053  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1950/3000]  eta: 0:26:28  lr: 0.000029  loss: 0.3070  time: 1.5050  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1955/3000]  eta: 0:26:20  lr: 0.000029  loss: 0.1147  time: 1.5034  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1955/3000]  eta: 0:26:20  lr: 0.000029  loss: 0.2063  time: 1.5032  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1960/3000]  eta: 0:26:13  lr: 0.000029  loss: 0.3286  time: 1.4873  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1960/3000]  eta: 0:26:12  lr: 0.000029  loss: 0.4568  time: 1.4871  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1965/3000]  eta: 0:26:05  lr: 0.000029  loss: 0.4832  time: 1.4887  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1965/3000]  eta: 0:26:05  lr: 0.000029  loss: 0.1809  time: 1.4883  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1970/3000]  eta: 0:25:58  lr: 0.000029  loss: 0.5628  time: 1.5265  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1970/3000]  eta: 0:25:58  lr: 0.000029  loss: 0.4852  time: 1.5263  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1975/3000]  eta: 0:25:50  lr: 0.000029  loss: 0.5842  time: 1.5345  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1975/3000]  eta: 0:25:50  lr: 0.000029  loss: 0.3269  time: 1.5343  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1980/3000]  eta: 0:25:43  lr: 0.000029  loss: 0.2412  time: 1.5330  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1980/3000]  eta: 0:25:42  lr: 0.000029  loss: 0.3093  time: 1.5327  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1985/3000]  eta: 0:25:35  lr: 0.000029  loss: 0.2656  time: 1.5335  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1985/3000]  eta: 0:25:35  lr: 0.000029  loss: 0.4026  time: 1.5333  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1990/3000]  eta: 0:25:28  lr: 0.000029  loss: 0.4366  time: 1.5324  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1990/3000]  eta: 0:25:28  lr: 0.000029  loss: 0.5836  time: 1.5322  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [1995/3000]  eta: 0:25:20  lr: 0.000029  loss: 0.4725  time: 1.5394  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [1995/3000]  eta: 0:25:20  lr: 0.000029  loss: 0.0417  time: 1.5391  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2000/3000]  eta: 0:25:13  lr: 0.000029  loss: 0.6729  time: 1.5620  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2000/3000]  eta: 0:25:13  lr: 0.000029  loss: 0.2890  time: 1.5618  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2005/3000]  eta: 0:25:05  lr: 0.000029  loss: 1.2223  time: 1.5381  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2005/3000]  eta: 0:25:05  lr: 0.000029  loss: 0.6866  time: 1.5378  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2010/3000]  eta: 0:24:58  lr: 0.000029  loss: 0.5890  time: 1.5319  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2010/3000]  eta: 0:24:57  lr: 0.000029  loss: 0.8366  time: 1.5316  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2015/3000]  eta: 0:24:50  lr: 0.000029  loss: 0.0793  time: 1.5184  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2015/3000]  eta: 0:24:50  lr: 0.000029  loss: 0.4425  time: 1.5181  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2020/3000]  eta: 0:24:43  lr: 0.000029  loss: 0.4178  time: 1.5132  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2020/3000]  eta: 0:24:42  lr: 0.000029  loss: 1.0392  time: 1.5126  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2025/3000]  eta: 0:24:35  lr: 0.000029  loss: 0.3872  time: 1.5036  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2025/3000]  eta: 0:24:35  lr: 0.000029  loss: 0.7779  time: 1.5031  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2030/3000]  eta: 0:24:27  lr: 0.000029  loss: 0.2219  time: 1.4943  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2030/3000]  eta: 0:24:27  lr: 0.000029  loss: 0.0928  time: 1.4938  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2035/3000]  eta: 0:24:20  lr: 0.000029  loss: 0.2334  time: 1.5085  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2035/3000]  eta: 0:24:20  lr: 0.000029  loss: 0.1419  time: 1.5079  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2040/3000]  eta: 0:24:12  lr: 0.000029  loss: 0.1379  time: 1.5016  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2040/3000]  eta: 0:24:12  lr: 0.000029  loss: 0.6155  time: 1.5015  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2045/3000]  eta: 0:24:05  lr: 0.000029  loss: 0.2502  time: 1.5242  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2045/3000]  eta: 0:24:04  lr: 0.000029  loss: 0.2334  time: 1.5240  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2050/3000]  eta: 0:23:57  lr: 0.000029  loss: 0.2839  time: 1.5365  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2050/3000]  eta: 0:23:57  lr: 0.000029  loss: 0.2419  time: 1.5363  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2055/3000]  eta: 0:23:50  lr: 0.000029  loss: 0.2807  time: 1.5214  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2055/3000]  eta: 0:23:49  lr: 0.000029  loss: 0.5706  time: 1.5212  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2060/3000]  eta: 0:23:42  lr: 0.000029  loss: 0.4615  time: 1.5121  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2060/3000]  eta: 0:23:42  lr: 0.000029  loss: 0.3821  time: 1.5117  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2065/3000]  eta: 0:23:34  lr: 0.000029  loss: 1.1800  time: 1.5064  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2065/3000]  eta: 0:23:34  lr: 0.000029  loss: 0.1235  time: 1.5061  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2070/3000]  eta: 0:23:27  lr: 0.000029  loss: 0.6368  time: 1.4992  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2070/3000]  eta: 0:23:27  lr: 0.000029  loss: 0.5157  time: 1.4989  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2075/3000]  eta: 0:23:19  lr: 0.000029  loss: 0.3090  time: 1.5111  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2075/3000]  eta: 0:23:19  lr: 0.000029  loss: 0.2978  time: 1.5109  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2080/3000]  eta: 0:23:12  lr: 0.000029  loss: 0.3979  time: 1.5383  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2080/3000]  eta: 0:23:12  lr: 0.000029  loss: 0.1149  time: 1.5381  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2085/3000]  eta: 0:23:04  lr: 0.000029  loss: 0.1166  time: 1.5249  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2085/3000]  eta: 0:23:04  lr: 0.000029  loss: 0.4131  time: 1.5247  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2090/3000]  eta: 0:22:57  lr: 0.000029  loss: 0.3702  time: 1.5286  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2090/3000]  eta: 0:22:56  lr: 0.000029  loss: 0.2964  time: 1.5283  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2095/3000]  eta: 0:22:49  lr: 0.000029  loss: 0.1177  time: 1.5218  data: 0.0000  max mem: 18151Train: data epoch: [4]  [2095/3000]  eta: 0:22:49  lr: 0.000029  loss: 0.5646  time: 1.5221  data: 0.0000  max mem: 18432

Train: data epoch: [4]  [2100/3000]  eta: 0:22:41  lr: 0.000029  loss: 0.7933  time: 1.4815  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2100/3000]  eta: 0:22:41  lr: 0.000029  loss: 0.6196  time: 1.4813  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2105/3000]  eta: 0:22:34  lr: 0.000029  loss: 0.8337  time: 1.5157  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2105/3000]  eta: 0:22:34  lr: 0.000029  loss: 0.1864  time: 1.5155  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2110/3000]  eta: 0:22:26  lr: 0.000029  loss: 0.1482  time: 1.4892  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2110/3000]  eta: 0:22:26  lr: 0.000029  loss: 0.4009  time: 1.4889  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2115/3000]  eta: 0:22:19  lr: 0.000029  loss: 0.3421  time: 1.4831  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2115/3000]  eta: 0:22:18  lr: 0.000029  loss: 0.2686  time: 1.4829  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2120/3000]  eta: 0:22:11  lr: 0.000029  loss: 0.3861  time: 1.5062  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2120/3000]  eta: 0:22:11  lr: 0.000029  loss: 0.2653  time: 1.5059  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2125/3000]  eta: 0:22:04  lr: 0.000029  loss: 0.4008  time: 1.5038  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2125/3000]  eta: 0:22:03  lr: 0.000029  loss: 0.4637  time: 1.5035  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2130/3000]  eta: 0:21:56  lr: 0.000029  loss: 0.0719  time: 1.5230  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2130/3000]  eta: 0:21:56  lr: 0.000029  loss: 0.0924  time: 1.5228  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2135/3000]  eta: 0:21:49  lr: 0.000029  loss: 0.5635  time: 1.5353  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2135/3000]  eta: 0:21:48  lr: 0.000029  loss: 0.1470  time: 1.5350  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2140/3000]  eta: 0:21:41  lr: 0.000029  loss: 1.0943  time: 1.5252  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2140/3000]  eta: 0:21:41  lr: 0.000029  loss: 0.2303  time: 1.5250  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2145/3000]  eta: 0:21:33  lr: 0.000029  loss: 0.3299  time: 1.5076  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2145/3000]  eta: 0:21:33  lr: 0.000029  loss: 0.3118  time: 1.5073  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2150/3000]  eta: 0:21:26  lr: 0.000029  loss: 0.2345  time: 1.5235  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2150/3000]  eta: 0:21:26  lr: 0.000029  loss: 0.3140  time: 1.5231  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2155/3000]  eta: 0:21:18  lr: 0.000029  loss: 0.6200  time: 1.5079  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2155/3000]  eta: 0:21:18  lr: 0.000029  loss: 0.4585  time: 1.5076  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2160/3000]  eta: 0:21:11  lr: 0.000029  loss: 0.4594  time: 1.5213  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2160/3000]  eta: 0:21:10  lr: 0.000029  loss: 0.7480  time: 1.5211  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2165/3000]  eta: 0:21:03  lr: 0.000029  loss: 0.2508  time: 1.5438  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2165/3000]  eta: 0:21:03  lr: 0.000029  loss: 0.6513  time: 1.5436  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2170/3000]  eta: 0:20:56  lr: 0.000029  loss: 0.4311  time: 1.5241  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2170/3000]  eta: 0:20:55  lr: 0.000029  loss: 0.4121  time: 1.5239  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2175/3000]  eta: 0:20:48  lr: 0.000029  loss: 0.2207  time: 1.5380  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2175/3000]  eta: 0:20:48  lr: 0.000029  loss: 0.5811  time: 1.5377  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2180/3000]  eta: 0:20:41  lr: 0.000029  loss: 0.3986  time: 1.5393  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2180/3000]  eta: 0:20:40  lr: 0.000029  loss: 0.2653  time: 1.5390  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2185/3000]  eta: 0:20:33  lr: 0.000029  loss: 0.5144  time: 1.5237  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2185/3000]  eta: 0:20:33  lr: 0.000029  loss: 0.1301  time: 1.5234  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2190/3000]  eta: 0:20:25  lr: 0.000029  loss: 0.0606  time: 1.5212  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2190/3000]  eta: 0:20:25  lr: 0.000029  loss: 1.0335  time: 1.5210  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2195/3000]  eta: 0:20:18  lr: 0.000029  loss: 0.8772  time: 1.5253  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2195/3000]  eta: 0:20:18  lr: 0.000029  loss: 0.7587  time: 1.5251  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2200/3000]  eta: 0:20:10  lr: 0.000029  loss: 0.3596  time: 1.5293  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2200/3000]  eta: 0:20:10  lr: 0.000029  loss: 0.2250  time: 1.5290  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2205/3000]  eta: 0:20:03  lr: 0.000029  loss: 0.2415  time: 1.5379  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2205/3000]  eta: 0:20:03  lr: 0.000029  loss: 0.5783  time: 1.5376  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2210/3000]  eta: 0:19:55  lr: 0.000029  loss: 0.4695  time: 1.5413  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2210/3000]  eta: 0:19:55  lr: 0.000029  loss: 0.7768  time: 1.5411  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2215/3000]  eta: 0:19:48  lr: 0.000029  loss: 0.7012  time: 1.5415  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2215/3000]  eta: 0:19:48  lr: 0.000029  loss: 0.1390  time: 1.5413  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2220/3000]  eta: 0:19:40  lr: 0.000029  loss: 0.2865  time: 1.5389  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2220/3000]  eta: 0:19:40  lr: 0.000029  loss: 0.2993  time: 1.5387  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2225/3000]  eta: 0:19:33  lr: 0.000029  loss: 0.2247  time: 1.5198  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2225/3000]  eta: 0:19:32  lr: 0.000029  loss: 0.3474  time: 1.5195  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2230/3000]  eta: 0:19:25  lr: 0.000029  loss: 0.4961  time: 1.5290  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2230/3000]  eta: 0:19:25  lr: 0.000029  loss: 0.5710  time: 1.5286  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2235/3000]  eta: 0:19:18  lr: 0.000029  loss: 0.2469  time: 1.4985  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2235/3000]  eta: 0:19:17  lr: 0.000029  loss: 0.3051  time: 1.4983  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2240/3000]  eta: 0:19:10  lr: 0.000029  loss: 0.4527  time: 1.5067  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2240/3000]  eta: 0:19:10  lr: 0.000029  loss: 0.5830  time: 1.5065  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2245/3000]  eta: 0:19:02  lr: 0.000029  loss: 0.2510  time: 1.5192  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2245/3000]  eta: 0:19:02  lr: 0.000029  loss: 0.3679  time: 1.5190  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2250/3000]  eta: 0:18:55  lr: 0.000029  loss: 0.5212  time: 1.5075  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2250/3000]  eta: 0:18:55  lr: 0.000029  loss: 0.3165  time: 1.5073  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2255/3000]  eta: 0:18:47  lr: 0.000029  loss: 0.3946  time: 1.5245  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2255/3000]  eta: 0:18:47  lr: 0.000029  loss: 0.2535  time: 1.5242  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2260/3000]  eta: 0:18:40  lr: 0.000029  loss: 0.4113  time: 1.4965  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2260/3000]  eta: 0:18:39  lr: 0.000029  loss: 0.1389  time: 1.4963  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2265/3000]  eta: 0:18:32  lr: 0.000029  loss: 0.6008  time: 1.4958  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2265/3000]  eta: 0:18:32  lr: 0.000029  loss: 0.1728  time: 1.4955  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2270/3000]  eta: 0:18:25  lr: 0.000029  loss: 0.1808  time: 1.5122  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2270/3000]  eta: 0:18:24  lr: 0.000029  loss: 0.5238  time: 1.5120  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2275/3000]  eta: 0:18:17  lr: 0.000029  loss: 0.1125  time: 1.4936  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2275/3000]  eta: 0:18:17  lr: 0.000029  loss: 0.4144  time: 1.4934  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2280/3000]  eta: 0:18:09  lr: 0.000029  loss: 0.8872  time: 1.4994  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2280/3000]  eta: 0:18:09  lr: 0.000029  loss: 0.0958  time: 1.4992  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2285/3000]  eta: 0:18:02  lr: 0.000029  loss: 0.1564  time: 1.4911  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2285/3000]  eta: 0:18:01  lr: 0.000029  loss: 0.3919  time: 1.4909  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2290/3000]  eta: 0:17:54  lr: 0.000029  loss: 0.2807  time: 1.4556  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2290/3000]  eta: 0:17:54  lr: 0.000029  loss: 0.3371  time: 1.4553  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2295/3000]  eta: 0:17:47  lr: 0.000029  loss: 0.0840  time: 1.4946  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2295/3000]  eta: 0:17:46  lr: 0.000029  loss: 0.1786  time: 1.4943  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2300/3000]  eta: 0:17:39  lr: 0.000029  loss: 0.4709  time: 1.5045  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2300/3000]  eta: 0:17:39  lr: 0.000029  loss: 0.2850  time: 1.5042  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2305/3000]  eta: 0:17:31  lr: 0.000029  loss: 0.2268  time: 1.5206  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2305/3000]  eta: 0:17:31  lr: 0.000029  loss: 0.1881  time: 1.5204  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2310/3000]  eta: 0:17:24  lr: 0.000029  loss: 0.2310  time: 1.5337  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2310/3000]  eta: 0:17:24  lr: 0.000029  loss: 0.1756  time: 1.5335  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2315/3000]  eta: 0:17:16  lr: 0.000029  loss: 0.5115  time: 1.4945  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2315/3000]  eta: 0:17:16  lr: 0.000029  loss: 0.6102  time: 1.4942  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2320/3000]  eta: 0:17:08  lr: 0.000029  loss: 0.6539  time: 1.4798  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2320/3000]  eta: 0:17:08  lr: 0.000029  loss: 0.4323  time: 1.4796  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2325/3000]  eta: 0:17:01  lr: 0.000029  loss: 0.4828  time: 1.4491  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2325/3000]  eta: 0:17:01  lr: 0.000029  loss: 0.1242  time: 1.4488  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2330/3000]  eta: 0:16:53  lr: 0.000029  loss: 0.2633  time: 1.4634  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2330/3000]  eta: 0:16:53  lr: 0.000029  loss: 0.2002  time: 1.4632  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2335/3000]  eta: 0:16:46  lr: 0.000029  loss: 0.4210  time: 1.5091  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2335/3000]  eta: 0:16:46  lr: 0.000029  loss: 0.3883  time: 1.5085  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2340/3000]  eta: 0:16:38  lr: 0.000029  loss: 0.2432  time: 1.5185  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2340/3000]  eta: 0:16:38  lr: 0.000029  loss: 0.1604  time: 1.5179  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2345/3000]  eta: 0:16:31  lr: 0.000029  loss: 0.4791  time: 1.5445  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2345/3000]  eta: 0:16:31  lr: 0.000029  loss: 0.1439  time: 1.5439  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2350/3000]  eta: 0:16:23  lr: 0.000029  loss: 0.1981  time: 1.5279  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2350/3000]  eta: 0:16:23  lr: 0.000029  loss: 0.3501  time: 1.5273  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2355/3000]  eta: 0:16:15  lr: 0.000029  loss: 0.3626  time: 1.4982  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2355/3000]  eta: 0:16:15  lr: 0.000029  loss: 0.2188  time: 1.4979  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2360/3000]  eta: 0:16:08  lr: 0.000029  loss: 0.5800  time: 1.5079  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2360/3000]  eta: 0:16:08  lr: 0.000029  loss: 0.2248  time: 1.5076  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2365/3000]  eta: 0:16:00  lr: 0.000029  loss: 0.6991  time: 1.4872  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2365/3000]  eta: 0:16:00  lr: 0.000029  loss: 0.2063  time: 1.4869  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2370/3000]  eta: 0:15:53  lr: 0.000029  loss: 0.3749  time: 1.4751  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2370/3000]  eta: 0:15:52  lr: 0.000029  loss: 0.1457  time: 1.4748  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2375/3000]  eta: 0:15:45  lr: 0.000029  loss: 0.2107  time: 1.4803  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2375/3000]  eta: 0:15:45  lr: 0.000029  loss: 0.2676  time: 1.4801  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2380/3000]  eta: 0:15:37  lr: 0.000029  loss: 0.8021  time: 1.4716  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2380/3000]  eta: 0:15:37  lr: 0.000029  loss: 0.2476  time: 1.4713  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2385/3000]  eta: 0:15:30  lr: 0.000029  loss: 0.6768  time: 1.4959  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2385/3000]  eta: 0:15:30  lr: 0.000029  loss: 0.1413  time: 1.4956  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2390/3000]  eta: 0:15:22  lr: 0.000029  loss: 0.5213  time: 1.5221  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2390/3000]  eta: 0:15:22  lr: 0.000029  loss: 0.5457  time: 1.5218  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2395/3000]  eta: 0:15:15  lr: 0.000029  loss: 0.3106  time: 1.5233  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2395/3000]  eta: 0:15:15  lr: 0.000029  loss: 0.7885  time: 1.5230  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2400/3000]  eta: 0:15:07  lr: 0.000029  loss: 0.3532  time: 1.5383  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2400/3000]  eta: 0:15:07  lr: 0.000029  loss: 0.5357  time: 1.5380  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2405/3000]  eta: 0:15:00  lr: 0.000029  loss: 0.6904  time: 1.5414  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2405/3000]  eta: 0:15:00  lr: 0.000029  loss: 0.1202  time: 1.5411  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2410/3000]  eta: 0:14:52  lr: 0.000029  loss: 0.6534  time: 1.5408  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2410/3000]  eta: 0:14:52  lr: 0.000029  loss: 0.3639  time: 1.5406  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2415/3000]  eta: 0:14:45  lr: 0.000029  loss: 0.3135  time: 1.5230  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2415/3000]  eta: 0:14:44  lr: 0.000029  loss: 0.2692  time: 1.5228  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2420/3000]  eta: 0:14:37  lr: 0.000029  loss: 0.5663  time: 1.5284  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2420/3000]  eta: 0:14:37  lr: 0.000029  loss: 0.3923  time: 1.5283  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2425/3000]  eta: 0:14:30  lr: 0.000029  loss: 0.3932  time: 1.5193  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2425/3000]  eta: 0:14:29  lr: 0.000029  loss: 0.2356  time: 1.5191  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2430/3000]  eta: 0:14:22  lr: 0.000029  loss: 0.6621  time: 1.5202  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2430/3000]  eta: 0:14:22  lr: 0.000029  loss: 0.5849  time: 1.5200  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2435/3000]  eta: 0:14:15  lr: 0.000029  loss: 0.4065  time: 1.5476  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2435/3000]  eta: 0:14:14  lr: 0.000029  loss: 0.2666  time: 1.5474  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2440/3000]  eta: 0:14:07  lr: 0.000029  loss: 0.2737  time: 1.5377  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2440/3000]  eta: 0:14:07  lr: 0.000029  loss: 0.7514  time: 1.5372  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2445/3000]  eta: 0:14:00  lr: 0.000029  loss: 0.3393  time: 1.5517  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2445/3000]  eta: 0:13:59  lr: 0.000029  loss: 0.6489  time: 1.5513  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2450/3000]  eta: 0:13:52  lr: 0.000029  loss: 0.2328  time: 1.5380  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2450/3000]  eta: 0:13:52  lr: 0.000029  loss: 0.0981  time: 1.5376  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2455/3000]  eta: 0:13:44  lr: 0.000029  loss: 0.4441  time: 1.5309  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2455/3000]  eta: 0:13:44  lr: 0.000029  loss: 0.4732  time: 1.5304  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2460/3000]  eta: 0:13:37  lr: 0.000029  loss: 0.1488  time: 1.5202  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2460/3000]  eta: 0:13:37  lr: 0.000029  loss: 0.1669  time: 1.5200  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2465/3000]  eta: 0:13:29  lr: 0.000029  loss: 0.1428  time: 1.5089  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2465/3000]  eta: 0:13:29  lr: 0.000029  loss: 0.7029  time: 1.5087  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2470/3000]  eta: 0:13:22  lr: 0.000029  loss: 0.1987  time: 1.5373  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2470/3000]  eta: 0:13:22  lr: 0.000029  loss: 0.4787  time: 1.5371  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2475/3000]  eta: 0:13:14  lr: 0.000029  loss: 0.7099  time: 1.5368  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2475/3000]  eta: 0:13:14  lr: 0.000029  loss: 0.1516  time: 1.5366  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2480/3000]  eta: 0:13:07  lr: 0.000029  loss: 0.2444  time: 1.5359  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2480/3000]  eta: 0:13:06  lr: 0.000029  loss: 0.2487  time: 1.5356  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2485/3000]  eta: 0:12:59  lr: 0.000029  loss: 0.7693  time: 1.5173  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2485/3000]  eta: 0:12:59  lr: 0.000029  loss: 0.9442  time: 1.5170  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2490/3000]  eta: 0:12:51  lr: 0.000029  loss: 0.2436  time: 1.4851  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2490/3000]  eta: 0:12:51  lr: 0.000029  loss: 0.2315  time: 1.4848  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2495/3000]  eta: 0:12:44  lr: 0.000029  loss: 0.5276  time: 1.4923  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2495/3000]  eta: 0:12:44  lr: 0.000029  loss: 0.3669  time: 1.4920  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2500/3000]  eta: 0:12:36  lr: 0.000029  loss: 0.3720  time: 1.5021  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2500/3000]  eta: 0:12:36  lr: 0.000029  loss: 0.2481  time: 1.5018  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2505/3000]  eta: 0:12:29  lr: 0.000029  loss: 0.3885  time: 1.5230  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2505/3000]  eta: 0:12:29  lr: 0.000029  loss: 0.4170  time: 1.5228  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2510/3000]  eta: 0:12:21  lr: 0.000029  loss: 0.1271  time: 1.4962  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2510/3000]  eta: 0:12:21  lr: 0.000029  loss: 0.1422  time: 1.4959  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2515/3000]  eta: 0:12:13  lr: 0.000029  loss: 0.2442  time: 1.5007  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2515/3000]  eta: 0:12:13  lr: 0.000029  loss: 0.2846  time: 1.5004  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2520/3000]  eta: 0:12:06  lr: 0.000029  loss: 0.6324  time: 1.5048  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2520/3000]  eta: 0:12:06  lr: 0.000029  loss: 0.5720  time: 1.5045  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2525/3000]  eta: 0:11:58  lr: 0.000029  loss: 0.1982  time: 1.4900  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2525/3000]  eta: 0:11:58  lr: 0.000029  loss: 0.6593  time: 1.4897  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2530/3000]  eta: 0:11:51  lr: 0.000029  loss: 0.1726  time: 1.5152  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2530/3000]  eta: 0:11:51  lr: 0.000029  loss: 0.3232  time: 1.5150  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2535/3000]  eta: 0:11:43  lr: 0.000029  loss: 0.0574  time: 1.4916  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2535/3000]  eta: 0:11:43  lr: 0.000029  loss: 0.1136  time: 1.4914  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2540/3000]  eta: 0:11:36  lr: 0.000029  loss: 0.4386  time: 1.4855  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2540/3000]  eta: 0:11:35  lr: 0.000029  loss: 0.2286  time: 1.4852  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2545/3000]  eta: 0:11:28  lr: 0.000029  loss: 0.3169  time: 1.5010  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2545/3000]  eta: 0:11:28  lr: 0.000029  loss: 0.9025  time: 1.5006  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2550/3000]  eta: 0:11:20  lr: 0.000029  loss: 0.3216  time: 1.5071  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2550/3000]  eta: 0:11:20  lr: 0.000029  loss: 0.5245  time: 1.5068  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2555/3000]  eta: 0:11:13  lr: 0.000029  loss: 0.2188  time: 1.5250  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2555/3000]  eta: 0:11:13  lr: 0.000029  loss: 0.2598  time: 1.5247  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2560/3000]  eta: 0:11:05  lr: 0.000029  loss: 0.2027  time: 1.5128  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2560/3000]  eta: 0:11:05  lr: 0.000029  loss: 0.1021  time: 1.5125  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2565/3000]  eta: 0:10:58  lr: 0.000029  loss: 0.2064  time: 1.5092  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2565/3000]  eta: 0:10:58  lr: 0.000029  loss: 0.6069  time: 1.5090  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2570/3000]  eta: 0:10:50  lr: 0.000029  loss: 0.2608  time: 1.5156  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2570/3000]  eta: 0:10:50  lr: 0.000029  loss: 0.4272  time: 1.5154  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2575/3000]  eta: 0:10:43  lr: 0.000029  loss: 0.5006  time: 1.4887  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2575/3000]  eta: 0:10:42  lr: 0.000029  loss: 0.4742  time: 1.4885  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2580/3000]  eta: 0:10:35  lr: 0.000029  loss: 0.5721  time: 1.4971  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2580/3000]  eta: 0:10:35  lr: 0.000029  loss: 0.4275  time: 1.4969  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2585/3000]  eta: 0:10:27  lr: 0.000029  loss: 0.4236  time: 1.4894  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2585/3000]  eta: 0:10:27  lr: 0.000029  loss: 0.5166  time: 1.4892  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2590/3000]  eta: 0:10:20  lr: 0.000029  loss: 0.2600  time: 1.5029  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2590/3000]  eta: 0:10:20  lr: 0.000029  loss: 0.2644  time: 1.5026  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2595/3000]  eta: 0:10:12  lr: 0.000029  loss: 0.1500  time: 1.5354  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2595/3000]  eta: 0:10:12  lr: 0.000029  loss: 0.5409  time: 1.5352  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2600/3000]  eta: 0:10:05  lr: 0.000029  loss: 0.3019  time: 1.5336  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2600/3000]  eta: 0:10:05  lr: 0.000029  loss: 0.3576  time: 1.5334  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2605/3000]  eta: 0:09:57  lr: 0.000029  loss: 0.1719  time: 1.5494  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2605/3000]  eta: 0:09:57  lr: 0.000029  loss: 0.2434  time: 1.5491  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2610/3000]  eta: 0:09:50  lr: 0.000029  loss: 0.3441  time: 1.5241  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2610/3000]  eta: 0:09:50  lr: 0.000029  loss: 0.1367  time: 1.5239  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2615/3000]  eta: 0:09:42  lr: 0.000029  loss: 0.3280  time: 1.5236  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2615/3000]  eta: 0:09:42  lr: 0.000029  loss: 0.1505  time: 1.5233  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2620/3000]  eta: 0:09:35  lr: 0.000029  loss: 0.5839  time: 1.5294  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2620/3000]  eta: 0:09:34  lr: 0.000029  loss: 0.1429  time: 1.5292  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2625/3000]  eta: 0:09:27  lr: 0.000029  loss: 0.2844  time: 1.5189  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2625/3000]  eta: 0:09:27  lr: 0.000029  loss: 0.1033  time: 1.5186  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2630/3000]  eta: 0:09:20  lr: 0.000029  loss: 0.2855  time: 1.5483  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2630/3000]  eta: 0:09:19  lr: 0.000029  loss: 0.2960  time: 1.5480  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2635/3000]  eta: 0:09:12  lr: 0.000029  loss: 0.1741  time: 1.5465  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2635/3000]  eta: 0:09:12  lr: 0.000029  loss: 0.5470  time: 1.5463  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2640/3000]  eta: 0:09:04  lr: 0.000029  loss: 0.1659  time: 1.5398  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2640/3000]  eta: 0:09:04  lr: 0.000029  loss: 0.3300  time: 1.5395  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2645/3000]  eta: 0:08:57  lr: 0.000029  loss: 0.5801  time: 1.5509  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2645/3000]  eta: 0:08:57  lr: 0.000029  loss: 0.4283  time: 1.5508  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2650/3000]  eta: 0:08:49  lr: 0.000029  loss: 0.3742  time: 1.5297  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2650/3000]  eta: 0:08:49  lr: 0.000029  loss: 0.4461  time: 1.5294  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2655/3000]  eta: 0:08:42  lr: 0.000029  loss: 0.4197  time: 1.5328  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2655/3000]  eta: 0:08:42  lr: 0.000029  loss: 1.1851  time: 1.5325  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2660/3000]  eta: 0:08:34  lr: 0.000029  loss: 0.6261  time: 1.5347  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2660/3000]  eta: 0:08:34  lr: 0.000029  loss: 1.2571  time: 1.5344  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2665/3000]  eta: 0:08:27  lr: 0.000029  loss: 0.3384  time: 1.5032  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2665/3000]  eta: 0:08:26  lr: 0.000029  loss: 0.4393  time: 1.5029  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2670/3000]  eta: 0:08:19  lr: 0.000029  loss: 0.1968  time: 1.4962  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2670/3000]  eta: 0:08:19  lr: 0.000029  loss: 0.1209  time: 1.4960  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2675/3000]  eta: 0:08:11  lr: 0.000029  loss: 0.3522  time: 1.4638  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2675/3000]  eta: 0:08:11  lr: 0.000029  loss: 0.4053  time: 1.4636  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2680/3000]  eta: 0:08:04  lr: 0.000029  loss: 0.3903  time: 1.4494  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2680/3000]  eta: 0:08:04  lr: 0.000029  loss: 0.1180  time: 1.4492  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2685/3000]  eta: 0:07:56  lr: 0.000029  loss: 1.6924  time: 1.4658  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2685/3000]  eta: 0:07:56  lr: 0.000029  loss: 0.2429  time: 1.4656  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2690/3000]  eta: 0:07:49  lr: 0.000029  loss: 0.4484  time: 1.4799  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2690/3000]  eta: 0:07:49  lr: 0.000029  loss: 0.2486  time: 1.4797  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2695/3000]  eta: 0:07:41  lr: 0.000029  loss: 0.3851  time: 1.5153  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2695/3000]  eta: 0:07:41  lr: 0.000029  loss: 0.4802  time: 1.5150  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2700/3000]  eta: 0:07:33  lr: 0.000029  loss: 0.2907  time: 1.5103  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2700/3000]  eta: 0:07:33  lr: 0.000029  loss: 0.5623  time: 1.5100  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2705/3000]  eta: 0:07:26  lr: 0.000029  loss: 0.7776  time: 1.5211  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2705/3000]  eta: 0:07:26  lr: 0.000029  loss: 0.4583  time: 1.5209  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2710/3000]  eta: 0:07:18  lr: 0.000029  loss: 0.7666  time: 1.5165  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2710/3000]  eta: 0:07:18  lr: 0.000029  loss: 0.2682  time: 1.5162  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2715/3000]  eta: 0:07:11  lr: 0.000029  loss: 0.4141  time: 1.5100  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2715/3000]  eta: 0:07:11  lr: 0.000029  loss: 1.0387  time: 1.5098  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2720/3000]  eta: 0:07:03  lr: 0.000029  loss: 0.3350  time: 1.5174  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2720/3000]  eta: 0:07:03  lr: 0.000029  loss: 0.1151  time: 1.5172  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2725/3000]  eta: 0:06:56  lr: 0.000029  loss: 0.5421  time: 1.5044  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2725/3000]  eta: 0:06:56  lr: 0.000029  loss: 0.5395  time: 1.5042  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2730/3000]  eta: 0:06:48  lr: 0.000029  loss: 0.3631  time: 1.4770  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2730/3000]  eta: 0:06:48  lr: 0.000029  loss: 0.4704  time: 1.4768  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2735/3000]  eta: 0:06:40  lr: 0.000029  loss: 0.3631  time: 1.4740  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2735/3000]  eta: 0:06:40  lr: 0.000029  loss: 0.5535  time: 1.4737  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2740/3000]  eta: 0:06:33  lr: 0.000029  loss: 0.3180  time: 1.4907  data: 0.0000  max mem: 18151Train: data epoch: [4]  [2740/3000]  eta: 0:06:33  lr: 0.000029  loss: 0.1167  time: 1.4909  data: 0.0000  max mem: 18432

Train: data epoch: [4]  [2745/3000]  eta: 0:06:25  lr: 0.000029  loss: 0.4260  time: 1.5069  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2745/3000]  eta: 0:06:25  lr: 0.000029  loss: 0.8252  time: 1.5067  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2750/3000]  eta: 0:06:18  lr: 0.000029  loss: 0.4261  time: 1.5535  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2750/3000]  eta: 0:06:18  lr: 0.000029  loss: 0.6526  time: 1.5534  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2755/3000]  eta: 0:06:10  lr: 0.000029  loss: 0.2327  time: 1.5443  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2755/3000]  eta: 0:06:10  lr: 0.000029  loss: 0.5667  time: 1.5441  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2760/3000]  eta: 0:06:03  lr: 0.000029  loss: 0.3177  time: 1.5476  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2760/3000]  eta: 0:06:03  lr: 0.000029  loss: 0.2278  time: 1.5474  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2765/3000]  eta: 0:05:55  lr: 0.000029  loss: 0.3931  time: 1.5479  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2765/3000]  eta: 0:05:55  lr: 0.000029  loss: 0.2354  time: 1.5476  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2770/3000]  eta: 0:05:48  lr: 0.000029  loss: 0.1566  time: 1.5261  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2770/3000]  eta: 0:05:48  lr: 0.000029  loss: 0.0956  time: 1.5259  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2775/3000]  eta: 0:05:40  lr: 0.000029  loss: 0.7897  time: 1.5114  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2775/3000]  eta: 0:05:40  lr: 0.000029  loss: 0.2099  time: 1.5112  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2780/3000]  eta: 0:05:32  lr: 0.000029  loss: 0.6624  time: 1.4977  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2780/3000]  eta: 0:05:32  lr: 0.000029  loss: 0.1224  time: 1.4974  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2785/3000]  eta: 0:05:25  lr: 0.000029  loss: 0.1852  time: 1.4914  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2785/3000]  eta: 0:05:25  lr: 0.000029  loss: 0.1390  time: 1.4911  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2790/3000]  eta: 0:05:17  lr: 0.000029  loss: 0.2715  time: 1.4966  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2790/3000]  eta: 0:05:17  lr: 0.000029  loss: 0.2820  time: 1.4964  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2795/3000]  eta: 0:05:10  lr: 0.000029  loss: 0.5757  time: 1.4956  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2795/3000]  eta: 0:05:10  lr: 0.000029  loss: 0.2862  time: 1.4953  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2800/3000]  eta: 0:05:02  lr: 0.000029  loss: 0.4817  time: 1.5082  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2800/3000]  eta: 0:05:02  lr: 0.000029  loss: 0.3583  time: 1.5080  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2805/3000]  eta: 0:04:55  lr: 0.000029  loss: 0.5189  time: 1.4756  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2805/3000]  eta: 0:04:54  lr: 0.000029  loss: 0.3076  time: 1.4753  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2810/3000]  eta: 0:04:47  lr: 0.000029  loss: 0.3887  time: 1.4839  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2810/3000]  eta: 0:04:47  lr: 0.000029  loss: 0.3583  time: 1.4837  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2815/3000]  eta: 0:04:39  lr: 0.000029  loss: 0.2848  time: 1.5238  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2815/3000]  eta: 0:04:39  lr: 0.000029  loss: 0.3640  time: 1.5235  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2820/3000]  eta: 0:04:32  lr: 0.000029  loss: 0.5589  time: 1.5171  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2820/3000]  eta: 0:04:32  lr: 0.000029  loss: 0.8400  time: 1.5169  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2825/3000]  eta: 0:04:24  lr: 0.000029  loss: 0.0741  time: 1.5628  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2825/3000]  eta: 0:04:24  lr: 0.000029  loss: 0.3385  time: 1.5636  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2830/3000]  eta: 0:04:17  lr: 0.000029  loss: 0.1374  time: 1.5650  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2830/3000]  eta: 0:04:17  lr: 0.000029  loss: 0.3463  time: 1.5647  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2835/3000]  eta: 0:04:09  lr: 0.000029  loss: 0.5912  time: 1.5560  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2835/3000]  eta: 0:04:09  lr: 0.000029  loss: 0.4494  time: 1.5558  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2840/3000]  eta: 0:04:02  lr: 0.000029  loss: 0.1811  time: 1.5276  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2840/3000]  eta: 0:04:02  lr: 0.000029  loss: 0.2805  time: 1.5274  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2845/3000]  eta: 0:03:54  lr: 0.000029  loss: 0.5823  time: 1.5160  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2845/3000]  eta: 0:03:54  lr: 0.000029  loss: 0.4069  time: 1.5148  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2850/3000]  eta: 0:03:47  lr: 0.000029  loss: 1.1129  time: 1.5000  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2850/3000]  eta: 0:03:46  lr: 0.000029  loss: 0.5452  time: 1.4998  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2855/3000]  eta: 0:03:39  lr: 0.000029  loss: 0.3627  time: 1.5051  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2855/3000]  eta: 0:03:39  lr: 0.000029  loss: 0.2898  time: 1.5048  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2860/3000]  eta: 0:03:31  lr: 0.000029  loss: 0.2603  time: 1.5426  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2860/3000]  eta: 0:03:31  lr: 0.000029  loss: 0.1772  time: 1.5423  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2865/3000]  eta: 0:03:24  lr: 0.000029  loss: 0.4597  time: 1.5548  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2865/3000]  eta: 0:03:24  lr: 0.000029  loss: 0.5827  time: 1.5545  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2870/3000]  eta: 0:03:16  lr: 0.000029  loss: 0.2261  time: 1.5530  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2870/3000]  eta: 0:03:16  lr: 0.000029  loss: 0.1450  time: 1.5527  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2875/3000]  eta: 0:03:09  lr: 0.000029  loss: 0.2807  time: 1.5518  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2875/3000]  eta: 0:03:09  lr: 0.000029  loss: 0.2330  time: 1.5515  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2880/3000]  eta: 0:03:01  lr: 0.000029  loss: 1.0645  time: 1.5405  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2880/3000]  eta: 0:03:01  lr: 0.000029  loss: 0.4584  time: 1.5402  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2885/3000]  eta: 0:02:54  lr: 0.000029  loss: 0.1454  time: 1.5204  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2885/3000]  eta: 0:02:54  lr: 0.000029  loss: 1.0689  time: 1.5202  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2890/3000]  eta: 0:02:46  lr: 0.000029  loss: 0.1745  time: 1.5214  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2890/3000]  eta: 0:02:46  lr: 0.000029  loss: 0.0425  time: 1.5213  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2895/3000]  eta: 0:02:38  lr: 0.000029  loss: 0.3216  time: 1.5018  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2895/3000]  eta: 0:02:38  lr: 0.000029  loss: 0.1669  time: 1.5017  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2900/3000]  eta: 0:02:31  lr: 0.000029  loss: 0.4434  time: 1.4900  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2900/3000]  eta: 0:02:31  lr: 0.000029  loss: 0.1484  time: 1.4899  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2905/3000]  eta: 0:02:23  lr: 0.000029  loss: 0.1951  time: 1.4869  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2905/3000]  eta: 0:02:23  lr: 0.000029  loss: 0.3535  time: 1.4867  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2910/3000]  eta: 0:02:16  lr: 0.000029  loss: 0.3733  time: 1.5028  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2910/3000]  eta: 0:02:16  lr: 0.000029  loss: 0.1472  time: 1.5026  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2915/3000]  eta: 0:02:08  lr: 0.000029  loss: 0.4204  time: 1.5139  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2915/3000]  eta: 0:02:08  lr: 0.000029  loss: 0.3676  time: 1.5138  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2920/3000]  eta: 0:02:01  lr: 0.000029  loss: 0.2566  time: 1.4965  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2920/3000]  eta: 0:02:01  lr: 0.000029  loss: 0.3089  time: 1.4962  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2925/3000]  eta: 0:01:53  lr: 0.000029  loss: 0.5081  time: 1.5099  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2925/3000]  eta: 0:01:53  lr: 0.000029  loss: 0.2025  time: 1.5097  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2930/3000]  eta: 0:01:45  lr: 0.000029  loss: 0.1714  time: 1.5011  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2930/3000]  eta: 0:01:45  lr: 0.000029  loss: 0.5214  time: 1.5010  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2935/3000]  eta: 0:01:38  lr: 0.000029  loss: 0.0881  time: 1.4893  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2935/3000]  eta: 0:01:38  lr: 0.000029  loss: 0.1761  time: 1.4891  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2940/3000]  eta: 0:01:30  lr: 0.000029  loss: 0.5497  time: 1.5299  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2940/3000]  eta: 0:01:30  lr: 0.000029  loss: 0.3070  time: 1.5297  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2945/3000]  eta: 0:01:23  lr: 0.000029  loss: 0.2438  time: 1.5382  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2945/3000]  eta: 0:01:23  lr: 0.000029  loss: 0.3373  time: 1.5380  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2950/3000]  eta: 0:01:15  lr: 0.000029  loss: 0.3657  time: 1.5386  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2950/3000]  eta: 0:01:15  lr: 0.000029  loss: 0.7154  time: 1.5384  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2955/3000]  eta: 0:01:08  lr: 0.000029  loss: 0.2555  time: 1.5479  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2955/3000]  eta: 0:01:08  lr: 0.000029  loss: 0.5138  time: 1.5476  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2960/3000]  eta: 0:01:00  lr: 0.000029  loss: 0.6369  time: 1.5457  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2960/3000]  eta: 0:01:00  lr: 0.000029  loss: 0.1501  time: 1.5455  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2965/3000]  eta: 0:00:52  lr: 0.000029  loss: 0.2745  time: 1.5270  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2965/3000]  eta: 0:00:52  lr: 0.000029  loss: 0.2902  time: 1.5268  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2970/3000]  eta: 0:00:45  lr: 0.000029  loss: 0.4721  time: 1.5263  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2970/3000]  eta: 0:00:45  lr: 0.000029  loss: 0.6651  time: 1.5260  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2975/3000]  eta: 0:00:37  lr: 0.000029  loss: 0.1851  time: 1.5135  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2975/3000]  eta: 0:00:37  lr: 0.000029  loss: 0.5926  time: 1.5133  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2980/3000]  eta: 0:00:30  lr: 0.000029  loss: 0.3415  time: 1.4956  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2980/3000]  eta: 0:00:30  lr: 0.000029  loss: 0.1290  time: 1.4954  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2985/3000]  eta: 0:00:22  lr: 0.000029  loss: 0.1977  time: 1.4642  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2985/3000]  eta: 0:00:22  lr: 0.000029  loss: 0.5852  time: 1.4639  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2990/3000]  eta: 0:00:15  lr: 0.000029  loss: 0.3157  time: 1.4711  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2990/3000]  eta: 0:00:15  lr: 0.000029  loss: 0.1569  time: 1.4709  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2995/3000]  eta: 0:00:07  lr: 0.000029  loss: 0.4170  time: 1.4697  data: 0.0000  max mem: 18432
Train: data epoch: [4]  [2995/3000]  eta: 0:00:07  lr: 0.000029  loss: 0.2440  time: 1.4695  data: 0.0000  max mem: 18151
Train: data epoch: [4]  [2999/3000]  eta: 0:00:01  lr: 0.000029  loss: 0.3622  time: 1.4739  data: 0.0000  max mem: 18432
Train: data epoch: [4] Total time: 1:15:40 (1.5136 s / it)
Train: data epoch: [4]  [2999/3000]  eta: 0:00:01  lr: 0.000029  loss: 0.2517  time: 1.4736  data: 0.0000  max mem: 18151
Train: data epoch: [4] Total time: 1:15:40 (1.5136 s / it)
2025-01-19 05:59:03,902 [INFO] Averaged stats: lr: 0.0000  loss: 0.4240
2025-01-19 05:59:03,908 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [4]  [0/1]  eta: 0:00:00    time: 0.7763  data: 0.4861  max mem: 18151
Eval: data epoch: [4] Total time: 0:00:00 (0.8953 s / it)
Eval: data epoch: [4]  [0/1]  eta: 0:00:00    time: 0.9329  data: 0.6399  max mem: 18432
Eval: data epoch: [4] Total time: 0:00:01 (1.0599 s / it)
2025-01-19 05:59:04,992 [INFO] Saving checkpoint at epoch 4 to outputs_stage1_only/202501182338/checkpoint_4.pth.
2025-01-19 05:59:07,388 [INFO] Training Phase
2025-01-19 05:59:07,396 [INFO] Start training epoch 5, 3000 iters per inner epoch.
Train: data epoch: [5]  [   0/3000]  eta: 1:19:28  lr: 0.000029  loss: 0.2911  time: 1.5895  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [   0/3000]  eta: 1:19:27  lr: 0.000029  loss: 0.7855  time: 1.5891  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [   5/3000]  eta: 1:11:34  lr: 0.000029  loss: 0.4367  time: 1.4340  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [   5/3000]  eta: 1:11:34  lr: 0.000029  loss: 0.3420  time: 1.4340  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [  10/3000]  eta: 1:13:31  lr: 0.000029  loss: 0.3405  time: 1.4754  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [  10/3000]  eta: 1:13:30  lr: 0.000029  loss: 0.7530  time: 1.4752  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [  15/3000]  eta: 1:14:52  lr: 0.000029  loss: 0.3294  time: 1.5052  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [  15/3000]  eta: 1:14:52  lr: 0.000029  loss: 0.3474  time: 1.5050  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [  20/3000]  eta: 1:14:29  lr: 0.000029  loss: 0.2948  time: 1.4952  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [  20/3000]  eta: 1:14:28  lr: 0.000029  loss: 0.4641  time: 1.4950  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [  25/3000]  eta: 1:14:54  lr: 0.000029  loss: 0.3521  time: 1.5338  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [  25/3000]  eta: 1:14:53  lr: 0.000029  loss: 0.1893  time: 1.5335  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [  30/3000]  eta: 1:14:41  lr: 0.000029  loss: 0.7882  time: 1.5276  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [  30/3000]  eta: 1:14:41  lr: 0.000029  loss: 0.3246  time: 1.5273  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [  35/3000]  eta: 1:14:01  lr: 0.000029  loss: 0.2146  time: 1.4922  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [  35/3000]  eta: 1:14:00  lr: 0.000029  loss: 0.2798  time: 1.4920  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [  40/3000]  eta: 1:14:15  lr: 0.000029  loss: 0.8360  time: 1.5112  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [  40/3000]  eta: 1:14:15  lr: 0.000029  loss: 0.9137  time: 1.5110  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [  45/3000]  eta: 1:14:13  lr: 0.000029  loss: 0.2457  time: 1.5022  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [  45/3000]  eta: 1:14:12  lr: 0.000029  loss: 0.4482  time: 1.5019  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [  50/3000]  eta: 1:14:03  lr: 0.000029  loss: 0.1433  time: 1.5016  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [  50/3000]  eta: 1:14:02  lr: 0.000029  loss: 0.2526  time: 1.5014  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [  55/3000]  eta: 1:14:06  lr: 0.000029  loss: 0.3553  time: 1.5315  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [  55/3000]  eta: 1:14:05  lr: 0.000029  loss: 0.4554  time: 1.5312  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [  60/3000]  eta: 1:13:44  lr: 0.000029  loss: 0.2157  time: 1.5043  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [  60/3000]  eta: 1:13:43  lr: 0.000029  loss: 0.8042  time: 1.5040  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [  65/3000]  eta: 1:13:22  lr: 0.000029  loss: 0.2823  time: 1.4834  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [  65/3000]  eta: 1:13:21  lr: 0.000029  loss: 0.4894  time: 1.4831  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [  70/3000]  eta: 1:13:07  lr: 0.000029  loss: 0.1942  time: 1.4755  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [  70/3000]  eta: 1:13:06  lr: 0.000029  loss: 0.4925  time: 1.4752  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [  75/3000]  eta: 1:13:08  lr: 0.000029  loss: 0.4619  time: 1.4741  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [  75/3000]  eta: 1:13:08  lr: 0.000029  loss: 0.5367  time: 1.4739  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [  80/3000]  eta: 1:13:01  lr: 0.000029  loss: 0.2349  time: 1.4866  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [  80/3000]  eta: 1:13:00  lr: 0.000029  loss: 0.4637  time: 1.4864  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [  85/3000]  eta: 1:13:04  lr: 0.000029  loss: 0.7343  time: 1.5185  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [  85/3000]  eta: 1:13:04  lr: 0.000029  loss: 0.6296  time: 1.5183  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [  90/3000]  eta: 1:13:10  lr: 0.000029  loss: 0.3560  time: 1.5485  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [  90/3000]  eta: 1:13:09  lr: 0.000029  loss: 0.3486  time: 1.5482  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [  95/3000]  eta: 1:13:15  lr: 0.000029  loss: 0.6039  time: 1.5605  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [  95/3000]  eta: 1:13:14  lr: 0.000029  loss: 0.5466  time: 1.5602  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 100/3000]  eta: 1:13:16  lr: 0.000029  loss: 0.7155  time: 1.5796  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 100/3000]  eta: 1:13:16  lr: 0.000029  loss: 0.4378  time: 1.5794  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 105/3000]  eta: 1:13:09  lr: 0.000029  loss: 0.2390  time: 1.5686  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 105/3000]  eta: 1:13:09  lr: 0.000029  loss: 0.5716  time: 1.5684  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 110/3000]  eta: 1:13:06  lr: 0.000029  loss: 0.3076  time: 1.5587  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 110/3000]  eta: 1:13:05  lr: 0.000029  loss: 0.4162  time: 1.5585  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 115/3000]  eta: 1:13:01  lr: 0.000029  loss: 0.5869  time: 1.5472  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 115/3000]  eta: 1:13:01  lr: 0.000029  loss: 0.3120  time: 1.5470  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 120/3000]  eta: 1:13:00  lr: 0.000029  loss: 0.2620  time: 1.5460  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 120/3000]  eta: 1:12:59  lr: 0.000029  loss: 0.5988  time: 1.5458  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 125/3000]  eta: 1:12:49  lr: 0.000029  loss: 0.1731  time: 1.5375  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 125/3000]  eta: 1:12:48  lr: 0.000029  loss: 0.1923  time: 1.5372  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 130/3000]  eta: 1:12:41  lr: 0.000029  loss: 0.4771  time: 1.5307  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 130/3000]  eta: 1:12:40  lr: 0.000029  loss: 0.3596  time: 1.5304  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 135/3000]  eta: 1:12:35  lr: 0.000029  loss: 0.8575  time: 1.5275  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 135/3000]  eta: 1:12:34  lr: 0.000029  loss: 0.2581  time: 1.5272  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 140/3000]  eta: 1:12:28  lr: 0.000029  loss: 0.4345  time: 1.5167  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 140/3000]  eta: 1:12:27  lr: 0.000029  loss: 0.4293  time: 1.5165  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 145/3000]  eta: 1:12:22  lr: 0.000029  loss: 0.1212  time: 1.5289  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 145/3000]  eta: 1:12:21  lr: 0.000029  loss: 0.4082  time: 1.5287  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 150/3000]  eta: 1:12:10  lr: 0.000029  loss: 0.2160  time: 1.5186  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 150/3000]  eta: 1:12:10  lr: 0.000029  loss: 0.6804  time: 1.5183  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 155/3000]  eta: 1:11:58  lr: 0.000029  loss: 0.2671  time: 1.5022  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 155/3000]  eta: 1:11:57  lr: 0.000029  loss: 0.4062  time: 1.5019  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 160/3000]  eta: 1:11:45  lr: 0.000029  loss: 0.3667  time: 1.4851  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 160/3000]  eta: 1:11:44  lr: 0.000029  loss: 0.2881  time: 1.4848  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 165/3000]  eta: 1:11:38  lr: 0.000029  loss: 0.2994  time: 1.4825  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 165/3000]  eta: 1:11:38  lr: 0.000029  loss: 0.2291  time: 1.4823  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 170/3000]  eta: 1:11:37  lr: 0.000029  loss: 0.8869  time: 1.5101  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 170/3000]  eta: 1:11:36  lr: 0.000029  loss: 0.7046  time: 1.5095  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 175/3000]  eta: 1:11:33  lr: 0.000029  loss: 0.2368  time: 1.5359  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 175/3000]  eta: 1:11:32  lr: 0.000029  loss: 0.6442  time: 1.5354  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 180/3000]  eta: 1:11:31  lr: 0.000029  loss: 0.5238  time: 1.5689  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 180/3000]  eta: 1:11:30  lr: 0.000029  loss: 0.3395  time: 1.5684  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 185/3000]  eta: 1:11:25  lr: 0.000029  loss: 0.4834  time: 1.5730  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 185/3000]  eta: 1:11:24  lr: 0.000029  loss: 0.8029  time: 1.5725  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 190/3000]  eta: 1:11:22  lr: 0.000029  loss: 0.5902  time: 1.5713  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 190/3000]  eta: 1:11:21  lr: 0.000029  loss: 0.2952  time: 1.5710  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 195/3000]  eta: 1:11:15  lr: 0.000029  loss: 0.9050  time: 1.5614  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 195/3000]  eta: 1:11:14  lr: 0.000029  loss: 0.5982  time: 1.5613  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 200/3000]  eta: 1:11:11  lr: 0.000029  loss: 0.5691  time: 1.5580  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 200/3000]  eta: 1:11:10  lr: 0.000029  loss: 0.9151  time: 1.5577  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 205/3000]  eta: 1:11:06  lr: 0.000029  loss: 0.2643  time: 1.5631  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 205/3000]  eta: 1:11:05  lr: 0.000029  loss: 0.4680  time: 1.5629  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 210/3000]  eta: 1:10:57  lr: 0.000029  loss: 0.4644  time: 1.5447  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 210/3000]  eta: 1:10:56  lr: 0.000029  loss: 0.2764  time: 1.5445  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 215/3000]  eta: 1:10:47  lr: 0.000029  loss: 0.8417  time: 1.5338  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 215/3000]  eta: 1:10:46  lr: 0.000029  loss: 0.3854  time: 1.5336  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 220/3000]  eta: 1:10:39  lr: 0.000029  loss: 0.8666  time: 1.5218  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 220/3000]  eta: 1:10:39  lr: 0.000029  loss: 0.6615  time: 1.5216  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 225/3000]  eta: 1:10:33  lr: 0.000029  loss: 0.5044  time: 1.5185  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 225/3000]  eta: 1:10:33  lr: 0.000029  loss: 0.5806  time: 1.5183  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 230/3000]  eta: 1:10:25  lr: 0.000029  loss: 0.6960  time: 1.5197  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 230/3000]  eta: 1:10:24  lr: 0.000029  loss: 0.6568  time: 1.5194  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 235/3000]  eta: 1:10:21  lr: 0.000029  loss: 0.6080  time: 1.5438  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 235/3000]  eta: 1:10:20  lr: 0.000029  loss: 0.4335  time: 1.5436  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 240/3000]  eta: 1:10:13  lr: 0.000029  loss: 0.4823  time: 1.5411  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 240/3000]  eta: 1:10:12  lr: 0.000029  loss: 1.0085  time: 1.5408  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 245/3000]  eta: 1:10:03  lr: 0.000029  loss: 0.4233  time: 1.5287  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 245/3000]  eta: 1:10:03  lr: 0.000029  loss: 0.3837  time: 1.5285  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 250/3000]  eta: 1:09:54  lr: 0.000029  loss: 0.1790  time: 1.5239  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 250/3000]  eta: 1:09:53  lr: 0.000029  loss: 0.3819  time: 1.5237  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 255/3000]  eta: 1:09:45  lr: 0.000029  loss: 0.2476  time: 1.5019  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 255/3000]  eta: 1:09:44  lr: 0.000029  loss: 1.0599  time: 1.5017  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 260/3000]  eta: 1:09:36  lr: 0.000029  loss: 0.1954  time: 1.4960  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 260/3000]  eta: 1:09:35  lr: 0.000029  loss: 0.1108  time: 1.4957  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 265/3000]  eta: 1:09:22  lr: 0.000029  loss: 0.4225  time: 1.4733  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 265/3000]  eta: 1:09:21  lr: 0.000029  loss: 0.3810  time: 1.4730  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 270/3000]  eta: 1:09:15  lr: 0.000029  loss: 0.3190  time: 1.4812  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 270/3000]  eta: 1:09:14  lr: 0.000029  loss: 0.5698  time: 1.4809  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 275/3000]  eta: 1:09:11  lr: 0.000029  loss: 0.5676  time: 1.5077  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 275/3000]  eta: 1:09:10  lr: 0.000029  loss: 0.6359  time: 1.5074  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 280/3000]  eta: 1:09:02  lr: 0.000029  loss: 0.3745  time: 1.5090  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 280/3000]  eta: 1:09:02  lr: 0.000029  loss: 0.5518  time: 1.5088  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 285/3000]  eta: 1:08:54  lr: 0.000029  loss: 0.3877  time: 1.5329  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 285/3000]  eta: 1:08:53  lr: 0.000029  loss: 0.0799  time: 1.5326  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 290/3000]  eta: 1:08:45  lr: 0.000029  loss: 0.3634  time: 1.5250  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 290/3000]  eta: 1:08:44  lr: 0.000029  loss: 0.5839  time: 1.5248  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 295/3000]  eta: 1:08:35  lr: 0.000029  loss: 0.4175  time: 1.4929  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 295/3000]  eta: 1:08:34  lr: 0.000029  loss: 0.2424  time: 1.4926  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 300/3000]  eta: 1:08:27  lr: 0.000029  loss: 0.3196  time: 1.4960  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 300/3000]  eta: 1:08:26  lr: 0.000029  loss: 0.1898  time: 1.4958  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 305/3000]  eta: 1:08:13  lr: 0.000029  loss: 0.9401  time: 1.4664  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 305/3000]  eta: 1:08:13  lr: 0.000029  loss: 0.7490  time: 1.4662  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 310/3000]  eta: 1:08:07  lr: 0.000029  loss: 0.0898  time: 1.4795  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 310/3000]  eta: 1:08:06  lr: 0.000029  loss: 0.2177  time: 1.4793  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 315/3000]  eta: 1:07:57  lr: 0.000029  loss: 0.4883  time: 1.4753  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 315/3000]  eta: 1:07:56  lr: 0.000029  loss: 0.2928  time: 1.4752  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 320/3000]  eta: 1:07:48  lr: 0.000029  loss: 0.1266  time: 1.4713  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 320/3000]  eta: 1:07:47  lr: 0.000029  loss: 0.4611  time: 1.4712  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 325/3000]  eta: 1:07:40  lr: 0.000029  loss: 0.6439  time: 1.5025  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 325/3000]  eta: 1:07:40  lr: 0.000029  loss: 0.2870  time: 1.5024  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 330/3000]  eta: 1:07:30  lr: 0.000029  loss: 0.7715  time: 1.4814  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 330/3000]  eta: 1:07:30  lr: 0.000029  loss: 0.3328  time: 1.4812  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 335/3000]  eta: 1:07:22  lr: 0.000029  loss: 0.3962  time: 1.4927  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 335/3000]  eta: 1:07:21  lr: 0.000029  loss: 0.2846  time: 1.4925  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 340/3000]  eta: 1:07:15  lr: 0.000029  loss: 0.1513  time: 1.5026  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 340/3000]  eta: 1:07:15  lr: 0.000029  loss: 0.3723  time: 1.5024  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 345/3000]  eta: 1:07:08  lr: 0.000029  loss: 0.3530  time: 1.5023  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 345/3000]  eta: 1:07:07  lr: 0.000029  loss: 0.2044  time: 1.5022  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 350/3000]  eta: 1:07:00  lr: 0.000029  loss: 0.4490  time: 1.5141  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 350/3000]  eta: 1:06:59  lr: 0.000029  loss: 0.3364  time: 1.5139  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 355/3000]  eta: 1:06:53  lr: 0.000029  loss: 0.2262  time: 1.5223  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 355/3000]  eta: 1:06:52  lr: 0.000029  loss: 0.5466  time: 1.5221  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 360/3000]  eta: 1:06:48  lr: 0.000029  loss: 0.5644  time: 1.5369  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 360/3000]  eta: 1:06:47  lr: 0.000029  loss: 0.5496  time: 1.5366  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 365/3000]  eta: 1:06:39  lr: 0.000029  loss: 0.7324  time: 1.5270  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 365/3000]  eta: 1:06:38  lr: 0.000029  loss: 0.4232  time: 1.5266  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 370/3000]  eta: 1:06:28  lr: 0.000029  loss: 0.1818  time: 1.5103  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 370/3000]  eta: 1:06:28  lr: 0.000029  loss: 0.6512  time: 1.5101  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 375/3000]  eta: 1:06:18  lr: 0.000029  loss: 0.0784  time: 1.4879  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 375/3000]  eta: 1:06:18  lr: 0.000029  loss: 0.1250  time: 1.4877  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 380/3000]  eta: 1:06:11  lr: 0.000029  loss: 0.1723  time: 1.4730  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 380/3000]  eta: 1:06:11  lr: 0.000029  loss: 0.5687  time: 1.4728  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 385/3000]  eta: 1:06:05  lr: 0.000029  loss: 0.1776  time: 1.4910  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 385/3000]  eta: 1:06:04  lr: 0.000029  loss: 0.4322  time: 1.4908  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 390/3000]  eta: 1:05:57  lr: 0.000029  loss: 0.5143  time: 1.5105  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 390/3000]  eta: 1:05:56  lr: 0.000029  loss: 0.5975  time: 1.5102  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 395/3000]  eta: 1:05:49  lr: 0.000029  loss: 0.0723  time: 1.5280  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 395/3000]  eta: 1:05:49  lr: 0.000029  loss: 0.3540  time: 1.5279  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 400/3000]  eta: 1:05:41  lr: 0.000029  loss: 0.1232  time: 1.5165  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 400/3000]  eta: 1:05:40  lr: 0.000029  loss: 0.3591  time: 1.5162  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 405/3000]  eta: 1:05:33  lr: 0.000029  loss: 0.0909  time: 1.5081  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 405/3000]  eta: 1:05:33  lr: 0.000029  loss: 0.3983  time: 1.5079  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 410/3000]  eta: 1:05:26  lr: 0.000029  loss: 0.1925  time: 1.5091  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 410/3000]  eta: 1:05:25  lr: 0.000029  loss: 0.5482  time: 1.5088  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 415/3000]  eta: 1:05:17  lr: 0.000029  loss: 0.2457  time: 1.5013  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 415/3000]  eta: 1:05:17  lr: 0.000029  loss: 0.6649  time: 1.5010  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 420/3000]  eta: 1:05:09  lr: 0.000029  loss: 0.9591  time: 1.5033  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 420/3000]  eta: 1:05:09  lr: 0.000029  loss: 0.2930  time: 1.5030  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 425/3000]  eta: 1:05:02  lr: 0.000029  loss: 0.4783  time: 1.5055  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 425/3000]  eta: 1:05:01  lr: 0.000029  loss: 0.3888  time: 1.5052  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 430/3000]  eta: 1:04:51  lr: 0.000029  loss: 0.3724  time: 1.4783  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 430/3000]  eta: 1:04:50  lr: 0.000029  loss: 0.9496  time: 1.4780  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 435/3000]  eta: 1:04:44  lr: 0.000029  loss: 0.6606  time: 1.4882  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 435/3000]  eta: 1:04:43  lr: 0.000029  loss: 0.4979  time: 1.4880  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 440/3000]  eta: 1:04:37  lr: 0.000029  loss: 0.1090  time: 1.4966  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 440/3000]  eta: 1:04:36  lr: 0.000029  loss: 0.5914  time: 1.4963  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 445/3000]  eta: 1:04:32  lr: 0.000029  loss: 0.3992  time: 1.5174  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 445/3000]  eta: 1:04:31  lr: 0.000029  loss: 0.2871  time: 1.5172  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 450/3000]  eta: 1:04:25  lr: 0.000029  loss: 0.4336  time: 1.5474  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 450/3000]  eta: 1:04:24  lr: 0.000029  loss: 0.5587  time: 1.5472  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 455/3000]  eta: 1:04:18  lr: 0.000029  loss: 0.2934  time: 1.5582  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 455/3000]  eta: 1:04:18  lr: 0.000029  loss: 0.4020  time: 1.5580  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 460/3000]  eta: 1:04:10  lr: 0.000029  loss: 0.5188  time: 1.5506  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 460/3000]  eta: 1:04:10  lr: 0.000029  loss: 0.2775  time: 1.5504  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 465/3000]  eta: 1:04:03  lr: 0.000029  loss: 0.2282  time: 1.5344  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 465/3000]  eta: 1:04:03  lr: 0.000029  loss: 1.1497  time: 1.5342  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 470/3000]  eta: 1:03:56  lr: 0.000029  loss: 0.5098  time: 1.5345  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 470/3000]  eta: 1:03:56  lr: 0.000029  loss: 0.4175  time: 1.5343  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 475/3000]  eta: 1:03:49  lr: 0.000029  loss: 0.5182  time: 1.5237  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 475/3000]  eta: 1:03:48  lr: 0.000029  loss: 0.6786  time: 1.5235  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 480/3000]  eta: 1:03:40  lr: 0.000029  loss: 0.5298  time: 1.5140  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 480/3000]  eta: 1:03:39  lr: 0.000029  loss: 0.4255  time: 1.5138  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 485/3000]  eta: 1:03:31  lr: 0.000029  loss: 0.2248  time: 1.4981  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 485/3000]  eta: 1:03:31  lr: 0.000029  loss: 0.1985  time: 1.4978  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 490/3000]  eta: 1:03:24  lr: 0.000029  loss: 0.1763  time: 1.5019  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 490/3000]  eta: 1:03:24  lr: 0.000029  loss: 0.1828  time: 1.5017  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 495/3000]  eta: 1:03:17  lr: 0.000029  loss: 0.3774  time: 1.5043  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 495/3000]  eta: 1:03:17  lr: 0.000029  loss: 0.3469  time: 1.5041  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 500/3000]  eta: 1:03:10  lr: 0.000029  loss: 0.2265  time: 1.5253  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 500/3000]  eta: 1:03:10  lr: 0.000029  loss: 0.2413  time: 1.5250  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 505/3000]  eta: 1:03:03  lr: 0.000029  loss: 0.1850  time: 1.5327  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 505/3000]  eta: 1:03:02  lr: 0.000029  loss: 1.1068  time: 1.5324  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 510/3000]  eta: 1:02:54  lr: 0.000029  loss: 0.5007  time: 1.5151  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 510/3000]  eta: 1:02:53  lr: 0.000029  loss: 0.2339  time: 1.5148  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 515/3000]  eta: 1:02:47  lr: 0.000029  loss: 0.3123  time: 1.5197  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 515/3000]  eta: 1:02:47  lr: 0.000029  loss: 0.3195  time: 1.5194  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 520/3000]  eta: 1:02:40  lr: 0.000029  loss: 0.2016  time: 1.5134  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 520/3000]  eta: 1:02:39  lr: 0.000029  loss: 0.5929  time: 1.5131  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 525/3000]  eta: 1:02:32  lr: 0.000029  loss: 0.3985  time: 1.5119  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 525/3000]  eta: 1:02:31  lr: 0.000029  loss: 0.4483  time: 1.5116  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 530/3000]  eta: 1:02:24  lr: 0.000029  loss: 0.4635  time: 1.5226  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 530/3000]  eta: 1:02:24  lr: 0.000029  loss: 0.4432  time: 1.5223  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 535/3000]  eta: 1:02:17  lr: 0.000029  loss: 1.0587  time: 1.5215  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 535/3000]  eta: 1:02:17  lr: 0.000029  loss: 0.2726  time: 1.5212  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 540/3000]  eta: 1:02:11  lr: 0.000029  loss: 0.3384  time: 1.5361  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 540/3000]  eta: 1:02:11  lr: 0.000029  loss: 0.3073  time: 1.5359  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 545/3000]  eta: 1:02:04  lr: 0.000029  loss: 0.3616  time: 1.5400  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 545/3000]  eta: 1:02:03  lr: 0.000029  loss: 0.3308  time: 1.5397  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 550/3000]  eta: 1:01:56  lr: 0.000029  loss: 0.4772  time: 1.5423  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 550/3000]  eta: 1:01:56  lr: 0.000029  loss: 0.3050  time: 1.5420  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 555/3000]  eta: 1:01:48  lr: 0.000029  loss: 0.4311  time: 1.5226  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 555/3000]  eta: 1:01:47  lr: 0.000029  loss: 0.1423  time: 1.5224  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 560/3000]  eta: 1:01:40  lr: 0.000029  loss: 0.5893  time: 1.5012  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 560/3000]  eta: 1:01:39  lr: 0.000029  loss: 1.0204  time: 1.5010  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 565/3000]  eta: 1:01:31  lr: 0.000029  loss: 0.4945  time: 1.4902  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 565/3000]  eta: 1:01:30  lr: 0.000029  loss: 0.6435  time: 1.4899  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 570/3000]  eta: 1:01:25  lr: 0.000029  loss: 0.4678  time: 1.5002  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 570/3000]  eta: 1:01:24  lr: 0.000029  loss: 0.5306  time: 1.5000  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 575/3000]  eta: 1:01:15  lr: 0.000029  loss: 0.3100  time: 1.4892  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 575/3000]  eta: 1:01:14  lr: 0.000029  loss: 0.3856  time: 1.4889  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 580/3000]  eta: 1:01:07  lr: 0.000029  loss: 0.3102  time: 1.4939  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 580/3000]  eta: 1:01:07  lr: 0.000029  loss: 0.4156  time: 1.4936  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 585/3000]  eta: 1:00:59  lr: 0.000029  loss: 0.0988  time: 1.4998  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 585/3000]  eta: 1:00:59  lr: 0.000029  loss: 0.5708  time: 1.4996  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 590/3000]  eta: 1:00:51  lr: 0.000029  loss: 0.6682  time: 1.4824  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 590/3000]  eta: 1:00:51  lr: 0.000029  loss: 0.2414  time: 1.4822  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 595/3000]  eta: 1:00:43  lr: 0.000029  loss: 0.2506  time: 1.4991  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 595/3000]  eta: 1:00:43  lr: 0.000029  loss: 0.0517  time: 1.4988  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 600/3000]  eta: 1:00:34  lr: 0.000029  loss: 0.0682  time: 1.4810  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 600/3000]  eta: 1:00:34  lr: 0.000029  loss: 0.2560  time: 1.4808  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 605/3000]  eta: 1:00:26  lr: 0.000029  loss: 0.5620  time: 1.4836  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 605/3000]  eta: 1:00:26  lr: 0.000029  loss: 0.4175  time: 1.4834  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 610/3000]  eta: 1:00:20  lr: 0.000029  loss: 0.8089  time: 1.4951  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 610/3000]  eta: 1:00:19  lr: 0.000029  loss: 0.4374  time: 1.4949  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 615/3000]  eta: 1:00:12  lr: 0.000029  loss: 0.2834  time: 1.5021  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 615/3000]  eta: 1:00:11  lr: 0.000029  loss: 0.6817  time: 1.5019  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 620/3000]  eta: 1:00:03  lr: 0.000029  loss: 0.5934  time: 1.5048  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 620/3000]  eta: 1:00:03  lr: 0.000029  loss: 0.4167  time: 1.5045  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 625/3000]  eta: 0:59:56  lr: 0.000029  loss: 0.5031  time: 1.5076  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 625/3000]  eta: 0:59:55  lr: 0.000029  loss: 0.5909  time: 1.5074  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 630/3000]  eta: 0:59:48  lr: 0.000029  loss: 0.2358  time: 1.4964  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 630/3000]  eta: 0:59:47  lr: 0.000029  loss: 0.4841  time: 1.4962  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 635/3000]  eta: 0:59:40  lr: 0.000029  loss: 0.2805  time: 1.4964  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 635/3000]  eta: 0:59:40  lr: 0.000029  loss: 0.6189  time: 1.4952  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 640/3000]  eta: 0:59:33  lr: 0.000029  loss: 0.1656  time: 1.5079  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 640/3000]  eta: 0:59:32  lr: 0.000029  loss: 0.6575  time: 1.5067  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 645/3000]  eta: 0:59:26  lr: 0.000029  loss: 0.1932  time: 1.5211  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 645/3000]  eta: 0:59:25  lr: 0.000029  loss: 0.2338  time: 1.5198  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 650/3000]  eta: 0:59:19  lr: 0.000029  loss: 0.1923  time: 1.5328  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 650/3000]  eta: 0:59:18  lr: 0.000029  loss: 0.2475  time: 1.5316  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 655/3000]  eta: 0:59:12  lr: 0.000029  loss: 0.3970  time: 1.5356  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 655/3000]  eta: 0:59:11  lr: 0.000029  loss: 0.3582  time: 1.5353  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 660/3000]  eta: 0:59:04  lr: 0.000029  loss: 0.0996  time: 1.5324  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 660/3000]  eta: 0:59:03  lr: 0.000029  loss: 0.5227  time: 1.5322  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 665/3000]  eta: 0:58:56  lr: 0.000029  loss: 0.1268  time: 1.5257  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 665/3000]  eta: 0:58:56  lr: 0.000029  loss: 0.2410  time: 1.5255  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 670/3000]  eta: 0:58:50  lr: 0.000029  loss: 0.0997  time: 1.5292  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 670/3000]  eta: 0:58:49  lr: 0.000029  loss: 0.3531  time: 1.5289  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 675/3000]  eta: 0:58:41  lr: 0.000029  loss: 0.1565  time: 1.5141  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 675/3000]  eta: 0:58:41  lr: 0.000029  loss: 0.3654  time: 1.5138  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 680/3000]  eta: 0:58:34  lr: 0.000029  loss: 0.6375  time: 1.5323  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 680/3000]  eta: 0:58:34  lr: 0.000029  loss: 0.3426  time: 1.5321  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 685/3000]  eta: 0:58:27  lr: 0.000029  loss: 0.5681  time: 1.5233  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 685/3000]  eta: 0:58:26  lr: 0.000029  loss: 0.2962  time: 1.5230  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 690/3000]  eta: 0:58:19  lr: 0.000029  loss: 0.6306  time: 1.5144  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 690/3000]  eta: 0:58:19  lr: 0.000029  loss: 0.6688  time: 1.5141  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 695/3000]  eta: 0:58:12  lr: 0.000029  loss: 0.0988  time: 1.5299  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 695/3000]  eta: 0:58:11  lr: 0.000029  loss: 0.4115  time: 1.5296  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 700/3000]  eta: 0:58:03  lr: 0.000029  loss: 0.1513  time: 1.4952  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 700/3000]  eta: 0:58:02  lr: 0.000029  loss: 0.6491  time: 1.4949  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 705/3000]  eta: 0:57:54  lr: 0.000029  loss: 0.1873  time: 1.4808  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 705/3000]  eta: 0:57:54  lr: 0.000029  loss: 0.1874  time: 1.4806  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 710/3000]  eta: 0:57:46  lr: 0.000029  loss: 0.3906  time: 1.4694  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 710/3000]  eta: 0:57:45  lr: 0.000029  loss: 0.8725  time: 1.4691  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 715/3000]  eta: 0:57:38  lr: 0.000029  loss: 0.3302  time: 1.4579  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 715/3000]  eta: 0:57:37  lr: 0.000029  loss: 0.2226  time: 1.4577  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 720/3000]  eta: 0:57:31  lr: 0.000029  loss: 0.1727  time: 1.4918  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 720/3000]  eta: 0:57:30  lr: 0.000029  loss: 0.2681  time: 1.4915  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 725/3000]  eta: 0:57:24  lr: 0.000029  loss: 0.1463  time: 1.5107  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 725/3000]  eta: 0:57:23  lr: 0.000029  loss: 0.3979  time: 1.5104  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 730/3000]  eta: 0:57:17  lr: 0.000029  loss: 0.2383  time: 1.5249  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 730/3000]  eta: 0:57:16  lr: 0.000029  loss: 0.4967  time: 1.5246  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 735/3000]  eta: 0:57:10  lr: 0.000029  loss: 0.3765  time: 1.5431  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 735/3000]  eta: 0:57:09  lr: 0.000029  loss: 0.6269  time: 1.5428  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 740/3000]  eta: 0:57:02  lr: 0.000029  loss: 0.4750  time: 1.5399  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 740/3000]  eta: 0:57:02  lr: 0.000029  loss: 0.5561  time: 1.5397  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 745/3000]  eta: 0:56:56  lr: 0.000029  loss: 0.3196  time: 1.5580  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 745/3000]  eta: 0:56:55  lr: 0.000029  loss: 0.5081  time: 1.5577  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 750/3000]  eta: 0:56:48  lr: 0.000029  loss: 0.6832  time: 1.5512  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 750/3000]  eta: 0:56:48  lr: 0.000029  loss: 0.3426  time: 1.5510  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 755/3000]  eta: 0:56:41  lr: 0.000029  loss: 0.1321  time: 1.5410  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 755/3000]  eta: 0:56:40  lr: 0.000029  loss: 0.0580  time: 1.5408  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 760/3000]  eta: 0:56:32  lr: 0.000029  loss: 0.5308  time: 1.5170  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 760/3000]  eta: 0:56:32  lr: 0.000029  loss: 0.2241  time: 1.5166  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 765/3000]  eta: 0:56:23  lr: 0.000029  loss: 0.1256  time: 1.4758  data: 0.0000  max mem: 18151Train: data epoch: [5]  [ 765/3000]  eta: 0:56:23  lr: 0.000029  loss: 0.1990  time: 1.4762  data: 0.0000  max mem: 18432

Train: data epoch: [5]  [ 770/3000]  eta: 0:56:15  lr: 0.000029  loss: 0.2873  time: 1.4623  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 770/3000]  eta: 0:56:14  lr: 0.000029  loss: 0.4659  time: 1.4621  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 775/3000]  eta: 0:56:07  lr: 0.000029  loss: 0.5213  time: 1.4468  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 775/3000]  eta: 0:56:06  lr: 0.000029  loss: 0.1236  time: 1.4465  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 780/3000]  eta: 0:55:58  lr: 0.000029  loss: 0.6373  time: 1.4470  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 780/3000]  eta: 0:55:58  lr: 0.000029  loss: 0.8315  time: 1.4468  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 785/3000]  eta: 0:55:51  lr: 0.000029  loss: 0.4956  time: 1.4806  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 785/3000]  eta: 0:55:51  lr: 0.000029  loss: 0.2157  time: 1.4804  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 790/3000]  eta: 0:55:44  lr: 0.000029  loss: 0.7288  time: 1.4999  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 790/3000]  eta: 0:55:43  lr: 0.000029  loss: 0.4957  time: 1.4998  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 795/3000]  eta: 0:55:37  lr: 0.000029  loss: 0.6005  time: 1.5234  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 795/3000]  eta: 0:55:36  lr: 0.000029  loss: 0.3863  time: 1.5231  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 800/3000]  eta: 0:55:29  lr: 0.000029  loss: 0.2401  time: 1.5304  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 800/3000]  eta: 0:55:28  lr: 0.000029  loss: 0.5647  time: 1.5301  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 805/3000]  eta: 0:55:21  lr: 0.000029  loss: 0.3512  time: 1.5079  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 805/3000]  eta: 0:55:20  lr: 0.000029  loss: 0.4487  time: 1.5077  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 810/3000]  eta: 0:55:13  lr: 0.000029  loss: 0.7374  time: 1.5019  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 810/3000]  eta: 0:55:13  lr: 0.000029  loss: 0.3305  time: 1.5016  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 815/3000]  eta: 0:55:06  lr: 0.000029  loss: 0.4514  time: 1.4977  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 815/3000]  eta: 0:55:05  lr: 0.000029  loss: 0.1616  time: 1.4974  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 820/3000]  eta: 0:54:58  lr: 0.000029  loss: 0.7514  time: 1.4997  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 820/3000]  eta: 0:54:57  lr: 0.000029  loss: 0.1922  time: 1.4995  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 825/3000]  eta: 0:54:51  lr: 0.000029  loss: 0.2356  time: 1.5133  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 825/3000]  eta: 0:54:50  lr: 0.000029  loss: 0.3896  time: 1.5130  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 830/3000]  eta: 0:54:43  lr: 0.000029  loss: 0.3727  time: 1.5224  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 830/3000]  eta: 0:54:43  lr: 0.000029  loss: 0.3664  time: 1.5221  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 835/3000]  eta: 0:54:36  lr: 0.000029  loss: 0.2717  time: 1.5206  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 835/3000]  eta: 0:54:35  lr: 0.000029  loss: 0.3757  time: 1.5204  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 840/3000]  eta: 0:54:28  lr: 0.000029  loss: 0.7434  time: 1.5301  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 840/3000]  eta: 0:54:28  lr: 0.000029  loss: 0.2840  time: 1.5298  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 845/3000]  eta: 0:54:21  lr: 0.000029  loss: 0.2909  time: 1.5306  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 845/3000]  eta: 0:54:21  lr: 0.000029  loss: 0.8973  time: 1.5304  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 850/3000]  eta: 0:54:13  lr: 0.000029  loss: 0.2576  time: 1.5032  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 850/3000]  eta: 0:54:12  lr: 0.000029  loss: 0.1376  time: 1.5030  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 855/3000]  eta: 0:54:06  lr: 0.000029  loss: 0.2721  time: 1.5153  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 855/3000]  eta: 0:54:05  lr: 0.000029  loss: 0.5333  time: 1.5151  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 860/3000]  eta: 0:53:58  lr: 0.000029  loss: 0.1233  time: 1.4999  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 860/3000]  eta: 0:53:57  lr: 0.000029  loss: 0.2839  time: 1.4996  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 865/3000]  eta: 0:53:50  lr: 0.000029  loss: 0.8086  time: 1.4932  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 865/3000]  eta: 0:53:49  lr: 0.000029  loss: 0.1513  time: 1.4929  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 870/3000]  eta: 0:53:42  lr: 0.000029  loss: 0.3439  time: 1.5004  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 870/3000]  eta: 0:53:41  lr: 0.000029  loss: 0.8275  time: 1.5001  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 875/3000]  eta: 0:53:35  lr: 0.000029  loss: 0.2199  time: 1.4951  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 875/3000]  eta: 0:53:34  lr: 0.000029  loss: 0.2369  time: 1.4948  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 880/3000]  eta: 0:53:27  lr: 0.000029  loss: 0.1904  time: 1.5030  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 880/3000]  eta: 0:53:26  lr: 0.000029  loss: 0.5662  time: 1.5027  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 885/3000]  eta: 0:53:20  lr: 0.000029  loss: 0.1918  time: 1.5124  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 885/3000]  eta: 0:53:19  lr: 0.000029  loss: 0.4659  time: 1.5121  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 890/3000]  eta: 0:53:12  lr: 0.000029  loss: 0.4701  time: 1.5204  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 890/3000]  eta: 0:53:11  lr: 0.000029  loss: 0.5311  time: 1.5201  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 895/3000]  eta: 0:53:04  lr: 0.000028  loss: 0.9093  time: 1.5099  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 895/3000]  eta: 0:53:04  lr: 0.000028  loss: 0.1581  time: 1.5096  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 900/3000]  eta: 0:52:57  lr: 0.000028  loss: 0.2371  time: 1.5217  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 900/3000]  eta: 0:52:56  lr: 0.000028  loss: 0.3179  time: 1.5213  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 905/3000]  eta: 0:52:49  lr: 0.000028  loss: 0.5221  time: 1.5047  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 905/3000]  eta: 0:52:48  lr: 0.000028  loss: 0.4982  time: 1.5045  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 910/3000]  eta: 0:52:42  lr: 0.000028  loss: 0.1935  time: 1.5156  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 910/3000]  eta: 0:52:41  lr: 0.000028  loss: 0.2813  time: 1.5154  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 915/3000]  eta: 0:52:34  lr: 0.000028  loss: 1.2553  time: 1.5161  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 915/3000]  eta: 0:52:34  lr: 0.000028  loss: 0.7868  time: 1.5159  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 920/3000]  eta: 0:52:27  lr: 0.000028  loss: 0.3944  time: 1.5148  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 920/3000]  eta: 0:52:26  lr: 0.000028  loss: 0.2555  time: 1.5146  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 925/3000]  eta: 0:52:19  lr: 0.000028  loss: 0.5014  time: 1.5284  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 925/3000]  eta: 0:52:19  lr: 0.000028  loss: 0.5785  time: 1.5281  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 930/3000]  eta: 0:52:12  lr: 0.000028  loss: 0.1628  time: 1.5197  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 930/3000]  eta: 0:52:11  lr: 0.000028  loss: 0.6308  time: 1.5195  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 935/3000]  eta: 0:52:04  lr: 0.000028  loss: 0.6017  time: 1.5119  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 935/3000]  eta: 0:52:03  lr: 0.000028  loss: 0.1555  time: 1.5117  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 940/3000]  eta: 0:51:56  lr: 0.000028  loss: 0.5437  time: 1.5036  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 940/3000]  eta: 0:51:56  lr: 0.000028  loss: 0.4978  time: 1.5035  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 945/3000]  eta: 0:51:49  lr: 0.000028  loss: 0.6365  time: 1.5204  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 945/3000]  eta: 0:51:49  lr: 0.000028  loss: 0.7265  time: 1.5202  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 950/3000]  eta: 0:51:42  lr: 0.000028  loss: 0.2407  time: 1.5284  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 950/3000]  eta: 0:51:42  lr: 0.000028  loss: 0.6481  time: 1.5281  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 955/3000]  eta: 0:51:35  lr: 0.000028  loss: 0.6731  time: 1.5558  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 955/3000]  eta: 0:51:35  lr: 0.000028  loss: 0.3949  time: 1.5555  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 960/3000]  eta: 0:51:28  lr: 0.000028  loss: 0.3598  time: 1.5612  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 960/3000]  eta: 0:51:27  lr: 0.000028  loss: 1.0157  time: 1.5609  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 965/3000]  eta: 0:51:21  lr: 0.000028  loss: 0.2633  time: 1.5540  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 965/3000]  eta: 0:51:20  lr: 0.000028  loss: 0.4585  time: 1.5539  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 970/3000]  eta: 0:51:13  lr: 0.000028  loss: 0.4554  time: 1.5381  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 970/3000]  eta: 0:51:12  lr: 0.000028  loss: 0.2315  time: 1.5379  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 975/3000]  eta: 0:51:05  lr: 0.000028  loss: 0.3385  time: 1.5122  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 975/3000]  eta: 0:51:04  lr: 0.000028  loss: 0.4081  time: 1.5119  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 980/3000]  eta: 0:50:57  lr: 0.000028  loss: 0.0887  time: 1.4999  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 980/3000]  eta: 0:50:56  lr: 0.000028  loss: 0.1301  time: 1.4996  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 985/3000]  eta: 0:50:49  lr: 0.000028  loss: 0.3564  time: 1.4849  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 985/3000]  eta: 0:50:49  lr: 0.000028  loss: 0.4385  time: 1.4847  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 990/3000]  eta: 0:50:43  lr: 0.000028  loss: 0.2008  time: 1.5144  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [ 990/3000]  eta: 0:50:42  lr: 0.000028  loss: 0.4058  time: 1.5140  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 995/3000]  eta: 0:50:35  lr: 0.000028  loss: 0.1114  time: 1.5304  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [ 995/3000]  eta: 0:50:35  lr: 0.000028  loss: 0.7244  time: 1.5308  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1000/3000]  eta: 0:50:28  lr: 0.000028  loss: 0.2851  time: 1.5522  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1000/3000]  eta: 0:50:28  lr: 0.000028  loss: 0.4564  time: 1.5520  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1005/3000]  eta: 0:50:21  lr: 0.000028  loss: 0.2430  time: 1.5522  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1005/3000]  eta: 0:50:20  lr: 0.000028  loss: 0.4545  time: 1.5518  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1010/3000]  eta: 0:50:13  lr: 0.000028  loss: 0.1910  time: 1.5355  data: 0.0000  max mem: 18151Train: data epoch: [5]  [1010/3000]  eta: 0:50:13  lr: 0.000028  loss: 0.3439  time: 1.5358  data: 0.0000  max mem: 18432

Train: data epoch: [5]  [1015/3000]  eta: 0:50:06  lr: 0.000028  loss: 0.2734  time: 1.5436  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1015/3000]  eta: 0:50:06  lr: 0.000028  loss: 0.8267  time: 1.5435  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1020/3000]  eta: 0:49:58  lr: 0.000028  loss: 0.5788  time: 1.5176  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1020/3000]  eta: 0:49:58  lr: 0.000028  loss: 0.4552  time: 1.5173  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1025/3000]  eta: 0:49:50  lr: 0.000028  loss: 0.5440  time: 1.5144  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1025/3000]  eta: 0:49:50  lr: 0.000028  loss: 0.4354  time: 1.5142  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1030/3000]  eta: 0:49:42  lr: 0.000028  loss: 0.3269  time: 1.4990  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1030/3000]  eta: 0:49:42  lr: 0.000028  loss: 0.5680  time: 1.4989  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1035/3000]  eta: 0:49:35  lr: 0.000028  loss: 0.8653  time: 1.4975  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1035/3000]  eta: 0:49:35  lr: 0.000028  loss: 0.4701  time: 1.4973  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1040/3000]  eta: 0:49:28  lr: 0.000028  loss: 0.1015  time: 1.5116  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1040/3000]  eta: 0:49:27  lr: 0.000028  loss: 0.3481  time: 1.5113  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1045/3000]  eta: 0:49:20  lr: 0.000028  loss: 0.4689  time: 1.5037  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1045/3000]  eta: 0:49:19  lr: 0.000028  loss: 0.2779  time: 1.5034  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1050/3000]  eta: 0:49:13  lr: 0.000028  loss: 0.2206  time: 1.5272  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1050/3000]  eta: 0:49:12  lr: 0.000028  loss: 0.8611  time: 1.5270  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1055/3000]  eta: 0:49:05  lr: 0.000028  loss: 0.6184  time: 1.5052  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1055/3000]  eta: 0:49:04  lr: 0.000028  loss: 0.2043  time: 1.5050  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1060/3000]  eta: 0:48:57  lr: 0.000028  loss: 0.4234  time: 1.5005  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1060/3000]  eta: 0:48:56  lr: 0.000028  loss: 0.1024  time: 1.5003  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1065/3000]  eta: 0:48:49  lr: 0.000028  loss: 0.4955  time: 1.5026  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1065/3000]  eta: 0:48:48  lr: 0.000028  loss: 0.2953  time: 1.5022  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1070/3000]  eta: 0:48:42  lr: 0.000028  loss: 0.3227  time: 1.5101  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1070/3000]  eta: 0:48:42  lr: 0.000028  loss: 0.6705  time: 1.5097  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1075/3000]  eta: 0:48:35  lr: 0.000028  loss: 0.1076  time: 1.5189  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1075/3000]  eta: 0:48:34  lr: 0.000028  loss: 0.3554  time: 1.5186  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1080/3000]  eta: 0:48:27  lr: 0.000028  loss: 0.1412  time: 1.5232  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1080/3000]  eta: 0:48:26  lr: 0.000028  loss: 0.1362  time: 1.5229  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1085/3000]  eta: 0:48:20  lr: 0.000028  loss: 0.2974  time: 1.5370  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1085/3000]  eta: 0:48:19  lr: 0.000028  loss: 0.3785  time: 1.5368  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1090/3000]  eta: 0:48:12  lr: 0.000028  loss: 0.2454  time: 1.5331  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1090/3000]  eta: 0:48:12  lr: 0.000028  loss: 0.4641  time: 1.5328  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1095/3000]  eta: 0:48:05  lr: 0.000028  loss: 0.2257  time: 1.5391  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1095/3000]  eta: 0:48:05  lr: 0.000028  loss: 0.4456  time: 1.5389  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1100/3000]  eta: 0:47:57  lr: 0.000028  loss: 0.6968  time: 1.5222  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1100/3000]  eta: 0:47:56  lr: 0.000028  loss: 0.1835  time: 1.5220  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1105/3000]  eta: 0:47:50  lr: 0.000028  loss: 0.3474  time: 1.5244  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1105/3000]  eta: 0:47:49  lr: 0.000028  loss: 0.1182  time: 1.5242  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1110/3000]  eta: 0:47:42  lr: 0.000028  loss: 0.9121  time: 1.5039  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1110/3000]  eta: 0:47:41  lr: 0.000028  loss: 0.1571  time: 1.5036  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1115/3000]  eta: 0:47:35  lr: 0.000028  loss: 0.9819  time: 1.5085  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1115/3000]  eta: 0:47:34  lr: 0.000028  loss: 0.5999  time: 1.5082  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1120/3000]  eta: 0:47:27  lr: 0.000028  loss: 0.1754  time: 1.5257  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1120/3000]  eta: 0:47:27  lr: 0.000028  loss: 0.2508  time: 1.5254  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1125/3000]  eta: 0:47:20  lr: 0.000028  loss: 0.6139  time: 1.5336  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1125/3000]  eta: 0:47:19  lr: 0.000028  loss: 0.5968  time: 1.5334  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1130/3000]  eta: 0:47:12  lr: 0.000028  loss: 0.5314  time: 1.5361  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1130/3000]  eta: 0:47:12  lr: 0.000028  loss: 0.2802  time: 1.5358  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1135/3000]  eta: 0:47:04  lr: 0.000028  loss: 0.2364  time: 1.5001  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1135/3000]  eta: 0:47:03  lr: 0.000028  loss: 0.1103  time: 1.4998  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1140/3000]  eta: 0:46:56  lr: 0.000028  loss: 0.1853  time: 1.5047  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1140/3000]  eta: 0:46:56  lr: 0.000028  loss: 0.5264  time: 1.5045  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1145/3000]  eta: 0:46:49  lr: 0.000028  loss: 0.4669  time: 1.5104  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1145/3000]  eta: 0:46:49  lr: 0.000028  loss: 0.4615  time: 1.5101  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1150/3000]  eta: 0:46:42  lr: 0.000028  loss: 0.2734  time: 1.5207  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1150/3000]  eta: 0:46:42  lr: 0.000028  loss: 1.1869  time: 1.5205  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1155/3000]  eta: 0:46:35  lr: 0.000028  loss: 0.4601  time: 1.5429  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1155/3000]  eta: 0:46:34  lr: 0.000028  loss: 0.3919  time: 1.5426  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1160/3000]  eta: 0:46:27  lr: 0.000028  loss: 0.2545  time: 1.5335  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1160/3000]  eta: 0:46:26  lr: 0.000028  loss: 0.2382  time: 1.5334  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1165/3000]  eta: 0:46:19  lr: 0.000028  loss: 0.3362  time: 1.5036  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1165/3000]  eta: 0:46:18  lr: 0.000028  loss: 0.1580  time: 1.5035  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1170/3000]  eta: 0:46:11  lr: 0.000028  loss: 0.1429  time: 1.4982  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1170/3000]  eta: 0:46:11  lr: 0.000028  loss: 0.0856  time: 1.4979  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1175/3000]  eta: 0:46:03  lr: 0.000028  loss: 0.2987  time: 1.4897  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1175/3000]  eta: 0:46:03  lr: 0.000028  loss: 0.3436  time: 1.4895  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1180/3000]  eta: 0:45:56  lr: 0.000028  loss: 1.0359  time: 1.5102  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1180/3000]  eta: 0:45:56  lr: 0.000028  loss: 0.2139  time: 1.5098  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1185/3000]  eta: 0:45:49  lr: 0.000028  loss: 0.6700  time: 1.5366  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1185/3000]  eta: 0:45:49  lr: 0.000028  loss: 1.3546  time: 1.5363  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1190/3000]  eta: 0:45:42  lr: 0.000028  loss: 0.3641  time: 1.5311  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1190/3000]  eta: 0:45:41  lr: 0.000028  loss: 0.3657  time: 1.5308  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1195/3000]  eta: 0:45:34  lr: 0.000028  loss: 0.2079  time: 1.5438  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1195/3000]  eta: 0:45:34  lr: 0.000028  loss: 1.3304  time: 1.5435  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1200/3000]  eta: 0:45:26  lr: 0.000028  loss: 0.2469  time: 1.5301  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1200/3000]  eta: 0:45:26  lr: 0.000028  loss: 0.3361  time: 1.5299  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1205/3000]  eta: 0:45:19  lr: 0.000028  loss: 0.0815  time: 1.5120  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1205/3000]  eta: 0:45:18  lr: 0.000028  loss: 0.2188  time: 1.5117  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1210/3000]  eta: 0:45:12  lr: 0.000028  loss: 0.3340  time: 1.5275  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1210/3000]  eta: 0:45:11  lr: 0.000028  loss: 0.6859  time: 1.5273  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1215/3000]  eta: 0:45:04  lr: 0.000028  loss: 0.4606  time: 1.5257  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1215/3000]  eta: 0:45:04  lr: 0.000028  loss: 0.6378  time: 1.5254  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1220/3000]  eta: 0:44:56  lr: 0.000028  loss: 0.1734  time: 1.5179  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1220/3000]  eta: 0:44:56  lr: 0.000028  loss: 0.2776  time: 1.5178  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1225/3000]  eta: 0:44:49  lr: 0.000028  loss: 0.3241  time: 1.5324  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1225/3000]  eta: 0:44:49  lr: 0.000028  loss: 0.2654  time: 1.5321  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1230/3000]  eta: 0:44:41  lr: 0.000028  loss: 0.3350  time: 1.5212  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1230/3000]  eta: 0:44:41  lr: 0.000028  loss: 0.8303  time: 1.5209  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1235/3000]  eta: 0:44:33  lr: 0.000028  loss: 0.3619  time: 1.4958  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1235/3000]  eta: 0:44:33  lr: 0.000028  loss: 0.4090  time: 1.4955  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1240/3000]  eta: 0:44:26  lr: 0.000028  loss: 0.9836  time: 1.5111  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1240/3000]  eta: 0:44:25  lr: 0.000028  loss: 0.1179  time: 1.5108  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1245/3000]  eta: 0:44:18  lr: 0.000028  loss: 0.2817  time: 1.5001  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1245/3000]  eta: 0:44:18  lr: 0.000028  loss: 0.2679  time: 1.4998  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1250/3000]  eta: 0:44:10  lr: 0.000028  loss: 0.4094  time: 1.4867  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1250/3000]  eta: 0:44:10  lr: 0.000028  loss: 0.4124  time: 1.4864  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1255/3000]  eta: 0:44:03  lr: 0.000028  loss: 0.3032  time: 1.5102  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1255/3000]  eta: 0:44:02  lr: 0.000028  loss: 1.4968  time: 1.5100  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1260/3000]  eta: 0:43:55  lr: 0.000028  loss: 1.0190  time: 1.5020  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1260/3000]  eta: 0:43:55  lr: 0.000028  loss: 0.1902  time: 1.5018  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1265/3000]  eta: 0:43:48  lr: 0.000028  loss: 0.2293  time: 1.5163  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1265/3000]  eta: 0:43:48  lr: 0.000028  loss: 0.3188  time: 1.5160  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1270/3000]  eta: 0:43:41  lr: 0.000028  loss: 0.2871  time: 1.5490  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1270/3000]  eta: 0:43:41  lr: 0.000028  loss: 0.3834  time: 1.5488  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1275/3000]  eta: 0:43:34  lr: 0.000028  loss: 0.6098  time: 1.5708  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1275/3000]  eta: 0:43:34  lr: 0.000028  loss: 0.3939  time: 1.5708  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1280/3000]  eta: 0:43:26  lr: 0.000028  loss: 0.2095  time: 1.5633  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1280/3000]  eta: 0:43:26  lr: 0.000028  loss: 0.8200  time: 1.5631  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1285/3000]  eta: 0:43:19  lr: 0.000028  loss: 0.3448  time: 1.5623  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1285/3000]  eta: 0:43:19  lr: 0.000028  loss: 0.2344  time: 1.5621  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1290/3000]  eta: 0:43:11  lr: 0.000028  loss: 0.2887  time: 1.5452  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1290/3000]  eta: 0:43:11  lr: 0.000028  loss: 0.6232  time: 1.5449  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1295/3000]  eta: 0:43:04  lr: 0.000028  loss: 0.5793  time: 1.5294  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1295/3000]  eta: 0:43:04  lr: 0.000028  loss: 0.5177  time: 1.5290  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1300/3000]  eta: 0:42:56  lr: 0.000028  loss: 0.6105  time: 1.5532  data: 0.0000  max mem: 18151Train: data epoch: [5]  [1300/3000]  eta: 0:42:57  lr: 0.000028  loss: 0.6698  time: 1.5535  data: 0.0000  max mem: 18432

Train: data epoch: [5]  [1305/3000]  eta: 0:42:50  lr: 0.000028  loss: 0.1421  time: 1.5556  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1305/3000]  eta: 0:42:49  lr: 0.000028  loss: 0.2701  time: 1.5553  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1310/3000]  eta: 0:42:42  lr: 0.000028  loss: 0.1577  time: 1.5328  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1310/3000]  eta: 0:42:41  lr: 0.000028  loss: 0.3596  time: 1.5326  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1315/3000]  eta: 0:42:34  lr: 0.000028  loss: 0.6527  time: 1.5355  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1315/3000]  eta: 0:42:34  lr: 0.000028  loss: 0.3503  time: 1.5352  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1320/3000]  eta: 0:42:26  lr: 0.000028  loss: 0.2617  time: 1.5077  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1320/3000]  eta: 0:42:26  lr: 0.000028  loss: 0.1556  time: 1.5075  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1325/3000]  eta: 0:42:19  lr: 0.000028  loss: 0.6677  time: 1.4965  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1325/3000]  eta: 0:42:18  lr: 0.000028  loss: 0.5744  time: 1.4962  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1330/3000]  eta: 0:42:12  lr: 0.000028  loss: 0.7808  time: 1.5267  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1330/3000]  eta: 0:42:11  lr: 0.000028  loss: 0.3475  time: 1.5264  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1335/3000]  eta: 0:42:04  lr: 0.000028  loss: 0.3905  time: 1.5198  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1335/3000]  eta: 0:42:04  lr: 0.000028  loss: 0.5626  time: 1.5196  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1340/3000]  eta: 0:41:57  lr: 0.000028  loss: 0.4259  time: 1.5488  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1340/3000]  eta: 0:41:56  lr: 0.000028  loss: 1.0918  time: 1.5487  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1345/3000]  eta: 0:41:49  lr: 0.000028  loss: 0.3439  time: 1.5497  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1345/3000]  eta: 0:41:49  lr: 0.000028  loss: 0.9393  time: 1.5494  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1350/3000]  eta: 0:41:42  lr: 0.000028  loss: 0.4968  time: 1.5446  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1350/3000]  eta: 0:41:41  lr: 0.000028  loss: 0.3680  time: 1.5443  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1355/3000]  eta: 0:41:34  lr: 0.000028  loss: 0.5213  time: 1.5466  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1355/3000]  eta: 0:41:34  lr: 0.000028  loss: 0.1119  time: 1.5463  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1360/3000]  eta: 0:41:26  lr: 0.000028  loss: 0.6614  time: 1.5023  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1360/3000]  eta: 0:41:26  lr: 0.000028  loss: 0.5562  time: 1.5020  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1365/3000]  eta: 0:41:18  lr: 0.000028  loss: 0.6390  time: 1.4797  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1365/3000]  eta: 0:41:18  lr: 0.000028  loss: 0.2591  time: 1.4795  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1370/3000]  eta: 0:41:11  lr: 0.000028  loss: 0.5593  time: 1.4787  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1370/3000]  eta: 0:41:10  lr: 0.000028  loss: 0.3409  time: 1.4784  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1375/3000]  eta: 0:41:03  lr: 0.000028  loss: 0.5652  time: 1.4864  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1375/3000]  eta: 0:41:03  lr: 0.000028  loss: 0.5079  time: 1.4862  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1380/3000]  eta: 0:40:56  lr: 0.000028  loss: 0.0901  time: 1.5321  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1380/3000]  eta: 0:40:56  lr: 0.000028  loss: 0.2826  time: 1.5319  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1385/3000]  eta: 0:40:48  lr: 0.000028  loss: 0.1815  time: 1.5361  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1385/3000]  eta: 0:40:48  lr: 0.000028  loss: 0.2083  time: 1.5358  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1390/3000]  eta: 0:40:41  lr: 0.000028  loss: 0.5686  time: 1.5388  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1390/3000]  eta: 0:40:40  lr: 0.000028  loss: 0.3569  time: 1.5385  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1395/3000]  eta: 0:40:33  lr: 0.000028  loss: 0.2181  time: 1.5257  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1395/3000]  eta: 0:40:33  lr: 0.000028  loss: 0.2168  time: 1.5254  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1400/3000]  eta: 0:40:26  lr: 0.000028  loss: 0.8245  time: 1.5045  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1400/3000]  eta: 0:40:25  lr: 0.000028  loss: 0.3031  time: 1.5042  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1405/3000]  eta: 0:40:18  lr: 0.000028  loss: 0.1895  time: 1.5043  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1405/3000]  eta: 0:40:17  lr: 0.000028  loss: 0.5365  time: 1.5041  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1410/3000]  eta: 0:40:10  lr: 0.000028  loss: 0.2273  time: 1.4946  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1410/3000]  eta: 0:40:10  lr: 0.000028  loss: 0.3663  time: 1.4944  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1415/3000]  eta: 0:40:02  lr: 0.000028  loss: 0.1686  time: 1.4872  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1415/3000]  eta: 0:40:02  lr: 0.000028  loss: 0.1943  time: 1.4870  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1420/3000]  eta: 0:39:55  lr: 0.000028  loss: 0.1299  time: 1.4935  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1420/3000]  eta: 0:39:54  lr: 0.000028  loss: 0.0847  time: 1.4934  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1425/3000]  eta: 0:39:47  lr: 0.000028  loss: 0.3346  time: 1.5204  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1425/3000]  eta: 0:39:47  lr: 0.000028  loss: 0.5702  time: 1.5202  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1430/3000]  eta: 0:39:40  lr: 0.000028  loss: 0.4197  time: 1.5061  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1430/3000]  eta: 0:39:39  lr: 0.000028  loss: 0.6462  time: 1.5059  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1435/3000]  eta: 0:39:32  lr: 0.000028  loss: 0.7819  time: 1.5081  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1435/3000]  eta: 0:39:31  lr: 0.000028  loss: 0.3731  time: 1.5078  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1440/3000]  eta: 0:39:25  lr: 0.000028  loss: 0.6777  time: 1.5234  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1440/3000]  eta: 0:39:24  lr: 0.000028  loss: 0.1952  time: 1.5231  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1445/3000]  eta: 0:39:17  lr: 0.000028  loss: 1.0025  time: 1.5209  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1445/3000]  eta: 0:39:17  lr: 0.000028  loss: 0.1562  time: 1.5206  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1450/3000]  eta: 0:39:09  lr: 0.000028  loss: 0.1080  time: 1.5244  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1450/3000]  eta: 0:39:09  lr: 0.000028  loss: 0.3068  time: 1.5240  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1455/3000]  eta: 0:39:02  lr: 0.000028  loss: 0.3238  time: 1.5468  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1455/3000]  eta: 0:39:02  lr: 0.000028  loss: 0.1142  time: 1.5466  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1460/3000]  eta: 0:38:54  lr: 0.000028  loss: 0.6472  time: 1.5110  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1460/3000]  eta: 0:38:54  lr: 0.000028  loss: 0.1951  time: 1.5107  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1465/3000]  eta: 0:38:46  lr: 0.000028  loss: 0.2454  time: 1.4932  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1465/3000]  eta: 0:38:46  lr: 0.000028  loss: 0.2197  time: 1.4930  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1470/3000]  eta: 0:38:39  lr: 0.000028  loss: 0.4203  time: 1.5093  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1470/3000]  eta: 0:38:39  lr: 0.000028  loss: 0.7128  time: 1.5091  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1475/3000]  eta: 0:38:32  lr: 0.000028  loss: 0.4474  time: 1.5048  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1475/3000]  eta: 0:38:31  lr: 0.000028  loss: 0.4842  time: 1.5045  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1480/3000]  eta: 0:38:24  lr: 0.000028  loss: 0.8289  time: 1.5463  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1480/3000]  eta: 0:38:24  lr: 0.000028  loss: 0.4276  time: 1.5461  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1485/3000]  eta: 0:38:17  lr: 0.000028  loss: 0.4684  time: 1.5668  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1485/3000]  eta: 0:38:17  lr: 0.000028  loss: 0.5956  time: 1.5665  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1490/3000]  eta: 0:38:10  lr: 0.000028  loss: 0.7611  time: 1.5782  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1490/3000]  eta: 0:38:09  lr: 0.000028  loss: 0.3690  time: 1.5779  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1495/3000]  eta: 0:38:03  lr: 0.000028  loss: 0.1705  time: 1.5807  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1495/3000]  eta: 0:38:02  lr: 0.000028  loss: 0.5373  time: 1.5805  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1500/3000]  eta: 0:37:55  lr: 0.000028  loss: 0.6506  time: 1.5737  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1500/3000]  eta: 0:37:55  lr: 0.000028  loss: 0.1799  time: 1.5735  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1505/3000]  eta: 0:37:48  lr: 0.000028  loss: 0.2712  time: 1.5714  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1505/3000]  eta: 0:37:48  lr: 0.000028  loss: 0.8297  time: 1.5712  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1510/3000]  eta: 0:37:40  lr: 0.000028  loss: 0.1941  time: 1.5328  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1510/3000]  eta: 0:37:39  lr: 0.000028  loss: 1.2595  time: 1.5325  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1515/3000]  eta: 0:37:32  lr: 0.000028  loss: 0.6426  time: 1.5196  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1515/3000]  eta: 0:37:32  lr: 0.000028  loss: 0.1486  time: 1.5193  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1520/3000]  eta: 0:37:25  lr: 0.000028  loss: 0.1823  time: 1.5110  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1520/3000]  eta: 0:37:24  lr: 0.000028  loss: 0.3582  time: 1.5106  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1525/3000]  eta: 0:37:17  lr: 0.000028  loss: 0.5108  time: 1.5012  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1525/3000]  eta: 0:37:17  lr: 0.000028  loss: 0.4005  time: 1.5009  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1530/3000]  eta: 0:37:10  lr: 0.000028  loss: 0.2846  time: 1.5230  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1530/3000]  eta: 0:37:09  lr: 0.000028  loss: 0.7173  time: 1.5227  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1535/3000]  eta: 0:37:02  lr: 0.000028  loss: 0.7662  time: 1.5131  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1535/3000]  eta: 0:37:01  lr: 0.000028  loss: 0.8316  time: 1.5129  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1540/3000]  eta: 0:36:54  lr: 0.000028  loss: 0.3041  time: 1.5022  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1540/3000]  eta: 0:36:54  lr: 0.000028  loss: 0.4321  time: 1.5021  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1545/3000]  eta: 0:36:47  lr: 0.000028  loss: 0.4603  time: 1.5195  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1545/3000]  eta: 0:36:47  lr: 0.000028  loss: 0.4971  time: 1.5198  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1550/3000]  eta: 0:36:40  lr: 0.000028  loss: 0.3199  time: 1.5331  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1550/3000]  eta: 0:36:39  lr: 0.000028  loss: 0.4877  time: 1.5329  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1555/3000]  eta: 0:36:32  lr: 0.000028  loss: 0.1400  time: 1.5444  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1555/3000]  eta: 0:36:32  lr: 0.000028  loss: 0.7326  time: 1.5442  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1560/3000]  eta: 0:36:24  lr: 0.000028  loss: 0.4504  time: 1.5366  data: 0.0000  max mem: 18432Train: data epoch: [5]  [1560/3000]  eta: 0:36:24  lr: 0.000028  loss: 0.5024  time: 1.5363  data: 0.0000  max mem: 18151

Train: data epoch: [5]  [1565/3000]  eta: 0:36:16  lr: 0.000028  loss: 0.7438  time: 1.4999  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1565/3000]  eta: 0:36:16  lr: 0.000028  loss: 0.1477  time: 1.4997  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1570/3000]  eta: 0:36:09  lr: 0.000028  loss: 0.3906  time: 1.5041  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1570/3000]  eta: 0:36:09  lr: 0.000028  loss: 0.9547  time: 1.5039  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1575/3000]  eta: 0:36:02  lr: 0.000028  loss: 0.2979  time: 1.5220  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1575/3000]  eta: 0:36:01  lr: 0.000028  loss: 0.7003  time: 1.5218  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1580/3000]  eta: 0:35:54  lr: 0.000028  loss: 0.4566  time: 1.5378  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1580/3000]  eta: 0:35:54  lr: 0.000028  loss: 0.1547  time: 1.5376  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1585/3000]  eta: 0:35:47  lr: 0.000028  loss: 0.4923  time: 1.5610  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1585/3000]  eta: 0:35:46  lr: 0.000028  loss: 0.5434  time: 1.5608  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1590/3000]  eta: 0:35:39  lr: 0.000028  loss: 0.8053  time: 1.5597  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1590/3000]  eta: 0:35:39  lr: 0.000028  loss: 0.2802  time: 1.5594  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1595/3000]  eta: 0:35:32  lr: 0.000028  loss: 0.7894  time: 1.5313  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1595/3000]  eta: 0:35:31  lr: 0.000028  loss: 0.1355  time: 1.5310  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1600/3000]  eta: 0:35:24  lr: 0.000028  loss: 0.4973  time: 1.5184  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1600/3000]  eta: 0:35:24  lr: 0.000028  loss: 0.2886  time: 1.5182  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1605/3000]  eta: 0:35:16  lr: 0.000028  loss: 0.1703  time: 1.5202  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1605/3000]  eta: 0:35:16  lr: 0.000028  loss: 0.3065  time: 1.5200  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1610/3000]  eta: 0:35:09  lr: 0.000028  loss: 0.4437  time: 1.5252  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1610/3000]  eta: 0:35:09  lr: 0.000028  loss: 1.2040  time: 1.5249  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1615/3000]  eta: 0:35:01  lr: 0.000028  loss: 0.3326  time: 1.5187  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1615/3000]  eta: 0:35:01  lr: 0.000028  loss: 0.9411  time: 1.5185  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1620/3000]  eta: 0:34:54  lr: 0.000028  loss: 0.1458  time: 1.5213  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1620/3000]  eta: 0:34:53  lr: 0.000028  loss: 0.2514  time: 1.5210  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1625/3000]  eta: 0:34:46  lr: 0.000028  loss: 0.2273  time: 1.5057  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1625/3000]  eta: 0:34:46  lr: 0.000028  loss: 0.1422  time: 1.5054  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1630/3000]  eta: 0:34:38  lr: 0.000028  loss: 0.6453  time: 1.4949  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1630/3000]  eta: 0:34:38  lr: 0.000028  loss: 0.8587  time: 1.4946  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1635/3000]  eta: 0:34:31  lr: 0.000028  loss: 0.3015  time: 1.5050  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1635/3000]  eta: 0:34:30  lr: 0.000028  loss: 0.3974  time: 1.5047  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1640/3000]  eta: 0:34:23  lr: 0.000028  loss: 0.3322  time: 1.5168  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1640/3000]  eta: 0:34:23  lr: 0.000028  loss: 0.1572  time: 1.5166  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1645/3000]  eta: 0:34:16  lr: 0.000028  loss: 0.1694  time: 1.5166  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1645/3000]  eta: 0:34:15  lr: 0.000028  loss: 0.3734  time: 1.5164  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1650/3000]  eta: 0:34:08  lr: 0.000028  loss: 0.3566  time: 1.5089  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1650/3000]  eta: 0:34:08  lr: 0.000028  loss: 0.1998  time: 1.5087  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1655/3000]  eta: 0:34:00  lr: 0.000028  loss: 0.2797  time: 1.5134  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1655/3000]  eta: 0:34:00  lr: 0.000028  loss: 0.4625  time: 1.5131  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1660/3000]  eta: 0:33:53  lr: 0.000028  loss: 0.6057  time: 1.5121  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1660/3000]  eta: 0:33:53  lr: 0.000028  loss: 0.4498  time: 1.5119  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1665/3000]  eta: 0:33:45  lr: 0.000028  loss: 0.2289  time: 1.5135  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1665/3000]  eta: 0:33:45  lr: 0.000028  loss: 0.1988  time: 1.5133  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1670/3000]  eta: 0:33:38  lr: 0.000028  loss: 0.2322  time: 1.5232  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1670/3000]  eta: 0:33:37  lr: 0.000028  loss: 0.3669  time: 1.5230  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1675/3000]  eta: 0:33:30  lr: 0.000028  loss: 0.2135  time: 1.5274  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1675/3000]  eta: 0:33:30  lr: 0.000028  loss: 0.8544  time: 1.5273  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1680/3000]  eta: 0:33:23  lr: 0.000028  loss: 0.2393  time: 1.5173  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1680/3000]  eta: 0:33:22  lr: 0.000028  loss: 0.6936  time: 1.5171  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1685/3000]  eta: 0:33:15  lr: 0.000028  loss: 0.3040  time: 1.5181  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1685/3000]  eta: 0:33:14  lr: 0.000028  loss: 0.6188  time: 1.5179  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1690/3000]  eta: 0:33:07  lr: 0.000028  loss: 0.4832  time: 1.5075  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1690/3000]  eta: 0:33:07  lr: 0.000028  loss: 0.6004  time: 1.5072  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1695/3000]  eta: 0:32:59  lr: 0.000028  loss: 0.7085  time: 1.4976  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1695/3000]  eta: 0:32:59  lr: 0.000028  loss: 0.1006  time: 1.4973  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1700/3000]  eta: 0:32:52  lr: 0.000028  loss: 0.3743  time: 1.5010  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1700/3000]  eta: 0:32:52  lr: 0.000028  loss: 0.1987  time: 1.5008  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1705/3000]  eta: 0:32:44  lr: 0.000028  loss: 0.1335  time: 1.5132  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1705/3000]  eta: 0:32:44  lr: 0.000028  loss: 0.5196  time: 1.5129  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1710/3000]  eta: 0:32:37  lr: 0.000028  loss: 0.7543  time: 1.5273  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1710/3000]  eta: 0:32:37  lr: 0.000028  loss: 0.5599  time: 1.5271  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1715/3000]  eta: 0:32:29  lr: 0.000028  loss: 0.2483  time: 1.5360  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1715/3000]  eta: 0:32:29  lr: 0.000028  loss: 0.1132  time: 1.5358  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1720/3000]  eta: 0:32:22  lr: 0.000028  loss: 0.3727  time: 1.5582  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1720/3000]  eta: 0:32:22  lr: 0.000028  loss: 0.8405  time: 1.5579  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1725/3000]  eta: 0:32:14  lr: 0.000028  loss: 0.9769  time: 1.5409  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1725/3000]  eta: 0:32:14  lr: 0.000028  loss: 0.1543  time: 1.5407  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1730/3000]  eta: 0:32:07  lr: 0.000028  loss: 0.2817  time: 1.5433  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1730/3000]  eta: 0:32:07  lr: 0.000028  loss: 0.2863  time: 1.5430  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1735/3000]  eta: 0:31:59  lr: 0.000028  loss: 0.5165  time: 1.5350  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1735/3000]  eta: 0:31:59  lr: 0.000028  loss: 0.4070  time: 1.5348  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1740/3000]  eta: 0:31:52  lr: 0.000028  loss: 0.4599  time: 1.5122  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1740/3000]  eta: 0:31:51  lr: 0.000028  loss: 0.4421  time: 1.5119  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1745/3000]  eta: 0:31:44  lr: 0.000028  loss: 0.0967  time: 1.5084  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1745/3000]  eta: 0:31:44  lr: 0.000028  loss: 0.3176  time: 1.5081  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1750/3000]  eta: 0:31:37  lr: 0.000028  loss: 0.1110  time: 1.5114  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1750/3000]  eta: 0:31:36  lr: 0.000028  loss: 0.5784  time: 1.5111  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1755/3000]  eta: 0:31:29  lr: 0.000028  loss: 0.0579  time: 1.5338  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1755/3000]  eta: 0:31:29  lr: 0.000028  loss: 0.6934  time: 1.5335  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1760/3000]  eta: 0:31:22  lr: 0.000028  loss: 0.1815  time: 1.5524  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1760/3000]  eta: 0:31:22  lr: 0.000028  loss: 0.6699  time: 1.5521  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1765/3000]  eta: 0:31:14  lr: 0.000028  loss: 0.8119  time: 1.5665  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1765/3000]  eta: 0:31:14  lr: 0.000028  loss: 0.7540  time: 1.5662  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1770/3000]  eta: 0:31:07  lr: 0.000028  loss: 0.2559  time: 1.5486  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1770/3000]  eta: 0:31:06  lr: 0.000028  loss: 0.1754  time: 1.5484  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1775/3000]  eta: 0:30:59  lr: 0.000028  loss: 0.2275  time: 1.5242  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1775/3000]  eta: 0:30:59  lr: 0.000028  loss: 0.5029  time: 1.5240  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1780/3000]  eta: 0:30:51  lr: 0.000028  loss: 0.1271  time: 1.4980  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1780/3000]  eta: 0:30:51  lr: 0.000028  loss: 0.4658  time: 1.4977  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1785/3000]  eta: 0:30:44  lr: 0.000028  loss: 0.0984  time: 1.5094  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1785/3000]  eta: 0:30:43  lr: 0.000028  loss: 0.4683  time: 1.5091  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1790/3000]  eta: 0:30:36  lr: 0.000028  loss: 0.7457  time: 1.5119  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1790/3000]  eta: 0:30:36  lr: 0.000028  loss: 0.4145  time: 1.5116  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1795/3000]  eta: 0:30:29  lr: 0.000028  loss: 0.3147  time: 1.5137  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1795/3000]  eta: 0:30:28  lr: 0.000028  loss: 0.5641  time: 1.5134  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1800/3000]  eta: 0:30:21  lr: 0.000028  loss: 0.2787  time: 1.5271  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1800/3000]  eta: 0:30:21  lr: 0.000028  loss: 0.8195  time: 1.5269  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1805/3000]  eta: 0:30:13  lr: 0.000028  loss: 0.2318  time: 1.5050  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1805/3000]  eta: 0:30:13  lr: 0.000028  loss: 0.7445  time: 1.5047  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1810/3000]  eta: 0:30:06  lr: 0.000028  loss: 0.1795  time: 1.4985  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1810/3000]  eta: 0:30:05  lr: 0.000028  loss: 0.2755  time: 1.4983  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1815/3000]  eta: 0:29:58  lr: 0.000028  loss: 0.1514  time: 1.5122  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1815/3000]  eta: 0:29:58  lr: 0.000028  loss: 0.1265  time: 1.5120  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1820/3000]  eta: 0:29:50  lr: 0.000028  loss: 0.6436  time: 1.4762  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1820/3000]  eta: 0:29:50  lr: 0.000028  loss: 0.3322  time: 1.4760  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1825/3000]  eta: 0:29:43  lr: 0.000028  loss: 0.3587  time: 1.4937  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1825/3000]  eta: 0:29:42  lr: 0.000028  loss: 0.1711  time: 1.4935  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1830/3000]  eta: 0:29:35  lr: 0.000028  loss: 0.3873  time: 1.4956  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1830/3000]  eta: 0:29:35  lr: 0.000028  loss: 0.4576  time: 1.4955  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1835/3000]  eta: 0:29:27  lr: 0.000028  loss: 0.1675  time: 1.4771  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1835/3000]  eta: 0:29:27  lr: 0.000028  loss: 0.3483  time: 1.4768  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1840/3000]  eta: 0:29:19  lr: 0.000028  loss: 0.4483  time: 1.4929  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1840/3000]  eta: 0:29:19  lr: 0.000028  loss: 0.2232  time: 1.4926  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1845/3000]  eta: 0:29:12  lr: 0.000028  loss: 0.7297  time: 1.4813  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1845/3000]  eta: 0:29:11  lr: 0.000028  loss: 0.2265  time: 1.4820  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1850/3000]  eta: 0:29:04  lr: 0.000028  loss: 0.5906  time: 1.4933  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1850/3000]  eta: 0:29:04  lr: 0.000028  loss: 0.2397  time: 1.4930  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1855/3000]  eta: 0:28:57  lr: 0.000028  loss: 0.4135  time: 1.5089  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1855/3000]  eta: 0:28:56  lr: 0.000028  loss: 0.4716  time: 1.5086  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1860/3000]  eta: 0:28:49  lr: 0.000028  loss: 0.3581  time: 1.5376  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1860/3000]  eta: 0:28:49  lr: 0.000028  loss: 0.8182  time: 1.5374  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1865/3000]  eta: 0:28:42  lr: 0.000028  loss: 0.6379  time: 1.5285  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1865/3000]  eta: 0:28:41  lr: 0.000028  loss: 0.3784  time: 1.5272  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1870/3000]  eta: 0:28:34  lr: 0.000028  loss: 0.3740  time: 1.5153  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1870/3000]  eta: 0:28:34  lr: 0.000028  loss: 0.6164  time: 1.5151  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1875/3000]  eta: 0:28:26  lr: 0.000028  loss: 0.6659  time: 1.5037  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1875/3000]  eta: 0:28:26  lr: 0.000028  loss: 0.8736  time: 1.5034  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1880/3000]  eta: 0:28:19  lr: 0.000028  loss: 0.3101  time: 1.5034  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1880/3000]  eta: 0:28:19  lr: 0.000028  loss: 0.4060  time: 1.5031  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1885/3000]  eta: 0:28:11  lr: 0.000028  loss: 0.3268  time: 1.5123  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1885/3000]  eta: 0:28:11  lr: 0.000028  loss: 0.3658  time: 1.5120  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1890/3000]  eta: 0:28:04  lr: 0.000028  loss: 0.4976  time: 1.5149  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1890/3000]  eta: 0:28:03  lr: 0.000028  loss: 0.8260  time: 1.5145  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1895/3000]  eta: 0:27:56  lr: 0.000028  loss: 0.8150  time: 1.5048  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1895/3000]  eta: 0:27:55  lr: 0.000028  loss: 0.2297  time: 1.5045  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1900/3000]  eta: 0:27:48  lr: 0.000028  loss: 0.0472  time: 1.4830  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1900/3000]  eta: 0:27:48  lr: 0.000028  loss: 0.2774  time: 1.4828  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1905/3000]  eta: 0:27:41  lr: 0.000028  loss: 0.5341  time: 1.5005  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1905/3000]  eta: 0:27:40  lr: 0.000028  loss: 0.4635  time: 1.5003  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1910/3000]  eta: 0:27:33  lr: 0.000028  loss: 0.5181  time: 1.4957  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1910/3000]  eta: 0:27:33  lr: 0.000028  loss: 0.6221  time: 1.4955  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1915/3000]  eta: 0:27:25  lr: 0.000028  loss: 0.3339  time: 1.5118  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1915/3000]  eta: 0:27:25  lr: 0.000028  loss: 0.1423  time: 1.5116  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1920/3000]  eta: 0:27:18  lr: 0.000028  loss: 0.1220  time: 1.4949  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1920/3000]  eta: 0:27:17  lr: 0.000028  loss: 0.1124  time: 1.4945  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1925/3000]  eta: 0:27:10  lr: 0.000028  loss: 0.7391  time: 1.4695  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1925/3000]  eta: 0:27:09  lr: 0.000028  loss: 0.5701  time: 1.4692  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1930/3000]  eta: 0:27:02  lr: 0.000028  loss: 0.1313  time: 1.4889  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1930/3000]  eta: 0:27:02  lr: 0.000028  loss: 0.7884  time: 1.4887  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1935/3000]  eta: 0:26:55  lr: 0.000028  loss: 0.8890  time: 1.4904  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1935/3000]  eta: 0:26:54  lr: 0.000028  loss: 0.8707  time: 1.4902  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1940/3000]  eta: 0:26:47  lr: 0.000028  loss: 0.2154  time: 1.5280  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1940/3000]  eta: 0:26:47  lr: 0.000028  loss: 0.4263  time: 1.5277  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1945/3000]  eta: 0:26:40  lr: 0.000028  loss: 1.4852  time: 1.5344  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1945/3000]  eta: 0:26:39  lr: 0.000028  loss: 0.1689  time: 1.5341  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1950/3000]  eta: 0:26:32  lr: 0.000028  loss: 0.3412  time: 1.5062  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1950/3000]  eta: 0:26:32  lr: 0.000028  loss: 0.2979  time: 1.5059  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1955/3000]  eta: 0:26:24  lr: 0.000028  loss: 0.5611  time: 1.5187  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1955/3000]  eta: 0:26:24  lr: 0.000028  loss: 0.8097  time: 1.5185  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1960/3000]  eta: 0:26:17  lr: 0.000028  loss: 0.1167  time: 1.4845  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1960/3000]  eta: 0:26:16  lr: 0.000028  loss: 0.1001  time: 1.4842  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1965/3000]  eta: 0:26:09  lr: 0.000028  loss: 0.2767  time: 1.5005  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1965/3000]  eta: 0:26:09  lr: 0.000028  loss: 1.0085  time: 1.5002  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1970/3000]  eta: 0:26:01  lr: 0.000028  loss: 0.3712  time: 1.5040  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1970/3000]  eta: 0:26:01  lr: 0.000028  loss: 0.2569  time: 1.5037  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1975/3000]  eta: 0:25:54  lr: 0.000028  loss: 0.0994  time: 1.4859  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1975/3000]  eta: 0:25:54  lr: 0.000028  loss: 0.4509  time: 1.4856  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1980/3000]  eta: 0:25:46  lr: 0.000028  loss: 0.5112  time: 1.5095  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1980/3000]  eta: 0:25:46  lr: 0.000028  loss: 0.5479  time: 1.5092  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1985/3000]  eta: 0:25:39  lr: 0.000028  loss: 0.5295  time: 1.5021  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1985/3000]  eta: 0:25:38  lr: 0.000028  loss: 0.5360  time: 1.5019  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1990/3000]  eta: 0:25:31  lr: 0.000028  loss: 0.3951  time: 1.5047  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1990/3000]  eta: 0:25:31  lr: 0.000028  loss: 0.6115  time: 1.5045  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [1995/3000]  eta: 0:25:23  lr: 0.000028  loss: 0.3525  time: 1.5153  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [1995/3000]  eta: 0:25:23  lr: 0.000028  loss: 0.2677  time: 1.5150  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2000/3000]  eta: 0:25:16  lr: 0.000028  loss: 0.5035  time: 1.5347  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2000/3000]  eta: 0:25:16  lr: 0.000028  loss: 0.5134  time: 1.5344  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2005/3000]  eta: 0:25:08  lr: 0.000028  loss: 0.1456  time: 1.5294  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2005/3000]  eta: 0:25:08  lr: 0.000028  loss: 0.5344  time: 1.5293  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2010/3000]  eta: 0:25:01  lr: 0.000028  loss: 0.5526  time: 1.5402  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2010/3000]  eta: 0:25:01  lr: 0.000028  loss: 0.1051  time: 1.5400  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2015/3000]  eta: 0:24:54  lr: 0.000028  loss: 0.7558  time: 1.5557  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2015/3000]  eta: 0:24:53  lr: 0.000028  loss: 0.5135  time: 1.5555  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2020/3000]  eta: 0:24:46  lr: 0.000028  loss: 0.4418  time: 1.5422  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2020/3000]  eta: 0:24:46  lr: 0.000028  loss: 0.5186  time: 1.5419  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2025/3000]  eta: 0:24:38  lr: 0.000028  loss: 0.2303  time: 1.5402  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2025/3000]  eta: 0:24:38  lr: 0.000028  loss: 0.3790  time: 1.5399  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2030/3000]  eta: 0:24:31  lr: 0.000028  loss: 0.2241  time: 1.5240  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2030/3000]  eta: 0:24:30  lr: 0.000028  loss: 0.2445  time: 1.5238  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2035/3000]  eta: 0:24:23  lr: 0.000028  loss: 0.7053  time: 1.5099  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2035/3000]  eta: 0:24:23  lr: 0.000028  loss: 0.5948  time: 1.5096  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2040/3000]  eta: 0:24:16  lr: 0.000028  loss: 0.7621  time: 1.5034  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2040/3000]  eta: 0:24:15  lr: 0.000028  loss: 0.2288  time: 1.5031  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2045/3000]  eta: 0:24:08  lr: 0.000028  loss: 0.1790  time: 1.4956  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2045/3000]  eta: 0:24:08  lr: 0.000028  loss: 0.6904  time: 1.4953  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2050/3000]  eta: 0:24:00  lr: 0.000028  loss: 0.1992  time: 1.5065  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2050/3000]  eta: 0:24:00  lr: 0.000028  loss: 0.2067  time: 1.5063  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2055/3000]  eta: 0:23:53  lr: 0.000028  loss: 0.1868  time: 1.4878  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2055/3000]  eta: 0:23:52  lr: 0.000028  loss: 0.5516  time: 1.4877  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2060/3000]  eta: 0:23:45  lr: 0.000028  loss: 0.4314  time: 1.4934  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2060/3000]  eta: 0:23:45  lr: 0.000028  loss: 0.5804  time: 1.4932  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2065/3000]  eta: 0:23:37  lr: 0.000028  loss: 0.1256  time: 1.5163  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2065/3000]  eta: 0:23:37  lr: 0.000028  loss: 0.5278  time: 1.5161  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2070/3000]  eta: 0:23:30  lr: 0.000028  loss: 0.1229  time: 1.5224  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2070/3000]  eta: 0:23:30  lr: 0.000028  loss: 0.5645  time: 1.5221  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2075/3000]  eta: 0:23:22  lr: 0.000028  loss: 0.5954  time: 1.5224  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2075/3000]  eta: 0:23:22  lr: 0.000028  loss: 0.1919  time: 1.5220  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2080/3000]  eta: 0:23:15  lr: 0.000028  loss: 0.6539  time: 1.5200  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2080/3000]  eta: 0:23:14  lr: 0.000028  loss: 0.2231  time: 1.5196  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2085/3000]  eta: 0:23:07  lr: 0.000028  loss: 0.5088  time: 1.4872  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2085/3000]  eta: 0:23:07  lr: 0.000028  loss: 0.1449  time: 1.4868  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2090/3000]  eta: 0:22:59  lr: 0.000028  loss: 0.4179  time: 1.4925  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2090/3000]  eta: 0:22:59  lr: 0.000028  loss: 0.6261  time: 1.4922  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2095/3000]  eta: 0:22:52  lr: 0.000028  loss: 0.6578  time: 1.4981  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2095/3000]  eta: 0:22:52  lr: 0.000028  loss: 0.1653  time: 1.4978  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2100/3000]  eta: 0:22:44  lr: 0.000028  loss: 0.1052  time: 1.4792  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2100/3000]  eta: 0:22:44  lr: 0.000028  loss: 0.1514  time: 1.4790  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2105/3000]  eta: 0:22:37  lr: 0.000028  loss: 0.1680  time: 1.5170  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2105/3000]  eta: 0:22:36  lr: 0.000028  loss: 0.5611  time: 1.5168  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2110/3000]  eta: 0:22:29  lr: 0.000028  loss: 0.4228  time: 1.5064  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2110/3000]  eta: 0:22:29  lr: 0.000028  loss: 0.4862  time: 1.5061  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2115/3000]  eta: 0:22:21  lr: 0.000028  loss: 0.1755  time: 1.5086  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2115/3000]  eta: 0:22:21  lr: 0.000028  loss: 0.3895  time: 1.5084  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2120/3000]  eta: 0:22:14  lr: 0.000028  loss: 0.0827  time: 1.5038  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2120/3000]  eta: 0:22:13  lr: 0.000028  loss: 0.5589  time: 1.5035  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2125/3000]  eta: 0:22:06  lr: 0.000028  loss: 0.2841  time: 1.4774  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2125/3000]  eta: 0:22:06  lr: 0.000028  loss: 0.5383  time: 1.4772  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2130/3000]  eta: 0:21:58  lr: 0.000028  loss: 0.3363  time: 1.4655  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2130/3000]  eta: 0:21:58  lr: 0.000028  loss: 0.1269  time: 1.4652  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2135/3000]  eta: 0:21:51  lr: 0.000028  loss: 0.4508  time: 1.4684  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2135/3000]  eta: 0:21:50  lr: 0.000028  loss: 1.6433  time: 1.4682  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2140/3000]  eta: 0:21:43  lr: 0.000028  loss: 0.2762  time: 1.4677  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2140/3000]  eta: 0:21:43  lr: 0.000028  loss: 0.1689  time: 1.4676  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2145/3000]  eta: 0:21:35  lr: 0.000028  loss: 0.3508  time: 1.4736  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2145/3000]  eta: 0:21:35  lr: 0.000028  loss: 0.3203  time: 1.4734  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2150/3000]  eta: 0:21:28  lr: 0.000028  loss: 0.2325  time: 1.4862  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2150/3000]  eta: 0:21:27  lr: 0.000028  loss: 0.2610  time: 1.4859  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2155/3000]  eta: 0:21:20  lr: 0.000028  loss: 0.5086  time: 1.4964  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2155/3000]  eta: 0:21:20  lr: 0.000028  loss: 0.5534  time: 1.4961  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2160/3000]  eta: 0:21:13  lr: 0.000028  loss: 0.2308  time: 1.5340  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2160/3000]  eta: 0:21:13  lr: 0.000028  loss: 0.5986  time: 1.5337  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2165/3000]  eta: 0:21:05  lr: 0.000028  loss: 0.4599  time: 1.5488  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2165/3000]  eta: 0:21:05  lr: 0.000028  loss: 0.7779  time: 1.5486  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2170/3000]  eta: 0:20:58  lr: 0.000028  loss: 0.3360  time: 1.5469  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2170/3000]  eta: 0:20:57  lr: 0.000028  loss: 0.5129  time: 1.5465  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2175/3000]  eta: 0:20:50  lr: 0.000028  loss: 0.1756  time: 1.5451  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2175/3000]  eta: 0:20:50  lr: 0.000028  loss: 0.4083  time: 1.5448  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2180/3000]  eta: 0:20:43  lr: 0.000028  loss: 0.1478  time: 1.5295  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2180/3000]  eta: 0:20:42  lr: 0.000028  loss: 0.6512  time: 1.5292  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2185/3000]  eta: 0:20:35  lr: 0.000028  loss: 0.3084  time: 1.5236  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2185/3000]  eta: 0:20:35  lr: 0.000028  loss: 0.5820  time: 1.5233  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2190/3000]  eta: 0:20:27  lr: 0.000028  loss: 0.6636  time: 1.5405  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2190/3000]  eta: 0:20:27  lr: 0.000028  loss: 0.2993  time: 1.5402  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2195/3000]  eta: 0:20:20  lr: 0.000028  loss: 0.3122  time: 1.5468  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2195/3000]  eta: 0:20:20  lr: 0.000028  loss: 0.7808  time: 1.5465  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2200/3000]  eta: 0:20:12  lr: 0.000028  loss: 0.2940  time: 1.5512  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2200/3000]  eta: 0:20:12  lr: 0.000028  loss: 0.2164  time: 1.5509  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2205/3000]  eta: 0:20:05  lr: 0.000028  loss: 0.3895  time: 1.5474  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2205/3000]  eta: 0:20:05  lr: 0.000028  loss: 0.1077  time: 1.5471  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2210/3000]  eta: 0:19:57  lr: 0.000028  loss: 0.0919  time: 1.5347  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2210/3000]  eta: 0:19:57  lr: 0.000028  loss: 0.2511  time: 1.5345  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2215/3000]  eta: 0:19:50  lr: 0.000028  loss: 0.5858  time: 1.5251  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2215/3000]  eta: 0:19:50  lr: 0.000028  loss: 0.5609  time: 1.5248  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2220/3000]  eta: 0:19:42  lr: 0.000028  loss: 0.5526  time: 1.5186  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2220/3000]  eta: 0:19:42  lr: 0.000028  loss: 0.6700  time: 1.5184  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2225/3000]  eta: 0:19:35  lr: 0.000028  loss: 0.2954  time: 1.5260  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2225/3000]  eta: 0:19:34  lr: 0.000028  loss: 0.4112  time: 1.5257  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2230/3000]  eta: 0:19:27  lr: 0.000028  loss: 0.3987  time: 1.5138  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2230/3000]  eta: 0:19:27  lr: 0.000028  loss: 0.2630  time: 1.5135  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2235/3000]  eta: 0:19:19  lr: 0.000028  loss: 0.4015  time: 1.5215  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2235/3000]  eta: 0:19:19  lr: 0.000028  loss: 0.3705  time: 1.5212  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2240/3000]  eta: 0:19:12  lr: 0.000028  loss: 0.3957  time: 1.5372  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2240/3000]  eta: 0:19:12  lr: 0.000028  loss: 0.1615  time: 1.5370  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2245/3000]  eta: 0:19:04  lr: 0.000028  loss: 0.8732  time: 1.5354  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2245/3000]  eta: 0:19:04  lr: 0.000028  loss: 0.2853  time: 1.5351  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2250/3000]  eta: 0:18:57  lr: 0.000028  loss: 0.1694  time: 1.5413  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2250/3000]  eta: 0:18:57  lr: 0.000028  loss: 0.7644  time: 1.5411  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2255/3000]  eta: 0:18:49  lr: 0.000028  loss: 0.4908  time: 1.5386  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2255/3000]  eta: 0:18:49  lr: 0.000028  loss: 0.3052  time: 1.5383  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2260/3000]  eta: 0:18:42  lr: 0.000028  loss: 0.6420  time: 1.5234  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2260/3000]  eta: 0:18:41  lr: 0.000028  loss: 0.2202  time: 1.5231  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2265/3000]  eta: 0:18:34  lr: 0.000028  loss: 1.4164  time: 1.5173  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2265/3000]  eta: 0:18:34  lr: 0.000028  loss: 0.7155  time: 1.5170  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2270/3000]  eta: 0:18:26  lr: 0.000028  loss: 0.1836  time: 1.5211  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2270/3000]  eta: 0:18:26  lr: 0.000028  loss: 0.1568  time: 1.5207  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2275/3000]  eta: 0:18:19  lr: 0.000028  loss: 0.1759  time: 1.4985  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2275/3000]  eta: 0:18:19  lr: 0.000028  loss: 0.1257  time: 1.4981  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2280/3000]  eta: 0:18:11  lr: 0.000028  loss: 0.3242  time: 1.5082  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2280/3000]  eta: 0:18:11  lr: 0.000028  loss: 0.3043  time: 1.5078  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2285/3000]  eta: 0:18:04  lr: 0.000028  loss: 0.3354  time: 1.5007  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2285/3000]  eta: 0:18:03  lr: 0.000028  loss: 0.6989  time: 1.5004  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2290/3000]  eta: 0:17:56  lr: 0.000028  loss: 0.2567  time: 1.5042  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2290/3000]  eta: 0:17:56  lr: 0.000028  loss: 0.8685  time: 1.5040  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2295/3000]  eta: 0:17:49  lr: 0.000028  loss: 0.4473  time: 1.5197  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2295/3000]  eta: 0:17:48  lr: 0.000028  loss: 0.2393  time: 1.5195  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2300/3000]  eta: 0:17:41  lr: 0.000028  loss: 0.6417  time: 1.5120  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2300/3000]  eta: 0:17:41  lr: 0.000028  loss: 0.2908  time: 1.5118  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2305/3000]  eta: 0:17:33  lr: 0.000028  loss: 0.3212  time: 1.5374  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2305/3000]  eta: 0:17:33  lr: 0.000028  loss: 0.4409  time: 1.5372  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2310/3000]  eta: 0:17:26  lr: 0.000028  loss: 0.2107  time: 1.5437  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2310/3000]  eta: 0:17:26  lr: 0.000028  loss: 0.7043  time: 1.5435  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2315/3000]  eta: 0:17:18  lr: 0.000028  loss: 0.4156  time: 1.5440  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2315/3000]  eta: 0:17:18  lr: 0.000028  loss: 0.1701  time: 1.5437  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2320/3000]  eta: 0:17:11  lr: 0.000028  loss: 0.4366  time: 1.5474  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2320/3000]  eta: 0:17:11  lr: 0.000028  loss: 0.5559  time: 1.5471  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2325/3000]  eta: 0:17:03  lr: 0.000028  loss: 0.2123  time: 1.5245  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2325/3000]  eta: 0:17:03  lr: 0.000028  loss: 0.3287  time: 1.5242  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2330/3000]  eta: 0:16:56  lr: 0.000028  loss: 0.3147  time: 1.5275  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2330/3000]  eta: 0:16:55  lr: 0.000028  loss: 0.8790  time: 1.5272  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2335/3000]  eta: 0:16:48  lr: 0.000028  loss: 0.5380  time: 1.5210  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2335/3000]  eta: 0:16:48  lr: 0.000028  loss: 0.4593  time: 1.5207  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2340/3000]  eta: 0:16:40  lr: 0.000028  loss: 0.5881  time: 1.5027  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2340/3000]  eta: 0:16:40  lr: 0.000028  loss: 0.2598  time: 1.5024  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2345/3000]  eta: 0:16:33  lr: 0.000028  loss: 0.3076  time: 1.5154  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2345/3000]  eta: 0:16:33  lr: 0.000028  loss: 0.5743  time: 1.5152  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2350/3000]  eta: 0:16:25  lr: 0.000028  loss: 0.4948  time: 1.4781  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2350/3000]  eta: 0:16:25  lr: 0.000028  loss: 0.5078  time: 1.4779  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2355/3000]  eta: 0:16:18  lr: 0.000028  loss: 0.6571  time: 1.4973  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2355/3000]  eta: 0:16:17  lr: 0.000028  loss: 0.2597  time: 1.4971  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2360/3000]  eta: 0:16:10  lr: 0.000028  loss: 0.5809  time: 1.5172  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2360/3000]  eta: 0:16:10  lr: 0.000028  loss: 1.2572  time: 1.5175  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2365/3000]  eta: 0:16:02  lr: 0.000028  loss: 0.5720  time: 1.4997  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2365/3000]  eta: 0:16:02  lr: 0.000028  loss: 0.4931  time: 1.4994  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2370/3000]  eta: 0:15:55  lr: 0.000028  loss: 0.2755  time: 1.5166  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2370/3000]  eta: 0:15:55  lr: 0.000028  loss: 0.3628  time: 1.5163  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2375/3000]  eta: 0:15:47  lr: 0.000028  loss: 0.4029  time: 1.4920  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2375/3000]  eta: 0:15:47  lr: 0.000028  loss: 0.0760  time: 1.4917  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2380/3000]  eta: 0:15:40  lr: 0.000028  loss: 0.3522  time: 1.4760  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2380/3000]  eta: 0:15:39  lr: 0.000028  loss: 0.4197  time: 1.4758  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2385/3000]  eta: 0:15:32  lr: 0.000028  loss: 0.3788  time: 1.4954  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2385/3000]  eta: 0:15:32  lr: 0.000028  loss: 0.4203  time: 1.4952  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2390/3000]  eta: 0:15:24  lr: 0.000028  loss: 0.4747  time: 1.5178  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2390/3000]  eta: 0:15:24  lr: 0.000028  loss: 0.5165  time: 1.5176  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2395/3000]  eta: 0:15:17  lr: 0.000028  loss: 1.1069  time: 1.5208  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2395/3000]  eta: 0:15:17  lr: 0.000028  loss: 0.5147  time: 1.5206  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2400/3000]  eta: 0:15:09  lr: 0.000028  loss: 0.2460  time: 1.5127  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2400/3000]  eta: 0:15:09  lr: 0.000028  loss: 0.1953  time: 1.5125  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2405/3000]  eta: 0:15:02  lr: 0.000028  loss: 0.1723  time: 1.4856  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2405/3000]  eta: 0:15:01  lr: 0.000028  loss: 0.4340  time: 1.4855  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2410/3000]  eta: 0:14:54  lr: 0.000028  loss: 0.1391  time: 1.4644  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2410/3000]  eta: 0:14:54  lr: 0.000028  loss: 0.4031  time: 1.4642  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2415/3000]  eta: 0:14:46  lr: 0.000028  loss: 0.6459  time: 1.4671  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2415/3000]  eta: 0:14:46  lr: 0.000028  loss: 0.3233  time: 1.4668  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2420/3000]  eta: 0:14:39  lr: 0.000028  loss: 0.2560  time: 1.4823  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2420/3000]  eta: 0:14:39  lr: 0.000028  loss: 0.7790  time: 1.4820  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2425/3000]  eta: 0:14:31  lr: 0.000028  loss: 0.2062  time: 1.5032  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2425/3000]  eta: 0:14:31  lr: 0.000028  loss: 0.4002  time: 1.5030  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2430/3000]  eta: 0:14:24  lr: 0.000028  loss: 0.5113  time: 1.5099  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2430/3000]  eta: 0:14:23  lr: 0.000028  loss: 0.3130  time: 1.5098  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2435/3000]  eta: 0:14:16  lr: 0.000028  loss: 0.8731  time: 1.5358  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2435/3000]  eta: 0:14:16  lr: 0.000028  loss: 0.3183  time: 1.5356  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2440/3000]  eta: 0:14:09  lr: 0.000028  loss: 0.1818  time: 1.5412  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2440/3000]  eta: 0:14:08  lr: 0.000028  loss: 0.3855  time: 1.5410  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2445/3000]  eta: 0:14:01  lr: 0.000028  loss: 0.7731  time: 1.5465  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2445/3000]  eta: 0:14:01  lr: 0.000028  loss: 0.3754  time: 1.5462  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2450/3000]  eta: 0:13:53  lr: 0.000028  loss: 0.0889  time: 1.5550  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2450/3000]  eta: 0:13:53  lr: 0.000028  loss: 0.2613  time: 1.5547  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2455/3000]  eta: 0:13:46  lr: 0.000028  loss: 0.1627  time: 1.5399  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2455/3000]  eta: 0:13:46  lr: 0.000028  loss: 0.3023  time: 1.5397  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2460/3000]  eta: 0:13:38  lr: 0.000028  loss: 0.1295  time: 1.5347  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2460/3000]  eta: 0:13:38  lr: 0.000028  loss: 0.2284  time: 1.5344  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2465/3000]  eta: 0:13:31  lr: 0.000028  loss: 0.1816  time: 1.5346  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2465/3000]  eta: 0:13:31  lr: 0.000028  loss: 0.3509  time: 1.5345  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2470/3000]  eta: 0:13:23  lr: 0.000028  loss: 1.0447  time: 1.5011  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2470/3000]  eta: 0:13:23  lr: 0.000028  loss: 0.4556  time: 1.5009  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2475/3000]  eta: 0:13:15  lr: 0.000028  loss: 0.6189  time: 1.5091  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2475/3000]  eta: 0:13:15  lr: 0.000028  loss: 0.3748  time: 1.5088  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2480/3000]  eta: 0:13:08  lr: 0.000028  loss: 0.4972  time: 1.5097  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2480/3000]  eta: 0:13:08  lr: 0.000028  loss: 0.5400  time: 1.5094  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2485/3000]  eta: 0:13:00  lr: 0.000028  loss: 0.2317  time: 1.5141  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2485/3000]  eta: 0:13:00  lr: 0.000028  loss: 0.8369  time: 1.5138  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2490/3000]  eta: 0:12:53  lr: 0.000028  loss: 0.3005  time: 1.5338  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2490/3000]  eta: 0:12:53  lr: 0.000028  loss: 0.3349  time: 1.5335  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2495/3000]  eta: 0:12:45  lr: 0.000028  loss: 0.0401  time: 1.5288  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2495/3000]  eta: 0:12:45  lr: 0.000028  loss: 0.4036  time: 1.5286  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2500/3000]  eta: 0:12:38  lr: 0.000028  loss: 0.0733  time: 1.5281  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2500/3000]  eta: 0:12:38  lr: 0.000028  loss: 0.3107  time: 1.5279  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2505/3000]  eta: 0:12:30  lr: 0.000028  loss: 0.6076  time: 1.4835  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2505/3000]  eta: 0:12:30  lr: 0.000028  loss: 0.4176  time: 1.4832  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2510/3000]  eta: 0:12:22  lr: 0.000028  loss: 0.3153  time: 1.4741  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2510/3000]  eta: 0:12:22  lr: 0.000028  loss: 0.6029  time: 1.4739  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2515/3000]  eta: 0:12:15  lr: 0.000028  loss: 0.1581  time: 1.4570  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2515/3000]  eta: 0:12:15  lr: 0.000028  loss: 0.3404  time: 1.4568  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2520/3000]  eta: 0:12:07  lr: 0.000028  loss: 0.6924  time: 1.4582  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2520/3000]  eta: 0:12:07  lr: 0.000028  loss: 0.0860  time: 1.4580  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2525/3000]  eta: 0:11:59  lr: 0.000028  loss: 0.3550  time: 1.4847  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2525/3000]  eta: 0:11:59  lr: 0.000028  loss: 0.3484  time: 1.4846  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2530/3000]  eta: 0:11:52  lr: 0.000028  loss: 1.4522  time: 1.5017  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2530/3000]  eta: 0:11:52  lr: 0.000028  loss: 0.3066  time: 1.5009  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2535/3000]  eta: 0:11:44  lr: 0.000028  loss: 0.5718  time: 1.5281  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2535/3000]  eta: 0:11:44  lr: 0.000028  loss: 0.2564  time: 1.5271  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2540/3000]  eta: 0:11:37  lr: 0.000028  loss: 0.7422  time: 1.5461  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2540/3000]  eta: 0:11:37  lr: 0.000028  loss: 0.2539  time: 1.5453  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2545/3000]  eta: 0:11:29  lr: 0.000028  loss: 0.5101  time: 1.5468  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2545/3000]  eta: 0:11:29  lr: 0.000028  loss: 0.6611  time: 1.5459  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2550/3000]  eta: 0:11:22  lr: 0.000028  loss: 0.4368  time: 1.5589  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2550/3000]  eta: 0:11:22  lr: 0.000028  loss: 1.1963  time: 1.5586  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2555/3000]  eta: 0:11:14  lr: 0.000028  loss: 0.4560  time: 1.5411  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2555/3000]  eta: 0:11:14  lr: 0.000028  loss: 0.2567  time: 1.5409  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2560/3000]  eta: 0:11:07  lr: 0.000028  loss: 0.3608  time: 1.5225  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2560/3000]  eta: 0:11:06  lr: 0.000028  loss: 0.1542  time: 1.5222  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2565/3000]  eta: 0:10:59  lr: 0.000028  loss: 0.2723  time: 1.5002  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2565/3000]  eta: 0:10:59  lr: 0.000028  loss: 0.8004  time: 1.4999  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2570/3000]  eta: 0:10:51  lr: 0.000028  loss: 0.1159  time: 1.4860  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2570/3000]  eta: 0:10:51  lr: 0.000028  loss: 1.1896  time: 1.4857  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2575/3000]  eta: 0:10:44  lr: 0.000028  loss: 0.3259  time: 1.4913  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2575/3000]  eta: 0:10:44  lr: 0.000028  loss: 0.0798  time: 1.4910  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2580/3000]  eta: 0:10:36  lr: 0.000028  loss: 0.0815  time: 1.4892  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2580/3000]  eta: 0:10:36  lr: 0.000028  loss: 0.3717  time: 1.4890  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2585/3000]  eta: 0:10:29  lr: 0.000028  loss: 0.5311  time: 1.5242  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2585/3000]  eta: 0:10:29  lr: 0.000028  loss: 0.4551  time: 1.5239  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2590/3000]  eta: 0:10:21  lr: 0.000028  loss: 0.2525  time: 1.5383  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2590/3000]  eta: 0:10:21  lr: 0.000028  loss: 0.5306  time: 1.5381  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2595/3000]  eta: 0:10:14  lr: 0.000028  loss: 0.1451  time: 1.5403  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2595/3000]  eta: 0:10:13  lr: 0.000028  loss: 0.5767  time: 1.5401  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2600/3000]  eta: 0:10:06  lr: 0.000028  loss: 0.3634  time: 1.5507  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2600/3000]  eta: 0:10:06  lr: 0.000028  loss: 0.2295  time: 1.5504  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2605/3000]  eta: 0:09:58  lr: 0.000028  loss: 0.5141  time: 1.5530  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2605/3000]  eta: 0:09:58  lr: 0.000028  loss: 0.4702  time: 1.5528  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2610/3000]  eta: 0:09:51  lr: 0.000028  loss: 0.3950  time: 1.5397  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2610/3000]  eta: 0:09:51  lr: 0.000028  loss: 0.5516  time: 1.5395  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2615/3000]  eta: 0:09:43  lr: 0.000028  loss: 0.0588  time: 1.5018  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2615/3000]  eta: 0:09:43  lr: 0.000028  loss: 0.2009  time: 1.5016  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2620/3000]  eta: 0:09:36  lr: 0.000028  loss: 0.5869  time: 1.4868  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2620/3000]  eta: 0:09:35  lr: 0.000028  loss: 0.4552  time: 1.4865  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2625/3000]  eta: 0:09:28  lr: 0.000028  loss: 0.4807  time: 1.4719  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2625/3000]  eta: 0:09:28  lr: 0.000028  loss: 0.2038  time: 1.4717  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2630/3000]  eta: 0:09:20  lr: 0.000028  loss: 0.2645  time: 1.4647  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2630/3000]  eta: 0:09:20  lr: 0.000028  loss: 0.8818  time: 1.4643  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2635/3000]  eta: 0:09:13  lr: 0.000028  loss: 0.2699  time: 1.4808  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2635/3000]  eta: 0:09:13  lr: 0.000028  loss: 0.2416  time: 1.4805  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2640/3000]  eta: 0:09:05  lr: 0.000028  loss: 0.4723  time: 1.4824  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2640/3000]  eta: 0:09:05  lr: 0.000028  loss: 0.5663  time: 1.4822  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2645/3000]  eta: 0:08:58  lr: 0.000028  loss: 0.1965  time: 1.5064  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2645/3000]  eta: 0:08:58  lr: 0.000028  loss: 0.2404  time: 1.5061  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2650/3000]  eta: 0:08:50  lr: 0.000028  loss: 0.4137  time: 1.5075  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2650/3000]  eta: 0:08:50  lr: 0.000028  loss: 0.2081  time: 1.5072  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2655/3000]  eta: 0:08:42  lr: 0.000028  loss: 0.5199  time: 1.5232  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2655/3000]  eta: 0:08:42  lr: 0.000028  loss: 0.2282  time: 1.5229  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2660/3000]  eta: 0:08:35  lr: 0.000028  loss: 0.3001  time: 1.5398  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2660/3000]  eta: 0:08:35  lr: 0.000028  loss: 0.1856  time: 1.5396  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2665/3000]  eta: 0:08:27  lr: 0.000028  loss: 0.5032  time: 1.5102  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2665/3000]  eta: 0:08:27  lr: 0.000028  loss: 0.3827  time: 1.5099  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2670/3000]  eta: 0:08:20  lr: 0.000028  loss: 0.1426  time: 1.5196  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2670/3000]  eta: 0:08:20  lr: 0.000028  loss: 0.2865  time: 1.5193  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2675/3000]  eta: 0:08:12  lr: 0.000028  loss: 0.1810  time: 1.5235  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2675/3000]  eta: 0:08:12  lr: 0.000028  loss: 0.2742  time: 1.5233  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2680/3000]  eta: 0:08:05  lr: 0.000028  loss: 0.4313  time: 1.5106  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2680/3000]  eta: 0:08:04  lr: 0.000028  loss: 0.5406  time: 1.5104  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2685/3000]  eta: 0:07:57  lr: 0.000028  loss: 0.8244  time: 1.5262  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2685/3000]  eta: 0:07:57  lr: 0.000028  loss: 0.4494  time: 1.5260  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2690/3000]  eta: 0:07:49  lr: 0.000028  loss: 0.1229  time: 1.5263  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2690/3000]  eta: 0:07:49  lr: 0.000028  loss: 0.7040  time: 1.5260  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2695/3000]  eta: 0:07:42  lr: 0.000028  loss: 0.4524  time: 1.5332  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2695/3000]  eta: 0:07:42  lr: 0.000028  loss: 0.2524  time: 1.5329  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2700/3000]  eta: 0:07:34  lr: 0.000028  loss: 0.6542  time: 1.5393  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2700/3000]  eta: 0:07:34  lr: 0.000028  loss: 0.2304  time: 1.5390  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2705/3000]  eta: 0:07:27  lr: 0.000028  loss: 0.4820  time: 1.5288  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2705/3000]  eta: 0:07:27  lr: 0.000028  loss: 0.3866  time: 1.5286  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2710/3000]  eta: 0:07:19  lr: 0.000028  loss: 0.6128  time: 1.5474  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2710/3000]  eta: 0:07:19  lr: 0.000028  loss: 0.5190  time: 1.5472  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2715/3000]  eta: 0:07:12  lr: 0.000028  loss: 0.1823  time: 1.5507  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2715/3000]  eta: 0:07:12  lr: 0.000028  loss: 0.3880  time: 1.5505  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2720/3000]  eta: 0:07:04  lr: 0.000028  loss: 0.3327  time: 1.5385  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2720/3000]  eta: 0:07:04  lr: 0.000028  loss: 1.2075  time: 1.5382  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2725/3000]  eta: 0:06:56  lr: 0.000028  loss: 0.1040  time: 1.5558  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2725/3000]  eta: 0:06:56  lr: 0.000028  loss: 0.9929  time: 1.5556  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2730/3000]  eta: 0:06:49  lr: 0.000028  loss: 0.1623  time: 1.5571  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2730/3000]  eta: 0:06:49  lr: 0.000028  loss: 0.2190  time: 1.5569  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2735/3000]  eta: 0:06:41  lr: 0.000028  loss: 0.6217  time: 1.5336  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2735/3000]  eta: 0:06:41  lr: 0.000028  loss: 0.3538  time: 1.5334  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2740/3000]  eta: 0:06:34  lr: 0.000028  loss: 0.6032  time: 1.5424  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2740/3000]  eta: 0:06:34  lr: 0.000028  loss: 0.5810  time: 1.5421  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2745/3000]  eta: 0:06:26  lr: 0.000028  loss: 0.6055  time: 1.5479  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2745/3000]  eta: 0:06:26  lr: 0.000028  loss: 0.4517  time: 1.5477  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2750/3000]  eta: 0:06:19  lr: 0.000028  loss: 0.3475  time: 1.5133  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2750/3000]  eta: 0:06:19  lr: 0.000028  loss: 0.0942  time: 1.5131  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2755/3000]  eta: 0:06:11  lr: 0.000028  loss: 0.2046  time: 1.5072  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2755/3000]  eta: 0:06:11  lr: 0.000028  loss: 0.4198  time: 1.5070  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2760/3000]  eta: 0:06:03  lr: 0.000028  loss: 0.5701  time: 1.4840  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2760/3000]  eta: 0:06:03  lr: 0.000028  loss: 0.2747  time: 1.4838  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2765/3000]  eta: 0:05:56  lr: 0.000028  loss: 0.3246  time: 1.4417  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2765/3000]  eta: 0:05:56  lr: 0.000028  loss: 0.5168  time: 1.4415  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2770/3000]  eta: 0:05:48  lr: 0.000028  loss: 0.1090  time: 1.4429  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2770/3000]  eta: 0:05:48  lr: 0.000028  loss: 0.4002  time: 1.4426  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2775/3000]  eta: 0:05:41  lr: 0.000028  loss: 0.2889  time: 1.4613  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2775/3000]  eta: 0:05:41  lr: 0.000028  loss: 0.1916  time: 1.4610  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2780/3000]  eta: 0:05:33  lr: 0.000028  loss: 0.3978  time: 1.4750  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2780/3000]  eta: 0:05:33  lr: 0.000028  loss: 0.2899  time: 1.4747  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2785/3000]  eta: 0:05:25  lr: 0.000028  loss: 0.9234  time: 1.5042  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2785/3000]  eta: 0:05:25  lr: 0.000028  loss: 0.7915  time: 1.5039  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2790/3000]  eta: 0:05:18  lr: 0.000028  loss: 0.4056  time: 1.5287  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2790/3000]  eta: 0:05:18  lr: 0.000028  loss: 0.2256  time: 1.5285  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2795/3000]  eta: 0:05:10  lr: 0.000028  loss: 0.5161  time: 1.5192  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2795/3000]  eta: 0:05:10  lr: 0.000028  loss: 1.0724  time: 1.5190  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2800/3000]  eta: 0:05:03  lr: 0.000028  loss: 0.2174  time: 1.5454  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2800/3000]  eta: 0:05:03  lr: 0.000028  loss: 0.2407  time: 1.5453  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2805/3000]  eta: 0:04:55  lr: 0.000028  loss: 1.4502  time: 1.5586  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2805/3000]  eta: 0:04:55  lr: 0.000028  loss: 0.5109  time: 1.5584  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2810/3000]  eta: 0:04:48  lr: 0.000028  loss: 0.3326  time: 1.5272  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2810/3000]  eta: 0:04:47  lr: 0.000028  loss: 0.4121  time: 1.5270  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2815/3000]  eta: 0:04:40  lr: 0.000028  loss: 0.3058  time: 1.5510  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2815/3000]  eta: 0:04:40  lr: 0.000028  loss: 0.4243  time: 1.5507  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2820/3000]  eta: 0:04:32  lr: 0.000028  loss: 0.3874  time: 1.5471  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2820/3000]  eta: 0:04:32  lr: 0.000028  loss: 0.4090  time: 1.5469  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2825/3000]  eta: 0:04:25  lr: 0.000028  loss: 0.4172  time: 1.5292  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2825/3000]  eta: 0:04:25  lr: 0.000028  loss: 0.4317  time: 1.5289  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2830/3000]  eta: 0:04:17  lr: 0.000028  loss: 0.4746  time: 1.5587  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2830/3000]  eta: 0:04:17  lr: 0.000028  loss: 0.2628  time: 1.5584  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2835/3000]  eta: 0:04:10  lr: 0.000028  loss: 0.3824  time: 1.5419  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2835/3000]  eta: 0:04:10  lr: 0.000028  loss: 0.7017  time: 1.5417  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2840/3000]  eta: 0:04:02  lr: 0.000028  loss: 0.2404  time: 1.5340  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2840/3000]  eta: 0:04:02  lr: 0.000028  loss: 0.3411  time: 1.5337  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2845/3000]  eta: 0:03:55  lr: 0.000028  loss: 0.1336  time: 1.5515  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2845/3000]  eta: 0:03:55  lr: 0.000028  loss: 0.5526  time: 1.5513  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2850/3000]  eta: 0:03:47  lr: 0.000028  loss: 0.1677  time: 1.5461  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2850/3000]  eta: 0:03:47  lr: 0.000028  loss: 0.2403  time: 1.5459  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2855/3000]  eta: 0:03:39  lr: 0.000028  loss: 0.1911  time: 1.5341  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2855/3000]  eta: 0:03:39  lr: 0.000028  loss: 0.4848  time: 1.5338  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2860/3000]  eta: 0:03:32  lr: 0.000028  loss: 0.4442  time: 1.5221  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2860/3000]  eta: 0:03:32  lr: 0.000028  loss: 0.1293  time: 1.5219  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2865/3000]  eta: 0:03:24  lr: 0.000028  loss: 0.2275  time: 1.4870  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2865/3000]  eta: 0:03:24  lr: 0.000028  loss: 0.3755  time: 1.4867  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2870/3000]  eta: 0:03:17  lr: 0.000028  loss: 0.4183  time: 1.4766  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2870/3000]  eta: 0:03:17  lr: 0.000028  loss: 0.6907  time: 1.4762  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2875/3000]  eta: 0:03:09  lr: 0.000028  loss: 0.4393  time: 1.4908  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2875/3000]  eta: 0:03:09  lr: 0.000028  loss: 0.6510  time: 1.4904  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2880/3000]  eta: 0:03:01  lr: 0.000028  loss: 0.2200  time: 1.5010  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2880/3000]  eta: 0:03:01  lr: 0.000028  loss: 0.3618  time: 1.5006  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2885/3000]  eta: 0:02:54  lr: 0.000028  loss: 0.3295  time: 1.5076  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2885/3000]  eta: 0:02:54  lr: 0.000028  loss: 0.2809  time: 1.5073  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2890/3000]  eta: 0:02:46  lr: 0.000028  loss: 0.1575  time: 1.5020  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2890/3000]  eta: 0:02:46  lr: 0.000028  loss: 0.3412  time: 1.5018  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2895/3000]  eta: 0:02:39  lr: 0.000028  loss: 0.4786  time: 1.4899  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2895/3000]  eta: 0:02:39  lr: 0.000028  loss: 0.0918  time: 1.4897  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2900/3000]  eta: 0:02:31  lr: 0.000028  loss: 0.2454  time: 1.4645  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2900/3000]  eta: 0:02:31  lr: 0.000028  loss: 0.3032  time: 1.4642  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2905/3000]  eta: 0:02:24  lr: 0.000028  loss: 0.3058  time: 1.4964  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2905/3000]  eta: 0:02:24  lr: 0.000028  loss: 0.7395  time: 1.4962  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2910/3000]  eta: 0:02:16  lr: 0.000028  loss: 0.1451  time: 1.4984  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2910/3000]  eta: 0:02:16  lr: 0.000028  loss: 0.3720  time: 1.4981  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2915/3000]  eta: 0:02:08  lr: 0.000028  loss: 0.5073  time: 1.5118  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2915/3000]  eta: 0:02:08  lr: 0.000028  loss: 0.5318  time: 1.5116  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2920/3000]  eta: 0:02:01  lr: 0.000028  loss: 1.0720  time: 1.5136  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2920/3000]  eta: 0:02:01  lr: 0.000028  loss: 1.0811  time: 1.5133  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2925/3000]  eta: 0:01:53  lr: 0.000028  loss: 1.2935  time: 1.4947  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2925/3000]  eta: 0:01:53  lr: 0.000028  loss: 0.6649  time: 1.4944  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2930/3000]  eta: 0:01:46  lr: 0.000028  loss: 0.5282  time: 1.5044  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2930/3000]  eta: 0:01:46  lr: 0.000028  loss: 0.1531  time: 1.5042  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2935/3000]  eta: 0:01:38  lr: 0.000028  loss: 0.7225  time: 1.4984  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2935/3000]  eta: 0:01:38  lr: 0.000028  loss: 0.3885  time: 1.4982  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2940/3000]  eta: 0:01:30  lr: 0.000028  loss: 0.3851  time: 1.5083  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2940/3000]  eta: 0:01:30  lr: 0.000028  loss: 0.3294  time: 1.5081  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2945/3000]  eta: 0:01:23  lr: 0.000028  loss: 0.9410  time: 1.5018  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2945/3000]  eta: 0:01:23  lr: 0.000028  loss: 0.3885  time: 1.5016  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2950/3000]  eta: 0:01:15  lr: 0.000028  loss: 0.2931  time: 1.4973  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2950/3000]  eta: 0:01:15  lr: 0.000028  loss: 0.3369  time: 1.4971  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2955/3000]  eta: 0:01:08  lr: 0.000028  loss: 0.2926  time: 1.4950  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2955/3000]  eta: 0:01:08  lr: 0.000028  loss: 1.6321  time: 1.4948  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2960/3000]  eta: 0:01:00  lr: 0.000028  loss: 0.5437  time: 1.5192  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2960/3000]  eta: 0:01:00  lr: 0.000028  loss: 0.1493  time: 1.5189  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2965/3000]  eta: 0:00:53  lr: 0.000028  loss: 0.4633  time: 1.5020  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2965/3000]  eta: 0:00:53  lr: 0.000028  loss: 0.4664  time: 1.5018  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2970/3000]  eta: 0:00:45  lr: 0.000028  loss: 0.6112  time: 1.5063  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2970/3000]  eta: 0:00:45  lr: 0.000028  loss: 0.6431  time: 1.5061  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2975/3000]  eta: 0:00:37  lr: 0.000028  loss: 0.4348  time: 1.5088  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2975/3000]  eta: 0:00:37  lr: 0.000028  loss: 1.2595  time: 1.5086  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2980/3000]  eta: 0:00:30  lr: 0.000028  loss: 0.4820  time: 1.4994  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2980/3000]  eta: 0:00:30  lr: 0.000028  loss: 0.2113  time: 1.4992  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2985/3000]  eta: 0:00:22  lr: 0.000028  loss: 0.4509  time: 1.5389  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2985/3000]  eta: 0:00:22  lr: 0.000028  loss: 0.1221  time: 1.5387  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2990/3000]  eta: 0:00:15  lr: 0.000028  loss: 0.2041  time: 1.5327  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2990/3000]  eta: 0:00:15  lr: 0.000028  loss: 0.1442  time: 1.5326  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2995/3000]  eta: 0:00:07  lr: 0.000028  loss: 0.4676  time: 1.5147  data: 0.0000  max mem: 18432
Train: data epoch: [5]  [2995/3000]  eta: 0:00:07  lr: 0.000028  loss: 0.5524  time: 1.5145  data: 0.0000  max mem: 18151
Train: data epoch: [5]  [2999/3000]  eta: 0:00:01  lr: 0.000028  loss: 0.3684  time: 1.5085  data: 0.0000  max mem: 18432
Train: data epoch: [5] Total time: 1:15:47 (1.5159 s / it)
Train: data epoch: [5]  [2999/3000]  eta: 0:00:01  lr: 0.000028  loss: 0.1448  time: 1.5083  data: 0.0000  max mem: 18151
Train: data epoch: [5] Total time: 1:15:47 (1.5159 s / it)
2025-01-19 07:14:54,979 [INFO] Averaged stats: lr: 0.0000  loss: 0.4256
2025-01-19 07:14:54,984 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [5]  [0/1]  eta: 0:00:00    time: 0.7795  data: 0.4858  max mem: 18151
Eval: data epoch: [5] Total time: 0:00:00 (0.9123 s / it)
Eval: data epoch: [5]  [0/1]  eta: 0:00:00    time: 0.9371  data: 0.6454  max mem: 18432
Eval: data epoch: [5] Total time: 0:00:01 (1.0817 s / it)
2025-01-19 07:14:56,091 [INFO] Saving checkpoint at epoch 5 to outputs_stage1_only/202501182338/checkpoint_5.pth.
2025-01-19 07:14:58,476 [INFO] Training Phase
2025-01-19 07:14:58,484 [INFO] Start training epoch 6, 3000 iters per inner epoch.
Train: data epoch: [6]  [   0/3000]  eta: 1:11:24  lr: 0.000028  loss: 0.1559  time: 1.4281  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [   0/3000]  eta: 1:11:25  lr: 0.000028  loss: 0.9415  time: 1.4284  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [   5/3000]  eta: 1:17:22  lr: 0.000028  loss: 0.7399  time: 1.5501  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [   5/3000]  eta: 1:17:21  lr: 0.000028  loss: 0.6496  time: 1.5499  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [  10/3000]  eta: 1:18:18  lr: 0.000028  loss: 0.2359  time: 1.5714  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [  10/3000]  eta: 1:18:17  lr: 0.000028  loss: 0.1997  time: 1.5711  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [  15/3000]  eta: 1:17:53  lr: 0.000028  loss: 0.1381  time: 1.5657  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [  15/3000]  eta: 1:17:52  lr: 0.000028  loss: 0.8914  time: 1.5655  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [  20/3000]  eta: 1:16:16  lr: 0.000028  loss: 0.1743  time: 1.5410  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [  20/3000]  eta: 1:16:15  lr: 0.000028  loss: 0.4404  time: 1.5407  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [  25/3000]  eta: 1:15:40  lr: 0.000028  loss: 0.2393  time: 1.5189  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [  25/3000]  eta: 1:15:39  lr: 0.000028  loss: 0.3648  time: 1.5186  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [  30/3000]  eta: 1:15:49  lr: 0.000028  loss: 0.3213  time: 1.5099  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [  30/3000]  eta: 1:15:48  lr: 0.000028  loss: 0.4873  time: 1.5096  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [  35/3000]  eta: 1:15:48  lr: 0.000028  loss: 0.3039  time: 1.5087  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [  35/3000]  eta: 1:15:47  lr: 0.000028  loss: 0.1172  time: 1.5084  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [  40/3000]  eta: 1:15:44  lr: 0.000028  loss: 0.3035  time: 1.5353  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [  40/3000]  eta: 1:15:44  lr: 0.000028  loss: 0.2761  time: 1.5350  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [  45/3000]  eta: 1:15:22  lr: 0.000028  loss: 0.1586  time: 1.5361  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [  45/3000]  eta: 1:15:21  lr: 0.000028  loss: 0.9143  time: 1.5360  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [  50/3000]  eta: 1:14:41  lr: 0.000028  loss: 0.3802  time: 1.5000  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [  50/3000]  eta: 1:14:41  lr: 0.000028  loss: 0.2542  time: 1.4998  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [  55/3000]  eta: 1:14:21  lr: 0.000028  loss: 0.5453  time: 1.4808  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [  55/3000]  eta: 1:14:20  lr: 0.000028  loss: 1.1333  time: 1.4805  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [  60/3000]  eta: 1:14:33  lr: 0.000028  loss: 0.0417  time: 1.4932  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [  60/3000]  eta: 1:14:32  lr: 0.000028  loss: 1.5454  time: 1.4929  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [  65/3000]  eta: 1:14:33  lr: 0.000028  loss: 0.5218  time: 1.5099  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [  65/3000]  eta: 1:14:32  lr: 0.000028  loss: 0.4052  time: 1.5096  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [  70/3000]  eta: 1:14:11  lr: 0.000028  loss: 0.6622  time: 1.5196  data: 0.0000  max mem: 18151Train: data epoch: [6]  [  70/3000]  eta: 1:14:12  lr: 0.000028  loss: 0.5895  time: 1.5199  data: 0.0000  max mem: 18432

Train: data epoch: [6]  [  75/3000]  eta: 1:14:06  lr: 0.000028  loss: 0.4785  time: 1.5348  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [  75/3000]  eta: 1:14:05  lr: 0.000028  loss: 0.5524  time: 1.5345  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [  80/3000]  eta: 1:14:05  lr: 0.000028  loss: 0.4226  time: 1.5245  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [  80/3000]  eta: 1:14:04  lr: 0.000028  loss: 0.3183  time: 1.5243  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [  85/3000]  eta: 1:14:02  lr: 0.000028  loss: 0.9948  time: 1.5239  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [  85/3000]  eta: 1:14:02  lr: 0.000028  loss: 0.1588  time: 1.5237  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [  90/3000]  eta: 1:13:47  lr: 0.000028  loss: 0.1453  time: 1.5285  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [  90/3000]  eta: 1:13:46  lr: 0.000028  loss: 0.3604  time: 1.5283  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [  95/3000]  eta: 1:13:29  lr: 0.000028  loss: 0.3925  time: 1.5098  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [  95/3000]  eta: 1:13:29  lr: 0.000028  loss: 0.4399  time: 1.5096  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 100/3000]  eta: 1:13:15  lr: 0.000028  loss: 0.1823  time: 1.4895  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 100/3000]  eta: 1:13:15  lr: 0.000028  loss: 0.1182  time: 1.4893  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 105/3000]  eta: 1:13:13  lr: 0.000028  loss: 1.0301  time: 1.4899  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 105/3000]  eta: 1:13:13  lr: 0.000028  loss: 0.2359  time: 1.4897  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 110/3000]  eta: 1:13:04  lr: 0.000028  loss: 0.6576  time: 1.4977  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 110/3000]  eta: 1:13:03  lr: 0.000028  loss: 0.2751  time: 1.4975  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 115/3000]  eta: 1:12:49  lr: 0.000028  loss: 0.4534  time: 1.4977  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 115/3000]  eta: 1:12:48  lr: 0.000028  loss: 0.1250  time: 1.4975  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 120/3000]  eta: 1:12:45  lr: 0.000028  loss: 1.0053  time: 1.5149  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 120/3000]  eta: 1:12:44  lr: 0.000028  loss: 0.2290  time: 1.5147  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 125/3000]  eta: 1:12:45  lr: 0.000028  loss: 0.3864  time: 1.5213  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 125/3000]  eta: 1:12:44  lr: 0.000028  loss: 0.5686  time: 1.5211  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 130/3000]  eta: 1:12:36  lr: 0.000028  loss: 0.4862  time: 1.5225  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 130/3000]  eta: 1:12:35  lr: 0.000028  loss: 0.2301  time: 1.5222  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 135/3000]  eta: 1:12:28  lr: 0.000028  loss: 0.5279  time: 1.5373  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 135/3000]  eta: 1:12:28  lr: 0.000028  loss: 0.4307  time: 1.5371  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 140/3000]  eta: 1:12:18  lr: 0.000028  loss: 0.3623  time: 1.5240  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 140/3000]  eta: 1:12:17  lr: 0.000028  loss: 0.3534  time: 1.5237  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 145/3000]  eta: 1:12:14  lr: 0.000028  loss: 0.4716  time: 1.5178  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 145/3000]  eta: 1:12:13  lr: 0.000028  loss: 0.3911  time: 1.5176  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 150/3000]  eta: 1:11:57  lr: 0.000028  loss: 0.1590  time: 1.4951  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 150/3000]  eta: 1:11:56  lr: 0.000028  loss: 0.0987  time: 1.4949  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 155/3000]  eta: 1:11:46  lr: 0.000028  loss: 0.8795  time: 1.4858  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 155/3000]  eta: 1:11:45  lr: 0.000028  loss: 0.2627  time: 1.4855  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 160/3000]  eta: 1:11:39  lr: 0.000028  loss: 0.5731  time: 1.4922  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 160/3000]  eta: 1:11:38  lr: 0.000028  loss: 0.7481  time: 1.4920  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 165/3000]  eta: 1:11:26  lr: 0.000028  loss: 0.3534  time: 1.4655  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 165/3000]  eta: 1:11:25  lr: 0.000028  loss: 0.2754  time: 1.4653  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 170/3000]  eta: 1:11:20  lr: 0.000028  loss: 0.2143  time: 1.4938  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 170/3000]  eta: 1:11:19  lr: 0.000028  loss: 0.4275  time: 1.4935  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 175/3000]  eta: 1:11:09  lr: 0.000028  loss: 1.3163  time: 1.4920  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 175/3000]  eta: 1:11:08  lr: 0.000028  loss: 0.1566  time: 1.4917  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 180/3000]  eta: 1:11:05  lr: 0.000028  loss: 1.2075  time: 1.5030  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 180/3000]  eta: 1:11:04  lr: 0.000028  loss: 0.2913  time: 1.5028  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 185/3000]  eta: 1:11:00  lr: 0.000028  loss: 0.6359  time: 1.5261  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 185/3000]  eta: 1:10:59  lr: 0.000028  loss: 0.6622  time: 1.5259  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 190/3000]  eta: 1:10:56  lr: 0.000028  loss: 0.2741  time: 1.5345  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 190/3000]  eta: 1:10:55  lr: 0.000028  loss: 1.0264  time: 1.5344  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 195/3000]  eta: 1:10:49  lr: 0.000028  loss: 0.4494  time: 1.5479  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 195/3000]  eta: 1:10:48  lr: 0.000028  loss: 0.5005  time: 1.5477  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 200/3000]  eta: 1:10:40  lr: 0.000028  loss: 0.6268  time: 1.5323  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 200/3000]  eta: 1:10:40  lr: 0.000028  loss: 0.4400  time: 1.5321  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 205/3000]  eta: 1:10:35  lr: 0.000028  loss: 0.3086  time: 1.5332  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 205/3000]  eta: 1:10:34  lr: 0.000028  loss: 0.3813  time: 1.5330  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 210/3000]  eta: 1:10:28  lr: 0.000028  loss: 0.5111  time: 1.5218  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 210/3000]  eta: 1:10:27  lr: 0.000028  loss: 1.7517  time: 1.5215  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 215/3000]  eta: 1:10:21  lr: 0.000028  loss: 0.8132  time: 1.5231  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 215/3000]  eta: 1:10:20  lr: 0.000028  loss: 0.4042  time: 1.5228  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 220/3000]  eta: 1:10:11  lr: 0.000028  loss: 0.2883  time: 1.5180  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 220/3000]  eta: 1:10:10  lr: 0.000028  loss: 1.3622  time: 1.5177  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 225/3000]  eta: 1:10:07  lr: 0.000028  loss: 1.1140  time: 1.5237  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 225/3000]  eta: 1:10:06  lr: 0.000028  loss: 0.4413  time: 1.5235  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 230/3000]  eta: 1:10:00  lr: 0.000028  loss: 0.5300  time: 1.5250  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 230/3000]  eta: 1:09:59  lr: 0.000028  loss: 0.5922  time: 1.5248  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 235/3000]  eta: 1:09:52  lr: 0.000028  loss: 0.6100  time: 1.5215  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 235/3000]  eta: 1:09:51  lr: 0.000028  loss: 0.1166  time: 1.5212  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 240/3000]  eta: 1:09:45  lr: 0.000028  loss: 0.3215  time: 1.5327  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 240/3000]  eta: 1:09:44  lr: 0.000028  loss: 0.5612  time: 1.5325  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 245/3000]  eta: 1:09:33  lr: 0.000028  loss: 0.6782  time: 1.5015  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 245/3000]  eta: 1:09:32  lr: 0.000028  loss: 0.3873  time: 1.5013  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 250/3000]  eta: 1:09:25  lr: 0.000028  loss: 0.4581  time: 1.4969  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 250/3000]  eta: 1:09:24  lr: 0.000028  loss: 0.1007  time: 1.4966  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 255/3000]  eta: 1:09:20  lr: 0.000028  loss: 0.2661  time: 1.5070  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 255/3000]  eta: 1:09:19  lr: 0.000028  loss: 0.5563  time: 1.5068  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 260/3000]  eta: 1:09:10  lr: 0.000028  loss: 0.3015  time: 1.4956  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 260/3000]  eta: 1:09:09  lr: 0.000028  loss: 0.3952  time: 1.4953  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 265/3000]  eta: 1:09:03  lr: 0.000028  loss: 0.4327  time: 1.5162  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 265/3000]  eta: 1:09:02  lr: 0.000028  loss: 0.2051  time: 1.5159  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 270/3000]  eta: 1:08:58  lr: 0.000028  loss: 0.4422  time: 1.5320  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 270/3000]  eta: 1:08:58  lr: 0.000028  loss: 0.4262  time: 1.5318  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 275/3000]  eta: 1:08:52  lr: 0.000028  loss: 0.2891  time: 1.5277  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 275/3000]  eta: 1:08:51  lr: 0.000028  loss: 0.1827  time: 1.5275  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 280/3000]  eta: 1:08:45  lr: 0.000028  loss: 0.3828  time: 1.5402  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 280/3000]  eta: 1:08:44  lr: 0.000028  loss: 0.6550  time: 1.5400  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 285/3000]  eta: 1:08:39  lr: 0.000028  loss: 0.4431  time: 1.5457  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 285/3000]  eta: 1:08:38  lr: 0.000028  loss: 0.7274  time: 1.5454  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 290/3000]  eta: 1:08:32  lr: 0.000028  loss: 0.4681  time: 1.5359  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 290/3000]  eta: 1:08:31  lr: 0.000028  loss: 0.2871  time: 1.5356  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 295/3000]  eta: 1:08:26  lr: 0.000028  loss: 0.4852  time: 1.5408  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 295/3000]  eta: 1:08:25  lr: 0.000028  loss: 0.4870  time: 1.5405  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 300/3000]  eta: 1:08:21  lr: 0.000028  loss: 0.2794  time: 1.5521  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 300/3000]  eta: 1:08:20  lr: 0.000028  loss: 0.5546  time: 1.5518  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 305/3000]  eta: 1:08:12  lr: 0.000028  loss: 0.4227  time: 1.5397  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 305/3000]  eta: 1:08:11  lr: 0.000028  loss: 0.2711  time: 1.5394  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 310/3000]  eta: 1:08:08  lr: 0.000028  loss: 0.1925  time: 1.5537  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 310/3000]  eta: 1:08:07  lr: 0.000028  loss: 0.5393  time: 1.5535  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 315/3000]  eta: 1:08:02  lr: 0.000028  loss: 0.1735  time: 1.5560  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 315/3000]  eta: 1:08:01  lr: 0.000028  loss: 0.3229  time: 1.5559  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 320/3000]  eta: 1:07:54  lr: 0.000028  loss: 0.8937  time: 1.5395  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 320/3000]  eta: 1:07:53  lr: 0.000028  loss: 0.1263  time: 1.5393  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 325/3000]  eta: 1:07:49  lr: 0.000028  loss: 0.5585  time: 1.5600  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 325/3000]  eta: 1:07:48  lr: 0.000028  loss: 0.2826  time: 1.5597  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 330/3000]  eta: 1:07:40  lr: 0.000028  loss: 0.1345  time: 1.5373  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 330/3000]  eta: 1:07:39  lr: 0.000028  loss: 0.2881  time: 1.5370  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 335/3000]  eta: 1:07:35  lr: 0.000028  loss: 0.8101  time: 1.5414  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 335/3000]  eta: 1:07:34  lr: 0.000028  loss: 0.4312  time: 1.5411  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 340/3000]  eta: 1:07:22  lr: 0.000028  loss: 0.6112  time: 1.5118  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 340/3000]  eta: 1:07:21  lr: 0.000028  loss: 0.2853  time: 1.5115  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 345/3000]  eta: 1:07:17  lr: 0.000028  loss: 0.4797  time: 1.5131  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 345/3000]  eta: 1:07:16  lr: 0.000028  loss: 0.4802  time: 1.5129  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 350/3000]  eta: 1:07:10  lr: 0.000028  loss: 0.1212  time: 1.5227  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 350/3000]  eta: 1:07:09  lr: 0.000028  loss: 0.4285  time: 1.5225  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 355/3000]  eta: 1:07:02  lr: 0.000028  loss: 0.6678  time: 1.5031  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 355/3000]  eta: 1:07:01  lr: 0.000028  loss: 0.2561  time: 1.5027  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 360/3000]  eta: 1:06:55  lr: 0.000028  loss: 0.3652  time: 1.5420  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 360/3000]  eta: 1:06:54  lr: 0.000028  loss: 0.0781  time: 1.5417  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 365/3000]  eta: 1:06:48  lr: 0.000028  loss: 0.2534  time: 1.5288  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 365/3000]  eta: 1:06:47  lr: 0.000028  loss: 0.2461  time: 1.5286  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 370/3000]  eta: 1:06:39  lr: 0.000028  loss: 0.3006  time: 1.5204  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 370/3000]  eta: 1:06:39  lr: 0.000028  loss: 0.1643  time: 1.5201  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 375/3000]  eta: 1:06:33  lr: 0.000028  loss: 0.3135  time: 1.5313  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 375/3000]  eta: 1:06:32  lr: 0.000028  loss: 0.3936  time: 1.5311  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 380/3000]  eta: 1:06:25  lr: 0.000028  loss: 0.4582  time: 1.5244  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 380/3000]  eta: 1:06:24  lr: 0.000028  loss: 0.3355  time: 1.5241  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 385/3000]  eta: 1:06:17  lr: 0.000028  loss: 0.8021  time: 1.5176  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 385/3000]  eta: 1:06:16  lr: 0.000028  loss: 0.5932  time: 1.5173  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 390/3000]  eta: 1:06:09  lr: 0.000028  loss: 1.3529  time: 1.5210  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 390/3000]  eta: 1:06:08  lr: 0.000028  loss: 0.4418  time: 1.5208  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 395/3000]  eta: 1:06:02  lr: 0.000028  loss: 0.1668  time: 1.5182  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 395/3000]  eta: 1:06:01  lr: 0.000028  loss: 0.3157  time: 1.5179  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 400/3000]  eta: 1:05:53  lr: 0.000028  loss: 0.1738  time: 1.5097  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 400/3000]  eta: 1:05:52  lr: 0.000028  loss: 0.5455  time: 1.5095  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 405/3000]  eta: 1:05:46  lr: 0.000028  loss: 0.5147  time: 1.5161  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 405/3000]  eta: 1:05:45  lr: 0.000028  loss: 0.6247  time: 1.5159  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 410/3000]  eta: 1:05:38  lr: 0.000028  loss: 1.0743  time: 1.5193  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 410/3000]  eta: 1:05:38  lr: 0.000028  loss: 0.1109  time: 1.5190  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 415/3000]  eta: 1:05:28  lr: 0.000028  loss: 0.2000  time: 1.4944  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 415/3000]  eta: 1:05:28  lr: 0.000028  loss: 0.2552  time: 1.4942  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 420/3000]  eta: 1:05:21  lr: 0.000028  loss: 0.7053  time: 1.5052  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 420/3000]  eta: 1:05:20  lr: 0.000028  loss: 0.4356  time: 1.5050  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 425/3000]  eta: 1:05:14  lr: 0.000028  loss: 0.6751  time: 1.5091  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 425/3000]  eta: 1:05:13  lr: 0.000028  loss: 0.1909  time: 1.5088  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 430/3000]  eta: 1:05:05  lr: 0.000028  loss: 0.6592  time: 1.4991  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 430/3000]  eta: 1:05:05  lr: 0.000028  loss: 0.5299  time: 1.4988  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 435/3000]  eta: 1:04:56  lr: 0.000028  loss: 0.2929  time: 1.5066  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 435/3000]  eta: 1:04:56  lr: 0.000028  loss: 0.4632  time: 1.5063  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 440/3000]  eta: 1:04:49  lr: 0.000028  loss: 0.1740  time: 1.5065  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 440/3000]  eta: 1:04:48  lr: 0.000028  loss: 0.5320  time: 1.5063  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 445/3000]  eta: 1:04:41  lr: 0.000028  loss: 0.2792  time: 1.4979  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 445/3000]  eta: 1:04:40  lr: 0.000028  loss: 0.2506  time: 1.4977  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 450/3000]  eta: 1:04:33  lr: 0.000028  loss: 0.0529  time: 1.5004  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 450/3000]  eta: 1:04:32  lr: 0.000028  loss: 0.5403  time: 1.5001  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 455/3000]  eta: 1:04:25  lr: 0.000028  loss: 0.3491  time: 1.5122  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 455/3000]  eta: 1:04:24  lr: 0.000028  loss: 0.1936  time: 1.5120  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 460/3000]  eta: 1:04:16  lr: 0.000028  loss: 0.6052  time: 1.5026  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 460/3000]  eta: 1:04:16  lr: 0.000028  loss: 0.1411  time: 1.5023  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 465/3000]  eta: 1:04:08  lr: 0.000028  loss: 0.2292  time: 1.4979  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 465/3000]  eta: 1:04:08  lr: 0.000028  loss: 0.2785  time: 1.4977  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 470/3000]  eta: 1:04:02  lr: 0.000028  loss: 0.2924  time: 1.5151  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 470/3000]  eta: 1:04:01  lr: 0.000028  loss: 0.4426  time: 1.5149  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 475/3000]  eta: 1:03:55  lr: 0.000028  loss: 0.4234  time: 1.5246  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 475/3000]  eta: 1:03:55  lr: 0.000028  loss: 0.2570  time: 1.5244  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 480/3000]  eta: 1:03:46  lr: 0.000028  loss: 0.4116  time: 1.5155  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 480/3000]  eta: 1:03:45  lr: 0.000028  loss: 0.1790  time: 1.5153  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 485/3000]  eta: 1:03:38  lr: 0.000028  loss: 0.6118  time: 1.5153  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 485/3000]  eta: 1:03:37  lr: 0.000028  loss: 0.7414  time: 1.5150  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 490/3000]  eta: 1:03:30  lr: 0.000028  loss: 0.4526  time: 1.5076  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 490/3000]  eta: 1:03:30  lr: 0.000028  loss: 0.2799  time: 1.5074  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 495/3000]  eta: 1:03:23  lr: 0.000028  loss: 0.4208  time: 1.4999  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 495/3000]  eta: 1:03:22  lr: 0.000028  loss: 0.7903  time: 1.4995  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 500/3000]  eta: 1:03:15  lr: 0.000028  loss: 0.4267  time: 1.5105  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 500/3000]  eta: 1:03:14  lr: 0.000028  loss: 0.3688  time: 1.5102  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 505/3000]  eta: 1:03:09  lr: 0.000028  loss: 0.6575  time: 1.5317  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 505/3000]  eta: 1:03:08  lr: 0.000028  loss: 0.6435  time: 1.5315  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 510/3000]  eta: 1:03:01  lr: 0.000028  loss: 0.9001  time: 1.5329  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 510/3000]  eta: 1:03:01  lr: 0.000028  loss: 0.1827  time: 1.5326  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 515/3000]  eta: 1:02:54  lr: 0.000028  loss: 1.2022  time: 1.5288  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 515/3000]  eta: 1:02:53  lr: 0.000028  loss: 0.5364  time: 1.5287  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 520/3000]  eta: 1:02:46  lr: 0.000028  loss: 0.2391  time: 1.5350  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 520/3000]  eta: 1:02:45  lr: 0.000028  loss: 0.1995  time: 1.5348  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 525/3000]  eta: 1:02:39  lr: 0.000028  loss: 0.5246  time: 1.5319  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 525/3000]  eta: 1:02:39  lr: 0.000028  loss: 0.1993  time: 1.5315  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 530/3000]  eta: 1:02:32  lr: 0.000028  loss: 0.1940  time: 1.5334  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 530/3000]  eta: 1:02:32  lr: 0.000028  loss: 0.4222  time: 1.5331  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 535/3000]  eta: 1:02:23  lr: 0.000028  loss: 0.2999  time: 1.5184  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 535/3000]  eta: 1:02:23  lr: 0.000028  loss: 0.3186  time: 1.5181  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 540/3000]  eta: 1:02:16  lr: 0.000028  loss: 0.5400  time: 1.5243  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 540/3000]  eta: 1:02:15  lr: 0.000028  loss: 0.0654  time: 1.5240  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 545/3000]  eta: 1:02:10  lr: 0.000028  loss: 0.5893  time: 1.5275  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 545/3000]  eta: 1:02:09  lr: 0.000028  loss: 0.3716  time: 1.5273  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 550/3000]  eta: 1:02:03  lr: 0.000028  loss: 1.0988  time: 1.5341  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 550/3000]  eta: 1:02:03  lr: 0.000028  loss: 0.4282  time: 1.5339  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 555/3000]  eta: 1:01:55  lr: 0.000028  loss: 0.7002  time: 1.5463  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 555/3000]  eta: 1:01:55  lr: 0.000028  loss: 0.2060  time: 1.5462  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 560/3000]  eta: 1:01:48  lr: 0.000028  loss: 0.5213  time: 1.5463  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 560/3000]  eta: 1:01:47  lr: 0.000028  loss: 0.2239  time: 1.5460  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 565/3000]  eta: 1:01:40  lr: 0.000028  loss: 0.4511  time: 1.5312  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 565/3000]  eta: 1:01:40  lr: 0.000028  loss: 0.5529  time: 1.5310  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 570/3000]  eta: 1:01:33  lr: 0.000028  loss: 0.4820  time: 1.5237  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 570/3000]  eta: 1:01:33  lr: 0.000028  loss: 0.1931  time: 1.5234  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 575/3000]  eta: 1:01:25  lr: 0.000028  loss: 0.4596  time: 1.5246  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 575/3000]  eta: 1:01:25  lr: 0.000028  loss: 0.3311  time: 1.5243  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 580/3000]  eta: 1:01:18  lr: 0.000028  loss: 0.5646  time: 1.5300  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 580/3000]  eta: 1:01:18  lr: 0.000028  loss: 0.5332  time: 1.5298  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 585/3000]  eta: 1:01:13  lr: 0.000028  loss: 0.1355  time: 1.5500  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 585/3000]  eta: 1:01:12  lr: 0.000028  loss: 0.1676  time: 1.5497  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 590/3000]  eta: 1:01:05  lr: 0.000028  loss: 1.4637  time: 1.5492  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 590/3000]  eta: 1:01:05  lr: 0.000028  loss: 0.4033  time: 1.5489  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 595/3000]  eta: 1:00:59  lr: 0.000028  loss: 0.3497  time: 1.5667  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 595/3000]  eta: 1:00:58  lr: 0.000028  loss: 0.7073  time: 1.5665  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 600/3000]  eta: 1:00:50  lr: 0.000028  loss: 0.4703  time: 1.5398  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 600/3000]  eta: 1:00:49  lr: 0.000028  loss: 0.3189  time: 1.5395  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 605/3000]  eta: 1:00:41  lr: 0.000028  loss: 0.4228  time: 1.5127  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 605/3000]  eta: 1:00:41  lr: 0.000028  loss: 0.2562  time: 1.5125  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 610/3000]  eta: 1:00:32  lr: 0.000028  loss: 0.2431  time: 1.4864  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 610/3000]  eta: 1:00:32  lr: 0.000028  loss: 0.1568  time: 1.4862  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 615/3000]  eta: 1:00:25  lr: 0.000028  loss: 0.4773  time: 1.4798  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 615/3000]  eta: 1:00:24  lr: 0.000028  loss: 0.2633  time: 1.4796  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 620/3000]  eta: 1:00:18  lr: 0.000028  loss: 0.3425  time: 1.5081  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 620/3000]  eta: 1:00:18  lr: 0.000028  loss: 0.0883  time: 1.5079  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 625/3000]  eta: 1:00:10  lr: 0.000028  loss: 0.2073  time: 1.5078  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 625/3000]  eta: 1:00:09  lr: 0.000028  loss: 0.3870  time: 1.5076  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 630/3000]  eta: 1:00:03  lr: 0.000028  loss: 0.4252  time: 1.5352  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 630/3000]  eta: 1:00:02  lr: 0.000028  loss: 0.2205  time: 1.5349  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 635/3000]  eta: 0:59:55  lr: 0.000028  loss: 0.2548  time: 1.5284  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 635/3000]  eta: 0:59:55  lr: 0.000028  loss: 0.2044  time: 1.5281  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 640/3000]  eta: 0:59:47  lr: 0.000028  loss: 0.4236  time: 1.5031  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 640/3000]  eta: 0:59:46  lr: 0.000028  loss: 0.1633  time: 1.5028  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 645/3000]  eta: 0:59:40  lr: 0.000028  loss: 0.7049  time: 1.5207  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 645/3000]  eta: 0:59:39  lr: 0.000028  loss: 0.0286  time: 1.5204  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 650/3000]  eta: 0:59:32  lr: 0.000028  loss: 0.6111  time: 1.5088  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 650/3000]  eta: 0:59:31  lr: 0.000028  loss: 1.1005  time: 1.5085  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 655/3000]  eta: 0:59:24  lr: 0.000028  loss: 0.1200  time: 1.5107  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 655/3000]  eta: 0:59:24  lr: 0.000028  loss: 0.2077  time: 1.5104  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 660/3000]  eta: 0:59:18  lr: 0.000028  loss: 0.2449  time: 1.5400  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 660/3000]  eta: 0:59:17  lr: 0.000028  loss: 0.2994  time: 1.5398  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 665/3000]  eta: 0:59:11  lr: 0.000028  loss: 0.5032  time: 1.5384  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 665/3000]  eta: 0:59:10  lr: 0.000028  loss: 0.5068  time: 1.5382  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 670/3000]  eta: 0:59:03  lr: 0.000028  loss: 0.9282  time: 1.5512  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 670/3000]  eta: 0:59:03  lr: 0.000028  loss: 0.4436  time: 1.5511  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 675/3000]  eta: 0:58:55  lr: 0.000028  loss: 0.7667  time: 1.5387  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 675/3000]  eta: 0:58:54  lr: 0.000028  loss: 0.5109  time: 1.5383  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 680/3000]  eta: 0:58:46  lr: 0.000028  loss: 0.7086  time: 1.5065  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 680/3000]  eta: 0:58:46  lr: 0.000028  loss: 0.3523  time: 1.5061  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 685/3000]  eta: 0:58:40  lr: 0.000028  loss: 0.1606  time: 1.5141  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 685/3000]  eta: 0:58:39  lr: 0.000028  loss: 0.3002  time: 1.5137  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 690/3000]  eta: 0:58:32  lr: 0.000028  loss: 0.4247  time: 1.5000  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 690/3000]  eta: 0:58:31  lr: 0.000028  loss: 0.5693  time: 1.4995  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 695/3000]  eta: 0:58:24  lr: 0.000028  loss: 0.6607  time: 1.5144  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 695/3000]  eta: 0:58:24  lr: 0.000028  loss: 0.4736  time: 1.5142  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 700/3000]  eta: 0:58:16  lr: 0.000028  loss: 0.4641  time: 1.5234  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 700/3000]  eta: 0:58:15  lr: 0.000028  loss: 0.4209  time: 1.5232  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 705/3000]  eta: 0:58:09  lr: 0.000028  loss: 0.5992  time: 1.5233  data: 0.0000  max mem: 18151Train: data epoch: [6]  [ 705/3000]  eta: 0:58:09  lr: 0.000028  loss: 0.3689  time: 1.5237  data: 0.0000  max mem: 18432

Train: data epoch: [6]  [ 710/3000]  eta: 0:58:02  lr: 0.000028  loss: 1.0872  time: 1.5301  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 710/3000]  eta: 0:58:01  lr: 0.000028  loss: 0.2958  time: 1.5299  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 715/3000]  eta: 0:57:55  lr: 0.000028  loss: 0.7020  time: 1.5337  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 715/3000]  eta: 0:57:54  lr: 0.000028  loss: 0.2427  time: 1.5334  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 720/3000]  eta: 0:57:47  lr: 0.000028  loss: 0.8398  time: 1.5402  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 720/3000]  eta: 0:57:46  lr: 0.000028  loss: 0.2079  time: 1.5400  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 725/3000]  eta: 0:57:39  lr: 0.000028  loss: 0.9583  time: 1.5253  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 725/3000]  eta: 0:57:39  lr: 0.000028  loss: 0.2873  time: 1.5251  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 730/3000]  eta: 0:57:32  lr: 0.000028  loss: 0.5767  time: 1.5255  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 730/3000]  eta: 0:57:31  lr: 0.000028  loss: 0.3034  time: 1.5252  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 735/3000]  eta: 0:57:24  lr: 0.000028  loss: 0.6982  time: 1.5185  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 735/3000]  eta: 0:57:23  lr: 0.000028  loss: 0.7876  time: 1.5183  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 740/3000]  eta: 0:57:16  lr: 0.000028  loss: 0.3948  time: 1.5211  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 740/3000]  eta: 0:57:16  lr: 0.000028  loss: 0.3613  time: 1.5209  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 745/3000]  eta: 0:57:08  lr: 0.000028  loss: 0.1427  time: 1.4980  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 745/3000]  eta: 0:57:07  lr: 0.000028  loss: 0.4055  time: 1.4978  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 750/3000]  eta: 0:57:01  lr: 0.000028  loss: 0.4646  time: 1.5087  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 750/3000]  eta: 0:57:00  lr: 0.000028  loss: 0.4813  time: 1.5084  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 755/3000]  eta: 0:56:52  lr: 0.000028  loss: 0.6997  time: 1.4900  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 755/3000]  eta: 0:56:51  lr: 0.000028  loss: 0.3828  time: 1.4898  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 760/3000]  eta: 0:56:44  lr: 0.000028  loss: 0.3589  time: 1.4940  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 760/3000]  eta: 0:56:44  lr: 0.000028  loss: 0.4392  time: 1.4936  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 765/3000]  eta: 0:56:38  lr: 0.000028  loss: 0.3259  time: 1.5316  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 765/3000]  eta: 0:56:37  lr: 0.000028  loss: 0.6453  time: 1.5313  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 770/3000]  eta: 0:56:29  lr: 0.000028  loss: 0.4582  time: 1.5053  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 770/3000]  eta: 0:56:29  lr: 0.000028  loss: 1.0083  time: 1.5050  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 775/3000]  eta: 0:56:22  lr: 0.000028  loss: 0.6277  time: 1.5250  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 775/3000]  eta: 0:56:21  lr: 0.000028  loss: 0.3272  time: 1.5248  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 780/3000]  eta: 0:56:14  lr: 0.000028  loss: 0.6421  time: 1.5162  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 780/3000]  eta: 0:56:13  lr: 0.000028  loss: 0.4734  time: 1.5159  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 785/3000]  eta: 0:56:07  lr: 0.000028  loss: 0.4033  time: 1.5061  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 785/3000]  eta: 0:56:06  lr: 0.000028  loss: 0.4229  time: 1.5059  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 790/3000]  eta: 0:55:59  lr: 0.000028  loss: 0.2276  time: 1.5182  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 790/3000]  eta: 0:55:58  lr: 0.000028  loss: 0.2165  time: 1.5181  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 795/3000]  eta: 0:55:52  lr: 0.000028  loss: 0.4341  time: 1.5245  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 795/3000]  eta: 0:55:51  lr: 0.000028  loss: 0.2055  time: 1.5243  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 800/3000]  eta: 0:55:45  lr: 0.000028  loss: 0.1994  time: 1.5413  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 800/3000]  eta: 0:55:44  lr: 0.000028  loss: 0.1724  time: 1.5411  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 805/3000]  eta: 0:55:38  lr: 0.000028  loss: 0.2225  time: 1.5557  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 805/3000]  eta: 0:55:38  lr: 0.000028  loss: 0.1822  time: 1.5555  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 810/3000]  eta: 0:55:31  lr: 0.000028  loss: 0.5818  time: 1.5659  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 810/3000]  eta: 0:55:30  lr: 0.000028  loss: 0.3534  time: 1.5656  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 815/3000]  eta: 0:55:23  lr: 0.000028  loss: 0.6969  time: 1.5619  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 815/3000]  eta: 0:55:23  lr: 0.000028  loss: 0.4280  time: 1.5616  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 820/3000]  eta: 0:55:16  lr: 0.000028  loss: 0.5477  time: 1.5522  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 820/3000]  eta: 0:55:15  lr: 0.000028  loss: 0.2800  time: 1.5519  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 825/3000]  eta: 0:55:09  lr: 0.000028  loss: 0.7834  time: 1.5437  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 825/3000]  eta: 0:55:08  lr: 0.000028  loss: 0.5073  time: 1.5434  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 830/3000]  eta: 0:55:01  lr: 0.000028  loss: 0.3610  time: 1.5364  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 830/3000]  eta: 0:55:01  lr: 0.000028  loss: 0.1799  time: 1.5361  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 835/3000]  eta: 0:54:53  lr: 0.000028  loss: 0.4200  time: 1.5207  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 835/3000]  eta: 0:54:52  lr: 0.000028  loss: 0.1905  time: 1.5204  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 840/3000]  eta: 0:54:46  lr: 0.000028  loss: 0.3972  time: 1.5295  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 840/3000]  eta: 0:54:45  lr: 0.000028  loss: 0.2620  time: 1.5292  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 845/3000]  eta: 0:54:39  lr: 0.000028  loss: 0.6437  time: 1.5242  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 845/3000]  eta: 0:54:38  lr: 0.000028  loss: 0.2862  time: 1.5240  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 850/3000]  eta: 0:54:31  lr: 0.000028  loss: 0.2129  time: 1.5223  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 850/3000]  eta: 0:54:30  lr: 0.000028  loss: 0.2342  time: 1.5220  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 855/3000]  eta: 0:54:23  lr: 0.000028  loss: 0.2703  time: 1.5349  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 855/3000]  eta: 0:54:23  lr: 0.000028  loss: 0.5842  time: 1.5347  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 860/3000]  eta: 0:54:16  lr: 0.000028  loss: 0.7104  time: 1.5251  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 860/3000]  eta: 0:54:15  lr: 0.000028  loss: 0.4729  time: 1.5249  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 865/3000]  eta: 0:54:08  lr: 0.000028  loss: 0.4705  time: 1.5217  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 865/3000]  eta: 0:54:08  lr: 0.000028  loss: 0.2392  time: 1.5214  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 870/3000]  eta: 0:54:01  lr: 0.000028  loss: 0.8744  time: 1.5251  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 870/3000]  eta: 0:54:00  lr: 0.000028  loss: 0.3544  time: 1.5248  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 875/3000]  eta: 0:53:52  lr: 0.000028  loss: 0.2702  time: 1.5083  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 875/3000]  eta: 0:53:52  lr: 0.000028  loss: 0.8283  time: 1.5071  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 880/3000]  eta: 0:53:44  lr: 0.000028  loss: 0.1427  time: 1.4998  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 880/3000]  eta: 0:53:44  lr: 0.000028  loss: 0.1554  time: 1.5011  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 885/3000]  eta: 0:53:36  lr: 0.000028  loss: 0.4674  time: 1.4936  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 885/3000]  eta: 0:53:36  lr: 0.000028  loss: 0.2191  time: 1.4925  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 890/3000]  eta: 0:53:28  lr: 0.000028  loss: 0.1504  time: 1.4858  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 890/3000]  eta: 0:53:28  lr: 0.000028  loss: 0.6020  time: 1.4847  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 895/3000]  eta: 0:53:20  lr: 0.000028  loss: 0.2158  time: 1.4783  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 895/3000]  eta: 0:53:19  lr: 0.000028  loss: 0.6197  time: 1.4780  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 900/3000]  eta: 0:53:12  lr: 0.000028  loss: 0.4450  time: 1.4841  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 900/3000]  eta: 0:53:12  lr: 0.000028  loss: 0.7536  time: 1.4839  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 905/3000]  eta: 0:53:05  lr: 0.000028  loss: 0.4138  time: 1.5003  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 905/3000]  eta: 0:53:04  lr: 0.000028  loss: 0.4676  time: 1.5001  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 910/3000]  eta: 0:52:57  lr: 0.000028  loss: 0.3135  time: 1.5072  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 910/3000]  eta: 0:52:57  lr: 0.000028  loss: 0.5767  time: 1.5070  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 915/3000]  eta: 0:52:49  lr: 0.000028  loss: 0.5099  time: 1.5205  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 915/3000]  eta: 0:52:49  lr: 0.000028  loss: 0.4247  time: 1.5203  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 920/3000]  eta: 0:52:42  lr: 0.000028  loss: 0.2086  time: 1.5244  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 920/3000]  eta: 0:52:41  lr: 0.000028  loss: 0.9611  time: 1.5242  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 925/3000]  eta: 0:52:34  lr: 0.000028  loss: 0.5576  time: 1.5048  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 925/3000]  eta: 0:52:33  lr: 0.000028  loss: 0.3147  time: 1.5045  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 930/3000]  eta: 0:52:26  lr: 0.000028  loss: 0.2383  time: 1.5032  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 930/3000]  eta: 0:52:26  lr: 0.000028  loss: 0.5280  time: 1.5029  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 935/3000]  eta: 0:52:19  lr: 0.000028  loss: 0.4969  time: 1.5232  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 935/3000]  eta: 0:52:18  lr: 0.000028  loss: 0.2094  time: 1.5230  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 940/3000]  eta: 0:52:11  lr: 0.000028  loss: 0.1468  time: 1.5121  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 940/3000]  eta: 0:52:11  lr: 0.000028  loss: 0.3464  time: 1.5118  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 945/3000]  eta: 0:52:04  lr: 0.000028  loss: 0.2484  time: 1.5344  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 945/3000]  eta: 0:52:04  lr: 0.000028  loss: 1.2347  time: 1.5341  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 950/3000]  eta: 0:51:57  lr: 0.000028  loss: 0.4261  time: 1.5396  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 950/3000]  eta: 0:51:56  lr: 0.000028  loss: 0.5054  time: 1.5394  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 955/3000]  eta: 0:51:49  lr: 0.000028  loss: 0.5087  time: 1.5298  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 955/3000]  eta: 0:51:48  lr: 0.000028  loss: 0.2676  time: 1.5295  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 960/3000]  eta: 0:51:41  lr: 0.000028  loss: 0.9060  time: 1.5245  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 960/3000]  eta: 0:51:40  lr: 0.000028  loss: 0.5109  time: 1.5243  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 965/3000]  eta: 0:51:33  lr: 0.000028  loss: 1.3593  time: 1.5102  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 965/3000]  eta: 0:51:33  lr: 0.000028  loss: 0.2853  time: 1.5099  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 970/3000]  eta: 0:51:26  lr: 0.000028  loss: 0.4421  time: 1.5196  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 970/3000]  eta: 0:51:26  lr: 0.000028  loss: 0.2714  time: 1.5194  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 975/3000]  eta: 0:51:18  lr: 0.000028  loss: 0.6059  time: 1.5180  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 975/3000]  eta: 0:51:18  lr: 0.000028  loss: 0.2466  time: 1.5177  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 980/3000]  eta: 0:51:11  lr: 0.000028  loss: 0.7484  time: 1.5351  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 980/3000]  eta: 0:51:11  lr: 0.000028  loss: 0.2894  time: 1.5350  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 985/3000]  eta: 0:51:04  lr: 0.000028  loss: 0.3100  time: 1.5421  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 985/3000]  eta: 0:51:03  lr: 0.000028  loss: 1.0165  time: 1.5419  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 990/3000]  eta: 0:50:55  lr: 0.000028  loss: 0.1866  time: 1.4933  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 990/3000]  eta: 0:50:54  lr: 0.000028  loss: 0.0704  time: 1.4931  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [ 995/3000]  eta: 0:50:47  lr: 0.000028  loss: 0.5001  time: 1.4894  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [ 995/3000]  eta: 0:50:46  lr: 0.000028  loss: 0.4364  time: 1.4892  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1000/3000]  eta: 0:50:39  lr: 0.000028  loss: 0.3307  time: 1.4690  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1000/3000]  eta: 0:50:38  lr: 0.000028  loss: 0.7855  time: 1.4688  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1005/3000]  eta: 0:50:32  lr: 0.000028  loss: 0.2875  time: 1.4814  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1005/3000]  eta: 0:50:31  lr: 0.000028  loss: 0.6675  time: 1.4812  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1010/3000]  eta: 0:50:25  lr: 0.000028  loss: 0.2414  time: 1.5269  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1010/3000]  eta: 0:50:24  lr: 0.000028  loss: 0.2072  time: 1.5266  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1015/3000]  eta: 0:50:17  lr: 0.000028  loss: 0.2730  time: 1.5276  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1015/3000]  eta: 0:50:16  lr: 0.000028  loss: 0.3986  time: 1.5274  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1020/3000]  eta: 0:50:09  lr: 0.000028  loss: 0.5463  time: 1.5426  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1020/3000]  eta: 0:50:09  lr: 0.000028  loss: 0.2020  time: 1.5423  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1025/3000]  eta: 0:50:02  lr: 0.000028  loss: 0.4156  time: 1.5250  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1025/3000]  eta: 0:50:01  lr: 0.000028  loss: 0.4755  time: 1.5247  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1030/3000]  eta: 0:49:54  lr: 0.000028  loss: 0.3003  time: 1.5129  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1030/3000]  eta: 0:49:53  lr: 0.000028  loss: 1.1001  time: 1.5126  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1035/3000]  eta: 0:49:46  lr: 0.000028  loss: 1.1990  time: 1.5200  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1035/3000]  eta: 0:49:46  lr: 0.000028  loss: 0.3110  time: 1.5197  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1040/3000]  eta: 0:49:39  lr: 0.000028  loss: 1.0268  time: 1.5226  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1040/3000]  eta: 0:49:38  lr: 0.000028  loss: 0.5072  time: 1.5224  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1045/3000]  eta: 0:49:32  lr: 0.000028  loss: 0.2153  time: 1.5346  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1045/3000]  eta: 0:49:31  lr: 0.000028  loss: 0.1389  time: 1.5344  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1050/3000]  eta: 0:49:24  lr: 0.000028  loss: 0.1963  time: 1.5439  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1050/3000]  eta: 0:49:24  lr: 0.000028  loss: 0.3250  time: 1.5437  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1055/3000]  eta: 0:49:17  lr: 0.000028  loss: 0.2399  time: 1.5382  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1055/3000]  eta: 0:49:16  lr: 0.000028  loss: 0.3618  time: 1.5380  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1060/3000]  eta: 0:49:09  lr: 0.000028  loss: 0.5759  time: 1.5474  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1060/3000]  eta: 0:49:09  lr: 0.000028  loss: 0.4374  time: 1.5472  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1065/3000]  eta: 0:49:02  lr: 0.000028  loss: 0.0849  time: 1.5509  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1065/3000]  eta: 0:49:02  lr: 0.000028  loss: 0.4432  time: 1.5508  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1070/3000]  eta: 0:48:55  lr: 0.000028  loss: 0.6262  time: 1.5510  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1070/3000]  eta: 0:48:55  lr: 0.000028  loss: 0.3394  time: 1.5506  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1075/3000]  eta: 0:48:47  lr: 0.000028  loss: 0.8330  time: 1.5567  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1075/3000]  eta: 0:48:47  lr: 0.000028  loss: 0.4561  time: 1.5564  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1080/3000]  eta: 0:48:39  lr: 0.000028  loss: 0.5087  time: 1.5231  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1080/3000]  eta: 0:48:39  lr: 0.000028  loss: 0.6265  time: 1.5228  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1085/3000]  eta: 0:48:32  lr: 0.000028  loss: 0.4863  time: 1.5198  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1085/3000]  eta: 0:48:31  lr: 0.000028  loss: 0.4926  time: 1.5195  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1090/3000]  eta: 0:48:24  lr: 0.000028  loss: 0.2874  time: 1.5071  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1090/3000]  eta: 0:48:24  lr: 0.000028  loss: 0.1406  time: 1.5069  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1095/3000]  eta: 0:48:17  lr: 0.000028  loss: 0.3002  time: 1.5110  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1095/3000]  eta: 0:48:16  lr: 0.000028  loss: 0.8804  time: 1.5107  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1100/3000]  eta: 0:48:09  lr: 0.000028  loss: 0.6905  time: 1.5346  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1100/3000]  eta: 0:48:09  lr: 0.000028  loss: 0.3934  time: 1.5343  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1105/3000]  eta: 0:48:01  lr: 0.000028  loss: 0.2851  time: 1.5021  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1105/3000]  eta: 0:48:00  lr: 0.000028  loss: 0.2276  time: 1.5019  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1110/3000]  eta: 0:47:53  lr: 0.000028  loss: 0.1315  time: 1.5087  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1110/3000]  eta: 0:47:53  lr: 0.000028  loss: 0.4010  time: 1.5084  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1115/3000]  eta: 0:47:46  lr: 0.000028  loss: 0.2951  time: 1.5209  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1115/3000]  eta: 0:47:46  lr: 0.000028  loss: 0.4720  time: 1.5207  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1120/3000]  eta: 0:47:38  lr: 0.000028  loss: 0.7415  time: 1.5135  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1120/3000]  eta: 0:47:38  lr: 0.000028  loss: 1.2757  time: 1.5133  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1125/3000]  eta: 0:47:31  lr: 0.000028  loss: 0.1061  time: 1.5332  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1125/3000]  eta: 0:47:30  lr: 0.000028  loss: 0.3141  time: 1.5329  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1130/3000]  eta: 0:47:24  lr: 0.000028  loss: 0.5152  time: 1.5521  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1130/3000]  eta: 0:47:23  lr: 0.000028  loss: 0.6992  time: 1.5519  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1135/3000]  eta: 0:47:16  lr: 0.000028  loss: 0.5724  time: 1.5376  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1135/3000]  eta: 0:47:16  lr: 0.000028  loss: 0.1948  time: 1.5374  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1140/3000]  eta: 0:47:09  lr: 0.000028  loss: 0.4267  time: 1.5342  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1140/3000]  eta: 0:47:08  lr: 0.000028  loss: 0.2541  time: 1.5338  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1145/3000]  eta: 0:47:01  lr: 0.000028  loss: 0.2024  time: 1.5253  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1145/3000]  eta: 0:47:00  lr: 0.000028  loss: 0.3500  time: 1.5250  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1150/3000]  eta: 0:46:53  lr: 0.000028  loss: 0.1245  time: 1.5135  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1150/3000]  eta: 0:46:53  lr: 0.000028  loss: 0.1279  time: 1.5130  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1155/3000]  eta: 0:46:46  lr: 0.000028  loss: 0.2692  time: 1.5183  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1155/3000]  eta: 0:46:45  lr: 0.000028  loss: 1.0979  time: 1.5179  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1160/3000]  eta: 0:46:38  lr: 0.000028  loss: 0.3579  time: 1.5310  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1160/3000]  eta: 0:46:38  lr: 0.000028  loss: 0.6706  time: 1.5308  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1165/3000]  eta: 0:46:31  lr: 0.000028  loss: 0.2743  time: 1.5389  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1165/3000]  eta: 0:46:30  lr: 0.000028  loss: 0.7530  time: 1.5386  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1170/3000]  eta: 0:46:23  lr: 0.000028  loss: 0.3257  time: 1.5141  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1170/3000]  eta: 0:46:22  lr: 0.000028  loss: 0.9300  time: 1.5138  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1175/3000]  eta: 0:46:15  lr: 0.000028  loss: 0.8065  time: 1.5025  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1175/3000]  eta: 0:46:14  lr: 0.000028  loss: 0.4154  time: 1.5022  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1180/3000]  eta: 0:46:07  lr: 0.000028  loss: 0.3417  time: 1.4926  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1180/3000]  eta: 0:46:07  lr: 0.000028  loss: 0.3050  time: 1.4923  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1185/3000]  eta: 0:46:00  lr: 0.000028  loss: 0.3506  time: 1.5004  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1185/3000]  eta: 0:45:59  lr: 0.000028  loss: 0.4068  time: 1.5002  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1190/3000]  eta: 0:45:52  lr: 0.000028  loss: 0.3410  time: 1.5093  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1190/3000]  eta: 0:45:51  lr: 0.000028  loss: 0.4644  time: 1.5091  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1195/3000]  eta: 0:45:45  lr: 0.000028  loss: 0.4134  time: 1.5231  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1195/3000]  eta: 0:45:44  lr: 0.000028  loss: 0.6929  time: 1.5230  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1200/3000]  eta: 0:45:36  lr: 0.000028  loss: 0.4757  time: 1.5086  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1200/3000]  eta: 0:45:36  lr: 0.000028  loss: 0.3908  time: 1.5084  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1205/3000]  eta: 0:45:28  lr: 0.000028  loss: 0.5804  time: 1.4861  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1205/3000]  eta: 0:45:28  lr: 0.000028  loss: 0.5425  time: 1.4859  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1210/3000]  eta: 0:45:21  lr: 0.000028  loss: 0.4738  time: 1.5130  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1210/3000]  eta: 0:45:21  lr: 0.000028  loss: 0.6213  time: 1.5128  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1215/3000]  eta: 0:45:14  lr: 0.000028  loss: 0.1482  time: 1.5076  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1215/3000]  eta: 0:45:13  lr: 0.000028  loss: 0.5233  time: 1.5074  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1220/3000]  eta: 0:45:06  lr: 0.000028  loss: 0.5264  time: 1.5199  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1220/3000]  eta: 0:45:05  lr: 0.000028  loss: 1.1587  time: 1.5197  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1225/3000]  eta: 0:44:58  lr: 0.000028  loss: 0.3555  time: 1.5269  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1225/3000]  eta: 0:44:58  lr: 0.000028  loss: 0.1022  time: 1.5267  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1230/3000]  eta: 0:44:51  lr: 0.000028  loss: 1.0213  time: 1.5149  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1230/3000]  eta: 0:44:50  lr: 0.000028  loss: 0.7483  time: 1.5147  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1235/3000]  eta: 0:44:43  lr: 0.000028  loss: 0.2814  time: 1.5182  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1235/3000]  eta: 0:44:43  lr: 0.000028  loss: 0.4964  time: 1.5180  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1240/3000]  eta: 0:44:35  lr: 0.000028  loss: 0.2622  time: 1.5125  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1240/3000]  eta: 0:44:35  lr: 0.000028  loss: 0.3185  time: 1.5123  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1245/3000]  eta: 0:44:27  lr: 0.000028  loss: 0.6724  time: 1.4946  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1245/3000]  eta: 0:44:26  lr: 0.000028  loss: 0.4107  time: 1.4944  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1250/3000]  eta: 0:44:20  lr: 0.000028  loss: 0.6121  time: 1.5069  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1250/3000]  eta: 0:44:19  lr: 0.000028  loss: 0.1266  time: 1.5066  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1255/3000]  eta: 0:44:12  lr: 0.000028  loss: 0.4572  time: 1.4946  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1255/3000]  eta: 0:44:12  lr: 0.000028  loss: 0.6158  time: 1.4943  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1260/3000]  eta: 0:44:04  lr: 0.000028  loss: 0.4694  time: 1.4920  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1260/3000]  eta: 0:44:04  lr: 0.000028  loss: 0.6171  time: 1.4918  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1265/3000]  eta: 0:43:56  lr: 0.000028  loss: 0.1669  time: 1.5023  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1265/3000]  eta: 0:43:56  lr: 0.000028  loss: 0.4528  time: 1.5020  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1270/3000]  eta: 0:43:49  lr: 0.000028  loss: 0.2557  time: 1.5054  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1270/3000]  eta: 0:43:49  lr: 0.000028  loss: 0.1611  time: 1.5052  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1275/3000]  eta: 0:43:42  lr: 0.000028  loss: 0.3731  time: 1.5156  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1275/3000]  eta: 0:43:41  lr: 0.000028  loss: 1.0724  time: 1.5153  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1280/3000]  eta: 0:43:34  lr: 0.000028  loss: 0.7714  time: 1.5225  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1280/3000]  eta: 0:43:33  lr: 0.000028  loss: 0.2838  time: 1.5222  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1285/3000]  eta: 0:43:26  lr: 0.000028  loss: 1.0531  time: 1.5253  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1285/3000]  eta: 0:43:25  lr: 0.000028  loss: 0.2187  time: 1.5251  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1290/3000]  eta: 0:43:18  lr: 0.000028  loss: 0.3048  time: 1.4916  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1290/3000]  eta: 0:43:17  lr: 0.000028  loss: 1.0504  time: 1.4913  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1295/3000]  eta: 0:43:10  lr: 0.000028  loss: 0.5560  time: 1.4853  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1295/3000]  eta: 0:43:10  lr: 0.000028  loss: 0.2128  time: 1.4849  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1300/3000]  eta: 0:43:02  lr: 0.000028  loss: 0.5245  time: 1.4844  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1300/3000]  eta: 0:43:02  lr: 0.000028  loss: 0.2074  time: 1.4841  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1305/3000]  eta: 0:42:55  lr: 0.000028  loss: 0.5645  time: 1.5135  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1305/3000]  eta: 0:42:55  lr: 0.000028  loss: 0.3178  time: 1.5130  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1310/3000]  eta: 0:42:48  lr: 0.000028  loss: 0.5254  time: 1.5284  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1310/3000]  eta: 0:42:47  lr: 0.000028  loss: 0.1888  time: 1.5280  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1315/3000]  eta: 0:42:40  lr: 0.000028  loss: 0.7745  time: 1.5281  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1315/3000]  eta: 0:42:40  lr: 0.000028  loss: 0.4865  time: 1.5279  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1320/3000]  eta: 0:42:33  lr: 0.000028  loss: 0.4884  time: 1.5554  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1320/3000]  eta: 0:42:33  lr: 0.000028  loss: 0.1982  time: 1.5551  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1325/3000]  eta: 0:42:26  lr: 0.000028  loss: 1.1285  time: 1.5452  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1325/3000]  eta: 0:42:25  lr: 0.000028  loss: 0.5209  time: 1.5450  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1330/3000]  eta: 0:42:18  lr: 0.000028  loss: 0.6168  time: 1.5546  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1330/3000]  eta: 0:42:18  lr: 0.000028  loss: 0.5275  time: 1.5544  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1335/3000]  eta: 0:42:10  lr: 0.000028  loss: 0.6340  time: 1.5452  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1335/3000]  eta: 0:42:10  lr: 0.000028  loss: 0.2115  time: 1.5450  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1340/3000]  eta: 0:42:03  lr: 0.000028  loss: 0.7538  time: 1.5295  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1340/3000]  eta: 0:42:02  lr: 0.000028  loss: 0.1732  time: 1.5293  data: 0.0000  max mem: 18151
Train: data epoch: [6]  [1345/3000]  eta: 0:41:55  lr: 0.000028  loss: 0.3910  time: 1.5248  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1345/3000]  eta: 0:41:55  lr: 0.000028  loss: 0.6607  time: 1.5246  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1350/3000]  eta: 0:41:47  lr: 0.000028  loss: 0.3655  time: 1.4997  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1350/3000]  eta: 0:41:47  lr: 0.000028  loss: 0.5357  time: 1.4993  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1355/3000]  eta: 0:41:39  lr: 0.000028  loss: 0.1308  time: 1.4900  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1355/3000]  eta: 0:41:39  lr: 0.000028  loss: 0.3178  time: 1.4897  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1360/3000]  eta: 0:41:31  lr: 0.000028  loss: 0.5852  time: 1.4737  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1360/3000]  eta: 0:41:31  lr: 0.000028  loss: 0.3417  time: 1.4734  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1365/3000]  eta: 0:41:23  lr: 0.000028  loss: 0.1979  time: 1.4580  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1365/3000]  eta: 0:41:23  lr: 0.000028  loss: 0.4058  time: 1.4577  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1370/3000]  eta: 0:41:16  lr: 0.000028  loss: 0.2153  time: 1.4825  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1370/3000]  eta: 0:41:16  lr: 0.000028  loss: 0.3855  time: 1.4823  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1375/3000]  eta: 0:41:08  lr: 0.000028  loss: 0.3801  time: 1.4939  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1375/3000]  eta: 0:41:08  lr: 0.000028  loss: 0.3523  time: 1.4936  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1380/3000]  eta: 0:41:00  lr: 0.000028  loss: 0.3058  time: 1.4974  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1380/3000]  eta: 0:41:00  lr: 0.000028  loss: 0.2368  time: 1.4971  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1385/3000]  eta: 0:40:53  lr: 0.000028  loss: 0.1111  time: 1.5157  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1385/3000]  eta: 0:40:52  lr: 0.000028  loss: 0.6748  time: 1.5154  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1390/3000]  eta: 0:40:45  lr: 0.000028  loss: 0.9699  time: 1.4738  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1390/3000]  eta: 0:40:44  lr: 0.000028  loss: 0.2195  time: 1.4736  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1395/3000]  eta: 0:40:37  lr: 0.000028  loss: 0.5038  time: 1.4882  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1395/3000]  eta: 0:40:37  lr: 0.000028  loss: 0.6920  time: 1.4880  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1400/3000]  eta: 0:40:30  lr: 0.000028  loss: 0.3089  time: 1.5101  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1400/3000]  eta: 0:40:29  lr: 0.000028  loss: 0.7843  time: 1.5099  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1405/3000]  eta: 0:40:22  lr: 0.000028  loss: 0.4035  time: 1.5166  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1405/3000]  eta: 0:40:22  lr: 0.000028  loss: 0.6647  time: 1.5164  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1410/3000]  eta: 0:40:14  lr: 0.000028  loss: 0.1257  time: 1.5254  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1410/3000]  eta: 0:40:14  lr: 0.000028  loss: 0.9770  time: 1.5251  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1415/3000]  eta: 0:40:07  lr: 0.000028  loss: 0.7801  time: 1.5160  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1415/3000]  eta: 0:40:06  lr: 0.000028  loss: 0.3431  time: 1.5158  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1420/3000]  eta: 0:39:59  lr: 0.000028  loss: 0.5339  time: 1.4886  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1420/3000]  eta: 0:39:58  lr: 0.000028  loss: 0.2494  time: 1.4884  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1425/3000]  eta: 0:39:51  lr: 0.000028  loss: 0.6001  time: 1.4708  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1425/3000]  eta: 0:39:51  lr: 0.000028  loss: 0.5603  time: 1.4706  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1430/3000]  eta: 0:39:44  lr: 0.000028  loss: 0.1193  time: 1.5101  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1430/3000]  eta: 0:39:43  lr: 0.000028  loss: 0.2628  time: 1.5099  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1435/3000]  eta: 0:39:36  lr: 0.000028  loss: 0.1913  time: 1.5183  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1435/3000]  eta: 0:39:36  lr: 0.000028  loss: 0.4313  time: 1.5181  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1440/3000]  eta: 0:39:29  lr: 0.000028  loss: 0.2316  time: 1.5344  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1440/3000]  eta: 0:39:28  lr: 0.000028  loss: 0.4765  time: 1.5342  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1445/3000]  eta: 0:39:21  lr: 0.000028  loss: 0.1757  time: 1.5412  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1445/3000]  eta: 0:39:21  lr: 0.000028  loss: 0.4973  time: 1.5410  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1450/3000]  eta: 0:39:13  lr: 0.000028  loss: 0.2715  time: 1.4969  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1450/3000]  eta: 0:39:13  lr: 0.000028  loss: 0.1375  time: 1.4966  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1455/3000]  eta: 0:39:05  lr: 0.000028  loss: 0.5178  time: 1.4874  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1455/3000]  eta: 0:39:05  lr: 0.000028  loss: 0.7965  time: 1.4871  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1460/3000]  eta: 0:38:58  lr: 0.000028  loss: 0.2137  time: 1.4857  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1460/3000]  eta: 0:38:57  lr: 0.000028  loss: 1.0473  time: 1.4856  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1465/3000]  eta: 0:38:50  lr: 0.000028  loss: 0.0794  time: 1.4855  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1465/3000]  eta: 0:38:50  lr: 0.000028  loss: 0.2284  time: 1.4853  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1470/3000]  eta: 0:38:43  lr: 0.000028  loss: 0.7226  time: 1.5275  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1470/3000]  eta: 0:38:42  lr: 0.000028  loss: 0.5372  time: 1.5273  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1475/3000]  eta: 0:38:36  lr: 0.000028  loss: 0.3546  time: 1.5471  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1475/3000]  eta: 0:38:35  lr: 0.000028  loss: 0.3620  time: 1.5469  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1480/3000]  eta: 0:38:28  lr: 0.000028  loss: 0.1594  time: 1.5377  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1480/3000]  eta: 0:38:27  lr: 0.000028  loss: 0.1712  time: 1.5375  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1485/3000]  eta: 0:38:20  lr: 0.000028  loss: 0.9086  time: 1.5410  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1485/3000]  eta: 0:38:20  lr: 0.000028  loss: 0.4781  time: 1.5407  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1490/3000]  eta: 0:38:13  lr: 0.000028  loss: 0.2978  time: 1.5382  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1490/3000]  eta: 0:38:12  lr: 0.000028  loss: 0.4733  time: 1.5380  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1495/3000]  eta: 0:38:05  lr: 0.000028  loss: 0.8289  time: 1.5271  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1495/3000]  eta: 0:38:05  lr: 0.000028  loss: 0.6193  time: 1.5269  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1500/3000]  eta: 0:37:58  lr: 0.000028  loss: 0.3633  time: 1.5290  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1500/3000]  eta: 0:37:57  lr: 0.000028  loss: 0.8206  time: 1.5288  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1505/3000]  eta: 0:37:50  lr: 0.000028  loss: 0.5090  time: 1.5163  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1505/3000]  eta: 0:37:49  lr: 0.000028  loss: 0.0723  time: 1.5161  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1510/3000]  eta: 0:37:42  lr: 0.000028  loss: 0.2792  time: 1.4974  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1510/3000]  eta: 0:37:42  lr: 0.000028  loss: 0.6201  time: 1.4972  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1515/3000]  eta: 0:37:34  lr: 0.000028  loss: 0.3066  time: 1.4943  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1515/3000]  eta: 0:37:34  lr: 0.000028  loss: 0.4728  time: 1.4940  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1520/3000]  eta: 0:37:27  lr: 0.000028  loss: 0.4083  time: 1.5169  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1520/3000]  eta: 0:37:27  lr: 0.000028  loss: 0.3480  time: 1.5166  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1525/3000]  eta: 0:37:20  lr: 0.000028  loss: 0.1935  time: 1.5340  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1525/3000]  eta: 0:37:19  lr: 0.000028  loss: 0.6449  time: 1.5338  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1530/3000]  eta: 0:37:12  lr: 0.000028  loss: 1.2166  time: 1.5532  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1530/3000]  eta: 0:37:12  lr: 0.000028  loss: 0.3956  time: 1.5529  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1535/3000]  eta: 0:37:05  lr: 0.000028  loss: 0.7673  time: 1.5698  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1535/3000]  eta: 0:37:05  lr: 0.000028  loss: 0.2680  time: 1.5695  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1540/3000]  eta: 0:36:57  lr: 0.000028  loss: 0.2110  time: 1.5443  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1540/3000]  eta: 0:36:57  lr: 0.000028  loss: 0.7167  time: 1.5442  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1545/3000]  eta: 0:36:49  lr: 0.000028  loss: 0.4255  time: 1.5282  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1545/3000]  eta: 0:36:49  lr: 0.000028  loss: 0.0600  time: 1.5280  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1550/3000]  eta: 0:36:42  lr: 0.000028  loss: 0.6135  time: 1.4958  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1550/3000]  eta: 0:36:41  lr: 0.000028  loss: 1.1230  time: 1.4956  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1555/3000]  eta: 0:36:34  lr: 0.000028  loss: 0.6070  time: 1.4843  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1555/3000]  eta: 0:36:34  lr: 0.000028  loss: 0.3798  time: 1.4841  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1560/3000]  eta: 0:36:27  lr: 0.000028  loss: 0.1774  time: 1.5025  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1560/3000]  eta: 0:36:26  lr: 0.000028  loss: 0.2334  time: 1.5023  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1565/3000]  eta: 0:36:19  lr: 0.000028  loss: 0.9930  time: 1.5104  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1565/3000]  eta: 0:36:19  lr: 0.000028  loss: 0.3751  time: 1.5101  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1570/3000]  eta: 0:36:11  lr: 0.000028  loss: 0.3661  time: 1.5171  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1570/3000]  eta: 0:36:11  lr: 0.000028  loss: 0.6458  time: 1.5169  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1575/3000]  eta: 0:36:03  lr: 0.000028  loss: 0.3122  time: 1.5008  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1575/3000]  eta: 0:36:03  lr: 0.000028  loss: 0.1188  time: 1.5006  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1580/3000]  eta: 0:35:56  lr: 0.000028  loss: 0.3468  time: 1.4850  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1580/3000]  eta: 0:35:55  lr: 0.000028  loss: 0.8213  time: 1.4847  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1585/3000]  eta: 0:35:48  lr: 0.000028  loss: 0.3724  time: 1.4660  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1585/3000]  eta: 0:35:47  lr: 0.000028  loss: 0.4150  time: 1.4658  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1590/3000]  eta: 0:35:40  lr: 0.000028  loss: 0.6544  time: 1.4749  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1590/3000]  eta: 0:35:40  lr: 0.000028  loss: 0.2387  time: 1.4748  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1595/3000]  eta: 0:35:32  lr: 0.000028  loss: 0.2362  time: 1.4908  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1595/3000]  eta: 0:35:32  lr: 0.000028  loss: 1.2586  time: 1.4905  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1600/3000]  eta: 0:35:25  lr: 0.000028  loss: 0.3747  time: 1.4805  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1600/3000]  eta: 0:35:24  lr: 0.000028  loss: 0.1487  time: 1.4803  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1605/3000]  eta: 0:35:17  lr: 0.000028  loss: 0.3669  time: 1.5145  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1605/3000]  eta: 0:35:17  lr: 0.000028  loss: 0.8757  time: 1.5142  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1610/3000]  eta: 0:35:10  lr: 0.000028  loss: 0.2982  time: 1.5324  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1610/3000]  eta: 0:35:09  lr: 0.000028  loss: 0.5522  time: 1.5319  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1615/3000]  eta: 0:35:02  lr: 0.000028  loss: 0.3726  time: 1.4945  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1615/3000]  eta: 0:35:01  lr: 0.000028  loss: 0.4514  time: 1.4942  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1620/3000]  eta: 0:34:54  lr: 0.000028  loss: 0.6873  time: 1.5233  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1620/3000]  eta: 0:34:54  lr: 0.000028  loss: 0.1915  time: 1.5230  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1625/3000]  eta: 0:34:46  lr: 0.000028  loss: 0.1926  time: 1.5138  data: 0.0000  max mem: 18190Train: data epoch: [6]  [1625/3000]  eta: 0:34:47  lr: 0.000028  loss: 0.1232  time: 1.5141  data: 0.0000  max mem: 18432

Train: data epoch: [6]  [1630/3000]  eta: 0:34:39  lr: 0.000028  loss: 0.2094  time: 1.5047  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1630/3000]  eta: 0:34:39  lr: 0.000028  loss: 0.2112  time: 1.5045  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1635/3000]  eta: 0:34:31  lr: 0.000028  loss: 0.2795  time: 1.5238  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1635/3000]  eta: 0:34:31  lr: 0.000028  loss: 0.4446  time: 1.5235  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1640/3000]  eta: 0:34:24  lr: 0.000028  loss: 0.1935  time: 1.4981  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1640/3000]  eta: 0:34:23  lr: 0.000028  loss: 0.2104  time: 1.4978  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1645/3000]  eta: 0:34:16  lr: 0.000028  loss: 0.9207  time: 1.4979  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1645/3000]  eta: 0:34:16  lr: 0.000028  loss: 0.3386  time: 1.4977  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1650/3000]  eta: 0:34:08  lr: 0.000028  loss: 0.1626  time: 1.4883  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1650/3000]  eta: 0:34:08  lr: 0.000028  loss: 0.5075  time: 1.4880  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1655/3000]  eta: 0:34:00  lr: 0.000028  loss: 0.0543  time: 1.4812  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1655/3000]  eta: 0:34:00  lr: 0.000028  loss: 0.6478  time: 1.4809  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1660/3000]  eta: 0:33:53  lr: 0.000028  loss: 0.3501  time: 1.5037  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1660/3000]  eta: 0:33:53  lr: 0.000028  loss: 0.3708  time: 1.5034  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1665/3000]  eta: 0:33:45  lr: 0.000028  loss: 0.6554  time: 1.4922  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1665/3000]  eta: 0:33:45  lr: 0.000028  loss: 0.5347  time: 1.4919  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1670/3000]  eta: 0:33:38  lr: 0.000028  loss: 0.2007  time: 1.5039  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1670/3000]  eta: 0:33:37  lr: 0.000028  loss: 0.0771  time: 1.5036  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1675/3000]  eta: 0:33:30  lr: 0.000028  loss: 0.4625  time: 1.5393  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1675/3000]  eta: 0:33:30  lr: 0.000028  loss: 0.4122  time: 1.5390  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1680/3000]  eta: 0:33:23  lr: 0.000028  loss: 0.7878  time: 1.5265  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1680/3000]  eta: 0:33:22  lr: 0.000028  loss: 0.1293  time: 1.5253  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1685/3000]  eta: 0:33:15  lr: 0.000028  loss: 0.1017  time: 1.5415  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1685/3000]  eta: 0:33:15  lr: 0.000028  loss: 0.9100  time: 1.5403  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1690/3000]  eta: 0:33:08  lr: 0.000028  loss: 0.2909  time: 1.5370  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1690/3000]  eta: 0:33:07  lr: 0.000028  loss: 0.2453  time: 1.5358  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1695/3000]  eta: 0:33:00  lr: 0.000028  loss: 0.8259  time: 1.5176  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1695/3000]  eta: 0:33:00  lr: 0.000028  loss: 0.4963  time: 1.5163  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1700/3000]  eta: 0:32:53  lr: 0.000028  loss: 0.3027  time: 1.5334  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1700/3000]  eta: 0:32:52  lr: 0.000028  loss: 0.7996  time: 1.5332  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1705/3000]  eta: 0:32:45  lr: 0.000028  loss: 0.4962  time: 1.5012  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1705/3000]  eta: 0:32:44  lr: 0.000028  loss: 0.2102  time: 1.5010  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1710/3000]  eta: 0:32:37  lr: 0.000028  loss: 0.2507  time: 1.4887  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1710/3000]  eta: 0:32:37  lr: 0.000028  loss: 0.3587  time: 1.4884  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1715/3000]  eta: 0:32:29  lr: 0.000028  loss: 0.3118  time: 1.4835  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1715/3000]  eta: 0:32:29  lr: 0.000028  loss: 0.9486  time: 1.4833  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1720/3000]  eta: 0:32:22  lr: 0.000028  loss: 0.7364  time: 1.4728  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1720/3000]  eta: 0:32:21  lr: 0.000028  loss: 0.3585  time: 1.4725  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1725/3000]  eta: 0:32:14  lr: 0.000028  loss: 0.1656  time: 1.4895  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1725/3000]  eta: 0:32:14  lr: 0.000028  loss: 0.9478  time: 1.4891  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1730/3000]  eta: 0:32:06  lr: 0.000028  loss: 0.7934  time: 1.4953  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1730/3000]  eta: 0:32:06  lr: 0.000028  loss: 0.7023  time: 1.4950  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1735/3000]  eta: 0:31:59  lr: 0.000028  loss: 0.3610  time: 1.4941  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1735/3000]  eta: 0:31:58  lr: 0.000028  loss: 0.6453  time: 1.4939  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1740/3000]  eta: 0:31:51  lr: 0.000028  loss: 0.0834  time: 1.4932  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1740/3000]  eta: 0:31:51  lr: 0.000028  loss: 0.1814  time: 1.4930  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1745/3000]  eta: 0:31:44  lr: 0.000028  loss: 0.4787  time: 1.5109  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1745/3000]  eta: 0:31:43  lr: 0.000028  loss: 0.5003  time: 1.5106  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1750/3000]  eta: 0:31:36  lr: 0.000028  loss: 0.4322  time: 1.5171  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1750/3000]  eta: 0:31:36  lr: 0.000028  loss: 0.2548  time: 1.5169  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1755/3000]  eta: 0:31:28  lr: 0.000028  loss: 0.2940  time: 1.5073  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1755/3000]  eta: 0:31:28  lr: 0.000028  loss: 0.1869  time: 1.5071  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1760/3000]  eta: 0:31:21  lr: 0.000028  loss: 0.1744  time: 1.5167  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1760/3000]  eta: 0:31:20  lr: 0.000028  loss: 0.9573  time: 1.5165  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1765/3000]  eta: 0:31:13  lr: 0.000028  loss: 0.3614  time: 1.5056  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1765/3000]  eta: 0:31:13  lr: 0.000028  loss: 0.2384  time: 1.5054  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1770/3000]  eta: 0:31:06  lr: 0.000028  loss: 0.6972  time: 1.5123  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1770/3000]  eta: 0:31:05  lr: 0.000028  loss: 0.4259  time: 1.5122  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1775/3000]  eta: 0:30:58  lr: 0.000028  loss: 0.6896  time: 1.5480  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1775/3000]  eta: 0:30:58  lr: 0.000028  loss: 0.1506  time: 1.5477  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1780/3000]  eta: 0:30:51  lr: 0.000028  loss: 0.4113  time: 1.5397  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1780/3000]  eta: 0:30:50  lr: 0.000028  loss: 0.1603  time: 1.5395  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1785/3000]  eta: 0:30:43  lr: 0.000028  loss: 0.3315  time: 1.5323  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1785/3000]  eta: 0:30:43  lr: 0.000028  loss: 0.2197  time: 1.5322  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1790/3000]  eta: 0:30:36  lr: 0.000028  loss: 0.3915  time: 1.5382  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1790/3000]  eta: 0:30:35  lr: 0.000028  loss: 0.3065  time: 1.5379  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1795/3000]  eta: 0:30:28  lr: 0.000028  loss: 0.5034  time: 1.5288  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1795/3000]  eta: 0:30:28  lr: 0.000028  loss: 0.2512  time: 1.5286  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1800/3000]  eta: 0:30:21  lr: 0.000028  loss: 0.6553  time: 1.5423  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1800/3000]  eta: 0:30:20  lr: 0.000028  loss: 0.4433  time: 1.5421  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1805/3000]  eta: 0:30:13  lr: 0.000028  loss: 0.0512  time: 1.5339  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1805/3000]  eta: 0:30:12  lr: 0.000028  loss: 0.3217  time: 1.5337  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1810/3000]  eta: 0:30:05  lr: 0.000028  loss: 0.7837  time: 1.5039  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1810/3000]  eta: 0:30:05  lr: 0.000028  loss: 0.2553  time: 1.5037  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1815/3000]  eta: 0:29:57  lr: 0.000028  loss: 0.1624  time: 1.5014  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1815/3000]  eta: 0:29:57  lr: 0.000028  loss: 0.2965  time: 1.5013  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1820/3000]  eta: 0:29:50  lr: 0.000028  loss: 0.1706  time: 1.5003  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1820/3000]  eta: 0:29:50  lr: 0.000028  loss: 0.5390  time: 1.5001  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1825/3000]  eta: 0:29:42  lr: 0.000028  loss: 0.3175  time: 1.5095  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1825/3000]  eta: 0:29:42  lr: 0.000028  loss: 0.9943  time: 1.5092  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1830/3000]  eta: 0:29:35  lr: 0.000028  loss: 0.5747  time: 1.5420  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1830/3000]  eta: 0:29:35  lr: 0.000028  loss: 0.1787  time: 1.5418  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1835/3000]  eta: 0:29:27  lr: 0.000028  loss: 0.2800  time: 1.5171  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1835/3000]  eta: 0:29:27  lr: 0.000028  loss: 0.5047  time: 1.5168  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1840/3000]  eta: 0:29:19  lr: 0.000028  loss: 0.3664  time: 1.4855  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1840/3000]  eta: 0:29:19  lr: 0.000028  loss: 0.8662  time: 1.4852  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1845/3000]  eta: 0:29:12  lr: 0.000028  loss: 0.0635  time: 1.4882  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1845/3000]  eta: 0:29:11  lr: 0.000028  loss: 0.1088  time: 1.4881  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1850/3000]  eta: 0:29:04  lr: 0.000028  loss: 0.3328  time: 1.4847  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1850/3000]  eta: 0:29:04  lr: 0.000028  loss: 0.4942  time: 1.4845  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1855/3000]  eta: 0:28:56  lr: 0.000028  loss: 0.2726  time: 1.4959  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1855/3000]  eta: 0:28:56  lr: 0.000028  loss: 0.1319  time: 1.4956  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1860/3000]  eta: 0:28:49  lr: 0.000028  loss: 0.3610  time: 1.5079  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1860/3000]  eta: 0:28:48  lr: 0.000028  loss: 0.8445  time: 1.5077  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1865/3000]  eta: 0:28:41  lr: 0.000028  loss: 0.6009  time: 1.5227  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1865/3000]  eta: 0:28:41  lr: 0.000028  loss: 0.7471  time: 1.5225  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1870/3000]  eta: 0:28:34  lr: 0.000028  loss: 0.0986  time: 1.4873  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1870/3000]  eta: 0:28:33  lr: 0.000028  loss: 0.3406  time: 1.4871  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1875/3000]  eta: 0:28:26  lr: 0.000028  loss: 0.2908  time: 1.4981  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1875/3000]  eta: 0:28:26  lr: 0.000028  loss: 0.3164  time: 1.4979  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1880/3000]  eta: 0:28:18  lr: 0.000028  loss: 0.1012  time: 1.5077  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1880/3000]  eta: 0:28:18  lr: 0.000028  loss: 0.3552  time: 1.5075  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1885/3000]  eta: 0:28:11  lr: 0.000028  loss: 0.4868  time: 1.4933  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1885/3000]  eta: 0:28:10  lr: 0.000028  loss: 0.4516  time: 1.4931  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1890/3000]  eta: 0:28:03  lr: 0.000028  loss: 0.6006  time: 1.5224  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1890/3000]  eta: 0:28:03  lr: 0.000028  loss: 0.6085  time: 1.5222  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1895/3000]  eta: 0:27:56  lr: 0.000028  loss: 0.3555  time: 1.5246  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1895/3000]  eta: 0:27:55  lr: 0.000028  loss: 0.2551  time: 1.5244  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1900/3000]  eta: 0:27:48  lr: 0.000028  loss: 0.1430  time: 1.5253  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1900/3000]  eta: 0:27:48  lr: 0.000028  loss: 0.6143  time: 1.5251  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1905/3000]  eta: 0:27:41  lr: 0.000028  loss: 0.5170  time: 1.5287  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1905/3000]  eta: 0:27:40  lr: 0.000028  loss: 0.5887  time: 1.5285  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1910/3000]  eta: 0:27:33  lr: 0.000028  loss: 0.2020  time: 1.5299  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1910/3000]  eta: 0:27:33  lr: 0.000028  loss: 0.5170  time: 1.5297  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1915/3000]  eta: 0:27:26  lr: 0.000028  loss: 0.4250  time: 1.5404  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1915/3000]  eta: 0:27:25  lr: 0.000028  loss: 0.3191  time: 1.5401  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1920/3000]  eta: 0:27:18  lr: 0.000028  loss: 0.9647  time: 1.5333  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1920/3000]  eta: 0:27:18  lr: 0.000028  loss: 0.1447  time: 1.5330  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1925/3000]  eta: 0:27:11  lr: 0.000028  loss: 0.8199  time: 1.5455  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1925/3000]  eta: 0:27:10  lr: 0.000028  loss: 0.4227  time: 1.5452  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1930/3000]  eta: 0:27:03  lr: 0.000028  loss: 0.3602  time: 1.5375  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1930/3000]  eta: 0:27:03  lr: 0.000028  loss: 0.2039  time: 1.5373  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1935/3000]  eta: 0:26:55  lr: 0.000028  loss: 0.1918  time: 1.5281  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1935/3000]  eta: 0:26:55  lr: 0.000028  loss: 0.3515  time: 1.5280  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1940/3000]  eta: 0:26:48  lr: 0.000028  loss: 0.4512  time: 1.5250  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1940/3000]  eta: 0:26:47  lr: 0.000028  loss: 0.5454  time: 1.5248  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1945/3000]  eta: 0:26:40  lr: 0.000028  loss: 0.3702  time: 1.5029  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1945/3000]  eta: 0:26:40  lr: 0.000028  loss: 0.4184  time: 1.5027  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1950/3000]  eta: 0:26:33  lr: 0.000028  loss: 0.6346  time: 1.5136  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1950/3000]  eta: 0:26:32  lr: 0.000028  loss: 0.6918  time: 1.5134  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1955/3000]  eta: 0:26:25  lr: 0.000028  loss: 0.4681  time: 1.5092  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1955/3000]  eta: 0:26:25  lr: 0.000028  loss: 0.7107  time: 1.5090  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1960/3000]  eta: 0:26:17  lr: 0.000028  loss: 0.4296  time: 1.5072  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1960/3000]  eta: 0:26:17  lr: 0.000028  loss: 0.2609  time: 1.5070  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1965/3000]  eta: 0:26:10  lr: 0.000028  loss: 0.3879  time: 1.5119  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1965/3000]  eta: 0:26:09  lr: 0.000028  loss: 0.5926  time: 1.5117  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1970/3000]  eta: 0:26:02  lr: 0.000028  loss: 0.1811  time: 1.5095  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1970/3000]  eta: 0:26:02  lr: 0.000028  loss: 0.4830  time: 1.5094  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1975/3000]  eta: 0:25:55  lr: 0.000028  loss: 1.2641  time: 1.5228  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1975/3000]  eta: 0:25:54  lr: 0.000028  loss: 0.2375  time: 1.5226  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1980/3000]  eta: 0:25:47  lr: 0.000028  loss: 0.5654  time: 1.5314  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1980/3000]  eta: 0:25:47  lr: 0.000028  loss: 0.1416  time: 1.5312  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1985/3000]  eta: 0:25:40  lr: 0.000028  loss: 0.7402  time: 1.5431  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1985/3000]  eta: 0:25:39  lr: 0.000028  loss: 0.8736  time: 1.5429  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1990/3000]  eta: 0:25:32  lr: 0.000028  loss: 0.4173  time: 1.5397  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1990/3000]  eta: 0:25:32  lr: 0.000028  loss: 0.7708  time: 1.5395  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [1995/3000]  eta: 0:25:25  lr: 0.000028  loss: 0.4731  time: 1.5367  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [1995/3000]  eta: 0:25:24  lr: 0.000028  loss: 0.6919  time: 1.5365  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2000/3000]  eta: 0:25:17  lr: 0.000028  loss: 0.4432  time: 1.5242  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2000/3000]  eta: 0:25:17  lr: 0.000028  loss: 0.0804  time: 1.5239  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2005/3000]  eta: 0:25:09  lr: 0.000028  loss: 0.5221  time: 1.5188  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2005/3000]  eta: 0:25:09  lr: 0.000028  loss: 1.3119  time: 1.5185  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2010/3000]  eta: 0:25:02  lr: 0.000028  loss: 0.6753  time: 1.5278  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2010/3000]  eta: 0:25:01  lr: 0.000028  loss: 0.4448  time: 1.5276  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2015/3000]  eta: 0:24:54  lr: 0.000028  loss: 0.1522  time: 1.5111  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2015/3000]  eta: 0:24:54  lr: 0.000028  loss: 0.4078  time: 1.5109  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2020/3000]  eta: 0:24:47  lr: 0.000028  loss: 0.1098  time: 1.5265  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2020/3000]  eta: 0:24:46  lr: 0.000028  loss: 0.2907  time: 1.5263  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2025/3000]  eta: 0:24:39  lr: 0.000028  loss: 0.9618  time: 1.5286  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2025/3000]  eta: 0:24:39  lr: 0.000028  loss: 0.2410  time: 1.5284  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2030/3000]  eta: 0:24:31  lr: 0.000028  loss: 0.3344  time: 1.5262  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2030/3000]  eta: 0:24:31  lr: 0.000028  loss: 0.4311  time: 1.5259  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2035/3000]  eta: 0:24:24  lr: 0.000028  loss: 0.2723  time: 1.5364  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2035/3000]  eta: 0:24:24  lr: 0.000028  loss: 0.5782  time: 1.5361  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2040/3000]  eta: 0:24:16  lr: 0.000028  loss: 0.1248  time: 1.5421  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2040/3000]  eta: 0:24:16  lr: 0.000028  loss: 0.3057  time: 1.5418  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2045/3000]  eta: 0:24:09  lr: 0.000028  loss: 0.2091  time: 1.5282  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2045/3000]  eta: 0:24:08  lr: 0.000028  loss: 0.1542  time: 1.5280  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2050/3000]  eta: 0:24:01  lr: 0.000028  loss: 0.1406  time: 1.5307  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2050/3000]  eta: 0:24:01  lr: 0.000028  loss: 0.1847  time: 1.5305  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2055/3000]  eta: 0:23:54  lr: 0.000028  loss: 0.2225  time: 1.5229  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2055/3000]  eta: 0:23:53  lr: 0.000028  loss: 0.3204  time: 1.5227  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2060/3000]  eta: 0:23:46  lr: 0.000028  loss: 0.1834  time: 1.4943  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2060/3000]  eta: 0:23:46  lr: 0.000028  loss: 0.4621  time: 1.4940  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2065/3000]  eta: 0:23:38  lr: 0.000028  loss: 0.3929  time: 1.5106  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2065/3000]  eta: 0:23:38  lr: 0.000028  loss: 0.5272  time: 1.5103  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2070/3000]  eta: 0:23:31  lr: 0.000028  loss: 0.3602  time: 1.5008  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2070/3000]  eta: 0:23:31  lr: 0.000028  loss: 0.2284  time: 1.5005  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2075/3000]  eta: 0:23:23  lr: 0.000028  loss: 0.3115  time: 1.5095  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2075/3000]  eta: 0:23:23  lr: 0.000028  loss: 0.2384  time: 1.5092  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2080/3000]  eta: 0:23:15  lr: 0.000028  loss: 0.4482  time: 1.5051  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2080/3000]  eta: 0:23:15  lr: 0.000028  loss: 0.3068  time: 1.5049  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2085/3000]  eta: 0:23:08  lr: 0.000028  loss: 0.3996  time: 1.4942  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2085/3000]  eta: 0:23:08  lr: 0.000028  loss: 0.1611  time: 1.4940  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2090/3000]  eta: 0:23:00  lr: 0.000028  loss: 0.7381  time: 1.4803  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2090/3000]  eta: 0:23:00  lr: 0.000028  loss: 0.1997  time: 1.4801  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2095/3000]  eta: 0:22:52  lr: 0.000028  loss: 0.7128  time: 1.4534  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2095/3000]  eta: 0:22:52  lr: 0.000028  loss: 0.2940  time: 1.4531  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2100/3000]  eta: 0:22:45  lr: 0.000028  loss: 0.3458  time: 1.4846  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2100/3000]  eta: 0:22:45  lr: 0.000028  loss: 0.4066  time: 1.4843  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2105/3000]  eta: 0:22:37  lr: 0.000028  loss: 0.5643  time: 1.5085  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2105/3000]  eta: 0:22:37  lr: 0.000028  loss: 0.6651  time: 1.5083  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2110/3000]  eta: 0:22:30  lr: 0.000028  loss: 0.6648  time: 1.5110  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2110/3000]  eta: 0:22:29  lr: 0.000028  loss: 0.2986  time: 1.5108  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2115/3000]  eta: 0:22:22  lr: 0.000028  loss: 0.1601  time: 1.5389  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2115/3000]  eta: 0:22:22  lr: 0.000028  loss: 0.1042  time: 1.5387  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2120/3000]  eta: 0:22:15  lr: 0.000028  loss: 0.3007  time: 1.5527  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2120/3000]  eta: 0:22:14  lr: 0.000028  loss: 0.7139  time: 1.5525  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2125/3000]  eta: 0:22:07  lr: 0.000028  loss: 0.1122  time: 1.5418  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2125/3000]  eta: 0:22:07  lr: 0.000028  loss: 0.5023  time: 1.5416  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2130/3000]  eta: 0:22:00  lr: 0.000028  loss: 0.1476  time: 1.5545  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2130/3000]  eta: 0:21:59  lr: 0.000028  loss: 1.1525  time: 1.5542  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2135/3000]  eta: 0:21:52  lr: 0.000028  loss: 0.4293  time: 1.5261  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2135/3000]  eta: 0:21:52  lr: 0.000028  loss: 0.5857  time: 1.5258  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2140/3000]  eta: 0:21:44  lr: 0.000028  loss: 0.3412  time: 1.5134  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2140/3000]  eta: 0:21:44  lr: 0.000028  loss: 0.5479  time: 1.5131  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2145/3000]  eta: 0:21:37  lr: 0.000028  loss: 0.1416  time: 1.5022  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2145/3000]  eta: 0:21:37  lr: 0.000028  loss: 0.0732  time: 1.5019  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2150/3000]  eta: 0:21:29  lr: 0.000028  loss: 0.3261  time: 1.4730  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2150/3000]  eta: 0:21:29  lr: 0.000028  loss: 0.5344  time: 1.4728  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2155/3000]  eta: 0:21:21  lr: 0.000028  loss: 0.0706  time: 1.4907  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2155/3000]  eta: 0:21:21  lr: 0.000028  loss: 0.1222  time: 1.4904  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2160/3000]  eta: 0:21:14  lr: 0.000028  loss: 0.9901  time: 1.4819  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2160/3000]  eta: 0:21:13  lr: 0.000028  loss: 0.4011  time: 1.4817  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2165/3000]  eta: 0:21:06  lr: 0.000028  loss: 0.3694  time: 1.4892  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2165/3000]  eta: 0:21:06  lr: 0.000028  loss: 0.1997  time: 1.4888  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2170/3000]  eta: 0:20:58  lr: 0.000028  loss: 0.1681  time: 1.4856  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2170/3000]  eta: 0:20:58  lr: 0.000028  loss: 0.2659  time: 1.4854  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2175/3000]  eta: 0:20:51  lr: 0.000028  loss: 0.2859  time: 1.4975  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2175/3000]  eta: 0:20:51  lr: 0.000028  loss: 0.3585  time: 1.4972  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2180/3000]  eta: 0:20:43  lr: 0.000028  loss: 0.4036  time: 1.4909  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2180/3000]  eta: 0:20:43  lr: 0.000028  loss: 0.5582  time: 1.4907  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2185/3000]  eta: 0:20:36  lr: 0.000028  loss: 0.1510  time: 1.4897  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2185/3000]  eta: 0:20:35  lr: 0.000028  loss: 0.4260  time: 1.4894  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2190/3000]  eta: 0:20:28  lr: 0.000028  loss: 0.3467  time: 1.5339  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2190/3000]  eta: 0:20:28  lr: 0.000028  loss: 0.0985  time: 1.5337  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2195/3000]  eta: 0:20:21  lr: 0.000028  loss: 0.3092  time: 1.5191  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2195/3000]  eta: 0:20:20  lr: 0.000028  loss: 0.6605  time: 1.5189  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2200/3000]  eta: 0:20:13  lr: 0.000028  loss: 0.1768  time: 1.5126  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2200/3000]  eta: 0:20:13  lr: 0.000028  loss: 0.5509  time: 1.5123  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2205/3000]  eta: 0:20:05  lr: 0.000028  loss: 0.3030  time: 1.5080  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2205/3000]  eta: 0:20:05  lr: 0.000028  loss: 0.0620  time: 1.5078  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2210/3000]  eta: 0:19:58  lr: 0.000028  loss: 0.6115  time: 1.5021  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2210/3000]  eta: 0:19:58  lr: 0.000028  loss: 0.0703  time: 1.5019  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2215/3000]  eta: 0:19:50  lr: 0.000028  loss: 0.3833  time: 1.5041  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2215/3000]  eta: 0:19:50  lr: 0.000028  loss: 0.5637  time: 1.5039  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2220/3000]  eta: 0:19:43  lr: 0.000028  loss: 0.2129  time: 1.5373  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2220/3000]  eta: 0:19:42  lr: 0.000028  loss: 0.2835  time: 1.5371  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2225/3000]  eta: 0:19:35  lr: 0.000028  loss: 0.1923  time: 1.5249  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2225/3000]  eta: 0:19:35  lr: 0.000028  loss: 0.3640  time: 1.5247  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2230/3000]  eta: 0:19:27  lr: 0.000028  loss: 0.4526  time: 1.5225  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2230/3000]  eta: 0:19:27  lr: 0.000028  loss: 0.4705  time: 1.5223  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2235/3000]  eta: 0:19:20  lr: 0.000028  loss: 0.6547  time: 1.5209  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2235/3000]  eta: 0:19:20  lr: 0.000028  loss: 0.2437  time: 1.5206  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2240/3000]  eta: 0:19:12  lr: 0.000028  loss: 0.7047  time: 1.5198  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2240/3000]  eta: 0:19:12  lr: 0.000028  loss: 0.1524  time: 1.5196  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2245/3000]  eta: 0:19:05  lr: 0.000028  loss: 0.6738  time: 1.5471  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2245/3000]  eta: 0:19:05  lr: 0.000028  loss: 0.3486  time: 1.5468  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2250/3000]  eta: 0:18:57  lr: 0.000028  loss: 0.1573  time: 1.5407  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2250/3000]  eta: 0:18:57  lr: 0.000028  loss: 0.0883  time: 1.5405  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2255/3000]  eta: 0:18:50  lr: 0.000028  loss: 0.2516  time: 1.5490  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2255/3000]  eta: 0:18:49  lr: 0.000028  loss: 0.4663  time: 1.5488  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2260/3000]  eta: 0:18:42  lr: 0.000028  loss: 0.4695  time: 1.5223  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2260/3000]  eta: 0:18:42  lr: 0.000028  loss: 0.3278  time: 1.5226  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2265/3000]  eta: 0:18:34  lr: 0.000028  loss: 0.1453  time: 1.5036  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2265/3000]  eta: 0:18:34  lr: 0.000028  loss: 0.4046  time: 1.5035  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2270/3000]  eta: 0:18:27  lr: 0.000028  loss: 0.2471  time: 1.5042  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2270/3000]  eta: 0:18:27  lr: 0.000028  loss: 0.7678  time: 1.5041  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2275/3000]  eta: 0:18:19  lr: 0.000028  loss: 0.0725  time: 1.4877  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2275/3000]  eta: 0:18:19  lr: 0.000028  loss: 0.2525  time: 1.4875  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2280/3000]  eta: 0:18:12  lr: 0.000028  loss: 0.2263  time: 1.4930  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2280/3000]  eta: 0:18:11  lr: 0.000028  loss: 0.6645  time: 1.4928  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2285/3000]  eta: 0:18:04  lr: 0.000028  loss: 0.4149  time: 1.5119  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2285/3000]  eta: 0:18:04  lr: 0.000028  loss: 0.6848  time: 1.5116  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2290/3000]  eta: 0:17:56  lr: 0.000028  loss: 0.0494  time: 1.5084  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2290/3000]  eta: 0:17:56  lr: 0.000028  loss: 1.0106  time: 1.5082  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2295/3000]  eta: 0:17:49  lr: 0.000028  loss: 0.2216  time: 1.5403  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2295/3000]  eta: 0:17:49  lr: 0.000028  loss: 0.5837  time: 1.5401  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2300/3000]  eta: 0:17:41  lr: 0.000028  loss: 0.0826  time: 1.5385  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2300/3000]  eta: 0:17:41  lr: 0.000028  loss: 0.4979  time: 1.5383  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2305/3000]  eta: 0:17:34  lr: 0.000028  loss: 0.3254  time: 1.5319  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2305/3000]  eta: 0:17:34  lr: 0.000028  loss: 0.4153  time: 1.5317  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2310/3000]  eta: 0:17:26  lr: 0.000028  loss: 1.0067  time: 1.5243  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2310/3000]  eta: 0:17:26  lr: 0.000028  loss: 0.2377  time: 1.5241  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2315/3000]  eta: 0:17:19  lr: 0.000028  loss: 0.3810  time: 1.5127  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2315/3000]  eta: 0:17:18  lr: 0.000028  loss: 0.4526  time: 1.5125  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2320/3000]  eta: 0:17:11  lr: 0.000028  loss: 0.2526  time: 1.5019  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2320/3000]  eta: 0:17:11  lr: 0.000028  loss: 0.6719  time: 1.5017  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2325/3000]  eta: 0:17:03  lr: 0.000028  loss: 0.9066  time: 1.4831  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2325/3000]  eta: 0:17:03  lr: 0.000028  loss: 0.5321  time: 1.4829  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2330/3000]  eta: 0:16:56  lr: 0.000028  loss: 0.3061  time: 1.4724  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2330/3000]  eta: 0:16:55  lr: 0.000028  loss: 0.4893  time: 1.4722  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2335/3000]  eta: 0:16:48  lr: 0.000028  loss: 0.4676  time: 1.4796  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2335/3000]  eta: 0:16:48  lr: 0.000028  loss: 1.3578  time: 1.4793  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2340/3000]  eta: 0:16:40  lr: 0.000028  loss: 0.3774  time: 1.4854  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2340/3000]  eta: 0:16:40  lr: 0.000028  loss: 0.2449  time: 1.4851  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2345/3000]  eta: 0:16:33  lr: 0.000028  loss: 0.1743  time: 1.5051  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2345/3000]  eta: 0:16:33  lr: 0.000028  loss: 0.2988  time: 1.5048  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2350/3000]  eta: 0:16:25  lr: 0.000028  loss: 0.2089  time: 1.5383  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2350/3000]  eta: 0:16:25  lr: 0.000028  loss: 0.3783  time: 1.5381  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2355/3000]  eta: 0:16:18  lr: 0.000028  loss: 0.8981  time: 1.5097  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2355/3000]  eta: 0:16:18  lr: 0.000028  loss: 0.3529  time: 1.5095  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2360/3000]  eta: 0:16:10  lr: 0.000028  loss: 1.1250  time: 1.5207  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2360/3000]  eta: 0:16:10  lr: 0.000028  loss: 0.2837  time: 1.5205  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2365/3000]  eta: 0:16:03  lr: 0.000028  loss: 0.7253  time: 1.5254  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2365/3000]  eta: 0:16:02  lr: 0.000028  loss: 0.1439  time: 1.5252  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2370/3000]  eta: 0:15:55  lr: 0.000028  loss: 0.9653  time: 1.5176  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2370/3000]  eta: 0:15:55  lr: 0.000028  loss: 0.5681  time: 1.5174  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2375/3000]  eta: 0:15:47  lr: 0.000028  loss: 0.1270  time: 1.5441  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2375/3000]  eta: 0:15:47  lr: 0.000028  loss: 0.5306  time: 1.5439  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2380/3000]  eta: 0:15:40  lr: 0.000028  loss: 0.0780  time: 1.5396  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2380/3000]  eta: 0:15:40  lr: 0.000028  loss: 0.3030  time: 1.5393  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2385/3000]  eta: 0:15:32  lr: 0.000028  loss: 0.3032  time: 1.5285  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2385/3000]  eta: 0:15:32  lr: 0.000028  loss: 0.5787  time: 1.5282  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2390/3000]  eta: 0:15:25  lr: 0.000028  loss: 0.6567  time: 1.5351  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2390/3000]  eta: 0:15:25  lr: 0.000028  loss: 0.2631  time: 1.5348  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2395/3000]  eta: 0:15:17  lr: 0.000028  loss: 0.4450  time: 1.5272  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2395/3000]  eta: 0:15:17  lr: 0.000028  loss: 0.9150  time: 1.5269  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2400/3000]  eta: 0:15:10  lr: 0.000028  loss: 0.3436  time: 1.5167  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2400/3000]  eta: 0:15:09  lr: 0.000028  loss: 0.1952  time: 1.5164  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2405/3000]  eta: 0:15:02  lr: 0.000028  loss: 0.1863  time: 1.5104  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2405/3000]  eta: 0:15:02  lr: 0.000028  loss: 0.1156  time: 1.5102  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2410/3000]  eta: 0:14:54  lr: 0.000028  loss: 0.4827  time: 1.5131  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2410/3000]  eta: 0:14:54  lr: 0.000028  loss: 0.1549  time: 1.5128  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2415/3000]  eta: 0:14:47  lr: 0.000028  loss: 0.2320  time: 1.5062  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2415/3000]  eta: 0:14:47  lr: 0.000028  loss: 0.5753  time: 1.5059  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2420/3000]  eta: 0:14:39  lr: 0.000028  loss: 0.7191  time: 1.5123  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2420/3000]  eta: 0:14:39  lr: 0.000028  loss: 0.7471  time: 1.5121  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2425/3000]  eta: 0:14:32  lr: 0.000028  loss: 0.1274  time: 1.5204  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2425/3000]  eta: 0:14:31  lr: 0.000028  loss: 0.2953  time: 1.5201  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2430/3000]  eta: 0:14:24  lr: 0.000028  loss: 0.2369  time: 1.4792  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2430/3000]  eta: 0:14:24  lr: 0.000028  loss: 0.2840  time: 1.4790  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2435/3000]  eta: 0:14:16  lr: 0.000028  loss: 0.5168  time: 1.4757  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2435/3000]  eta: 0:14:16  lr: 0.000028  loss: 0.6590  time: 1.4755  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2440/3000]  eta: 0:14:09  lr: 0.000028  loss: 0.3354  time: 1.4896  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2440/3000]  eta: 0:14:09  lr: 0.000028  loss: 0.5682  time: 1.4894  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2445/3000]  eta: 0:14:01  lr: 0.000028  loss: 0.2104  time: 1.4913  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2445/3000]  eta: 0:14:01  lr: 0.000028  loss: 0.5570  time: 1.4910  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2450/3000]  eta: 0:13:54  lr: 0.000028  loss: 0.1442  time: 1.5229  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2450/3000]  eta: 0:13:53  lr: 0.000028  loss: 0.2145  time: 1.5227  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2455/3000]  eta: 0:13:46  lr: 0.000028  loss: 0.3156  time: 1.5305  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2455/3000]  eta: 0:13:46  lr: 0.000028  loss: 0.3224  time: 1.5303  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2460/3000]  eta: 0:13:38  lr: 0.000028  loss: 0.3297  time: 1.5163  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2460/3000]  eta: 0:13:38  lr: 0.000028  loss: 1.1818  time: 1.5161  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2465/3000]  eta: 0:13:31  lr: 0.000028  loss: 0.2998  time: 1.4952  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2465/3000]  eta: 0:13:31  lr: 0.000028  loss: 0.5880  time: 1.4950  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2470/3000]  eta: 0:13:23  lr: 0.000028  loss: 0.3796  time: 1.4980  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2470/3000]  eta: 0:13:23  lr: 0.000028  loss: 0.1637  time: 1.4978  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2475/3000]  eta: 0:13:16  lr: 0.000028  loss: 0.3302  time: 1.4973  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2475/3000]  eta: 0:13:15  lr: 0.000028  loss: 0.2044  time: 1.4971  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2480/3000]  eta: 0:13:08  lr: 0.000028  loss: 0.4874  time: 1.5092  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2480/3000]  eta: 0:13:08  lr: 0.000028  loss: 0.4246  time: 1.5088  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2485/3000]  eta: 0:13:00  lr: 0.000028  loss: 1.0311  time: 1.5233  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2485/3000]  eta: 0:13:00  lr: 0.000028  loss: 0.4677  time: 1.5231  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2490/3000]  eta: 0:12:53  lr: 0.000028  loss: 0.4280  time: 1.5186  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2490/3000]  eta: 0:12:53  lr: 0.000028  loss: 0.6086  time: 1.5183  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2495/3000]  eta: 0:12:45  lr: 0.000028  loss: 0.6039  time: 1.5390  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2495/3000]  eta: 0:12:45  lr: 0.000028  loss: 0.2294  time: 1.5387  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2500/3000]  eta: 0:12:38  lr: 0.000028  loss: 0.2422  time: 1.5421  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2500/3000]  eta: 0:12:38  lr: 0.000028  loss: 0.4684  time: 1.5419  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2505/3000]  eta: 0:12:30  lr: 0.000028  loss: 0.6125  time: 1.5470  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2505/3000]  eta: 0:12:30  lr: 0.000028  loss: 0.0971  time: 1.5467  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2510/3000]  eta: 0:12:23  lr: 0.000028  loss: 0.5751  time: 1.5591  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2510/3000]  eta: 0:12:23  lr: 0.000028  loss: 0.2386  time: 1.5588  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2515/3000]  eta: 0:12:15  lr: 0.000028  loss: 0.3362  time: 1.5388  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2515/3000]  eta: 0:12:15  lr: 0.000028  loss: 0.2803  time: 1.5385  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2520/3000]  eta: 0:12:08  lr: 0.000028  loss: 0.4032  time: 1.5459  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2520/3000]  eta: 0:12:07  lr: 0.000028  loss: 0.1111  time: 1.5455  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2525/3000]  eta: 0:12:00  lr: 0.000028  loss: 0.1439  time: 1.5457  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2525/3000]  eta: 0:12:00  lr: 0.000028  loss: 0.2299  time: 1.5455  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2530/3000]  eta: 0:11:52  lr: 0.000028  loss: 0.4905  time: 1.5287  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2530/3000]  eta: 0:11:52  lr: 0.000028  loss: 0.9816  time: 1.5284  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2535/3000]  eta: 0:11:45  lr: 0.000028  loss: 0.6755  time: 1.5449  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2535/3000]  eta: 0:11:45  lr: 0.000028  loss: 0.3444  time: 1.5446  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2540/3000]  eta: 0:11:37  lr: 0.000028  loss: 0.5619  time: 1.5373  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2540/3000]  eta: 0:11:37  lr: 0.000028  loss: 0.3575  time: 1.5371  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2545/3000]  eta: 0:11:30  lr: 0.000028  loss: 0.3118  time: 1.5480  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2545/3000]  eta: 0:11:30  lr: 0.000028  loss: 0.2791  time: 1.5478  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2550/3000]  eta: 0:11:22  lr: 0.000028  loss: 0.5104  time: 1.5674  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2550/3000]  eta: 0:11:22  lr: 0.000028  loss: 0.6447  time: 1.5672  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2555/3000]  eta: 0:11:15  lr: 0.000028  loss: 0.4148  time: 1.5628  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2555/3000]  eta: 0:11:15  lr: 0.000028  loss: 0.0947  time: 1.5625  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2560/3000]  eta: 0:11:07  lr: 0.000028  loss: 0.6126  time: 1.5755  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2560/3000]  eta: 0:11:07  lr: 0.000028  loss: 0.5568  time: 1.5752  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2565/3000]  eta: 0:11:00  lr: 0.000028  loss: 0.4288  time: 1.5655  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2565/3000]  eta: 0:10:59  lr: 0.000028  loss: 0.7564  time: 1.5652  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2570/3000]  eta: 0:10:52  lr: 0.000028  loss: 0.3107  time: 1.5288  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2570/3000]  eta: 0:10:52  lr: 0.000028  loss: 0.0956  time: 1.5285  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2575/3000]  eta: 0:10:44  lr: 0.000028  loss: 0.3145  time: 1.5003  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2575/3000]  eta: 0:10:44  lr: 0.000028  loss: 0.9892  time: 1.5000  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2580/3000]  eta: 0:10:37  lr: 0.000028  loss: 0.2617  time: 1.4809  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2580/3000]  eta: 0:10:37  lr: 0.000028  loss: 0.2366  time: 1.4807  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2585/3000]  eta: 0:10:29  lr: 0.000028  loss: 0.2077  time: 1.4529  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2585/3000]  eta: 0:10:29  lr: 0.000028  loss: 0.7754  time: 1.4528  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2590/3000]  eta: 0:10:21  lr: 0.000028  loss: 0.2249  time: 1.4748  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2590/3000]  eta: 0:10:21  lr: 0.000028  loss: 0.3888  time: 1.4745  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2595/3000]  eta: 0:10:14  lr: 0.000028  loss: 0.5522  time: 1.4748  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2595/3000]  eta: 0:10:14  lr: 0.000028  loss: 0.4948  time: 1.4746  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2600/3000]  eta: 0:10:06  lr: 0.000028  loss: 0.2487  time: 1.4727  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2600/3000]  eta: 0:10:06  lr: 0.000028  loss: 0.3889  time: 1.4725  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2605/3000]  eta: 0:09:59  lr: 0.000028  loss: 0.1904  time: 1.4978  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2605/3000]  eta: 0:09:59  lr: 0.000028  loss: 0.9350  time: 1.4976  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2610/3000]  eta: 0:09:51  lr: 0.000028  loss: 0.2159  time: 1.4848  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2610/3000]  eta: 0:09:51  lr: 0.000028  loss: 0.4361  time: 1.4845  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2615/3000]  eta: 0:09:43  lr: 0.000028  loss: 0.7480  time: 1.4963  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2615/3000]  eta: 0:09:43  lr: 0.000028  loss: 0.1097  time: 1.4960  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2620/3000]  eta: 0:09:36  lr: 0.000028  loss: 0.5428  time: 1.5015  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2620/3000]  eta: 0:09:36  lr: 0.000028  loss: 0.8376  time: 1.5012  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2625/3000]  eta: 0:09:28  lr: 0.000028  loss: 0.2203  time: 1.5007  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2625/3000]  eta: 0:09:28  lr: 0.000028  loss: 0.2707  time: 1.5003  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2630/3000]  eta: 0:09:21  lr: 0.000028  loss: 0.3722  time: 1.4891  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2630/3000]  eta: 0:09:21  lr: 0.000028  loss: 0.2012  time: 1.4888  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2635/3000]  eta: 0:09:13  lr: 0.000028  loss: 0.2842  time: 1.4992  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2635/3000]  eta: 0:09:13  lr: 0.000028  loss: 0.1489  time: 1.4990  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2640/3000]  eta: 0:09:05  lr: 0.000028  loss: 0.2240  time: 1.4963  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2640/3000]  eta: 0:09:05  lr: 0.000028  loss: 0.7840  time: 1.4960  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2645/3000]  eta: 0:08:58  lr: 0.000028  loss: 0.0422  time: 1.5107  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2645/3000]  eta: 0:08:58  lr: 0.000028  loss: 1.1772  time: 1.5111  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2650/3000]  eta: 0:08:50  lr: 0.000028  loss: 0.6543  time: 1.5332  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2650/3000]  eta: 0:08:50  lr: 0.000028  loss: 0.3449  time: 1.5329  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2655/3000]  eta: 0:08:43  lr: 0.000028  loss: 0.2494  time: 1.5292  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2655/3000]  eta: 0:08:43  lr: 0.000028  loss: 0.7287  time: 1.5290  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2660/3000]  eta: 0:08:35  lr: 0.000028  loss: 0.3236  time: 1.5156  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2660/3000]  eta: 0:08:35  lr: 0.000028  loss: 0.9771  time: 1.5154  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2665/3000]  eta: 0:08:28  lr: 0.000028  loss: 0.2990  time: 1.4901  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2665/3000]  eta: 0:08:27  lr: 0.000028  loss: 0.5165  time: 1.4900  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2670/3000]  eta: 0:08:20  lr: 0.000028  loss: 0.4225  time: 1.4707  data: 0.0000  max mem: 18432Train: data epoch: [6]  [2670/3000]  eta: 0:08:20  lr: 0.000028  loss: 0.2133  time: 1.4704  data: 0.0000  max mem: 18190

Train: data epoch: [6]  [2675/3000]  eta: 0:08:12  lr: 0.000028  loss: 0.6729  time: 1.4848  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2675/3000]  eta: 0:08:12  lr: 0.000028  loss: 0.7894  time: 1.4846  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2680/3000]  eta: 0:08:05  lr: 0.000028  loss: 0.1034  time: 1.5204  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2680/3000]  eta: 0:08:05  lr: 0.000028  loss: 0.2427  time: 1.5201  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2685/3000]  eta: 0:07:57  lr: 0.000028  loss: 0.2956  time: 1.5255  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2685/3000]  eta: 0:07:57  lr: 0.000028  loss: 0.4368  time: 1.5252  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2690/3000]  eta: 0:07:50  lr: 0.000028  loss: 0.3645  time: 1.5471  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2690/3000]  eta: 0:07:50  lr: 0.000028  loss: 0.1994  time: 1.5469  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2695/3000]  eta: 0:07:42  lr: 0.000028  loss: 0.4045  time: 1.5379  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2695/3000]  eta: 0:07:42  lr: 0.000028  loss: 0.2140  time: 1.5376  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2700/3000]  eta: 0:07:34  lr: 0.000028  loss: 0.8446  time: 1.5142  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2700/3000]  eta: 0:07:34  lr: 0.000028  loss: 0.5383  time: 1.5139  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2705/3000]  eta: 0:07:27  lr: 0.000027  loss: 0.4585  time: 1.5218  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2705/3000]  eta: 0:07:27  lr: 0.000027  loss: 0.3841  time: 1.5214  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2710/3000]  eta: 0:07:19  lr: 0.000027  loss: 0.5366  time: 1.5326  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2710/3000]  eta: 0:07:19  lr: 0.000027  loss: 0.2562  time: 1.5323  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2715/3000]  eta: 0:07:12  lr: 0.000027  loss: 0.5733  time: 1.5344  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2715/3000]  eta: 0:07:12  lr: 0.000027  loss: 0.3336  time: 1.5341  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2720/3000]  eta: 0:07:04  lr: 0.000027  loss: 0.1893  time: 1.5258  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2720/3000]  eta: 0:07:04  lr: 0.000027  loss: 0.6210  time: 1.5255  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2725/3000]  eta: 0:06:57  lr: 0.000027  loss: 0.9966  time: 1.5344  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2725/3000]  eta: 0:06:57  lr: 0.000027  loss: 0.6473  time: 1.5342  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2730/3000]  eta: 0:06:49  lr: 0.000027  loss: 0.2097  time: 1.5266  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2730/3000]  eta: 0:06:49  lr: 0.000027  loss: 0.4063  time: 1.5264  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2735/3000]  eta: 0:06:41  lr: 0.000027  loss: 0.4286  time: 1.5128  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2735/3000]  eta: 0:06:41  lr: 0.000027  loss: 0.5081  time: 1.5126  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2740/3000]  eta: 0:06:34  lr: 0.000027  loss: 0.1164  time: 1.5068  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2740/3000]  eta: 0:06:34  lr: 0.000027  loss: 0.8508  time: 1.5066  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2745/3000]  eta: 0:06:26  lr: 0.000027  loss: 0.5803  time: 1.4895  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2745/3000]  eta: 0:06:26  lr: 0.000027  loss: 0.1898  time: 1.4893  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2750/3000]  eta: 0:06:19  lr: 0.000027  loss: 0.4245  time: 1.4736  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2750/3000]  eta: 0:06:19  lr: 0.000027  loss: 0.6749  time: 1.4734  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2755/3000]  eta: 0:06:11  lr: 0.000027  loss: 0.2328  time: 1.4577  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2755/3000]  eta: 0:06:11  lr: 0.000027  loss: 0.1506  time: 1.4574  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2760/3000]  eta: 0:06:03  lr: 0.000027  loss: 0.1351  time: 1.4760  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2760/3000]  eta: 0:06:03  lr: 0.000027  loss: 0.7314  time: 1.4758  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2765/3000]  eta: 0:05:56  lr: 0.000027  loss: 0.4673  time: 1.4755  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2765/3000]  eta: 0:05:56  lr: 0.000027  loss: 0.1989  time: 1.4752  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2770/3000]  eta: 0:05:48  lr: 0.000027  loss: 0.1448  time: 1.4793  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2770/3000]  eta: 0:05:48  lr: 0.000027  loss: 0.4095  time: 1.4790  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2775/3000]  eta: 0:05:41  lr: 0.000027  loss: 0.1903  time: 1.4908  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2775/3000]  eta: 0:05:41  lr: 0.000027  loss: 0.5538  time: 1.4906  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2780/3000]  eta: 0:05:33  lr: 0.000027  loss: 0.2869  time: 1.4906  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2780/3000]  eta: 0:05:33  lr: 0.000027  loss: 0.6264  time: 1.4905  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2785/3000]  eta: 0:05:25  lr: 0.000027  loss: 0.4750  time: 1.4961  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2785/3000]  eta: 0:05:25  lr: 0.000027  loss: 0.3453  time: 1.4958  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2790/3000]  eta: 0:05:18  lr: 0.000027  loss: 0.7750  time: 1.4958  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2790/3000]  eta: 0:05:18  lr: 0.000027  loss: 0.2068  time: 1.4955  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2795/3000]  eta: 0:05:10  lr: 0.000027  loss: 0.9449  time: 1.4928  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2795/3000]  eta: 0:05:10  lr: 0.000027  loss: 0.3824  time: 1.4926  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2800/3000]  eta: 0:05:03  lr: 0.000027  loss: 0.1265  time: 1.4748  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2800/3000]  eta: 0:05:03  lr: 0.000027  loss: 0.2407  time: 1.4745  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2805/3000]  eta: 0:04:55  lr: 0.000027  loss: 0.3400  time: 1.4675  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2805/3000]  eta: 0:04:55  lr: 0.000027  loss: 0.5572  time: 1.4672  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2810/3000]  eta: 0:04:47  lr: 0.000027  loss: 0.6077  time: 1.4646  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2810/3000]  eta: 0:04:47  lr: 0.000027  loss: 0.5707  time: 1.4644  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2815/3000]  eta: 0:04:40  lr: 0.000027  loss: 0.5207  time: 1.4616  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2815/3000]  eta: 0:04:40  lr: 0.000027  loss: 0.1719  time: 1.4614  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2820/3000]  eta: 0:04:32  lr: 0.000027  loss: 0.0683  time: 1.4719  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2820/3000]  eta: 0:04:32  lr: 0.000027  loss: 0.5683  time: 1.4717  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2825/3000]  eta: 0:04:25  lr: 0.000027  loss: 0.4860  time: 1.4814  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2825/3000]  eta: 0:04:25  lr: 0.000027  loss: 0.1159  time: 1.4812  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2830/3000]  eta: 0:04:17  lr: 0.000027  loss: 0.6029  time: 1.4814  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2830/3000]  eta: 0:04:17  lr: 0.000027  loss: 0.2011  time: 1.4813  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2835/3000]  eta: 0:04:10  lr: 0.000027  loss: 0.2052  time: 1.4974  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2835/3000]  eta: 0:04:09  lr: 0.000027  loss: 0.6127  time: 1.4971  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2840/3000]  eta: 0:04:02  lr: 0.000027  loss: 0.1950  time: 1.5062  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2840/3000]  eta: 0:04:02  lr: 0.000027  loss: 0.3974  time: 1.5060  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2845/3000]  eta: 0:03:54  lr: 0.000027  loss: 0.3786  time: 1.4950  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2845/3000]  eta: 0:03:54  lr: 0.000027  loss: 0.3147  time: 1.4947  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2850/3000]  eta: 0:03:47  lr: 0.000027  loss: 0.3223  time: 1.4812  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2850/3000]  eta: 0:03:47  lr: 0.000027  loss: 0.9286  time: 1.4809  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2855/3000]  eta: 0:03:39  lr: 0.000027  loss: 0.1292  time: 1.4763  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2855/3000]  eta: 0:03:39  lr: 0.000027  loss: 0.5719  time: 1.4762  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2860/3000]  eta: 0:03:32  lr: 0.000027  loss: 0.3394  time: 1.4808  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2860/3000]  eta: 0:03:32  lr: 0.000027  loss: 0.1222  time: 1.4806  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2865/3000]  eta: 0:03:24  lr: 0.000027  loss: 0.3266  time: 1.4818  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2865/3000]  eta: 0:03:24  lr: 0.000027  loss: 0.2700  time: 1.4816  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2870/3000]  eta: 0:03:16  lr: 0.000027  loss: 0.3108  time: 1.4900  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2870/3000]  eta: 0:03:16  lr: 0.000027  loss: 0.2593  time: 1.4898  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2875/3000]  eta: 0:03:09  lr: 0.000027  loss: 0.1913  time: 1.5003  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2875/3000]  eta: 0:03:09  lr: 0.000027  loss: 0.4673  time: 1.5000  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2880/3000]  eta: 0:03:01  lr: 0.000027  loss: 0.1354  time: 1.5078  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2880/3000]  eta: 0:03:01  lr: 0.000027  loss: 0.3552  time: 1.5076  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2885/3000]  eta: 0:02:54  lr: 0.000027  loss: 0.4185  time: 1.5125  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2885/3000]  eta: 0:02:54  lr: 0.000027  loss: 0.4459  time: 1.5123  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2890/3000]  eta: 0:02:46  lr: 0.000027  loss: 0.5354  time: 1.5347  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2890/3000]  eta: 0:02:46  lr: 0.000027  loss: 0.4055  time: 1.5345  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2895/3000]  eta: 0:02:39  lr: 0.000027  loss: 0.3358  time: 1.5173  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2895/3000]  eta: 0:02:39  lr: 0.000027  loss: 0.1472  time: 1.5170  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2900/3000]  eta: 0:02:31  lr: 0.000027  loss: 0.3274  time: 1.4986  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2900/3000]  eta: 0:02:31  lr: 0.000027  loss: 0.3804  time: 1.4973  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2905/3000]  eta: 0:02:23  lr: 0.000027  loss: 0.3783  time: 1.5181  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2905/3000]  eta: 0:02:23  lr: 0.000027  loss: 0.8744  time: 1.5168  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2910/3000]  eta: 0:02:16  lr: 0.000027  loss: 0.8398  time: 1.5110  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2910/3000]  eta: 0:02:16  lr: 0.000027  loss: 0.6092  time: 1.5097  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2915/3000]  eta: 0:02:08  lr: 0.000027  loss: 0.3359  time: 1.5100  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2915/3000]  eta: 0:02:08  lr: 0.000027  loss: 0.1074  time: 1.5087  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2920/3000]  eta: 0:02:01  lr: 0.000027  loss: 0.1384  time: 1.5255  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2920/3000]  eta: 0:02:01  lr: 0.000027  loss: 0.6463  time: 1.5252  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2925/3000]  eta: 0:01:53  lr: 0.000027  loss: 0.2629  time: 1.5057  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2925/3000]  eta: 0:01:53  lr: 0.000027  loss: 0.3587  time: 1.5055  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2930/3000]  eta: 0:01:46  lr: 0.000027  loss: 0.1902  time: 1.4922  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2930/3000]  eta: 0:01:46  lr: 0.000027  loss: 0.1587  time: 1.4920  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2935/3000]  eta: 0:01:38  lr: 0.000027  loss: 0.3091  time: 1.5084  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2935/3000]  eta: 0:01:38  lr: 0.000027  loss: 0.4004  time: 1.5082  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2940/3000]  eta: 0:01:30  lr: 0.000027  loss: 0.7694  time: 1.4860  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2940/3000]  eta: 0:01:30  lr: 0.000027  loss: 0.1837  time: 1.4857  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2945/3000]  eta: 0:01:23  lr: 0.000027  loss: 0.4806  time: 1.4814  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2945/3000]  eta: 0:01:23  lr: 0.000027  loss: 0.2763  time: 1.4818  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2950/3000]  eta: 0:01:15  lr: 0.000027  loss: 0.1290  time: 1.4924  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2950/3000]  eta: 0:01:15  lr: 0.000027  loss: 0.2438  time: 1.4921  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2955/3000]  eta: 0:01:08  lr: 0.000027  loss: 0.1516  time: 1.5051  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2955/3000]  eta: 0:01:08  lr: 0.000027  loss: 0.9752  time: 1.5048  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2960/3000]  eta: 0:01:00  lr: 0.000027  loss: 0.4577  time: 1.5190  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2960/3000]  eta: 0:01:00  lr: 0.000027  loss: 0.6843  time: 1.5187  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2965/3000]  eta: 0:00:53  lr: 0.000027  loss: 0.7101  time: 1.5391  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2965/3000]  eta: 0:00:53  lr: 0.000027  loss: 0.0807  time: 1.5389  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2970/3000]  eta: 0:00:45  lr: 0.000027  loss: 0.1385  time: 1.5542  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2970/3000]  eta: 0:00:45  lr: 0.000027  loss: 0.8039  time: 1.5539  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2975/3000]  eta: 0:00:37  lr: 0.000027  loss: 0.1602  time: 1.5402  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2975/3000]  eta: 0:00:37  lr: 0.000027  loss: 0.2198  time: 1.5399  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2980/3000]  eta: 0:00:30  lr: 0.000027  loss: 0.1656  time: 1.5407  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2980/3000]  eta: 0:00:30  lr: 0.000027  loss: 0.2976  time: 1.5404  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2985/3000]  eta: 0:00:22  lr: 0.000027  loss: 0.2893  time: 1.5178  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2985/3000]  eta: 0:00:22  lr: 0.000027  loss: 0.1959  time: 1.5177  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2990/3000]  eta: 0:00:15  lr: 0.000027  loss: 0.5149  time: 1.5111  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2990/3000]  eta: 0:00:15  lr: 0.000027  loss: 0.3622  time: 1.5108  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2995/3000]  eta: 0:00:07  lr: 0.000027  loss: 0.1917  time: 1.5044  data: 0.0000  max mem: 18432
Train: data epoch: [6]  [2995/3000]  eta: 0:00:07  lr: 0.000027  loss: 0.1044  time: 1.5042  data: 0.0000  max mem: 18190
Train: data epoch: [6]  [2999/3000]  eta: 0:00:01  lr: 0.000027  loss: 0.6315  time: 1.5072  data: 0.0000  max mem: 18432
Train: data epoch: [6] Total time: 1:15:45 (1.5152 s / it)
Train: data epoch: [6]  [2999/3000]  eta: 0:00:01  lr: 0.000027  loss: 0.3570  time: 1.5070  data: 0.0000  max mem: 18190
Train: data epoch: [6] Total time: 1:15:45 (1.5152 s / it)
2025-01-19 08:30:44,176 [INFO] Averaged stats: lr: 0.0000  loss: 0.4311
2025-01-19 08:30:44,182 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [6]  [0/1]  eta: 0:00:00    time: 0.7852  data: 0.4981  max mem: 18190
Eval: data epoch: [6] Total time: 0:00:00 (0.9001 s / it)
Eval: data epoch: [6]  [0/1]  eta: 0:00:00    time: 0.9470  data: 0.6561  max mem: 18432
Eval: data epoch: [6] Total time: 0:00:01 (1.0900 s / it)
2025-01-19 08:30:45,297 [INFO] Saving checkpoint at epoch 6 to outputs_stage1_only/202501182338/checkpoint_6.pth.
2025-01-19 08:30:47,673 [INFO] Training Phase
2025-01-19 08:30:47,681 [INFO] Start training epoch 7, 3000 iters per inner epoch.
Train: data epoch: [7]  [   0/3000]  eta: 1:23:26  lr: 0.000027  loss: 0.5144  time: 1.6687  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [   0/3000]  eta: 1:23:22  lr: 0.000027  loss: 0.4756  time: 1.6676  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [   5/3000]  eta: 1:19:37  lr: 0.000027  loss: 0.5663  time: 1.5952  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [   5/3000]  eta: 1:19:36  lr: 0.000027  loss: 0.4005  time: 1.5948  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [  10/3000]  eta: 1:18:13  lr: 0.000027  loss: 0.0899  time: 1.5697  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [  10/3000]  eta: 1:18:12  lr: 0.000027  loss: 0.2511  time: 1.5695  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [  15/3000]  eta: 1:17:50  lr: 0.000027  loss: 0.5231  time: 1.5645  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [  15/3000]  eta: 1:17:49  lr: 0.000027  loss: 1.0500  time: 1.5642  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [  20/3000]  eta: 1:15:27  lr: 0.000027  loss: 0.4209  time: 1.5118  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [  20/3000]  eta: 1:15:26  lr: 0.000027  loss: 0.0915  time: 1.5115  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [  25/3000]  eta: 1:15:44  lr: 0.000027  loss: 0.3278  time: 1.5071  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [  25/3000]  eta: 1:15:43  lr: 0.000027  loss: 0.0566  time: 1.5068  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [  30/3000]  eta: 1:15:15  lr: 0.000027  loss: 0.5364  time: 1.4935  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [  30/3000]  eta: 1:15:15  lr: 0.000027  loss: 0.3499  time: 1.4932  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [  35/3000]  eta: 1:15:33  lr: 0.000027  loss: 0.7365  time: 1.5007  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [  35/3000]  eta: 1:15:32  lr: 0.000027  loss: 0.1598  time: 1.5004  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [  40/3000]  eta: 1:15:11  lr: 0.000027  loss: 0.1018  time: 1.5290  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [  40/3000]  eta: 1:15:10  lr: 0.000027  loss: 0.5129  time: 1.5287  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [  45/3000]  eta: 1:14:53  lr: 0.000027  loss: 0.3122  time: 1.5120  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [  45/3000]  eta: 1:14:52  lr: 0.000027  loss: 0.7473  time: 1.5117  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [  50/3000]  eta: 1:14:48  lr: 0.000027  loss: 0.4908  time: 1.5232  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [  50/3000]  eta: 1:14:47  lr: 0.000027  loss: 0.3892  time: 1.5230  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [  55/3000]  eta: 1:14:27  lr: 0.000027  loss: 0.8104  time: 1.4951  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [  55/3000]  eta: 1:14:26  lr: 0.000027  loss: 0.6065  time: 1.4948  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [  60/3000]  eta: 1:14:12  lr: 0.000027  loss: 0.1785  time: 1.4950  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [  60/3000]  eta: 1:14:11  lr: 0.000027  loss: 0.2113  time: 1.4948  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [  65/3000]  eta: 1:14:04  lr: 0.000027  loss: 0.7196  time: 1.4990  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [  65/3000]  eta: 1:14:03  lr: 0.000027  loss: 0.6428  time: 1.4988  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [  70/3000]  eta: 1:13:46  lr: 0.000027  loss: 0.2811  time: 1.4828  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [  70/3000]  eta: 1:13:45  lr: 0.000027  loss: 0.4166  time: 1.4826  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [  75/3000]  eta: 1:13:41  lr: 0.000027  loss: 0.2543  time: 1.4972  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [  75/3000]  eta: 1:13:41  lr: 0.000027  loss: 0.1720  time: 1.4971  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [  80/3000]  eta: 1:13:41  lr: 0.000027  loss: 0.3515  time: 1.5129  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [  80/3000]  eta: 1:13:40  lr: 0.000027  loss: 0.3940  time: 1.5126  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [  85/3000]  eta: 1:13:27  lr: 0.000027  loss: 0.3888  time: 1.5043  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [  85/3000]  eta: 1:13:26  lr: 0.000027  loss: 0.3061  time: 1.5040  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [  90/3000]  eta: 1:13:31  lr: 0.000027  loss: 0.2371  time: 1.5346  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [  90/3000]  eta: 1:13:30  lr: 0.000027  loss: 0.8458  time: 1.5343  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [  95/3000]  eta: 1:13:34  lr: 0.000027  loss: 0.4089  time: 1.5489  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [  95/3000]  eta: 1:13:33  lr: 0.000027  loss: 0.6249  time: 1.5486  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 100/3000]  eta: 1:13:14  lr: 0.000027  loss: 0.1576  time: 1.5201  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 100/3000]  eta: 1:13:13  lr: 0.000027  loss: 0.3600  time: 1.5199  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 105/3000]  eta: 1:13:02  lr: 0.000027  loss: 0.2157  time: 1.5222  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 105/3000]  eta: 1:13:01  lr: 0.000027  loss: 0.2601  time: 1.5220  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 110/3000]  eta: 1:12:57  lr: 0.000027  loss: 0.3923  time: 1.5093  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 110/3000]  eta: 1:12:56  lr: 0.000027  loss: 0.1532  time: 1.5091  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 115/3000]  eta: 1:12:41  lr: 0.000027  loss: 0.2882  time: 1.4752  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 115/3000]  eta: 1:12:40  lr: 0.000027  loss: 0.4429  time: 1.4750  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 120/3000]  eta: 1:12:21  lr: 0.000027  loss: 0.0867  time: 1.4689  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 120/3000]  eta: 1:12:21  lr: 0.000027  loss: 0.1714  time: 1.4686  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 125/3000]  eta: 1:12:22  lr: 0.000027  loss: 0.1596  time: 1.4934  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 125/3000]  eta: 1:12:22  lr: 0.000027  loss: 1.1945  time: 1.4932  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 130/3000]  eta: 1:12:19  lr: 0.000027  loss: 0.4687  time: 1.4967  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 130/3000]  eta: 1:12:18  lr: 0.000027  loss: 0.2249  time: 1.4965  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 135/3000]  eta: 1:12:13  lr: 0.000027  loss: 0.6095  time: 1.5157  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 135/3000]  eta: 1:12:12  lr: 0.000027  loss: 0.3665  time: 1.5155  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 140/3000]  eta: 1:12:05  lr: 0.000027  loss: 0.3755  time: 1.5407  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 140/3000]  eta: 1:12:04  lr: 0.000027  loss: 0.4975  time: 1.5406  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 145/3000]  eta: 1:11:59  lr: 0.000027  loss: 0.3836  time: 1.5277  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 145/3000]  eta: 1:11:58  lr: 0.000027  loss: 0.3513  time: 1.5274  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 150/3000]  eta: 1:11:48  lr: 0.000027  loss: 0.1181  time: 1.5111  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 150/3000]  eta: 1:11:48  lr: 0.000027  loss: 0.2453  time: 1.5108  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 155/3000]  eta: 1:11:38  lr: 0.000027  loss: 0.1362  time: 1.5008  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 155/3000]  eta: 1:11:37  lr: 0.000027  loss: 0.4499  time: 1.5005  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 160/3000]  eta: 1:11:34  lr: 0.000027  loss: 0.3804  time: 1.5117  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 160/3000]  eta: 1:11:34  lr: 0.000027  loss: 0.3691  time: 1.5113  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 165/3000]  eta: 1:11:24  lr: 0.000027  loss: 0.8394  time: 1.4998  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 165/3000]  eta: 1:11:23  lr: 0.000027  loss: 0.1577  time: 1.4995  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 170/3000]  eta: 1:11:16  lr: 0.000027  loss: 0.3598  time: 1.5058  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 170/3000]  eta: 1:11:15  lr: 0.000027  loss: 0.5211  time: 1.5055  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 175/3000]  eta: 1:11:10  lr: 0.000027  loss: 0.5944  time: 1.5171  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 175/3000]  eta: 1:11:09  lr: 0.000027  loss: 0.5548  time: 1.5169  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 180/3000]  eta: 1:11:05  lr: 0.000027  loss: 0.4697  time: 1.5158  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 180/3000]  eta: 1:11:04  lr: 0.000027  loss: 0.4615  time: 1.5156  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 185/3000]  eta: 1:10:55  lr: 0.000027  loss: 0.2659  time: 1.5146  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 185/3000]  eta: 1:10:54  lr: 0.000027  loss: 0.3105  time: 1.5143  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 190/3000]  eta: 1:10:47  lr: 0.000027  loss: 0.5327  time: 1.5142  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 190/3000]  eta: 1:10:46  lr: 0.000027  loss: 0.3013  time: 1.5139  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 195/3000]  eta: 1:10:41  lr: 0.000027  loss: 0.5149  time: 1.5165  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 195/3000]  eta: 1:10:40  lr: 0.000027  loss: 0.1820  time: 1.5162  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 200/3000]  eta: 1:10:34  lr: 0.000027  loss: 0.2528  time: 1.5112  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 200/3000]  eta: 1:10:34  lr: 0.000027  loss: 0.2429  time: 1.5110  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 205/3000]  eta: 1:10:23  lr: 0.000027  loss: 0.1252  time: 1.5062  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 205/3000]  eta: 1:10:22  lr: 0.000027  loss: 0.2299  time: 1.5060  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 210/3000]  eta: 1:10:21  lr: 0.000027  loss: 0.3091  time: 1.5281  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 210/3000]  eta: 1:10:20  lr: 0.000027  loss: 0.1297  time: 1.5279  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 215/3000]  eta: 1:10:17  lr: 0.000027  loss: 0.1558  time: 1.5359  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 215/3000]  eta: 1:10:16  lr: 0.000027  loss: 0.8639  time: 1.5357  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 220/3000]  eta: 1:10:10  lr: 0.000027  loss: 0.4149  time: 1.5358  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 220/3000]  eta: 1:10:09  lr: 0.000027  loss: 0.5977  time: 1.5355  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 225/3000]  eta: 1:10:03  lr: 0.000027  loss: 0.2814  time: 1.5504  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 225/3000]  eta: 1:10:02  lr: 0.000027  loss: 0.0715  time: 1.5501  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 230/3000]  eta: 1:09:56  lr: 0.000027  loss: 0.1559  time: 1.5341  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 230/3000]  eta: 1:09:55  lr: 0.000027  loss: 0.2493  time: 1.5339  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 235/3000]  eta: 1:09:48  lr: 0.000027  loss: 0.3344  time: 1.5189  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 235/3000]  eta: 1:09:47  lr: 0.000027  loss: 0.4958  time: 1.5186  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 240/3000]  eta: 1:09:42  lr: 0.000027  loss: 0.3625  time: 1.5233  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 240/3000]  eta: 1:09:41  lr: 0.000027  loss: 0.2911  time: 1.5231  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 245/3000]  eta: 1:09:37  lr: 0.000027  loss: 0.8955  time: 1.5360  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 245/3000]  eta: 1:09:36  lr: 0.000027  loss: 0.2440  time: 1.5357  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 250/3000]  eta: 1:09:30  lr: 0.000027  loss: 0.9516  time: 1.5376  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 250/3000]  eta: 1:09:30  lr: 0.000027  loss: 0.6205  time: 1.5373  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 255/3000]  eta: 1:09:24  lr: 0.000027  loss: 0.2277  time: 1.5445  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 255/3000]  eta: 1:09:23  lr: 0.000027  loss: 0.3943  time: 1.5442  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 260/3000]  eta: 1:09:18  lr: 0.000027  loss: 0.1477  time: 1.5485  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 260/3000]  eta: 1:09:18  lr: 0.000027  loss: 0.3341  time: 1.5482  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 265/3000]  eta: 1:09:09  lr: 0.000027  loss: 0.4204  time: 1.5296  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 265/3000]  eta: 1:09:09  lr: 0.000027  loss: 0.0790  time: 1.5293  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 270/3000]  eta: 1:09:05  lr: 0.000027  loss: 0.3575  time: 1.5412  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 270/3000]  eta: 1:09:04  lr: 0.000027  loss: 0.3857  time: 1.5409  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 275/3000]  eta: 1:08:57  lr: 0.000027  loss: 0.3280  time: 1.5327  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 275/3000]  eta: 1:08:56  lr: 0.000027  loss: 0.7640  time: 1.5324  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 280/3000]  eta: 1:08:52  lr: 0.000027  loss: 0.4979  time: 1.5390  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 280/3000]  eta: 1:08:51  lr: 0.000027  loss: 0.4543  time: 1.5387  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 285/3000]  eta: 1:08:47  lr: 0.000027  loss: 0.7508  time: 1.5590  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 285/3000]  eta: 1:08:46  lr: 0.000027  loss: 0.2592  time: 1.5586  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 290/3000]  eta: 1:08:39  lr: 0.000027  loss: 0.3515  time: 1.5409  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 290/3000]  eta: 1:08:38  lr: 0.000027  loss: 0.3173  time: 1.5405  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 295/3000]  eta: 1:08:30  lr: 0.000027  loss: 0.5354  time: 1.5396  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 295/3000]  eta: 1:08:29  lr: 0.000027  loss: 0.1454  time: 1.5393  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 300/3000]  eta: 1:08:20  lr: 0.000027  loss: 0.2021  time: 1.5098  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 300/3000]  eta: 1:08:19  lr: 0.000027  loss: 0.6095  time: 1.5085  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 305/3000]  eta: 1:08:13  lr: 0.000027  loss: 0.6415  time: 1.4981  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 305/3000]  eta: 1:08:12  lr: 0.000027  loss: 0.6278  time: 1.4969  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 310/3000]  eta: 1:08:07  lr: 0.000027  loss: 0.4460  time: 1.5133  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 310/3000]  eta: 1:08:06  lr: 0.000027  loss: 0.2918  time: 1.5120  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 315/3000]  eta: 1:08:00  lr: 0.000027  loss: 0.4815  time: 1.5227  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 315/3000]  eta: 1:07:59  lr: 0.000027  loss: 0.0722  time: 1.5215  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 320/3000]  eta: 1:07:55  lr: 0.000027  loss: 0.1972  time: 1.5502  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 320/3000]  eta: 1:07:54  lr: 0.000027  loss: 0.5443  time: 1.5500  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 325/3000]  eta: 1:07:50  lr: 0.000027  loss: 0.4506  time: 1.5679  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 325/3000]  eta: 1:07:50  lr: 0.000027  loss: 0.3565  time: 1.5676  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 330/3000]  eta: 1:07:43  lr: 0.000027  loss: 0.7838  time: 1.5559  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 330/3000]  eta: 1:07:42  lr: 0.000027  loss: 0.2937  time: 1.5556  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 335/3000]  eta: 1:07:36  lr: 0.000027  loss: 0.7191  time: 1.5557  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 335/3000]  eta: 1:07:35  lr: 0.000027  loss: 0.0818  time: 1.5554  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 340/3000]  eta: 1:07:26  lr: 0.000027  loss: 0.6687  time: 1.5325  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 340/3000]  eta: 1:07:26  lr: 0.000027  loss: 0.4716  time: 1.5322  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 345/3000]  eta: 1:07:18  lr: 0.000027  loss: 0.3953  time: 1.5075  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 345/3000]  eta: 1:07:17  lr: 0.000027  loss: 0.1222  time: 1.5073  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 350/3000]  eta: 1:07:12  lr: 0.000027  loss: 0.6538  time: 1.5205  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 350/3000]  eta: 1:07:11  lr: 0.000027  loss: 0.4519  time: 1.5202  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 355/3000]  eta: 1:07:06  lr: 0.000027  loss: 0.5685  time: 1.5250  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 355/3000]  eta: 1:07:05  lr: 0.000027  loss: 0.5045  time: 1.5248  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 360/3000]  eta: 1:06:59  lr: 0.000027  loss: 0.4793  time: 1.5433  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 360/3000]  eta: 1:06:58  lr: 0.000027  loss: 0.3096  time: 1.5431  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 365/3000]  eta: 1:06:50  lr: 0.000027  loss: 0.1521  time: 1.5400  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 365/3000]  eta: 1:06:49  lr: 0.000027  loss: 0.1528  time: 1.5398  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 370/3000]  eta: 1:06:42  lr: 0.000027  loss: 0.9633  time: 1.5249  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 370/3000]  eta: 1:06:41  lr: 0.000027  loss: 0.1104  time: 1.5247  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 375/3000]  eta: 1:06:36  lr: 0.000027  loss: 1.0515  time: 1.5259  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 375/3000]  eta: 1:06:35  lr: 0.000027  loss: 0.5410  time: 1.5256  data: 0.0000  max mem: 18190
Train: data epoch: [7]  [ 380/3000]  eta: 1:06:28  lr: 0.000027  loss: 0.4337  time: 1.5170  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 380/3000]  eta: 1:06:27  lr: 0.000027  loss: 0.2053  time: 1.5167  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 385/3000]  eta: 1:06:20  lr: 0.000027  loss: 0.9311  time: 1.5279  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 385/3000]  eta: 1:06:20  lr: 0.000027  loss: 0.2332  time: 1.5277  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 390/3000]  eta: 1:06:12  lr: 0.000027  loss: 0.4739  time: 1.5232  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 390/3000]  eta: 1:06:11  lr: 0.000027  loss: 0.7056  time: 1.5229  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 395/3000]  eta: 1:06:03  lr: 0.000027  loss: 0.3266  time: 1.5067  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 395/3000]  eta: 1:06:02  lr: 0.000027  loss: 0.7361  time: 1.5065  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 400/3000]  eta: 1:05:55  lr: 0.000027  loss: 0.7527  time: 1.5041  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 400/3000]  eta: 1:05:54  lr: 0.000027  loss: 0.2988  time: 1.5039  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 405/3000]  eta: 1:05:45  lr: 0.000027  loss: 0.5027  time: 1.4859  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 405/3000]  eta: 1:05:45  lr: 0.000027  loss: 0.2804  time: 1.4856  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 410/3000]  eta: 1:05:38  lr: 0.000027  loss: 0.2019  time: 1.4952  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 410/3000]  eta: 1:05:37  lr: 0.000027  loss: 0.9098  time: 1.4950  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 415/3000]  eta: 1:05:29  lr: 0.000027  loss: 0.4090  time: 1.4952  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 415/3000]  eta: 1:05:29  lr: 0.000027  loss: 0.1697  time: 1.4949  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 420/3000]  eta: 1:05:23  lr: 0.000027  loss: 0.3145  time: 1.5057  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 420/3000]  eta: 1:05:22  lr: 0.000027  loss: 0.3179  time: 1.5055  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 425/3000]  eta: 1:05:14  lr: 0.000027  loss: 0.5750  time: 1.5108  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 425/3000]  eta: 1:05:13  lr: 0.000027  loss: 0.6131  time: 1.5105  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 430/3000]  eta: 1:05:03  lr: 0.000027  loss: 0.7887  time: 1.4846  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 430/3000]  eta: 1:05:02  lr: 0.000027  loss: 0.5066  time: 1.4843  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 435/3000]  eta: 1:04:54  lr: 0.000027  loss: 0.3177  time: 1.4751  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 435/3000]  eta: 1:04:53  lr: 0.000027  loss: 0.6221  time: 1.4749  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 440/3000]  eta: 1:04:44  lr: 0.000027  loss: 0.4443  time: 1.4467  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 440/3000]  eta: 1:04:43  lr: 0.000027  loss: 0.6877  time: 1.4463  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 445/3000]  eta: 1:04:38  lr: 0.000027  loss: 0.6092  time: 1.4765  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 445/3000]  eta: 1:04:38  lr: 0.000027  loss: 0.9084  time: 1.4763  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 450/3000]  eta: 1:04:33  lr: 0.000027  loss: 0.3113  time: 1.5178  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 450/3000]  eta: 1:04:32  lr: 0.000027  loss: 0.6121  time: 1.5175  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 455/3000]  eta: 1:04:25  lr: 0.000027  loss: 0.1412  time: 1.5300  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 455/3000]  eta: 1:04:24  lr: 0.000027  loss: 0.3306  time: 1.5297  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 460/3000]  eta: 1:04:15  lr: 0.000027  loss: 0.3154  time: 1.5320  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 460/3000]  eta: 1:04:14  lr: 0.000027  loss: 0.3446  time: 1.5318  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 465/3000]  eta: 1:04:07  lr: 0.000027  loss: 0.4004  time: 1.5132  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 465/3000]  eta: 1:04:07  lr: 0.000027  loss: 0.3776  time: 1.5129  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 470/3000]  eta: 1:04:01  lr: 0.000027  loss: 0.2301  time: 1.5016  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 470/3000]  eta: 1:04:00  lr: 0.000027  loss: 0.2282  time: 1.5014  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 475/3000]  eta: 1:03:53  lr: 0.000027  loss: 0.3429  time: 1.5087  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 475/3000]  eta: 1:03:52  lr: 0.000027  loss: 0.8375  time: 1.5084  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 480/3000]  eta: 1:03:46  lr: 0.000027  loss: 0.2580  time: 1.5331  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 480/3000]  eta: 1:03:46  lr: 0.000027  loss: 0.1688  time: 1.5328  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 485/3000]  eta: 1:03:37  lr: 0.000027  loss: 0.3659  time: 1.5210  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 485/3000]  eta: 1:03:37  lr: 0.000027  loss: 0.7288  time: 1.5208  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 490/3000]  eta: 1:03:30  lr: 0.000027  loss: 0.2841  time: 1.5151  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 490/3000]  eta: 1:03:29  lr: 0.000027  loss: 0.5212  time: 1.5149  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 495/3000]  eta: 1:03:21  lr: 0.000027  loss: 0.2575  time: 1.5025  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 495/3000]  eta: 1:03:20  lr: 0.000027  loss: 0.5554  time: 1.5024  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 500/3000]  eta: 1:03:11  lr: 0.000027  loss: 0.0788  time: 1.4733  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 500/3000]  eta: 1:03:11  lr: 0.000027  loss: 0.1108  time: 1.4731  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 505/3000]  eta: 1:03:05  lr: 0.000027  loss: 0.0992  time: 1.4960  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 505/3000]  eta: 1:03:04  lr: 0.000027  loss: 0.3653  time: 1.4958  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 510/3000]  eta: 1:02:58  lr: 0.000027  loss: 0.3385  time: 1.5049  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 510/3000]  eta: 1:02:57  lr: 0.000027  loss: 0.6945  time: 1.5046  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 515/3000]  eta: 1:02:48  lr: 0.000027  loss: 0.4861  time: 1.4920  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 515/3000]  eta: 1:02:48  lr: 0.000027  loss: 0.5772  time: 1.4917  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 520/3000]  eta: 1:02:40  lr: 0.000027  loss: 0.3049  time: 1.5065  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 520/3000]  eta: 1:02:39  lr: 0.000027  loss: 0.2468  time: 1.5062  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 525/3000]  eta: 1:02:32  lr: 0.000027  loss: 0.7346  time: 1.4862  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 525/3000]  eta: 1:02:31  lr: 0.000027  loss: 0.4748  time: 1.4858  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 530/3000]  eta: 1:02:24  lr: 0.000027  loss: 0.4610  time: 1.4790  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 530/3000]  eta: 1:02:24  lr: 0.000027  loss: 0.1530  time: 1.4788  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 535/3000]  eta: 1:02:18  lr: 0.000027  loss: 0.4527  time: 1.5173  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 535/3000]  eta: 1:02:17  lr: 0.000027  loss: 0.1908  time: 1.5170  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 540/3000]  eta: 1:02:09  lr: 0.000027  loss: 0.8423  time: 1.5064  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 540/3000]  eta: 1:02:08  lr: 0.000027  loss: 0.2658  time: 1.5062  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 545/3000]  eta: 1:02:00  lr: 0.000027  loss: 0.2558  time: 1.5060  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 545/3000]  eta: 1:02:00  lr: 0.000027  loss: 0.4983  time: 1.5058  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 550/3000]  eta: 1:01:53  lr: 0.000027  loss: 0.3405  time: 1.5076  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 550/3000]  eta: 1:01:52  lr: 0.000027  loss: 0.3108  time: 1.5073  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 555/3000]  eta: 1:01:44  lr: 0.000027  loss: 0.2890  time: 1.4693  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 555/3000]  eta: 1:01:43  lr: 0.000027  loss: 0.2020  time: 1.4690  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 560/3000]  eta: 1:01:37  lr: 0.000027  loss: 0.3590  time: 1.4975  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 560/3000]  eta: 1:01:36  lr: 0.000027  loss: 0.2006  time: 1.4972  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 565/3000]  eta: 1:01:30  lr: 0.000027  loss: 0.6356  time: 1.5110  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 565/3000]  eta: 1:01:29  lr: 0.000027  loss: 1.0302  time: 1.5108  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 570/3000]  eta: 1:01:22  lr: 0.000027  loss: 0.1009  time: 1.5071  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 570/3000]  eta: 1:01:21  lr: 0.000027  loss: 0.5512  time: 1.5069  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 575/3000]  eta: 1:01:14  lr: 0.000027  loss: 0.2903  time: 1.5276  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 575/3000]  eta: 1:01:14  lr: 0.000027  loss: 0.5829  time: 1.5273  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 580/3000]  eta: 1:01:06  lr: 0.000027  loss: 0.3009  time: 1.5130  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 580/3000]  eta: 1:01:06  lr: 0.000027  loss: 0.5876  time: 1.5127  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 585/3000]  eta: 1:00:58  lr: 0.000027  loss: 0.5879  time: 1.5019  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 585/3000]  eta: 1:00:58  lr: 0.000027  loss: 0.3952  time: 1.5016  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 590/3000]  eta: 1:00:51  lr: 0.000027  loss: 0.3141  time: 1.5054  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 590/3000]  eta: 1:00:50  lr: 0.000027  loss: 0.5751  time: 1.5051  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 595/3000]  eta: 1:00:41  lr: 0.000027  loss: 0.7988  time: 1.4820  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 595/3000]  eta: 1:00:41  lr: 0.000027  loss: 0.1148  time: 1.4818  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 600/3000]  eta: 1:00:33  lr: 0.000027  loss: 0.1340  time: 1.4772  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 600/3000]  eta: 1:00:32  lr: 0.000027  loss: 0.7020  time: 1.4771  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 605/3000]  eta: 1:00:27  lr: 0.000027  loss: 0.8209  time: 1.4995  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 605/3000]  eta: 1:00:26  lr: 0.000027  loss: 0.2361  time: 1.4992  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 610/3000]  eta: 1:00:20  lr: 0.000027  loss: 0.6764  time: 1.5003  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 610/3000]  eta: 1:00:19  lr: 0.000027  loss: 0.2947  time: 1.5001  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 615/3000]  eta: 1:00:13  lr: 0.000027  loss: 0.2974  time: 1.5386  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 615/3000]  eta: 1:00:12  lr: 0.000027  loss: 0.3216  time: 1.5383  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 620/3000]  eta: 1:00:05  lr: 0.000027  loss: 0.2321  time: 1.5449  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 620/3000]  eta: 1:00:04  lr: 0.000027  loss: 0.4266  time: 1.5446  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 625/3000]  eta: 0:59:58  lr: 0.000027  loss: 0.8226  time: 1.5326  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 625/3000]  eta: 0:59:57  lr: 0.000027  loss: 0.3493  time: 1.5323  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 630/3000]  eta: 0:59:51  lr: 0.000027  loss: 0.8388  time: 1.5393  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 630/3000]  eta: 0:59:50  lr: 0.000027  loss: 0.5782  time: 1.5390  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 635/3000]  eta: 0:59:43  lr: 0.000027  loss: 1.1714  time: 1.5254  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 635/3000]  eta: 0:59:43  lr: 0.000027  loss: 0.4697  time: 1.5251  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 640/3000]  eta: 0:59:37  lr: 0.000027  loss: 0.4042  time: 1.5400  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 640/3000]  eta: 0:59:36  lr: 0.000027  loss: 0.9497  time: 1.5398  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 645/3000]  eta: 0:59:30  lr: 0.000027  loss: 0.1529  time: 1.5543  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 645/3000]  eta: 0:59:30  lr: 0.000027  loss: 0.2064  time: 1.5541  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 650/3000]  eta: 0:59:23  lr: 0.000027  loss: 0.2905  time: 1.5516  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 650/3000]  eta: 0:59:23  lr: 0.000027  loss: 0.1386  time: 1.5513  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 655/3000]  eta: 0:59:15  lr: 0.000027  loss: 0.1178  time: 1.5358  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 655/3000]  eta: 0:59:14  lr: 0.000027  loss: 0.7093  time: 1.5355  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 660/3000]  eta: 0:59:07  lr: 0.000027  loss: 0.1838  time: 1.5179  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 660/3000]  eta: 0:59:06  lr: 0.000027  loss: 0.3033  time: 1.5176  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 665/3000]  eta: 0:58:59  lr: 0.000027  loss: 0.5031  time: 1.4993  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 665/3000]  eta: 0:58:58  lr: 0.000027  loss: 0.2351  time: 1.4991  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 670/3000]  eta: 0:58:50  lr: 0.000027  loss: 0.4217  time: 1.4688  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 670/3000]  eta: 0:58:49  lr: 0.000027  loss: 0.2905  time: 1.4685  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 675/3000]  eta: 0:58:42  lr: 0.000027  loss: 0.2302  time: 1.4805  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 675/3000]  eta: 0:58:41  lr: 0.000027  loss: 0.2433  time: 1.4802  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 680/3000]  eta: 0:58:35  lr: 0.000027  loss: 0.1820  time: 1.4925  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 680/3000]  eta: 0:58:34  lr: 0.000027  loss: 0.2755  time: 1.4923  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 685/3000]  eta: 0:58:25  lr: 0.000027  loss: 0.2704  time: 1.4702  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 685/3000]  eta: 0:58:25  lr: 0.000027  loss: 0.2826  time: 1.4698  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 690/3000]  eta: 0:58:18  lr: 0.000027  loss: 0.1695  time: 1.4985  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 690/3000]  eta: 0:58:18  lr: 0.000027  loss: 0.3838  time: 1.4982  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 695/3000]  eta: 0:58:11  lr: 0.000027  loss: 0.1858  time: 1.5006  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 695/3000]  eta: 0:58:10  lr: 0.000027  loss: 0.0277  time: 1.5003  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 700/3000]  eta: 0:58:04  lr: 0.000027  loss: 0.3074  time: 1.5117  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 700/3000]  eta: 0:58:03  lr: 0.000027  loss: 0.2380  time: 1.5114  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 705/3000]  eta: 0:57:57  lr: 0.000027  loss: 0.7223  time: 1.5496  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 705/3000]  eta: 0:57:57  lr: 0.000027  loss: 0.1568  time: 1.5494  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 710/3000]  eta: 0:57:50  lr: 0.000027  loss: 0.4704  time: 1.5452  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 710/3000]  eta: 0:57:49  lr: 0.000027  loss: 0.2874  time: 1.5449  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 715/3000]  eta: 0:57:43  lr: 0.000027  loss: 0.3652  time: 1.5516  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 715/3000]  eta: 0:57:42  lr: 0.000027  loss: 0.2117  time: 1.5514  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 720/3000]  eta: 0:57:36  lr: 0.000027  loss: 0.4748  time: 1.5446  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 720/3000]  eta: 0:57:35  lr: 0.000027  loss: 0.3540  time: 1.5443  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 725/3000]  eta: 0:57:27  lr: 0.000027  loss: 0.5584  time: 1.5110  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 725/3000]  eta: 0:57:26  lr: 0.000027  loss: 0.1894  time: 1.5107  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 730/3000]  eta: 0:57:19  lr: 0.000027  loss: 0.1547  time: 1.5012  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 730/3000]  eta: 0:57:18  lr: 0.000027  loss: 0.2076  time: 1.5011  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 735/3000]  eta: 0:57:11  lr: 0.000027  loss: 0.3723  time: 1.5009  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 735/3000]  eta: 0:57:11  lr: 0.000027  loss: 0.7147  time: 1.5008  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 740/3000]  eta: 0:57:05  lr: 0.000027  loss: 0.7456  time: 1.5037  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 740/3000]  eta: 0:57:04  lr: 0.000027  loss: 0.5264  time: 1.5035  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 745/3000]  eta: 0:56:58  lr: 0.000027  loss: 0.3417  time: 1.5375  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 745/3000]  eta: 0:56:57  lr: 0.000027  loss: 0.1364  time: 1.5373  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 750/3000]  eta: 0:56:51  lr: 0.000027  loss: 0.7905  time: 1.5585  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 750/3000]  eta: 0:56:50  lr: 0.000027  loss: 0.2108  time: 1.5582  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 755/3000]  eta: 0:56:43  lr: 0.000027  loss: 0.5009  time: 1.5512  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 755/3000]  eta: 0:56:43  lr: 0.000027  loss: 0.4007  time: 1.5510  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 760/3000]  eta: 0:56:36  lr: 0.000027  loss: 0.0953  time: 1.5446  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 760/3000]  eta: 0:56:35  lr: 0.000027  loss: 0.5730  time: 1.5443  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 765/3000]  eta: 0:56:27  lr: 0.000027  loss: 0.2939  time: 1.5135  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 765/3000]  eta: 0:56:27  lr: 0.000027  loss: 0.6676  time: 1.5132  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 770/3000]  eta: 0:56:19  lr: 0.000027  loss: 0.3894  time: 1.4838  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 770/3000]  eta: 0:56:18  lr: 0.000027  loss: 0.9890  time: 1.4836  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 775/3000]  eta: 0:56:11  lr: 0.000027  loss: 0.2858  time: 1.4812  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 775/3000]  eta: 0:56:10  lr: 0.000027  loss: 0.8559  time: 1.4810  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 780/3000]  eta: 0:56:03  lr: 0.000027  loss: 0.1490  time: 1.4600  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 780/3000]  eta: 0:56:02  lr: 0.000027  loss: 0.1583  time: 1.4597  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 785/3000]  eta: 0:55:56  lr: 0.000027  loss: 0.5154  time: 1.4924  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 785/3000]  eta: 0:55:55  lr: 0.000027  loss: 0.0586  time: 1.4921  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 790/3000]  eta: 0:55:47  lr: 0.000027  loss: 0.1017  time: 1.4945  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 790/3000]  eta: 0:55:47  lr: 0.000027  loss: 0.1144  time: 1.4943  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 795/3000]  eta: 0:55:40  lr: 0.000027  loss: 0.4789  time: 1.5087  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 795/3000]  eta: 0:55:40  lr: 0.000027  loss: 0.4764  time: 1.5085  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 800/3000]  eta: 0:55:31  lr: 0.000027  loss: 0.2319  time: 1.5007  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 800/3000]  eta: 0:55:31  lr: 0.000027  loss: 0.1420  time: 1.5005  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 805/3000]  eta: 0:55:23  lr: 0.000027  loss: 0.3857  time: 1.4706  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 805/3000]  eta: 0:55:22  lr: 0.000027  loss: 0.4948  time: 1.4704  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 810/3000]  eta: 0:55:16  lr: 0.000027  loss: 0.4250  time: 1.5012  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 810/3000]  eta: 0:55:16  lr: 0.000027  loss: 0.2650  time: 1.5009  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 815/3000]  eta: 0:55:09  lr: 0.000027  loss: 0.6159  time: 1.4964  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 815/3000]  eta: 0:55:08  lr: 0.000027  loss: 0.5795  time: 1.4961  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 820/3000]  eta: 0:55:01  lr: 0.000027  loss: 0.4124  time: 1.5142  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 820/3000]  eta: 0:55:01  lr: 0.000027  loss: 0.2102  time: 1.5139  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 825/3000]  eta: 0:54:54  lr: 0.000027  loss: 0.4351  time: 1.5352  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 825/3000]  eta: 0:54:53  lr: 0.000027  loss: 0.5521  time: 1.5349  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 830/3000]  eta: 0:54:45  lr: 0.000027  loss: 0.2225  time: 1.5000  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 830/3000]  eta: 0:54:45  lr: 0.000027  loss: 0.1543  time: 1.4998  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 835/3000]  eta: 0:54:38  lr: 0.000027  loss: 0.2942  time: 1.4943  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 835/3000]  eta: 0:54:37  lr: 0.000027  loss: 0.5688  time: 1.4940  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 840/3000]  eta: 0:54:30  lr: 0.000027  loss: 0.4784  time: 1.4910  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 840/3000]  eta: 0:54:29  lr: 0.000027  loss: 1.0099  time: 1.4907  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 845/3000]  eta: 0:54:23  lr: 0.000027  loss: 0.2506  time: 1.4988  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 845/3000]  eta: 0:54:22  lr: 0.000027  loss: 1.0587  time: 1.4986  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 850/3000]  eta: 0:54:16  lr: 0.000027  loss: 0.1406  time: 1.5249  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 850/3000]  eta: 0:54:15  lr: 0.000027  loss: 0.6832  time: 1.5247  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 855/3000]  eta: 0:54:08  lr: 0.000027  loss: 0.4219  time: 1.5341  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 855/3000]  eta: 0:54:08  lr: 0.000027  loss: 0.0705  time: 1.5339  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 860/3000]  eta: 0:54:01  lr: 0.000027  loss: 0.4152  time: 1.5417  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 860/3000]  eta: 0:54:00  lr: 0.000027  loss: 0.4026  time: 1.5414  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 865/3000]  eta: 0:53:54  lr: 0.000027  loss: 0.9658  time: 1.5347  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 865/3000]  eta: 0:53:53  lr: 0.000027  loss: 0.9963  time: 1.5345  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 870/3000]  eta: 0:53:47  lr: 0.000027  loss: 0.5817  time: 1.5409  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 870/3000]  eta: 0:53:46  lr: 0.000027  loss: 0.4929  time: 1.5407  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 875/3000]  eta: 0:53:38  lr: 0.000027  loss: 0.2347  time: 1.5122  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 875/3000]  eta: 0:53:37  lr: 0.000027  loss: 0.6282  time: 1.5120  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 880/3000]  eta: 0:53:31  lr: 0.000027  loss: 0.3144  time: 1.5225  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 880/3000]  eta: 0:53:30  lr: 0.000027  loss: 0.8588  time: 1.5223  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 885/3000]  eta: 0:53:23  lr: 0.000027  loss: 0.4082  time: 1.5159  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 885/3000]  eta: 0:53:23  lr: 0.000027  loss: 0.4500  time: 1.5157  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 890/3000]  eta: 0:53:16  lr: 0.000027  loss: 0.6240  time: 1.5033  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 890/3000]  eta: 0:53:15  lr: 0.000027  loss: 0.2105  time: 1.5031  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 895/3000]  eta: 0:53:08  lr: 0.000027  loss: 0.2268  time: 1.5320  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 895/3000]  eta: 0:53:08  lr: 0.000027  loss: 0.3634  time: 1.5317  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 900/3000]  eta: 0:53:01  lr: 0.000027  loss: 0.6733  time: 1.5290  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 900/3000]  eta: 0:53:01  lr: 0.000027  loss: 0.4634  time: 1.5288  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 905/3000]  eta: 0:52:53  lr: 0.000027  loss: 0.1453  time: 1.5228  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 905/3000]  eta: 0:52:53  lr: 0.000027  loss: 0.4644  time: 1.5226  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 910/3000]  eta: 0:52:46  lr: 0.000027  loss: 0.4318  time: 1.5295  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 910/3000]  eta: 0:52:46  lr: 0.000027  loss: 0.2894  time: 1.5293  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 915/3000]  eta: 0:52:39  lr: 0.000027  loss: 0.8649  time: 1.5248  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 915/3000]  eta: 0:52:38  lr: 0.000027  loss: 1.1719  time: 1.5246  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 920/3000]  eta: 0:52:31  lr: 0.000027  loss: 0.2879  time: 1.5192  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 920/3000]  eta: 0:52:30  lr: 0.000027  loss: 0.1801  time: 1.5189  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 925/3000]  eta: 0:52:24  lr: 0.000027  loss: 0.2407  time: 1.5338  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 925/3000]  eta: 0:52:23  lr: 0.000027  loss: 0.7828  time: 1.5336  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 930/3000]  eta: 0:52:17  lr: 0.000027  loss: 0.2114  time: 1.5360  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 930/3000]  eta: 0:52:16  lr: 0.000027  loss: 0.2364  time: 1.5357  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 935/3000]  eta: 0:52:09  lr: 0.000027  loss: 0.3403  time: 1.5333  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 935/3000]  eta: 0:52:09  lr: 0.000027  loss: 0.4931  time: 1.5331  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 940/3000]  eta: 0:52:02  lr: 0.000027  loss: 1.1824  time: 1.5394  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 940/3000]  eta: 0:52:01  lr: 0.000027  loss: 0.1881  time: 1.5391  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 945/3000]  eta: 0:51:54  lr: 0.000027  loss: 0.4921  time: 1.5332  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 945/3000]  eta: 0:51:54  lr: 0.000027  loss: 0.5966  time: 1.5329  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 950/3000]  eta: 0:51:47  lr: 0.000027  loss: 0.3224  time: 1.5280  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 950/3000]  eta: 0:51:46  lr: 0.000027  loss: 0.4739  time: 1.5278  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 955/3000]  eta: 0:51:39  lr: 0.000027  loss: 0.1688  time: 1.5167  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 955/3000]  eta: 0:51:38  lr: 0.000027  loss: 0.1656  time: 1.5164  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 960/3000]  eta: 0:51:31  lr: 0.000027  loss: 0.5336  time: 1.5080  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 960/3000]  eta: 0:51:31  lr: 0.000027  loss: 0.4401  time: 1.5078  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 965/3000]  eta: 0:51:24  lr: 0.000027  loss: 0.4689  time: 1.5183  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 965/3000]  eta: 0:51:24  lr: 0.000027  loss: 0.6206  time: 1.5180  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 970/3000]  eta: 0:51:17  lr: 0.000027  loss: 1.1852  time: 1.5352  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 970/3000]  eta: 0:51:17  lr: 0.000027  loss: 1.8965  time: 1.5350  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 975/3000]  eta: 0:51:10  lr: 0.000027  loss: 0.3479  time: 1.5402  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 975/3000]  eta: 0:51:09  lr: 0.000027  loss: 0.3793  time: 1.5399  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 980/3000]  eta: 0:51:02  lr: 0.000027  loss: 0.6621  time: 1.5518  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 980/3000]  eta: 0:51:02  lr: 0.000027  loss: 0.4467  time: 1.5515  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 985/3000]  eta: 0:50:55  lr: 0.000027  loss: 0.4419  time: 1.5409  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 985/3000]  eta: 0:50:54  lr: 0.000027  loss: 0.3872  time: 1.5407  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 990/3000]  eta: 0:50:47  lr: 0.000027  loss: 0.4686  time: 1.5235  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 990/3000]  eta: 0:50:47  lr: 0.000027  loss: 0.2678  time: 1.5232  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [ 995/3000]  eta: 0:50:40  lr: 0.000027  loss: 0.8656  time: 1.5395  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [ 995/3000]  eta: 0:50:40  lr: 0.000027  loss: 0.3055  time: 1.5393  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1000/3000]  eta: 0:50:33  lr: 0.000027  loss: 0.3886  time: 1.5378  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1000/3000]  eta: 0:50:32  lr: 0.000027  loss: 0.3243  time: 1.5376  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1005/3000]  eta: 0:50:24  lr: 0.000027  loss: 0.1698  time: 1.5138  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1005/3000]  eta: 0:50:24  lr: 0.000027  loss: 0.1745  time: 1.5136  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1010/3000]  eta: 0:50:18  lr: 0.000027  loss: 1.1751  time: 1.5291  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1010/3000]  eta: 0:50:17  lr: 0.000027  loss: 0.6802  time: 1.5289  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1015/3000]  eta: 0:50:10  lr: 0.000027  loss: 0.6691  time: 1.5218  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1015/3000]  eta: 0:50:10  lr: 0.000027  loss: 0.4032  time: 1.5216  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1020/3000]  eta: 0:50:02  lr: 0.000027  loss: 0.2141  time: 1.5129  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1020/3000]  eta: 0:50:02  lr: 0.000027  loss: 0.7223  time: 1.5126  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1025/3000]  eta: 0:49:55  lr: 0.000027  loss: 0.5863  time: 1.5298  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1025/3000]  eta: 0:49:54  lr: 0.000027  loss: 0.4319  time: 1.5295  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1030/3000]  eta: 0:49:48  lr: 0.000027  loss: 0.8578  time: 1.5272  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1030/3000]  eta: 0:49:47  lr: 0.000027  loss: 0.3889  time: 1.5267  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1035/3000]  eta: 0:49:40  lr: 0.000027  loss: 0.5521  time: 1.5192  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1035/3000]  eta: 0:49:39  lr: 0.000027  loss: 0.0604  time: 1.5187  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1040/3000]  eta: 0:49:32  lr: 0.000027  loss: 0.1628  time: 1.5231  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1040/3000]  eta: 0:49:32  lr: 0.000027  loss: 0.2111  time: 1.5228  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1045/3000]  eta: 0:49:25  lr: 0.000027  loss: 0.6197  time: 1.5250  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1045/3000]  eta: 0:49:24  lr: 0.000027  loss: 0.0761  time: 1.5247  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1050/3000]  eta: 0:49:17  lr: 0.000027  loss: 0.2576  time: 1.5125  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1050/3000]  eta: 0:49:17  lr: 0.000027  loss: 0.8589  time: 1.5123  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1055/3000]  eta: 0:49:10  lr: 0.000027  loss: 0.6431  time: 1.5179  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1055/3000]  eta: 0:49:09  lr: 0.000027  loss: 0.1609  time: 1.5176  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1060/3000]  eta: 0:49:02  lr: 0.000027  loss: 0.8622  time: 1.5056  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1060/3000]  eta: 0:49:01  lr: 0.000027  loss: 0.0421  time: 1.5053  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1065/3000]  eta: 0:48:54  lr: 0.000027  loss: 0.6671  time: 1.5082  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1065/3000]  eta: 0:48:53  lr: 0.000027  loss: 0.7205  time: 1.5080  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1070/3000]  eta: 0:48:47  lr: 0.000027  loss: 0.1108  time: 1.5131  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1070/3000]  eta: 0:48:46  lr: 0.000027  loss: 0.6136  time: 1.5128  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1075/3000]  eta: 0:48:40  lr: 0.000027  loss: 0.5346  time: 1.5301  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1075/3000]  eta: 0:48:39  lr: 0.000027  loss: 0.2166  time: 1.5299  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1080/3000]  eta: 0:48:32  lr: 0.000027  loss: 0.5098  time: 1.5492  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1080/3000]  eta: 0:48:32  lr: 0.000027  loss: 0.4251  time: 1.5490  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1085/3000]  eta: 0:48:25  lr: 0.000027  loss: 0.9277  time: 1.5551  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1085/3000]  eta: 0:48:24  lr: 0.000027  loss: 0.2830  time: 1.5548  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1090/3000]  eta: 0:48:18  lr: 0.000027  loss: 0.2835  time: 1.5555  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1090/3000]  eta: 0:48:17  lr: 0.000027  loss: 0.5506  time: 1.5552  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1095/3000]  eta: 0:48:10  lr: 0.000027  loss: 0.2985  time: 1.5222  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1095/3000]  eta: 0:48:09  lr: 0.000027  loss: 0.2095  time: 1.5218  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1100/3000]  eta: 0:48:01  lr: 0.000027  loss: 0.4266  time: 1.4954  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1100/3000]  eta: 0:48:01  lr: 0.000027  loss: 0.2903  time: 1.4951  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1105/3000]  eta: 0:47:53  lr: 0.000027  loss: 0.5006  time: 1.4724  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1105/3000]  eta: 0:47:53  lr: 0.000027  loss: 0.1986  time: 1.4721  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1110/3000]  eta: 0:47:45  lr: 0.000027  loss: 0.4079  time: 1.4547  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1110/3000]  eta: 0:47:45  lr: 0.000027  loss: 0.7561  time: 1.4545  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1115/3000]  eta: 0:47:38  lr: 0.000027  loss: 0.2618  time: 1.4770  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1115/3000]  eta: 0:47:37  lr: 0.000027  loss: 0.2576  time: 1.4768  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1120/3000]  eta: 0:47:30  lr: 0.000027  loss: 0.6337  time: 1.4856  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1120/3000]  eta: 0:47:29  lr: 0.000027  loss: 0.4367  time: 1.4854  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1125/3000]  eta: 0:47:22  lr: 0.000027  loss: 0.1678  time: 1.5068  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1125/3000]  eta: 0:47:22  lr: 0.000027  loss: 0.1976  time: 1.5066  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1130/3000]  eta: 0:47:14  lr: 0.000027  loss: 0.5972  time: 1.4894  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1130/3000]  eta: 0:47:14  lr: 0.000027  loss: 0.3184  time: 1.4891  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1135/3000]  eta: 0:47:05  lr: 0.000027  loss: 0.1298  time: 1.4519  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1135/3000]  eta: 0:47:05  lr: 0.000027  loss: 0.2469  time: 1.4517  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1140/3000]  eta: 0:46:57  lr: 0.000027  loss: 0.2005  time: 1.4406  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1140/3000]  eta: 0:46:57  lr: 0.000027  loss: 0.8592  time: 1.4403  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1145/3000]  eta: 0:46:50  lr: 0.000027  loss: 1.5573  time: 1.4400  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1145/3000]  eta: 0:46:49  lr: 0.000027  loss: 0.5118  time: 1.4397  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1150/3000]  eta: 0:46:42  lr: 0.000027  loss: 0.1593  time: 1.4776  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1150/3000]  eta: 0:46:42  lr: 0.000027  loss: 0.3599  time: 1.4773  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1155/3000]  eta: 0:46:35  lr: 0.000027  loss: 0.2838  time: 1.5139  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1155/3000]  eta: 0:46:35  lr: 0.000027  loss: 0.4984  time: 1.5137  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1160/3000]  eta: 0:46:27  lr: 0.000027  loss: 0.3134  time: 1.5280  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1160/3000]  eta: 0:46:27  lr: 0.000027  loss: 0.3157  time: 1.5278  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1165/3000]  eta: 0:46:20  lr: 0.000027  loss: 0.4774  time: 1.5394  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1165/3000]  eta: 0:46:20  lr: 0.000027  loss: 0.1376  time: 1.5391  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1170/3000]  eta: 0:46:12  lr: 0.000027  loss: 0.2018  time: 1.5159  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1170/3000]  eta: 0:46:12  lr: 0.000027  loss: 0.0953  time: 1.5157  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1175/3000]  eta: 0:46:04  lr: 0.000027  loss: 0.4526  time: 1.5061  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1175/3000]  eta: 0:46:04  lr: 0.000027  loss: 0.7813  time: 1.5057  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1180/3000]  eta: 0:45:57  lr: 0.000027  loss: 0.6513  time: 1.5009  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1180/3000]  eta: 0:45:56  lr: 0.000027  loss: 0.8210  time: 1.5006  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1185/3000]  eta: 0:45:49  lr: 0.000027  loss: 0.2740  time: 1.4864  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1185/3000]  eta: 0:45:48  lr: 0.000027  loss: 0.4340  time: 1.4861  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1190/3000]  eta: 0:45:42  lr: 0.000027  loss: 0.0950  time: 1.5045  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1190/3000]  eta: 0:45:41  lr: 0.000027  loss: 0.3782  time: 1.5042  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1195/3000]  eta: 0:45:34  lr: 0.000027  loss: 0.1786  time: 1.5169  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1195/3000]  eta: 0:45:34  lr: 0.000027  loss: 0.2798  time: 1.5167  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1200/3000]  eta: 0:45:27  lr: 0.000027  loss: 0.6939  time: 1.5300  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1200/3000]  eta: 0:45:26  lr: 0.000027  loss: 0.7037  time: 1.5297  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1205/3000]  eta: 0:45:19  lr: 0.000027  loss: 0.2972  time: 1.5415  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1205/3000]  eta: 0:45:19  lr: 0.000027  loss: 0.5944  time: 1.5413  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1210/3000]  eta: 0:45:12  lr: 0.000027  loss: 0.3333  time: 1.5363  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1210/3000]  eta: 0:45:11  lr: 0.000027  loss: 0.5133  time: 1.5361  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1215/3000]  eta: 0:45:04  lr: 0.000027  loss: 1.4506  time: 1.5153  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1215/3000]  eta: 0:45:03  lr: 0.000027  loss: 0.6563  time: 1.5150  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1220/3000]  eta: 0:44:56  lr: 0.000027  loss: 1.0670  time: 1.5170  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1220/3000]  eta: 0:44:56  lr: 0.000027  loss: 0.3690  time: 1.5167  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1225/3000]  eta: 0:44:49  lr: 0.000027  loss: 0.2610  time: 1.4966  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1225/3000]  eta: 0:44:48  lr: 0.000027  loss: 1.0128  time: 1.4963  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1230/3000]  eta: 0:44:40  lr: 0.000027  loss: 0.5644  time: 1.4653  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1230/3000]  eta: 0:44:40  lr: 0.000027  loss: 0.3377  time: 1.4650  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1235/3000]  eta: 0:44:33  lr: 0.000027  loss: 0.3490  time: 1.4838  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1235/3000]  eta: 0:44:32  lr: 0.000027  loss: 0.2812  time: 1.4835  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1240/3000]  eta: 0:44:25  lr: 0.000027  loss: 0.2258  time: 1.4723  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1240/3000]  eta: 0:44:24  lr: 0.000027  loss: 0.4437  time: 1.4721  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1245/3000]  eta: 0:44:17  lr: 0.000027  loss: 0.2387  time: 1.4722  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1245/3000]  eta: 0:44:17  lr: 0.000027  loss: 0.3515  time: 1.4719  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1250/3000]  eta: 0:44:10  lr: 0.000027  loss: 0.7446  time: 1.5112  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1250/3000]  eta: 0:44:09  lr: 0.000027  loss: 0.2123  time: 1.5109  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1255/3000]  eta: 0:44:02  lr: 0.000027  loss: 0.5687  time: 1.5173  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1255/3000]  eta: 0:44:02  lr: 0.000027  loss: 0.2839  time: 1.5171  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1260/3000]  eta: 0:43:55  lr: 0.000027  loss: 0.7310  time: 1.5232  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1260/3000]  eta: 0:43:54  lr: 0.000027  loss: 0.2347  time: 1.5229  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1265/3000]  eta: 0:43:47  lr: 0.000027  loss: 0.0773  time: 1.5228  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1265/3000]  eta: 0:43:47  lr: 0.000027  loss: 0.1948  time: 1.5226  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1270/3000]  eta: 0:43:39  lr: 0.000027  loss: 0.9653  time: 1.4969  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1270/3000]  eta: 0:43:39  lr: 0.000027  loss: 0.2629  time: 1.4967  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1275/3000]  eta: 0:43:32  lr: 0.000027  loss: 0.1852  time: 1.5029  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1275/3000]  eta: 0:43:31  lr: 0.000027  loss: 0.3223  time: 1.5027  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1280/3000]  eta: 0:43:24  lr: 0.000027  loss: 0.5244  time: 1.5073  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1280/3000]  eta: 0:43:24  lr: 0.000027  loss: 0.5008  time: 1.5070  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1285/3000]  eta: 0:43:17  lr: 0.000027  loss: 0.2078  time: 1.5401  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1285/3000]  eta: 0:43:17  lr: 0.000027  loss: 0.5085  time: 1.5398  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1290/3000]  eta: 0:43:10  lr: 0.000027  loss: 0.9430  time: 1.5440  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1290/3000]  eta: 0:43:09  lr: 0.000027  loss: 0.3510  time: 1.5438  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1295/3000]  eta: 0:43:02  lr: 0.000027  loss: 0.5099  time: 1.5479  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1295/3000]  eta: 0:43:02  lr: 0.000027  loss: 0.5008  time: 1.5476  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1300/3000]  eta: 0:42:55  lr: 0.000027  loss: 0.3695  time: 1.5484  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1300/3000]  eta: 0:42:55  lr: 0.000027  loss: 0.4840  time: 1.5482  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1305/3000]  eta: 0:42:47  lr: 0.000027  loss: 0.4582  time: 1.5249  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1305/3000]  eta: 0:42:47  lr: 0.000027  loss: 0.3960  time: 1.5247  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1310/3000]  eta: 0:42:40  lr: 0.000027  loss: 0.4691  time: 1.5445  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1310/3000]  eta: 0:42:40  lr: 0.000027  loss: 0.3277  time: 1.5442  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1315/3000]  eta: 0:42:33  lr: 0.000027  loss: 0.3606  time: 1.5339  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1315/3000]  eta: 0:42:32  lr: 0.000027  loss: 0.4445  time: 1.5337  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1320/3000]  eta: 0:42:25  lr: 0.000027  loss: 0.3880  time: 1.5325  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1320/3000]  eta: 0:42:25  lr: 0.000027  loss: 0.1756  time: 1.5322  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1325/3000]  eta: 0:42:18  lr: 0.000027  loss: 0.6251  time: 1.5411  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1325/3000]  eta: 0:42:17  lr: 0.000027  loss: 0.4987  time: 1.5409  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1330/3000]  eta: 0:42:10  lr: 0.000027  loss: 0.2850  time: 1.5418  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1330/3000]  eta: 0:42:10  lr: 0.000027  loss: 0.4412  time: 1.5415  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1335/3000]  eta: 0:42:02  lr: 0.000027  loss: 0.6731  time: 1.5186  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1335/3000]  eta: 0:42:02  lr: 0.000027  loss: 0.1463  time: 1.5183  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1340/3000]  eta: 0:41:55  lr: 0.000027  loss: 0.5634  time: 1.5115  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1340/3000]  eta: 0:41:54  lr: 0.000027  loss: 0.7313  time: 1.5113  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1345/3000]  eta: 0:41:47  lr: 0.000027  loss: 0.4004  time: 1.5059  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1345/3000]  eta: 0:41:47  lr: 0.000027  loss: 0.3170  time: 1.5056  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1350/3000]  eta: 0:41:40  lr: 0.000027  loss: 0.2792  time: 1.4960  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1350/3000]  eta: 0:41:39  lr: 0.000027  loss: 0.6172  time: 1.4957  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1355/3000]  eta: 0:41:32  lr: 0.000027  loss: 0.4414  time: 1.5050  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1355/3000]  eta: 0:41:31  lr: 0.000027  loss: 0.3523  time: 1.5048  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1360/3000]  eta: 0:41:24  lr: 0.000027  loss: 0.6118  time: 1.5139  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1360/3000]  eta: 0:41:24  lr: 0.000027  loss: 0.1477  time: 1.5136  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1365/3000]  eta: 0:41:17  lr: 0.000027  loss: 0.1823  time: 1.5136  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1365/3000]  eta: 0:41:16  lr: 0.000027  loss: 0.8200  time: 1.5132  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1370/3000]  eta: 0:41:09  lr: 0.000027  loss: 0.3981  time: 1.5195  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1370/3000]  eta: 0:41:09  lr: 0.000027  loss: 0.5847  time: 1.5191  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1375/3000]  eta: 0:41:02  lr: 0.000027  loss: 0.1199  time: 1.5273  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1375/3000]  eta: 0:41:01  lr: 0.000027  loss: 0.4193  time: 1.5270  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1380/3000]  eta: 0:40:54  lr: 0.000027  loss: 0.3052  time: 1.5107  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1380/3000]  eta: 0:40:54  lr: 0.000027  loss: 0.3234  time: 1.5105  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1385/3000]  eta: 0:40:46  lr: 0.000027  loss: 0.3366  time: 1.4982  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1385/3000]  eta: 0:40:46  lr: 0.000027  loss: 0.3073  time: 1.4980  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1390/3000]  eta: 0:40:38  lr: 0.000027  loss: 0.0374  time: 1.4710  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1390/3000]  eta: 0:40:38  lr: 0.000027  loss: 0.2339  time: 1.4707  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1395/3000]  eta: 0:40:31  lr: 0.000027  loss: 0.0738  time: 1.4713  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1395/3000]  eta: 0:40:30  lr: 0.000027  loss: 0.6322  time: 1.4711  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1400/3000]  eta: 0:40:23  lr: 0.000027  loss: 0.3402  time: 1.4945  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1400/3000]  eta: 0:40:23  lr: 0.000027  loss: 0.2772  time: 1.4943  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1405/3000]  eta: 0:40:16  lr: 0.000027  loss: 1.1688  time: 1.5165  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1405/3000]  eta: 0:40:15  lr: 0.000027  loss: 0.4576  time: 1.5163  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1410/3000]  eta: 0:40:08  lr: 0.000027  loss: 0.1490  time: 1.5165  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1410/3000]  eta: 0:40:07  lr: 0.000027  loss: 0.1913  time: 1.5163  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1415/3000]  eta: 0:40:00  lr: 0.000027  loss: 0.5564  time: 1.5228  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1415/3000]  eta: 0:40:00  lr: 0.000027  loss: 0.3721  time: 1.5225  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1420/3000]  eta: 0:39:53  lr: 0.000027  loss: 0.5603  time: 1.5205  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1420/3000]  eta: 0:39:53  lr: 0.000027  loss: 0.4304  time: 1.5202  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1425/3000]  eta: 0:39:46  lr: 0.000027  loss: 0.3618  time: 1.5277  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1425/3000]  eta: 0:39:45  lr: 0.000027  loss: 0.4493  time: 1.5274  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1430/3000]  eta: 0:39:38  lr: 0.000027  loss: 0.3130  time: 1.5339  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1430/3000]  eta: 0:39:38  lr: 0.000027  loss: 0.0444  time: 1.5337  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1435/3000]  eta: 0:39:30  lr: 0.000027  loss: 0.2276  time: 1.5079  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1435/3000]  eta: 0:39:30  lr: 0.000027  loss: 0.1757  time: 1.5077  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1440/3000]  eta: 0:39:23  lr: 0.000027  loss: 0.8122  time: 1.5156  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1440/3000]  eta: 0:39:22  lr: 0.000027  loss: 0.5793  time: 1.5153  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1445/3000]  eta: 0:39:16  lr: 0.000027  loss: 0.5002  time: 1.5144  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1445/3000]  eta: 0:39:15  lr: 0.000027  loss: 0.4775  time: 1.5141  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1450/3000]  eta: 0:39:08  lr: 0.000027  loss: 0.3433  time: 1.5346  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1450/3000]  eta: 0:39:08  lr: 0.000027  loss: 0.1917  time: 1.5343  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1455/3000]  eta: 0:39:01  lr: 0.000027  loss: 0.1183  time: 1.5665  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1455/3000]  eta: 0:39:00  lr: 0.000027  loss: 0.2228  time: 1.5663  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1460/3000]  eta: 0:38:53  lr: 0.000027  loss: 0.3152  time: 1.5583  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1460/3000]  eta: 0:38:53  lr: 0.000027  loss: 0.5350  time: 1.5581  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1465/3000]  eta: 0:38:46  lr: 0.000027  loss: 0.1517  time: 1.5449  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1465/3000]  eta: 0:38:45  lr: 0.000027  loss: 0.5252  time: 1.5447  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1470/3000]  eta: 0:38:38  lr: 0.000027  loss: 0.4788  time: 1.5316  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1470/3000]  eta: 0:38:38  lr: 0.000027  loss: 0.8826  time: 1.5314  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1475/3000]  eta: 0:38:31  lr: 0.000027  loss: 0.1442  time: 1.5264  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1475/3000]  eta: 0:38:30  lr: 0.000027  loss: 0.1144  time: 1.5262  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1480/3000]  eta: 0:38:23  lr: 0.000027  loss: 0.6362  time: 1.5202  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1480/3000]  eta: 0:38:23  lr: 0.000027  loss: 0.3210  time: 1.5200  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1485/3000]  eta: 0:38:16  lr: 0.000027  loss: 0.6149  time: 1.5132  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1485/3000]  eta: 0:38:15  lr: 0.000027  loss: 0.4490  time: 1.5130  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1490/3000]  eta: 0:38:08  lr: 0.000027  loss: 0.3713  time: 1.5188  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1490/3000]  eta: 0:38:07  lr: 0.000027  loss: 0.2136  time: 1.5186  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1495/3000]  eta: 0:38:00  lr: 0.000027  loss: 0.2499  time: 1.5134  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1495/3000]  eta: 0:38:00  lr: 0.000027  loss: 0.4154  time: 1.5131  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1500/3000]  eta: 0:37:53  lr: 0.000027  loss: 0.6379  time: 1.5286  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1500/3000]  eta: 0:37:53  lr: 0.000027  loss: 0.5381  time: 1.5284  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1505/3000]  eta: 0:37:45  lr: 0.000027  loss: 0.2100  time: 1.5284  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1505/3000]  eta: 0:37:45  lr: 0.000027  loss: 0.1285  time: 1.5281  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1510/3000]  eta: 0:37:38  lr: 0.000027  loss: 0.2343  time: 1.5440  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1510/3000]  eta: 0:37:38  lr: 0.000027  loss: 0.2778  time: 1.5438  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1515/3000]  eta: 0:37:31  lr: 0.000027  loss: 0.7004  time: 1.5417  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1515/3000]  eta: 0:37:30  lr: 0.000027  loss: 0.2171  time: 1.5415  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1520/3000]  eta: 0:37:23  lr: 0.000027  loss: 0.7451  time: 1.5233  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1520/3000]  eta: 0:37:23  lr: 0.000027  loss: 0.6040  time: 1.5230  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1525/3000]  eta: 0:37:15  lr: 0.000027  loss: 0.5630  time: 1.5206  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1525/3000]  eta: 0:37:15  lr: 0.000027  loss: 0.5074  time: 1.5203  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1530/3000]  eta: 0:37:08  lr: 0.000027  loss: 0.3374  time: 1.5022  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1530/3000]  eta: 0:37:07  lr: 0.000027  loss: 0.3259  time: 1.5020  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1535/3000]  eta: 0:37:00  lr: 0.000027  loss: 0.1188  time: 1.5021  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1535/3000]  eta: 0:37:00  lr: 0.000027  loss: 0.5009  time: 1.5019  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1540/3000]  eta: 0:36:53  lr: 0.000027  loss: 0.3604  time: 1.5162  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1540/3000]  eta: 0:36:52  lr: 0.000027  loss: 0.3610  time: 1.5161  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1545/3000]  eta: 0:36:45  lr: 0.000027  loss: 0.3964  time: 1.5210  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1545/3000]  eta: 0:36:45  lr: 0.000027  loss: 0.3981  time: 1.5207  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1550/3000]  eta: 0:36:37  lr: 0.000027  loss: 0.5780  time: 1.5184  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1550/3000]  eta: 0:36:37  lr: 0.000027  loss: 0.1516  time: 1.5182  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1555/3000]  eta: 0:36:30  lr: 0.000027  loss: 0.5808  time: 1.5140  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1555/3000]  eta: 0:36:29  lr: 0.000027  loss: 0.3598  time: 1.5137  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1560/3000]  eta: 0:36:22  lr: 0.000027  loss: 0.5030  time: 1.5020  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1560/3000]  eta: 0:36:22  lr: 0.000027  loss: 0.5384  time: 1.5016  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1565/3000]  eta: 0:36:15  lr: 0.000027  loss: 0.3855  time: 1.5195  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1565/3000]  eta: 0:36:14  lr: 0.000027  loss: 0.3424  time: 1.5193  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1570/3000]  eta: 0:36:07  lr: 0.000027  loss: 0.4172  time: 1.5119  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1570/3000]  eta: 0:36:07  lr: 0.000027  loss: 0.4038  time: 1.5117  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1575/3000]  eta: 0:35:59  lr: 0.000027  loss: 0.1381  time: 1.5099  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1575/3000]  eta: 0:35:59  lr: 0.000027  loss: 0.2643  time: 1.5096  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1580/3000]  eta: 0:35:52  lr: 0.000027  loss: 0.5366  time: 1.5032  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1580/3000]  eta: 0:35:51  lr: 0.000027  loss: 0.1703  time: 1.5030  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1585/3000]  eta: 0:35:44  lr: 0.000027  loss: 0.3181  time: 1.5004  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1585/3000]  eta: 0:35:44  lr: 0.000027  loss: 0.6679  time: 1.5002  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1590/3000]  eta: 0:35:36  lr: 0.000027  loss: 0.5193  time: 1.5111  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1590/3000]  eta: 0:35:36  lr: 0.000027  loss: 0.3589  time: 1.5109  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1595/3000]  eta: 0:35:29  lr: 0.000027  loss: 0.8451  time: 1.5149  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1595/3000]  eta: 0:35:29  lr: 0.000027  loss: 0.4059  time: 1.5146  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1600/3000]  eta: 0:35:21  lr: 0.000027  loss: 0.3045  time: 1.5254  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1600/3000]  eta: 0:35:21  lr: 0.000027  loss: 0.1463  time: 1.5251  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1605/3000]  eta: 0:35:14  lr: 0.000027  loss: 0.6449  time: 1.5213  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1605/3000]  eta: 0:35:14  lr: 0.000027  loss: 0.0992  time: 1.5211  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1610/3000]  eta: 0:35:06  lr: 0.000027  loss: 0.3067  time: 1.5294  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1610/3000]  eta: 0:35:06  lr: 0.000027  loss: 0.5183  time: 1.5292  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1615/3000]  eta: 0:34:59  lr: 0.000027  loss: 0.8362  time: 1.5386  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1615/3000]  eta: 0:34:59  lr: 0.000027  loss: 0.2123  time: 1.5384  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1620/3000]  eta: 0:34:51  lr: 0.000027  loss: 0.1952  time: 1.5270  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1620/3000]  eta: 0:34:51  lr: 0.000027  loss: 0.3093  time: 1.5268  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1625/3000]  eta: 0:34:44  lr: 0.000027  loss: 0.4338  time: 1.5153  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1625/3000]  eta: 0:34:43  lr: 0.000027  loss: 0.3284  time: 1.5151  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1630/3000]  eta: 0:34:36  lr: 0.000027  loss: 0.7488  time: 1.4984  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1630/3000]  eta: 0:34:35  lr: 0.000027  loss: 0.1425  time: 1.4981  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1635/3000]  eta: 0:34:28  lr: 0.000027  loss: 0.1491  time: 1.4969  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1635/3000]  eta: 0:34:28  lr: 0.000027  loss: 0.1194  time: 1.4966  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1640/3000]  eta: 0:34:21  lr: 0.000027  loss: 0.1443  time: 1.4972  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1640/3000]  eta: 0:34:20  lr: 0.000027  loss: 0.2977  time: 1.4970  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1645/3000]  eta: 0:34:13  lr: 0.000027  loss: 0.6315  time: 1.4835  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1645/3000]  eta: 0:34:12  lr: 0.000027  loss: 0.5405  time: 1.4832  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1650/3000]  eta: 0:34:05  lr: 0.000027  loss: 0.7541  time: 1.5091  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1650/3000]  eta: 0:34:05  lr: 0.000027  loss: 0.6672  time: 1.5088  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1655/3000]  eta: 0:33:58  lr: 0.000027  loss: 0.2452  time: 1.5186  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1655/3000]  eta: 0:33:58  lr: 0.000027  loss: 0.2026  time: 1.5183  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1660/3000]  eta: 0:33:51  lr: 0.000027  loss: 0.9499  time: 1.5369  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1660/3000]  eta: 0:33:50  lr: 0.000027  loss: 0.3063  time: 1.5366  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1665/3000]  eta: 0:33:43  lr: 0.000027  loss: 0.2521  time: 1.5409  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1665/3000]  eta: 0:33:43  lr: 0.000027  loss: 0.3652  time: 1.5407  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1670/3000]  eta: 0:33:36  lr: 0.000027  loss: 0.5918  time: 1.5471  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1670/3000]  eta: 0:33:35  lr: 0.000027  loss: 0.2814  time: 1.5467  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1675/3000]  eta: 0:33:28  lr: 0.000027  loss: 0.2657  time: 1.5260  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1675/3000]  eta: 0:33:28  lr: 0.000027  loss: 0.0784  time: 1.5256  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1680/3000]  eta: 0:33:20  lr: 0.000027  loss: 1.0642  time: 1.5235  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1680/3000]  eta: 0:33:20  lr: 0.000027  loss: 0.7003  time: 1.5231  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1685/3000]  eta: 0:33:13  lr: 0.000027  loss: 0.5429  time: 1.5497  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1685/3000]  eta: 0:33:13  lr: 0.000027  loss: 0.7240  time: 1.5493  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1690/3000]  eta: 0:33:05  lr: 0.000027  loss: 0.7186  time: 1.5270  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1690/3000]  eta: 0:33:05  lr: 0.000027  loss: 0.3184  time: 1.5268  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1695/3000]  eta: 0:32:58  lr: 0.000027  loss: 0.2261  time: 1.5325  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1695/3000]  eta: 0:32:58  lr: 0.000027  loss: 0.5726  time: 1.5323  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1700/3000]  eta: 0:32:50  lr: 0.000027  loss: 0.3981  time: 1.4971  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1700/3000]  eta: 0:32:50  lr: 0.000027  loss: 0.3300  time: 1.4969  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1705/3000]  eta: 0:32:42  lr: 0.000027  loss: 0.5634  time: 1.4712  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1705/3000]  eta: 0:32:42  lr: 0.000027  loss: 0.1997  time: 1.4709  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1710/3000]  eta: 0:32:35  lr: 0.000027  loss: 0.5472  time: 1.4876  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1710/3000]  eta: 0:32:34  lr: 0.000027  loss: 0.2354  time: 1.4874  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1715/3000]  eta: 0:32:27  lr: 0.000027  loss: 0.2882  time: 1.4937  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1715/3000]  eta: 0:32:27  lr: 0.000027  loss: 0.3241  time: 1.4934  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1720/3000]  eta: 0:32:19  lr: 0.000027  loss: 0.3807  time: 1.5038  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1720/3000]  eta: 0:32:19  lr: 0.000027  loss: 0.1977  time: 1.5036  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1725/3000]  eta: 0:32:12  lr: 0.000027  loss: 0.1933  time: 1.4955  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1725/3000]  eta: 0:32:11  lr: 0.000027  loss: 0.4404  time: 1.4952  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1730/3000]  eta: 0:32:04  lr: 0.000027  loss: 0.9024  time: 1.4749  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1730/3000]  eta: 0:32:03  lr: 0.000027  loss: 0.6231  time: 1.4747  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1735/3000]  eta: 0:31:56  lr: 0.000027  loss: 0.4452  time: 1.4600  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1735/3000]  eta: 0:31:56  lr: 0.000027  loss: 0.1885  time: 1.4596  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1740/3000]  eta: 0:31:49  lr: 0.000027  loss: 1.0599  time: 1.4795  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1740/3000]  eta: 0:31:48  lr: 0.000027  loss: 0.4183  time: 1.4792  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1745/3000]  eta: 0:31:41  lr: 0.000027  loss: 0.4036  time: 1.4844  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1745/3000]  eta: 0:31:40  lr: 0.000027  loss: 0.4446  time: 1.4843  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1750/3000]  eta: 0:31:33  lr: 0.000027  loss: 0.5850  time: 1.5134  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1750/3000]  eta: 0:31:33  lr: 0.000027  loss: 0.6513  time: 1.5132  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1755/3000]  eta: 0:31:26  lr: 0.000027  loss: 0.3420  time: 1.5246  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1755/3000]  eta: 0:31:26  lr: 0.000027  loss: 0.7572  time: 1.5244  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1760/3000]  eta: 0:31:18  lr: 0.000027  loss: 0.2892  time: 1.5176  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1760/3000]  eta: 0:31:18  lr: 0.000027  loss: 0.1187  time: 1.5173  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1765/3000]  eta: 0:31:11  lr: 0.000027  loss: 0.9629  time: 1.5353  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1765/3000]  eta: 0:31:10  lr: 0.000027  loss: 0.4688  time: 1.5350  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1770/3000]  eta: 0:31:03  lr: 0.000027  loss: 0.2444  time: 1.5313  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1770/3000]  eta: 0:31:03  lr: 0.000027  loss: 0.4984  time: 1.5310  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1775/3000]  eta: 0:30:56  lr: 0.000027  loss: 0.2952  time: 1.5415  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1775/3000]  eta: 0:30:56  lr: 0.000027  loss: 0.3398  time: 1.5413  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1780/3000]  eta: 0:30:48  lr: 0.000027  loss: 0.2528  time: 1.5481  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1780/3000]  eta: 0:30:48  lr: 0.000027  loss: 0.6117  time: 1.5479  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1785/3000]  eta: 0:30:41  lr: 0.000027  loss: 0.9436  time: 1.5498  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1785/3000]  eta: 0:30:41  lr: 0.000027  loss: 0.5796  time: 1.5495  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1790/3000]  eta: 0:30:33  lr: 0.000027  loss: 0.2457  time: 1.5102  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1790/3000]  eta: 0:30:33  lr: 0.000027  loss: 0.1162  time: 1.5099  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1795/3000]  eta: 0:30:26  lr: 0.000027  loss: 0.4687  time: 1.5130  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1795/3000]  eta: 0:30:25  lr: 0.000027  loss: 0.4347  time: 1.5128  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1800/3000]  eta: 0:30:18  lr: 0.000027  loss: 0.8296  time: 1.5226  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1800/3000]  eta: 0:30:18  lr: 0.000027  loss: 0.1541  time: 1.5223  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1805/3000]  eta: 0:30:11  lr: 0.000027  loss: 0.4637  time: 1.5183  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1805/3000]  eta: 0:30:10  lr: 0.000027  loss: 0.3480  time: 1.5181  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1810/3000]  eta: 0:30:03  lr: 0.000027  loss: 0.8093  time: 1.5276  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1810/3000]  eta: 0:30:03  lr: 0.000027  loss: 0.8438  time: 1.5273  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1815/3000]  eta: 0:29:55  lr: 0.000027  loss: 0.6345  time: 1.5176  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1815/3000]  eta: 0:29:55  lr: 0.000027  loss: 0.3083  time: 1.5173  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1820/3000]  eta: 0:29:48  lr: 0.000027  loss: 0.7139  time: 1.5118  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1820/3000]  eta: 0:29:48  lr: 0.000027  loss: 0.3106  time: 1.5115  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1825/3000]  eta: 0:29:40  lr: 0.000027  loss: 0.3057  time: 1.5010  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1825/3000]  eta: 0:29:40  lr: 0.000027  loss: 0.1092  time: 1.5007  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1830/3000]  eta: 0:29:32  lr: 0.000027  loss: 0.3158  time: 1.5031  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1830/3000]  eta: 0:29:32  lr: 0.000027  loss: 0.4470  time: 1.5029  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1835/3000]  eta: 0:29:25  lr: 0.000027  loss: 1.5102  time: 1.5093  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1835/3000]  eta: 0:29:25  lr: 0.000027  loss: 0.4661  time: 1.5090  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1840/3000]  eta: 0:29:17  lr: 0.000027  loss: 0.6976  time: 1.5042  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1840/3000]  eta: 0:29:17  lr: 0.000027  loss: 0.2701  time: 1.5040  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1845/3000]  eta: 0:29:10  lr: 0.000027  loss: 0.6852  time: 1.4981  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1845/3000]  eta: 0:29:09  lr: 0.000027  loss: 0.1179  time: 1.4979  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1850/3000]  eta: 0:29:02  lr: 0.000027  loss: 0.2034  time: 1.5259  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1850/3000]  eta: 0:29:02  lr: 0.000027  loss: 0.2753  time: 1.5256  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1855/3000]  eta: 0:28:54  lr: 0.000027  loss: 0.2670  time: 1.4979  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1855/3000]  eta: 0:28:54  lr: 0.000027  loss: 0.1264  time: 1.4977  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1860/3000]  eta: 0:28:47  lr: 0.000027  loss: 0.1184  time: 1.5023  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1860/3000]  eta: 0:28:47  lr: 0.000027  loss: 0.5243  time: 1.5020  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1865/3000]  eta: 0:28:40  lr: 0.000027  loss: 0.3440  time: 1.5307  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1865/3000]  eta: 0:28:39  lr: 0.000027  loss: 0.5985  time: 1.5305  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1870/3000]  eta: 0:28:32  lr: 0.000027  loss: 0.1591  time: 1.5098  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1870/3000]  eta: 0:28:32  lr: 0.000027  loss: 1.0266  time: 1.5095  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1875/3000]  eta: 0:28:24  lr: 0.000027  loss: 0.3844  time: 1.5329  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1875/3000]  eta: 0:28:24  lr: 0.000027  loss: 0.5834  time: 1.5327  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1880/3000]  eta: 0:28:17  lr: 0.000027  loss: 0.2156  time: 1.5313  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1880/3000]  eta: 0:28:17  lr: 0.000027  loss: 0.3157  time: 1.5310  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1885/3000]  eta: 0:28:09  lr: 0.000027  loss: 0.5838  time: 1.5194  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1885/3000]  eta: 0:28:09  lr: 0.000027  loss: 0.1095  time: 1.5191  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1890/3000]  eta: 0:28:02  lr: 0.000027  loss: 0.4535  time: 1.5292  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1890/3000]  eta: 0:28:01  lr: 0.000027  loss: 0.5369  time: 1.5289  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1895/3000]  eta: 0:27:54  lr: 0.000027  loss: 0.3406  time: 1.5300  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1895/3000]  eta: 0:27:54  lr: 0.000027  loss: 0.1825  time: 1.5297  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1900/3000]  eta: 0:27:47  lr: 0.000027  loss: 0.8943  time: 1.5266  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1900/3000]  eta: 0:27:46  lr: 0.000027  loss: 0.4566  time: 1.5264  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1905/3000]  eta: 0:27:39  lr: 0.000027  loss: 0.2374  time: 1.5162  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1905/3000]  eta: 0:27:39  lr: 0.000027  loss: 0.5055  time: 1.5160  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1910/3000]  eta: 0:27:31  lr: 0.000027  loss: 0.1313  time: 1.4921  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1910/3000]  eta: 0:27:31  lr: 0.000027  loss: 0.3854  time: 1.4919  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1915/3000]  eta: 0:27:23  lr: 0.000027  loss: 0.1171  time: 1.4770  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1915/3000]  eta: 0:27:23  lr: 0.000027  loss: 0.1486  time: 1.4768  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1920/3000]  eta: 0:27:16  lr: 0.000027  loss: 0.1558  time: 1.4729  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1920/3000]  eta: 0:27:16  lr: 0.000027  loss: 0.5364  time: 1.4719  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1925/3000]  eta: 0:27:08  lr: 0.000027  loss: 0.3450  time: 1.4747  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1925/3000]  eta: 0:27:08  lr: 0.000027  loss: 0.4670  time: 1.4735  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1930/3000]  eta: 0:27:01  lr: 0.000027  loss: 0.1309  time: 1.5070  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1930/3000]  eta: 0:27:00  lr: 0.000027  loss: 0.2483  time: 1.5059  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1935/3000]  eta: 0:26:53  lr: 0.000027  loss: 0.1068  time: 1.5338  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1935/3000]  eta: 0:26:53  lr: 0.000027  loss: 0.6304  time: 1.5327  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1940/3000]  eta: 0:26:46  lr: 0.000027  loss: 0.6643  time: 1.5509  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1940/3000]  eta: 0:26:46  lr: 0.000027  loss: 0.1951  time: 1.5506  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1945/3000]  eta: 0:26:39  lr: 0.000027  loss: 0.3820  time: 1.5740  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1945/3000]  eta: 0:26:38  lr: 0.000027  loss: 1.1378  time: 1.5737  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1950/3000]  eta: 0:26:31  lr: 0.000027  loss: 0.2504  time: 1.5683  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1950/3000]  eta: 0:26:31  lr: 0.000027  loss: 0.4183  time: 1.5680  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1955/3000]  eta: 0:26:23  lr: 0.000027  loss: 0.0983  time: 1.5429  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1955/3000]  eta: 0:26:23  lr: 0.000027  loss: 0.5243  time: 1.5426  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1960/3000]  eta: 0:26:16  lr: 0.000027  loss: 0.5042  time: 1.5490  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1960/3000]  eta: 0:26:16  lr: 0.000027  loss: 0.5319  time: 1.5486  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1965/3000]  eta: 0:26:09  lr: 0.000027  loss: 0.8138  time: 1.5544  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1965/3000]  eta: 0:26:08  lr: 0.000027  loss: 0.4091  time: 1.5541  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1970/3000]  eta: 0:26:01  lr: 0.000027  loss: 0.0793  time: 1.5353  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1970/3000]  eta: 0:26:01  lr: 0.000027  loss: 0.5562  time: 1.5351  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1975/3000]  eta: 0:25:53  lr: 0.000027  loss: 0.8121  time: 1.5428  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1975/3000]  eta: 0:25:53  lr: 0.000027  loss: 0.6146  time: 1.5425  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1980/3000]  eta: 0:25:46  lr: 0.000027  loss: 0.2314  time: 1.5392  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1980/3000]  eta: 0:25:46  lr: 0.000027  loss: 0.6278  time: 1.5390  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1985/3000]  eta: 0:25:38  lr: 0.000027  loss: 0.2261  time: 1.5316  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1985/3000]  eta: 0:25:38  lr: 0.000027  loss: 0.1054  time: 1.5313  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [1990/3000]  eta: 0:25:31  lr: 0.000027  loss: 0.3241  time: 1.5620  data: 0.0000  max mem: 18406Train: data epoch: [7]  [1990/3000]  eta: 0:25:31  lr: 0.000027  loss: 1.4635  time: 1.5623  data: 0.0000  max mem: 18432

Train: data epoch: [7]  [1995/3000]  eta: 0:25:23  lr: 0.000027  loss: 0.6135  time: 1.5590  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [1995/3000]  eta: 0:25:23  lr: 0.000027  loss: 0.1513  time: 1.5588  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2000/3000]  eta: 0:25:16  lr: 0.000027  loss: 0.6935  time: 1.5564  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2000/3000]  eta: 0:25:16  lr: 0.000027  loss: 1.1162  time: 1.5561  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2005/3000]  eta: 0:25:09  lr: 0.000027  loss: 0.3242  time: 1.5617  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2005/3000]  eta: 0:25:08  lr: 0.000027  loss: 0.3575  time: 1.5615  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2010/3000]  eta: 0:25:01  lr: 0.000027  loss: 0.2725  time: 1.5438  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2010/3000]  eta: 0:25:01  lr: 0.000027  loss: 0.1681  time: 1.5438  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2015/3000]  eta: 0:24:53  lr: 0.000027  loss: 0.3002  time: 1.5339  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2015/3000]  eta: 0:24:53  lr: 0.000027  loss: 0.1969  time: 1.5337  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2020/3000]  eta: 0:24:46  lr: 0.000027  loss: 0.3438  time: 1.5213  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2020/3000]  eta: 0:24:45  lr: 0.000027  loss: 0.3496  time: 1.5210  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2025/3000]  eta: 0:24:38  lr: 0.000027  loss: 0.5061  time: 1.4987  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2025/3000]  eta: 0:24:38  lr: 0.000027  loss: 0.5999  time: 1.4985  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2030/3000]  eta: 0:24:31  lr: 0.000027  loss: 0.3169  time: 1.5068  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2030/3000]  eta: 0:24:30  lr: 0.000027  loss: 0.6192  time: 1.5065  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2035/3000]  eta: 0:24:23  lr: 0.000027  loss: 0.5532  time: 1.5132  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2035/3000]  eta: 0:24:23  lr: 0.000027  loss: 0.5299  time: 1.5129  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2040/3000]  eta: 0:24:15  lr: 0.000027  loss: 0.4175  time: 1.5128  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2040/3000]  eta: 0:24:15  lr: 0.000027  loss: 0.1598  time: 1.5126  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2045/3000]  eta: 0:24:08  lr: 0.000027  loss: 1.1865  time: 1.5077  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2045/3000]  eta: 0:24:07  lr: 0.000027  loss: 0.9003  time: 1.5074  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2050/3000]  eta: 0:24:00  lr: 0.000027  loss: 0.2811  time: 1.5047  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2050/3000]  eta: 0:24:00  lr: 0.000027  loss: 0.5983  time: 1.5044  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2055/3000]  eta: 0:23:52  lr: 0.000027  loss: 0.8414  time: 1.4873  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2055/3000]  eta: 0:23:52  lr: 0.000027  loss: 0.3317  time: 1.4870  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2060/3000]  eta: 0:23:45  lr: 0.000027  loss: 0.1260  time: 1.4752  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2060/3000]  eta: 0:23:44  lr: 0.000027  loss: 0.6544  time: 1.4749  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2065/3000]  eta: 0:23:37  lr: 0.000027  loss: 1.1364  time: 1.5020  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2065/3000]  eta: 0:23:37  lr: 0.000027  loss: 0.6161  time: 1.5018  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2070/3000]  eta: 0:23:30  lr: 0.000027  loss: 0.1715  time: 1.4973  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2070/3000]  eta: 0:23:29  lr: 0.000027  loss: 0.2635  time: 1.4970  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2075/3000]  eta: 0:23:22  lr: 0.000027  loss: 0.6429  time: 1.5147  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2075/3000]  eta: 0:23:22  lr: 0.000027  loss: 0.2170  time: 1.5144  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2080/3000]  eta: 0:23:14  lr: 0.000027  loss: 0.7158  time: 1.5160  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2080/3000]  eta: 0:23:14  lr: 0.000027  loss: 0.1164  time: 1.5158  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2085/3000]  eta: 0:23:07  lr: 0.000027  loss: 0.5238  time: 1.5173  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2085/3000]  eta: 0:23:07  lr: 0.000027  loss: 0.3953  time: 1.5170  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2090/3000]  eta: 0:22:59  lr: 0.000027  loss: 0.1706  time: 1.5247  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2090/3000]  eta: 0:22:59  lr: 0.000027  loss: 1.0824  time: 1.5245  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2095/3000]  eta: 0:22:52  lr: 0.000027  loss: 0.2699  time: 1.5152  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2095/3000]  eta: 0:22:51  lr: 0.000027  loss: 0.3698  time: 1.5149  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2100/3000]  eta: 0:22:44  lr: 0.000027  loss: 0.3616  time: 1.5148  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2100/3000]  eta: 0:22:44  lr: 0.000027  loss: 0.9608  time: 1.5146  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2105/3000]  eta: 0:22:36  lr: 0.000027  loss: 0.5521  time: 1.4865  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2105/3000]  eta: 0:22:36  lr: 0.000027  loss: 0.2659  time: 1.4863  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2110/3000]  eta: 0:22:29  lr: 0.000027  loss: 0.5920  time: 1.4720  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2110/3000]  eta: 0:22:28  lr: 0.000027  loss: 0.8286  time: 1.4717  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2115/3000]  eta: 0:22:21  lr: 0.000027  loss: 0.1757  time: 1.4828  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2115/3000]  eta: 0:22:21  lr: 0.000027  loss: 0.5018  time: 1.4826  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2120/3000]  eta: 0:22:13  lr: 0.000027  loss: 0.1700  time: 1.4831  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2120/3000]  eta: 0:22:13  lr: 0.000027  loss: 1.7982  time: 1.4829  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2125/3000]  eta: 0:22:06  lr: 0.000027  loss: 0.3274  time: 1.4962  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2125/3000]  eta: 0:22:06  lr: 0.000027  loss: 0.1235  time: 1.4959  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2130/3000]  eta: 0:21:58  lr: 0.000027  loss: 0.5947  time: 1.5063  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2130/3000]  eta: 0:21:58  lr: 0.000027  loss: 0.6283  time: 1.5061  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2135/3000]  eta: 0:21:51  lr: 0.000027  loss: 0.5263  time: 1.5086  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2135/3000]  eta: 0:21:50  lr: 0.000027  loss: 0.9364  time: 1.5082  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2140/3000]  eta: 0:21:43  lr: 0.000027  loss: 0.2274  time: 1.5137  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2140/3000]  eta: 0:21:43  lr: 0.000027  loss: 0.2915  time: 1.5132  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2145/3000]  eta: 0:21:35  lr: 0.000027  loss: 0.3836  time: 1.5013  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2145/3000]  eta: 0:21:35  lr: 0.000027  loss: 0.2174  time: 1.5009  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2150/3000]  eta: 0:21:28  lr: 0.000027  loss: 0.2222  time: 1.5009  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2150/3000]  eta: 0:21:28  lr: 0.000027  loss: 0.3965  time: 1.5004  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2155/3000]  eta: 0:21:20  lr: 0.000027  loss: 0.6344  time: 1.4915  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2155/3000]  eta: 0:21:20  lr: 0.000027  loss: 0.5655  time: 1.4912  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2160/3000]  eta: 0:21:12  lr: 0.000027  loss: 0.2736  time: 1.4827  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2160/3000]  eta: 0:21:12  lr: 0.000027  loss: 0.2982  time: 1.4824  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2165/3000]  eta: 0:21:05  lr: 0.000027  loss: 0.2840  time: 1.5052  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2165/3000]  eta: 0:21:05  lr: 0.000027  loss: 0.4022  time: 1.5049  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2170/3000]  eta: 0:20:57  lr: 0.000027  loss: 0.5286  time: 1.5098  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2170/3000]  eta: 0:20:57  lr: 0.000027  loss: 0.5262  time: 1.5095  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2175/3000]  eta: 0:20:50  lr: 0.000027  loss: 0.2204  time: 1.5327  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2175/3000]  eta: 0:20:50  lr: 0.000027  loss: 0.5826  time: 1.5324  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2180/3000]  eta: 0:20:42  lr: 0.000027  loss: 0.4736  time: 1.5547  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2180/3000]  eta: 0:20:42  lr: 0.000027  loss: 0.5538  time: 1.5544  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2185/3000]  eta: 0:20:35  lr: 0.000027  loss: 0.5739  time: 1.5310  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2185/3000]  eta: 0:20:35  lr: 0.000027  loss: 0.2040  time: 1.5307  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2190/3000]  eta: 0:20:27  lr: 0.000027  loss: 0.5015  time: 1.5143  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2190/3000]  eta: 0:20:27  lr: 0.000027  loss: 0.5537  time: 1.5141  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2195/3000]  eta: 0:20:20  lr: 0.000027  loss: 0.5724  time: 1.5038  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2195/3000]  eta: 0:20:19  lr: 0.000027  loss: 0.1151  time: 1.5035  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2200/3000]  eta: 0:20:12  lr: 0.000027  loss: 0.6013  time: 1.4956  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2200/3000]  eta: 0:20:12  lr: 0.000027  loss: 0.2302  time: 1.4954  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2205/3000]  eta: 0:20:04  lr: 0.000027  loss: 0.3733  time: 1.4934  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2205/3000]  eta: 0:20:04  lr: 0.000027  loss: 0.2518  time: 1.4932  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2210/3000]  eta: 0:19:57  lr: 0.000027  loss: 0.9388  time: 1.4997  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2210/3000]  eta: 0:19:57  lr: 0.000027  loss: 0.3917  time: 1.4995  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2215/3000]  eta: 0:19:49  lr: 0.000027  loss: 0.0865  time: 1.4801  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2215/3000]  eta: 0:19:49  lr: 0.000027  loss: 0.6264  time: 1.4798  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2220/3000]  eta: 0:19:41  lr: 0.000027  loss: 0.4817  time: 1.4900  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2220/3000]  eta: 0:19:42  lr: 0.000027  loss: 1.0177  time: 1.4903  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2225/3000]  eta: 0:19:34  lr: 0.000027  loss: 0.4775  time: 1.4933  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2225/3000]  eta: 0:19:34  lr: 0.000027  loss: 0.1832  time: 1.4931  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2230/3000]  eta: 0:19:26  lr: 0.000027  loss: 0.4335  time: 1.4908  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2230/3000]  eta: 0:19:26  lr: 0.000027  loss: 0.3081  time: 1.4906  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2235/3000]  eta: 0:19:19  lr: 0.000027  loss: 0.9620  time: 1.5078  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2235/3000]  eta: 0:19:18  lr: 0.000027  loss: 0.7994  time: 1.5076  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2240/3000]  eta: 0:19:11  lr: 0.000027  loss: 0.2202  time: 1.5137  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2240/3000]  eta: 0:19:11  lr: 0.000027  loss: 0.4113  time: 1.5136  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2245/3000]  eta: 0:19:04  lr: 0.000027  loss: 0.2002  time: 1.5281  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2245/3000]  eta: 0:19:03  lr: 0.000027  loss: 0.3161  time: 1.5278  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2250/3000]  eta: 0:18:56  lr: 0.000027  loss: 0.3537  time: 1.5270  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2250/3000]  eta: 0:18:56  lr: 0.000027  loss: 0.3942  time: 1.5268  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2255/3000]  eta: 0:18:48  lr: 0.000027  loss: 0.7587  time: 1.5361  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2255/3000]  eta: 0:18:48  lr: 0.000027  loss: 0.5148  time: 1.5359  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2260/3000]  eta: 0:18:41  lr: 0.000027  loss: 0.4319  time: 1.5243  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2260/3000]  eta: 0:18:41  lr: 0.000027  loss: 0.6626  time: 1.5241  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2265/3000]  eta: 0:18:33  lr: 0.000027  loss: 0.5459  time: 1.5305  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2265/3000]  eta: 0:18:33  lr: 0.000027  loss: 0.4701  time: 1.5303  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2270/3000]  eta: 0:18:26  lr: 0.000027  loss: 0.5279  time: 1.5515  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2270/3000]  eta: 0:18:26  lr: 0.000027  loss: 0.1340  time: 1.5513  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2275/3000]  eta: 0:18:18  lr: 0.000027  loss: 0.2555  time: 1.5482  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2275/3000]  eta: 0:18:18  lr: 0.000027  loss: 1.8227  time: 1.5479  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2280/3000]  eta: 0:18:11  lr: 0.000027  loss: 1.2817  time: 1.5272  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2280/3000]  eta: 0:18:11  lr: 0.000027  loss: 0.5313  time: 1.5270  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2285/3000]  eta: 0:18:03  lr: 0.000027  loss: 0.2430  time: 1.5061  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2285/3000]  eta: 0:18:03  lr: 0.000027  loss: 0.7214  time: 1.5058  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2290/3000]  eta: 0:17:56  lr: 0.000027  loss: 0.4088  time: 1.5010  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2290/3000]  eta: 0:17:55  lr: 0.000027  loss: 0.5696  time: 1.5008  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2295/3000]  eta: 0:17:48  lr: 0.000027  loss: 0.2600  time: 1.5118  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2295/3000]  eta: 0:17:48  lr: 0.000027  loss: 0.3414  time: 1.5116  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2300/3000]  eta: 0:17:40  lr: 0.000027  loss: 0.1512  time: 1.5215  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2300/3000]  eta: 0:17:40  lr: 0.000027  loss: 0.1686  time: 1.5213  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2305/3000]  eta: 0:17:33  lr: 0.000027  loss: 0.6443  time: 1.5307  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2305/3000]  eta: 0:17:33  lr: 0.000027  loss: 0.1586  time: 1.5305  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2310/3000]  eta: 0:17:25  lr: 0.000027  loss: 0.2320  time: 1.5206  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2310/3000]  eta: 0:17:25  lr: 0.000027  loss: 0.6676  time: 1.5203  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2315/3000]  eta: 0:17:17  lr: 0.000027  loss: 0.3075  time: 1.4728  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2315/3000]  eta: 0:17:17  lr: 0.000027  loss: 0.4008  time: 1.4726  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2320/3000]  eta: 0:17:10  lr: 0.000027  loss: 0.1583  time: 1.4778  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2320/3000]  eta: 0:17:10  lr: 0.000027  loss: 0.3244  time: 1.4775  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2325/3000]  eta: 0:17:02  lr: 0.000027  loss: 0.3070  time: 1.4918  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2325/3000]  eta: 0:17:02  lr: 0.000027  loss: 0.5726  time: 1.4915  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2330/3000]  eta: 0:16:55  lr: 0.000027  loss: 0.8967  time: 1.4891  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2330/3000]  eta: 0:16:55  lr: 0.000027  loss: 0.1996  time: 1.4888  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2335/3000]  eta: 0:16:47  lr: 0.000027  loss: 0.4642  time: 1.4941  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2335/3000]  eta: 0:16:47  lr: 0.000027  loss: 0.2018  time: 1.4939  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2340/3000]  eta: 0:16:39  lr: 0.000027  loss: 0.4151  time: 1.4898  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2340/3000]  eta: 0:16:39  lr: 0.000027  loss: 0.2181  time: 1.4895  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2345/3000]  eta: 0:16:32  lr: 0.000027  loss: 0.2308  time: 1.4647  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2345/3000]  eta: 0:16:32  lr: 0.000027  loss: 0.4023  time: 1.4645  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2350/3000]  eta: 0:16:24  lr: 0.000027  loss: 0.1873  time: 1.4417  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2350/3000]  eta: 0:16:24  lr: 0.000027  loss: 0.0873  time: 1.4415  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2355/3000]  eta: 0:16:16  lr: 0.000027  loss: 0.1161  time: 1.4678  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2355/3000]  eta: 0:16:16  lr: 0.000027  loss: 0.2124  time: 1.4675  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2360/3000]  eta: 0:16:09  lr: 0.000027  loss: 0.6855  time: 1.4847  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2360/3000]  eta: 0:16:09  lr: 0.000027  loss: 0.4634  time: 1.4845  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2365/3000]  eta: 0:16:01  lr: 0.000027  loss: 0.7641  time: 1.4900  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2365/3000]  eta: 0:16:01  lr: 0.000027  loss: 0.3017  time: 1.4898  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2370/3000]  eta: 0:15:54  lr: 0.000027  loss: 0.4658  time: 1.5154  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2370/3000]  eta: 0:15:54  lr: 0.000027  loss: 0.4057  time: 1.5151  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2375/3000]  eta: 0:15:46  lr: 0.000027  loss: 0.2171  time: 1.4959  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2375/3000]  eta: 0:15:46  lr: 0.000027  loss: 0.1967  time: 1.4956  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2380/3000]  eta: 0:15:38  lr: 0.000027  loss: 1.1691  time: 1.4740  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2380/3000]  eta: 0:15:38  lr: 0.000027  loss: 0.6698  time: 1.4737  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2385/3000]  eta: 0:15:31  lr: 0.000027  loss: 0.2397  time: 1.4816  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2385/3000]  eta: 0:15:31  lr: 0.000027  loss: 0.2652  time: 1.4813  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2390/3000]  eta: 0:15:23  lr: 0.000027  loss: 0.5305  time: 1.4771  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2390/3000]  eta: 0:15:23  lr: 0.000027  loss: 0.2949  time: 1.4768  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2395/3000]  eta: 0:15:16  lr: 0.000027  loss: 1.1623  time: 1.5024  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2395/3000]  eta: 0:15:16  lr: 0.000027  loss: 0.9732  time: 1.5022  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2400/3000]  eta: 0:15:08  lr: 0.000027  loss: 0.4024  time: 1.5003  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2400/3000]  eta: 0:15:08  lr: 0.000027  loss: 0.4838  time: 1.5000  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2405/3000]  eta: 0:15:01  lr: 0.000027  loss: 0.1276  time: 1.4970  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2405/3000]  eta: 0:15:00  lr: 0.000027  loss: 0.4317  time: 1.4968  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2410/3000]  eta: 0:14:53  lr: 0.000027  loss: 0.7057  time: 1.5028  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2410/3000]  eta: 0:14:53  lr: 0.000027  loss: 0.4864  time: 1.5026  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2415/3000]  eta: 0:14:45  lr: 0.000027  loss: 0.4112  time: 1.4970  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2415/3000]  eta: 0:14:45  lr: 0.000027  loss: 0.6790  time: 1.4968  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2420/3000]  eta: 0:14:38  lr: 0.000027  loss: 0.3187  time: 1.4791  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2420/3000]  eta: 0:14:37  lr: 0.000027  loss: 0.4943  time: 1.4789  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2425/3000]  eta: 0:14:30  lr: 0.000027  loss: 0.2219  time: 1.4748  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2425/3000]  eta: 0:14:30  lr: 0.000027  loss: 0.3248  time: 1.4747  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2430/3000]  eta: 0:14:22  lr: 0.000027  loss: 0.3612  time: 1.4759  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2430/3000]  eta: 0:14:22  lr: 0.000027  loss: 0.3501  time: 1.4758  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2435/3000]  eta: 0:14:15  lr: 0.000027  loss: 0.9228  time: 1.4837  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2435/3000]  eta: 0:14:15  lr: 0.000027  loss: 0.6533  time: 1.4835  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2440/3000]  eta: 0:14:07  lr: 0.000027  loss: 0.5833  time: 1.5347  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2440/3000]  eta: 0:14:07  lr: 0.000027  loss: 0.1859  time: 1.5345  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2445/3000]  eta: 0:14:00  lr: 0.000027  loss: 0.6788  time: 1.5467  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2445/3000]  eta: 0:14:00  lr: 0.000027  loss: 0.4466  time: 1.5464  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2450/3000]  eta: 0:13:52  lr: 0.000027  loss: 0.2379  time: 1.5393  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2450/3000]  eta: 0:13:52  lr: 0.000027  loss: 0.1020  time: 1.5390  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2455/3000]  eta: 0:13:45  lr: 0.000027  loss: 0.1805  time: 1.5187  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2455/3000]  eta: 0:13:45  lr: 0.000027  loss: 0.0879  time: 1.5185  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2460/3000]  eta: 0:13:37  lr: 0.000027  loss: 0.3876  time: 1.4948  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2460/3000]  eta: 0:13:37  lr: 0.000027  loss: 0.2534  time: 1.4945  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2465/3000]  eta: 0:13:29  lr: 0.000027  loss: 0.6796  time: 1.4782  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2465/3000]  eta: 0:13:29  lr: 0.000027  loss: 0.3176  time: 1.4779  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2470/3000]  eta: 0:13:22  lr: 0.000027  loss: 0.2346  time: 1.4899  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2470/3000]  eta: 0:13:22  lr: 0.000027  loss: 0.2898  time: 1.4896  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2475/3000]  eta: 0:13:14  lr: 0.000027  loss: 0.1360  time: 1.5191  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2475/3000]  eta: 0:13:14  lr: 0.000027  loss: 0.4477  time: 1.5188  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2480/3000]  eta: 0:13:07  lr: 0.000027  loss: 0.2292  time: 1.5395  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2480/3000]  eta: 0:13:07  lr: 0.000027  loss: 0.6013  time: 1.5393  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2485/3000]  eta: 0:12:59  lr: 0.000027  loss: 0.1696  time: 1.5324  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2485/3000]  eta: 0:12:59  lr: 0.000027  loss: 0.0702  time: 1.5322  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2490/3000]  eta: 0:12:52  lr: 0.000027  loss: 0.1224  time: 1.5303  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2490/3000]  eta: 0:12:52  lr: 0.000027  loss: 0.4467  time: 1.5300  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2495/3000]  eta: 0:12:44  lr: 0.000027  loss: 0.5429  time: 1.5231  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2495/3000]  eta: 0:12:44  lr: 0.000027  loss: 0.2737  time: 1.5229  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2500/3000]  eta: 0:12:37  lr: 0.000027  loss: 0.2382  time: 1.4962  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2500/3000]  eta: 0:12:36  lr: 0.000027  loss: 0.4963  time: 1.4960  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2505/3000]  eta: 0:12:29  lr: 0.000027  loss: 0.7961  time: 1.4882  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2505/3000]  eta: 0:12:29  lr: 0.000027  loss: 0.4560  time: 1.4880  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2510/3000]  eta: 0:12:21  lr: 0.000027  loss: 0.9369  time: 1.4732  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2510/3000]  eta: 0:12:21  lr: 0.000027  loss: 0.3330  time: 1.4730  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2515/3000]  eta: 0:12:14  lr: 0.000027  loss: 0.4603  time: 1.4462  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2515/3000]  eta: 0:12:14  lr: 0.000027  loss: 0.0995  time: 1.4459  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2520/3000]  eta: 0:12:06  lr: 0.000027  loss: 0.6221  time: 1.4310  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2520/3000]  eta: 0:12:06  lr: 0.000027  loss: 0.3376  time: 1.4301  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2525/3000]  eta: 0:11:58  lr: 0.000027  loss: 0.3020  time: 1.4614  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2525/3000]  eta: 0:11:58  lr: 0.000027  loss: 0.2921  time: 1.4605  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2530/3000]  eta: 0:11:51  lr: 0.000027  loss: 0.4074  time: 1.4731  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2530/3000]  eta: 0:11:51  lr: 0.000027  loss: 0.3271  time: 1.4722  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2535/3000]  eta: 0:11:43  lr: 0.000027  loss: 0.4199  time: 1.4985  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2535/3000]  eta: 0:11:43  lr: 0.000027  loss: 0.3250  time: 1.4976  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2540/3000]  eta: 0:11:36  lr: 0.000027  loss: 0.5772  time: 1.5280  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2540/3000]  eta: 0:11:36  lr: 0.000027  loss: 0.2149  time: 1.5277  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2545/3000]  eta: 0:11:28  lr: 0.000027  loss: 0.3963  time: 1.5369  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2545/3000]  eta: 0:11:28  lr: 0.000027  loss: 0.4637  time: 1.5366  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2550/3000]  eta: 0:11:21  lr: 0.000027  loss: 0.2638  time: 1.5388  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2550/3000]  eta: 0:11:21  lr: 0.000027  loss: 0.4637  time: 1.5386  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2555/3000]  eta: 0:11:13  lr: 0.000027  loss: 0.6429  time: 1.5475  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2555/3000]  eta: 0:11:13  lr: 0.000027  loss: 0.6019  time: 1.5473  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2560/3000]  eta: 0:11:06  lr: 0.000027  loss: 0.1904  time: 1.5545  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2560/3000]  eta: 0:11:05  lr: 0.000027  loss: 0.4614  time: 1.5542  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2565/3000]  eta: 0:10:58  lr: 0.000027  loss: 0.1920  time: 1.5314  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2565/3000]  eta: 0:10:58  lr: 0.000027  loss: 0.2638  time: 1.5310  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2570/3000]  eta: 0:10:50  lr: 0.000027  loss: 0.4916  time: 1.5157  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2570/3000]  eta: 0:10:50  lr: 0.000027  loss: 0.1074  time: 1.5154  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2575/3000]  eta: 0:10:43  lr: 0.000027  loss: 0.5594  time: 1.4867  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2575/3000]  eta: 0:10:43  lr: 0.000027  loss: 0.0432  time: 1.4864  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2580/3000]  eta: 0:10:35  lr: 0.000027  loss: 0.3895  time: 1.4790  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2580/3000]  eta: 0:10:35  lr: 0.000027  loss: 0.5556  time: 1.4788  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2585/3000]  eta: 0:10:28  lr: 0.000027  loss: 0.2582  time: 1.4951  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2585/3000]  eta: 0:10:28  lr: 0.000027  loss: 0.3781  time: 1.4949  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2590/3000]  eta: 0:10:20  lr: 0.000027  loss: 0.4346  time: 1.5153  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2590/3000]  eta: 0:10:20  lr: 0.000027  loss: 0.1994  time: 1.5151  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2595/3000]  eta: 0:10:13  lr: 0.000027  loss: 0.5475  time: 1.5207  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2595/3000]  eta: 0:10:12  lr: 0.000027  loss: 0.4737  time: 1.5205  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2600/3000]  eta: 0:10:05  lr: 0.000027  loss: 0.0842  time: 1.5207  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2600/3000]  eta: 0:10:05  lr: 0.000027  loss: 0.4029  time: 1.5205  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2605/3000]  eta: 0:09:57  lr: 0.000027  loss: 0.2279  time: 1.5237  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2605/3000]  eta: 0:09:57  lr: 0.000027  loss: 0.5480  time: 1.5234  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2610/3000]  eta: 0:09:50  lr: 0.000027  loss: 0.8252  time: 1.5133  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2610/3000]  eta: 0:09:50  lr: 0.000027  loss: 0.4369  time: 1.5131  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2615/3000]  eta: 0:09:42  lr: 0.000027  loss: 0.3528  time: 1.5035  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2615/3000]  eta: 0:09:42  lr: 0.000027  loss: 0.5776  time: 1.5031  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2620/3000]  eta: 0:09:35  lr: 0.000027  loss: 0.0929  time: 1.4979  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2620/3000]  eta: 0:09:35  lr: 0.000027  loss: 0.1546  time: 1.4976  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2625/3000]  eta: 0:09:27  lr: 0.000027  loss: 0.3615  time: 1.5006  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2625/3000]  eta: 0:09:27  lr: 0.000027  loss: 0.4714  time: 1.5003  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2630/3000]  eta: 0:09:19  lr: 0.000027  loss: 0.5010  time: 1.4857  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2630/3000]  eta: 0:09:19  lr: 0.000027  loss: 0.1434  time: 1.4853  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2635/3000]  eta: 0:09:12  lr: 0.000027  loss: 0.1705  time: 1.5067  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2635/3000]  eta: 0:09:12  lr: 0.000027  loss: 0.9952  time: 1.5065  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2640/3000]  eta: 0:09:04  lr: 0.000027  loss: 0.3451  time: 1.5146  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2640/3000]  eta: 0:09:04  lr: 0.000027  loss: 0.1909  time: 1.5145  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2645/3000]  eta: 0:08:57  lr: 0.000027  loss: 0.5222  time: 1.5071  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2645/3000]  eta: 0:08:57  lr: 0.000027  loss: 0.9309  time: 1.5068  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2650/3000]  eta: 0:08:49  lr: 0.000027  loss: 0.0937  time: 1.5277  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2650/3000]  eta: 0:08:49  lr: 0.000027  loss: 0.2245  time: 1.5275  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2655/3000]  eta: 0:08:42  lr: 0.000027  loss: 0.6575  time: 1.5125  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2655/3000]  eta: 0:08:42  lr: 0.000027  loss: 0.2468  time: 1.5122  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2660/3000]  eta: 0:08:34  lr: 0.000027  loss: 0.2753  time: 1.5074  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2660/3000]  eta: 0:08:34  lr: 0.000027  loss: 0.8735  time: 1.5071  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2665/3000]  eta: 0:08:27  lr: 0.000027  loss: 0.4561  time: 1.4984  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2665/3000]  eta: 0:08:26  lr: 0.000027  loss: 0.1245  time: 1.4982  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2670/3000]  eta: 0:08:19  lr: 0.000027  loss: 0.2327  time: 1.5153  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2670/3000]  eta: 0:08:19  lr: 0.000027  loss: 0.4378  time: 1.5151  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2675/3000]  eta: 0:08:11  lr: 0.000027  loss: 0.3200  time: 1.5354  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2675/3000]  eta: 0:08:11  lr: 0.000027  loss: 0.2051  time: 1.5351  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2680/3000]  eta: 0:08:04  lr: 0.000027  loss: 0.5684  time: 1.5321  data: 0.0000  max mem: 18432
Train: data epoch: [7]  [2680/3000]  eta: 0:08:04  lr: 0.000027  loss: 0.1047  time: 1.5319  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2685/3000]  eta: 0:07:56  lr: 0.000027  loss: 0.0540  time: 1.5267  data: 0.0000  max mem: 18596Train: data epoch: [7]  [2685/3000]  eta: 0:07:56  lr: 0.000027  loss: 0.2449  time: 1.5264  data: 0.0000  max mem: 18406

Train: data epoch: [7]  [2690/3000]  eta: 0:07:49  lr: 0.000027  loss: 0.4048  time: 1.5066  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2690/3000]  eta: 0:07:49  lr: 0.000027  loss: 0.4530  time: 1.5064  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2695/3000]  eta: 0:07:41  lr: 0.000027  loss: 0.3388  time: 1.5084  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2695/3000]  eta: 0:07:41  lr: 0.000027  loss: 0.4149  time: 1.5082  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2700/3000]  eta: 0:07:34  lr: 0.000027  loss: 0.7602  time: 1.5156  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2700/3000]  eta: 0:07:34  lr: 0.000027  loss: 0.4709  time: 1.5154  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2705/3000]  eta: 0:07:26  lr: 0.000027  loss: 0.2306  time: 1.5200  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2705/3000]  eta: 0:07:26  lr: 0.000027  loss: 0.5793  time: 1.5199  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2710/3000]  eta: 0:07:18  lr: 0.000027  loss: 0.4945  time: 1.5074  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2710/3000]  eta: 0:07:18  lr: 0.000027  loss: 0.3890  time: 1.5072  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2715/3000]  eta: 0:07:11  lr: 0.000027  loss: 0.1115  time: 1.5000  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2715/3000]  eta: 0:07:11  lr: 0.000027  loss: 0.1479  time: 1.4998  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2720/3000]  eta: 0:07:03  lr: 0.000027  loss: 0.0731  time: 1.5055  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2720/3000]  eta: 0:07:03  lr: 0.000027  loss: 0.5366  time: 1.5053  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2725/3000]  eta: 0:06:56  lr: 0.000027  loss: 0.4777  time: 1.5241  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2725/3000]  eta: 0:06:56  lr: 0.000027  loss: 0.7929  time: 1.5239  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2730/3000]  eta: 0:06:48  lr: 0.000027  loss: 0.5613  time: 1.5482  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2730/3000]  eta: 0:06:48  lr: 0.000027  loss: 0.2675  time: 1.5480  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2735/3000]  eta: 0:06:41  lr: 0.000027  loss: 0.0793  time: 1.5311  data: 0.0000  max mem: 18406Train: data epoch: [7]  [2735/3000]  eta: 0:06:41  lr: 0.000027  loss: 0.7637  time: 1.5314  data: 0.0000  max mem: 18596

Train: data epoch: [7]  [2740/3000]  eta: 0:06:33  lr: 0.000027  loss: 0.5590  time: 1.5274  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2740/3000]  eta: 0:06:33  lr: 0.000027  loss: 0.6228  time: 1.5272  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2745/3000]  eta: 0:06:26  lr: 0.000027  loss: 0.0529  time: 1.5198  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2745/3000]  eta: 0:06:25  lr: 0.000027  loss: 0.4045  time: 1.5196  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2750/3000]  eta: 0:06:18  lr: 0.000027  loss: 0.1796  time: 1.5183  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2750/3000]  eta: 0:06:18  lr: 0.000027  loss: 0.4510  time: 1.5181  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2755/3000]  eta: 0:06:10  lr: 0.000027  loss: 0.2864  time: 1.5392  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2755/3000]  eta: 0:06:10  lr: 0.000027  loss: 0.3474  time: 1.5390  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2760/3000]  eta: 0:06:03  lr: 0.000027  loss: 0.4780  time: 1.5417  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2760/3000]  eta: 0:06:03  lr: 0.000027  loss: 0.5705  time: 1.5415  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2765/3000]  eta: 0:05:55  lr: 0.000027  loss: 1.1024  time: 1.5479  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2765/3000]  eta: 0:05:55  lr: 0.000027  loss: 0.1864  time: 1.5476  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2770/3000]  eta: 0:05:48  lr: 0.000027  loss: 0.0882  time: 1.5359  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2770/3000]  eta: 0:05:48  lr: 0.000027  loss: 0.2150  time: 1.5356  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2775/3000]  eta: 0:05:40  lr: 0.000027  loss: 0.6648  time: 1.5354  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2775/3000]  eta: 0:05:40  lr: 0.000027  loss: 0.6234  time: 1.5353  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2780/3000]  eta: 0:05:33  lr: 0.000027  loss: 0.2300  time: 1.5155  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2780/3000]  eta: 0:05:33  lr: 0.000027  loss: 0.4633  time: 1.5158  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2785/3000]  eta: 0:05:25  lr: 0.000027  loss: 0.1918  time: 1.4965  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2785/3000]  eta: 0:05:25  lr: 0.000027  loss: 0.4158  time: 1.4963  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2790/3000]  eta: 0:05:17  lr: 0.000027  loss: 0.0919  time: 1.4830  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2790/3000]  eta: 0:05:17  lr: 0.000027  loss: 0.4246  time: 1.4828  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2795/3000]  eta: 0:05:10  lr: 0.000027  loss: 0.4811  time: 1.4591  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2795/3000]  eta: 0:05:10  lr: 0.000027  loss: 0.4275  time: 1.4588  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2800/3000]  eta: 0:05:02  lr: 0.000027  loss: 0.6263  time: 1.4737  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2800/3000]  eta: 0:05:02  lr: 0.000027  loss: 0.0945  time: 1.4735  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2805/3000]  eta: 0:04:55  lr: 0.000027  loss: 0.1663  time: 1.4744  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2805/3000]  eta: 0:04:55  lr: 0.000027  loss: 0.3093  time: 1.4742  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2810/3000]  eta: 0:04:47  lr: 0.000027  loss: 0.8132  time: 1.4965  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2810/3000]  eta: 0:04:47  lr: 0.000027  loss: 0.5267  time: 1.4962  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2815/3000]  eta: 0:04:40  lr: 0.000027  loss: 0.3684  time: 1.5185  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2815/3000]  eta: 0:04:39  lr: 0.000027  loss: 0.2863  time: 1.5183  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2820/3000]  eta: 0:04:32  lr: 0.000027  loss: 0.3496  time: 1.5094  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2820/3000]  eta: 0:04:32  lr: 0.000027  loss: 0.5170  time: 1.5093  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2825/3000]  eta: 0:04:24  lr: 0.000027  loss: 0.3960  time: 1.4950  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2825/3000]  eta: 0:04:24  lr: 0.000027  loss: 0.2051  time: 1.4948  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2830/3000]  eta: 0:04:17  lr: 0.000027  loss: 0.0976  time: 1.4671  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2830/3000]  eta: 0:04:17  lr: 0.000027  loss: 0.1261  time: 1.4669  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2835/3000]  eta: 0:04:09  lr: 0.000027  loss: 0.1555  time: 1.4431  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2835/3000]  eta: 0:04:09  lr: 0.000027  loss: 0.6210  time: 1.4428  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2840/3000]  eta: 0:04:02  lr: 0.000027  loss: 0.6645  time: 1.4324  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2840/3000]  eta: 0:04:02  lr: 0.000027  loss: 0.4521  time: 1.4320  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2845/3000]  eta: 0:03:54  lr: 0.000027  loss: 0.3680  time: 1.4597  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2845/3000]  eta: 0:03:54  lr: 0.000027  loss: 0.4168  time: 1.4595  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2850/3000]  eta: 0:03:46  lr: 0.000027  loss: 0.4185  time: 1.4787  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2850/3000]  eta: 0:03:46  lr: 0.000027  loss: 0.1987  time: 1.4784  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2855/3000]  eta: 0:03:39  lr: 0.000027  loss: 0.2418  time: 1.4935  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2855/3000]  eta: 0:03:39  lr: 0.000027  loss: 0.3946  time: 1.4933  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2860/3000]  eta: 0:03:31  lr: 0.000027  loss: 0.2442  time: 1.5001  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2860/3000]  eta: 0:03:31  lr: 0.000027  loss: 0.5234  time: 1.4999  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2865/3000]  eta: 0:03:24  lr: 0.000027  loss: 0.2807  time: 1.4907  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2865/3000]  eta: 0:03:24  lr: 0.000027  loss: 0.3926  time: 1.4905  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2870/3000]  eta: 0:03:16  lr: 0.000027  loss: 0.1144  time: 1.4995  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2870/3000]  eta: 0:03:16  lr: 0.000027  loss: 0.7367  time: 1.4992  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2875/3000]  eta: 0:03:09  lr: 0.000027  loss: 0.5531  time: 1.5114  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2875/3000]  eta: 0:03:09  lr: 0.000027  loss: 0.7068  time: 1.5111  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2880/3000]  eta: 0:03:01  lr: 0.000027  loss: 0.1377  time: 1.4926  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2880/3000]  eta: 0:03:01  lr: 0.000027  loss: 0.3884  time: 1.4924  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2885/3000]  eta: 0:02:53  lr: 0.000027  loss: 0.3759  time: 1.4847  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2885/3000]  eta: 0:02:53  lr: 0.000027  loss: 0.4356  time: 1.4844  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2890/3000]  eta: 0:02:46  lr: 0.000027  loss: 0.3252  time: 1.4930  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2890/3000]  eta: 0:02:46  lr: 0.000027  loss: 0.6106  time: 1.4927  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2895/3000]  eta: 0:02:38  lr: 0.000027  loss: 0.4621  time: 1.4932  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2895/3000]  eta: 0:02:38  lr: 0.000027  loss: 0.4608  time: 1.4930  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2900/3000]  eta: 0:02:31  lr: 0.000027  loss: 0.5966  time: 1.5183  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2900/3000]  eta: 0:02:31  lr: 0.000027  loss: 0.1906  time: 1.5181  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2905/3000]  eta: 0:02:23  lr: 0.000027  loss: 0.1536  time: 1.5378  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2905/3000]  eta: 0:02:23  lr: 0.000027  loss: 0.1440  time: 1.5375  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2910/3000]  eta: 0:02:16  lr: 0.000027  loss: 0.8794  time: 1.5188  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2910/3000]  eta: 0:02:16  lr: 0.000027  loss: 0.1216  time: 1.5185  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2915/3000]  eta: 0:02:08  lr: 0.000027  loss: 0.2003  time: 1.5193  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2915/3000]  eta: 0:02:08  lr: 0.000027  loss: 0.7887  time: 1.5190  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2920/3000]  eta: 0:02:01  lr: 0.000027  loss: 0.2718  time: 1.5077  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2920/3000]  eta: 0:02:01  lr: 0.000027  loss: 0.2941  time: 1.5075  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2925/3000]  eta: 0:01:53  lr: 0.000027  loss: 0.7425  time: 1.5131  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2925/3000]  eta: 0:01:53  lr: 0.000027  loss: 0.7660  time: 1.5130  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2930/3000]  eta: 0:01:45  lr: 0.000027  loss: 0.4922  time: 1.5158  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2930/3000]  eta: 0:01:45  lr: 0.000027  loss: 0.5779  time: 1.5156  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2935/3000]  eta: 0:01:38  lr: 0.000027  loss: 0.5961  time: 1.5253  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2935/3000]  eta: 0:01:38  lr: 0.000027  loss: 0.8822  time: 1.5252  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2940/3000]  eta: 0:01:30  lr: 0.000027  loss: 0.2212  time: 1.5383  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2940/3000]  eta: 0:01:30  lr: 0.000027  loss: 0.6116  time: 1.5380  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2945/3000]  eta: 0:01:23  lr: 0.000027  loss: 0.2553  time: 1.5328  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2945/3000]  eta: 0:01:23  lr: 0.000027  loss: 0.4033  time: 1.5325  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2950/3000]  eta: 0:01:15  lr: 0.000027  loss: 0.6319  time: 1.5336  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2950/3000]  eta: 0:01:15  lr: 0.000027  loss: 0.0860  time: 1.5333  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2955/3000]  eta: 0:01:08  lr: 0.000027  loss: 0.4753  time: 1.5175  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2955/3000]  eta: 0:01:08  lr: 0.000027  loss: 0.6225  time: 1.5172  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2960/3000]  eta: 0:01:00  lr: 0.000027  loss: 0.6446  time: 1.5288  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2960/3000]  eta: 0:01:00  lr: 0.000027  loss: 0.5205  time: 1.5285  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2965/3000]  eta: 0:00:52  lr: 0.000027  loss: 0.3771  time: 1.5351  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2965/3000]  eta: 0:00:52  lr: 0.000027  loss: 0.3580  time: 1.5349  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2970/3000]  eta: 0:00:45  lr: 0.000027  loss: 0.1905  time: 1.5551  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2970/3000]  eta: 0:00:45  lr: 0.000027  loss: 0.2714  time: 1.5549  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2975/3000]  eta: 0:00:37  lr: 0.000027  loss: 0.5109  time: 1.5630  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2975/3000]  eta: 0:00:37  lr: 0.000027  loss: 0.6387  time: 1.5628  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2980/3000]  eta: 0:00:30  lr: 0.000027  loss: 0.2529  time: 1.5555  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2980/3000]  eta: 0:00:30  lr: 0.000027  loss: 1.0968  time: 1.5553  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2985/3000]  eta: 0:00:22  lr: 0.000027  loss: 0.2605  time: 1.5562  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2985/3000]  eta: 0:00:22  lr: 0.000027  loss: 0.3155  time: 1.5559  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2990/3000]  eta: 0:00:15  lr: 0.000027  loss: 0.9365  time: 1.5258  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2990/3000]  eta: 0:00:15  lr: 0.000027  loss: 0.3103  time: 1.5255  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2995/3000]  eta: 0:00:07  lr: 0.000027  loss: 0.2869  time: 1.5205  data: 0.0000  max mem: 18596
Train: data epoch: [7]  [2995/3000]  eta: 0:00:07  lr: 0.000027  loss: 0.3530  time: 1.5202  data: 0.0000  max mem: 18406
Train: data epoch: [7]  [2999/3000]  eta: 0:00:01  lr: 0.000027  loss: 0.5321  time: 1.5151  data: 0.0000  max mem: 18596
Train: data epoch: [7] Total time: 1:15:40 (1.5136 s / it)
Train: data epoch: [7]  [2999/3000]  eta: 0:00:01  lr: 0.000027  loss: 0.3305  time: 1.5149  data: 0.0000  max mem: 18406
Train: data epoch: [7] Total time: 1:15:40 (1.5136 s / it)
2025-01-19 09:46:28,344 [INFO] Averaged stats: lr: 0.0000  loss: 0.4239
2025-01-19 09:46:28,348 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [7]  [0/1]  eta: 0:00:00    time: 0.7921  data: 0.4926  max mem: 18406
Eval: data epoch: [7] Total time: 0:00:00 (0.9193 s / it)
Eval: data epoch: [7]  [0/1]  eta: 0:00:00    time: 0.9606  data: 0.6710  max mem: 18596
Eval: data epoch: [7] Total time: 0:00:01 (1.0995 s / it)
2025-01-19 09:46:29,474 [INFO] Saving checkpoint at epoch 7 to outputs_stage1_only/202501182338/checkpoint_7.pth.
2025-01-19 09:46:31,848 [INFO] Training Phase
2025-01-19 09:46:31,856 [INFO] Start training epoch 8, 3000 iters per inner epoch.
Train: data epoch: [8]  [   0/3000]  eta: 1:20:13  lr: 0.000027  loss: 0.8530  time: 1.6044  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [   0/3000]  eta: 1:20:10  lr: 0.000027  loss: 0.1924  time: 1.6034  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [   5/3000]  eta: 1:18:41  lr: 0.000027  loss: 0.2064  time: 1.5764  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [   5/3000]  eta: 1:18:40  lr: 0.000027  loss: 0.4118  time: 1.5761  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [  10/3000]  eta: 1:17:24  lr: 0.000027  loss: 0.7436  time: 1.5532  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [  10/3000]  eta: 1:17:22  lr: 0.000027  loss: 0.3626  time: 1.5528  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [  15/3000]  eta: 1:16:51  lr: 0.000027  loss: 0.4190  time: 1.5448  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [  15/3000]  eta: 1:16:50  lr: 0.000027  loss: 0.3183  time: 1.5445  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [  20/3000]  eta: 1:16:24  lr: 0.000027  loss: 0.4058  time: 1.5352  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [  20/3000]  eta: 1:16:23  lr: 0.000027  loss: 0.3661  time: 1.5349  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [  25/3000]  eta: 1:15:39  lr: 0.000027  loss: 0.1826  time: 1.5108  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [  25/3000]  eta: 1:15:38  lr: 0.000027  loss: 0.7324  time: 1.5106  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [  30/3000]  eta: 1:15:06  lr: 0.000027  loss: 0.5322  time: 1.4978  data: 0.0000  max mem: 18406Train: data epoch: [8]  [  30/3000]  eta: 1:15:07  lr: 0.000027  loss: 0.3057  time: 1.4981  data: 0.0000  max mem: 18596

Train: data epoch: [8]  [  35/3000]  eta: 1:15:07  lr: 0.000027  loss: 0.4153  time: 1.5008  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [  35/3000]  eta: 1:15:06  lr: 0.000027  loss: 0.4608  time: 1.5005  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [  40/3000]  eta: 1:15:05  lr: 0.000027  loss: 1.4887  time: 1.5051  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [  40/3000]  eta: 1:15:04  lr: 0.000027  loss: 0.6932  time: 1.5049  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [  45/3000]  eta: 1:15:01  lr: 0.000027  loss: 0.3468  time: 1.5201  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [  45/3000]  eta: 1:15:00  lr: 0.000027  loss: 0.9034  time: 1.5198  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [  50/3000]  eta: 1:15:06  lr: 0.000027  loss: 0.2880  time: 1.5432  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [  50/3000]  eta: 1:15:05  lr: 0.000027  loss: 0.3598  time: 1.5430  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [  55/3000]  eta: 1:14:52  lr: 0.000027  loss: 0.8796  time: 1.5350  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [  55/3000]  eta: 1:14:51  lr: 0.000027  loss: 0.9766  time: 1.5347  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [  60/3000]  eta: 1:14:41  lr: 0.000027  loss: 0.0662  time: 1.5286  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [  60/3000]  eta: 1:14:40  lr: 0.000027  loss: 0.3140  time: 1.5284  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [  65/3000]  eta: 1:14:39  lr: 0.000027  loss: 0.9138  time: 1.5323  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [  65/3000]  eta: 1:14:38  lr: 0.000027  loss: 0.2585  time: 1.5321  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [  70/3000]  eta: 1:14:19  lr: 0.000027  loss: 1.0584  time: 1.5073  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [  70/3000]  eta: 1:14:18  lr: 0.000027  loss: 0.3717  time: 1.5071  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [  75/3000]  eta: 1:14:06  lr: 0.000027  loss: 0.1171  time: 1.5048  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [  75/3000]  eta: 1:14:05  lr: 0.000027  loss: 0.3314  time: 1.5046  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [  80/3000]  eta: 1:13:53  lr: 0.000027  loss: 0.2957  time: 1.4999  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [  80/3000]  eta: 1:13:52  lr: 0.000027  loss: 0.3455  time: 1.4997  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [  85/3000]  eta: 1:13:55  lr: 0.000027  loss: 0.6094  time: 1.5073  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [  85/3000]  eta: 1:13:55  lr: 0.000027  loss: 0.4630  time: 1.5070  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [  90/3000]  eta: 1:13:44  lr: 0.000027  loss: 0.3119  time: 1.5150  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [  90/3000]  eta: 1:13:43  lr: 0.000027  loss: 0.4504  time: 1.5148  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [  95/3000]  eta: 1:13:21  lr: 0.000027  loss: 0.4189  time: 1.4971  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [  95/3000]  eta: 1:13:21  lr: 0.000027  loss: 0.3878  time: 1.4968  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 100/3000]  eta: 1:13:13  lr: 0.000027  loss: 0.5267  time: 1.5016  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 100/3000]  eta: 1:13:12  lr: 0.000027  loss: 0.7705  time: 1.5013  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 105/3000]  eta: 1:13:04  lr: 0.000027  loss: 0.2145  time: 1.4830  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 105/3000]  eta: 1:13:03  lr: 0.000027  loss: 0.3402  time: 1.4828  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 110/3000]  eta: 1:12:51  lr: 0.000027  loss: 0.6216  time: 1.4771  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 110/3000]  eta: 1:12:50  lr: 0.000027  loss: 0.3352  time: 1.4769  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 115/3000]  eta: 1:12:48  lr: 0.000027  loss: 0.3796  time: 1.5081  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 115/3000]  eta: 1:12:47  lr: 0.000027  loss: 0.2607  time: 1.5074  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 120/3000]  eta: 1:12:41  lr: 0.000027  loss: 0.1383  time: 1.5111  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 120/3000]  eta: 1:12:40  lr: 0.000027  loss: 0.2336  time: 1.5104  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 125/3000]  eta: 1:12:30  lr: 0.000027  loss: 0.3190  time: 1.5074  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 125/3000]  eta: 1:12:29  lr: 0.000027  loss: 0.4246  time: 1.5067  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 130/3000]  eta: 1:12:20  lr: 0.000027  loss: 0.2239  time: 1.5098  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 130/3000]  eta: 1:12:19  lr: 0.000027  loss: 0.3359  time: 1.5091  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 135/3000]  eta: 1:12:06  lr: 0.000027  loss: 0.7740  time: 1.4878  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 135/3000]  eta: 1:12:05  lr: 0.000027  loss: 0.7523  time: 1.4876  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 140/3000]  eta: 1:12:06  lr: 0.000027  loss: 0.3936  time: 1.5023  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 140/3000]  eta: 1:12:05  lr: 0.000027  loss: 0.6255  time: 1.5021  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 145/3000]  eta: 1:12:02  lr: 0.000027  loss: 0.2484  time: 1.5185  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 145/3000]  eta: 1:12:01  lr: 0.000027  loss: 0.3352  time: 1.5182  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 150/3000]  eta: 1:11:53  lr: 0.000027  loss: 0.2197  time: 1.5226  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 150/3000]  eta: 1:11:52  lr: 0.000027  loss: 0.2802  time: 1.5223  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 155/3000]  eta: 1:11:51  lr: 0.000027  loss: 0.3312  time: 1.5504  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 155/3000]  eta: 1:11:50  lr: 0.000027  loss: 0.2455  time: 1.5501  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 160/3000]  eta: 1:11:34  lr: 0.000027  loss: 0.4029  time: 1.5087  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 160/3000]  eta: 1:11:33  lr: 0.000027  loss: 0.2445  time: 1.5084  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 165/3000]  eta: 1:11:21  lr: 0.000027  loss: 0.5117  time: 1.4819  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 165/3000]  eta: 1:11:20  lr: 0.000027  loss: 0.0923  time: 1.4817  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 170/3000]  eta: 1:11:16  lr: 0.000027  loss: 0.6167  time: 1.4918  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 170/3000]  eta: 1:11:15  lr: 0.000027  loss: 0.3458  time: 1.4916  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 175/3000]  eta: 1:11:07  lr: 0.000027  loss: 0.5112  time: 1.4748  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 175/3000]  eta: 1:11:06  lr: 0.000027  loss: 0.7449  time: 1.4745  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 180/3000]  eta: 1:11:00  lr: 0.000027  loss: 0.1937  time: 1.5016  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 180/3000]  eta: 1:11:00  lr: 0.000027  loss: 0.1679  time: 1.5013  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 185/3000]  eta: 1:10:56  lr: 0.000027  loss: 0.8067  time: 1.5277  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 185/3000]  eta: 1:10:55  lr: 0.000027  loss: 0.3269  time: 1.5274  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 190/3000]  eta: 1:10:48  lr: 0.000027  loss: 0.2695  time: 1.5205  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 190/3000]  eta: 1:10:47  lr: 0.000027  loss: 0.1450  time: 1.5202  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 195/3000]  eta: 1:10:41  lr: 0.000027  loss: 0.3046  time: 1.5245  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 195/3000]  eta: 1:10:40  lr: 0.000027  loss: 0.5594  time: 1.5243  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 200/3000]  eta: 1:10:32  lr: 0.000027  loss: 0.7235  time: 1.5170  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 200/3000]  eta: 1:10:31  lr: 0.000027  loss: 0.1969  time: 1.5167  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 205/3000]  eta: 1:10:26  lr: 0.000027  loss: 0.2875  time: 1.5123  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 205/3000]  eta: 1:10:25  lr: 0.000027  loss: 0.2035  time: 1.5121  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 210/3000]  eta: 1:10:15  lr: 0.000027  loss: 0.6430  time: 1.5014  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 210/3000]  eta: 1:10:14  lr: 0.000027  loss: 0.4713  time: 1.5012  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 215/3000]  eta: 1:10:05  lr: 0.000027  loss: 0.2965  time: 1.4908  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 215/3000]  eta: 1:10:05  lr: 0.000027  loss: 0.5925  time: 1.4905  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 220/3000]  eta: 1:09:55  lr: 0.000027  loss: 0.1605  time: 1.4834  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 220/3000]  eta: 1:09:54  lr: 0.000027  loss: 0.2614  time: 1.4831  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 225/3000]  eta: 1:09:45  lr: 0.000027  loss: 0.3294  time: 1.4683  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 225/3000]  eta: 1:09:44  lr: 0.000027  loss: 0.2655  time: 1.4680  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 230/3000]  eta: 1:09:38  lr: 0.000027  loss: 0.3798  time: 1.4821  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 230/3000]  eta: 1:09:37  lr: 0.000027  loss: 0.5057  time: 1.4817  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 235/3000]  eta: 1:09:31  lr: 0.000027  loss: 0.4255  time: 1.4942  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 235/3000]  eta: 1:09:31  lr: 0.000027  loss: 0.7972  time: 1.4939  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 240/3000]  eta: 1:09:25  lr: 0.000027  loss: 0.6419  time: 1.5096  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 240/3000]  eta: 1:09:24  lr: 0.000027  loss: 0.1966  time: 1.5093  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 245/3000]  eta: 1:09:21  lr: 0.000027  loss: 0.5097  time: 1.5377  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 245/3000]  eta: 1:09:20  lr: 0.000027  loss: 0.7656  time: 1.5375  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 250/3000]  eta: 1:09:15  lr: 0.000027  loss: 0.1695  time: 1.5420  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 250/3000]  eta: 1:09:14  lr: 0.000027  loss: 0.1744  time: 1.5418  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 255/3000]  eta: 1:09:11  lr: 0.000027  loss: 0.8186  time: 1.5528  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 255/3000]  eta: 1:09:10  lr: 0.000027  loss: 0.4548  time: 1.5526  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 260/3000]  eta: 1:09:03  lr: 0.000027  loss: 0.3933  time: 1.5510  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 260/3000]  eta: 1:09:02  lr: 0.000027  loss: 0.1842  time: 1.5508  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 265/3000]  eta: 1:08:54  lr: 0.000027  loss: 0.3071  time: 1.5262  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 265/3000]  eta: 1:08:53  lr: 0.000027  loss: 0.1278  time: 1.5259  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 270/3000]  eta: 1:08:49  lr: 0.000027  loss: 0.4084  time: 1.5297  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 270/3000]  eta: 1:08:48  lr: 0.000027  loss: 0.1427  time: 1.5294  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 275/3000]  eta: 1:08:44  lr: 0.000027  loss: 0.9415  time: 1.5304  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 275/3000]  eta: 1:08:43  lr: 0.000027  loss: 0.1523  time: 1.5301  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 280/3000]  eta: 1:08:37  lr: 0.000027  loss: 0.3816  time: 1.5338  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 280/3000]  eta: 1:08:36  lr: 0.000027  loss: 0.3200  time: 1.5336  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 285/3000]  eta: 1:08:29  lr: 0.000027  loss: 0.1920  time: 1.5370  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 285/3000]  eta: 1:08:28  lr: 0.000027  loss: 0.1547  time: 1.5367  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 290/3000]  eta: 1:08:22  lr: 0.000027  loss: 0.5042  time: 1.5337  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 290/3000]  eta: 1:08:22  lr: 0.000027  loss: 0.7046  time: 1.5334  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 295/3000]  eta: 1:08:17  lr: 0.000027  loss: 0.4890  time: 1.5290  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 295/3000]  eta: 1:08:16  lr: 0.000027  loss: 0.8236  time: 1.5288  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 300/3000]  eta: 1:08:12  lr: 0.000027  loss: 0.1263  time: 1.5439  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 300/3000]  eta: 1:08:11  lr: 0.000027  loss: 0.5203  time: 1.5437  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 305/3000]  eta: 1:08:03  lr: 0.000027  loss: 0.9905  time: 1.5387  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 305/3000]  eta: 1:08:02  lr: 0.000027  loss: 0.2110  time: 1.5385  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 310/3000]  eta: 1:07:55  lr: 0.000027  loss: 0.2086  time: 1.5307  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 310/3000]  eta: 1:07:54  lr: 0.000027  loss: 0.2072  time: 1.5304  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 315/3000]  eta: 1:07:49  lr: 0.000027  loss: 0.3361  time: 1.5291  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 315/3000]  eta: 1:07:48  lr: 0.000027  loss: 0.6408  time: 1.5289  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 320/3000]  eta: 1:07:42  lr: 0.000027  loss: 0.4283  time: 1.5180  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 320/3000]  eta: 1:07:41  lr: 0.000027  loss: 0.1870  time: 1.5177  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 325/3000]  eta: 1:07:35  lr: 0.000027  loss: 0.4552  time: 1.5290  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 325/3000]  eta: 1:07:34  lr: 0.000027  loss: 0.4505  time: 1.5288  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 330/3000]  eta: 1:07:28  lr: 0.000027  loss: 0.5403  time: 1.5326  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 330/3000]  eta: 1:07:27  lr: 0.000027  loss: 0.4074  time: 1.5324  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 335/3000]  eta: 1:07:18  lr: 0.000027  loss: 1.2310  time: 1.5146  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 335/3000]  eta: 1:07:18  lr: 0.000027  loss: 0.3461  time: 1.5143  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 340/3000]  eta: 1:07:10  lr: 0.000027  loss: 1.4963  time: 1.5054  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 340/3000]  eta: 1:07:10  lr: 0.000027  loss: 0.2055  time: 1.5052  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 345/3000]  eta: 1:07:04  lr: 0.000027  loss: 0.4404  time: 1.5118  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 345/3000]  eta: 1:07:03  lr: 0.000027  loss: 0.4084  time: 1.5116  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 350/3000]  eta: 1:06:54  lr: 0.000027  loss: 0.3345  time: 1.4957  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 350/3000]  eta: 1:06:53  lr: 0.000027  loss: 0.3331  time: 1.4954  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 355/3000]  eta: 1:06:43  lr: 0.000027  loss: 0.3323  time: 1.4850  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 355/3000]  eta: 1:06:43  lr: 0.000027  loss: 0.4952  time: 1.4848  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 360/3000]  eta: 1:06:36  lr: 0.000027  loss: 0.3642  time: 1.4866  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 360/3000]  eta: 1:06:35  lr: 0.000027  loss: 0.1387  time: 1.4862  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 365/3000]  eta: 1:06:28  lr: 0.000027  loss: 0.2549  time: 1.4803  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 365/3000]  eta: 1:06:28  lr: 0.000027  loss: 0.9631  time: 1.4800  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 370/3000]  eta: 1:06:21  lr: 0.000027  loss: 0.4997  time: 1.4930  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 370/3000]  eta: 1:06:20  lr: 0.000027  loss: 0.4321  time: 1.4928  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 375/3000]  eta: 1:06:14  lr: 0.000027  loss: 0.2011  time: 1.5174  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 375/3000]  eta: 1:06:13  lr: 0.000027  loss: 0.2454  time: 1.5172  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 380/3000]  eta: 1:06:06  lr: 0.000027  loss: 0.4776  time: 1.5208  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 380/3000]  eta: 1:06:06  lr: 0.000027  loss: 0.1861  time: 1.5206  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 385/3000]  eta: 1:05:56  lr: 0.000027  loss: 0.3728  time: 1.5009  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 385/3000]  eta: 1:05:56  lr: 0.000027  loss: 0.1639  time: 1.5007  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 390/3000]  eta: 1:05:51  lr: 0.000027  loss: 0.5618  time: 1.5190  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 390/3000]  eta: 1:05:50  lr: 0.000027  loss: 0.6574  time: 1.5188  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 395/3000]  eta: 1:05:43  lr: 0.000027  loss: 0.4881  time: 1.5100  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 395/3000]  eta: 1:05:42  lr: 0.000027  loss: 0.6710  time: 1.5098  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 400/3000]  eta: 1:05:37  lr: 0.000027  loss: 0.4492  time: 1.5238  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 400/3000]  eta: 1:05:37  lr: 0.000027  loss: 0.9805  time: 1.5235  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 405/3000]  eta: 1:05:31  lr: 0.000027  loss: 0.3621  time: 1.5548  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 405/3000]  eta: 1:05:31  lr: 0.000027  loss: 0.4471  time: 1.5545  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 410/3000]  eta: 1:05:23  lr: 0.000027  loss: 0.7149  time: 1.5329  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 410/3000]  eta: 1:05:23  lr: 0.000027  loss: 0.4077  time: 1.5327  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 415/3000]  eta: 1:05:15  lr: 0.000027  loss: 0.5593  time: 1.5334  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 415/3000]  eta: 1:05:14  lr: 0.000027  loss: 0.1565  time: 1.5331  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 420/3000]  eta: 1:05:08  lr: 0.000027  loss: 0.5710  time: 1.5216  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 420/3000]  eta: 1:05:07  lr: 0.000027  loss: 0.8911  time: 1.5213  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 425/3000]  eta: 1:05:01  lr: 0.000027  loss: 0.7202  time: 1.5118  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 425/3000]  eta: 1:05:00  lr: 0.000027  loss: 0.3751  time: 1.5117  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 430/3000]  eta: 1:04:54  lr: 0.000027  loss: 0.2962  time: 1.5202  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 430/3000]  eta: 1:04:53  lr: 0.000027  loss: 0.4843  time: 1.5200  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 435/3000]  eta: 1:04:48  lr: 0.000027  loss: 0.1464  time: 1.5383  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 435/3000]  eta: 1:04:47  lr: 0.000027  loss: 0.4607  time: 1.5381  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 440/3000]  eta: 1:04:41  lr: 0.000027  loss: 0.5631  time: 1.5420  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 440/3000]  eta: 1:04:40  lr: 0.000027  loss: 0.4402  time: 1.5418  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 445/3000]  eta: 1:04:33  lr: 0.000027  loss: 0.2328  time: 1.5335  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 445/3000]  eta: 1:04:32  lr: 0.000027  loss: 0.3758  time: 1.5332  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 450/3000]  eta: 1:04:24  lr: 0.000027  loss: 0.5224  time: 1.5258  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 450/3000]  eta: 1:04:24  lr: 0.000027  loss: 0.2614  time: 1.5256  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 455/3000]  eta: 1:04:16  lr: 0.000027  loss: 0.6776  time: 1.5090  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 455/3000]  eta: 1:04:16  lr: 0.000027  loss: 0.2569  time: 1.5088  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 460/3000]  eta: 1:04:08  lr: 0.000027  loss: 0.2484  time: 1.4924  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 460/3000]  eta: 1:04:07  lr: 0.000027  loss: 0.1290  time: 1.4922  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 465/3000]  eta: 1:04:02  lr: 0.000027  loss: 0.4764  time: 1.5093  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 465/3000]  eta: 1:04:01  lr: 0.000027  loss: 0.6746  time: 1.5091  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 470/3000]  eta: 1:03:54  lr: 0.000027  loss: 0.4890  time: 1.5124  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 470/3000]  eta: 1:03:53  lr: 0.000027  loss: 1.1772  time: 1.5121  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 475/3000]  eta: 1:03:46  lr: 0.000027  loss: 0.6051  time: 1.5162  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 475/3000]  eta: 1:03:46  lr: 0.000027  loss: 0.1611  time: 1.5160  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 480/3000]  eta: 1:03:38  lr: 0.000027  loss: 0.4154  time: 1.5224  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 480/3000]  eta: 1:03:38  lr: 0.000027  loss: 0.3829  time: 1.5221  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 485/3000]  eta: 1:03:30  lr: 0.000027  loss: 0.2834  time: 1.5026  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 485/3000]  eta: 1:03:29  lr: 0.000027  loss: 0.2913  time: 1.5023  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 490/3000]  eta: 1:03:23  lr: 0.000027  loss: 0.7775  time: 1.5104  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 490/3000]  eta: 1:03:22  lr: 0.000027  loss: 0.3485  time: 1.5101  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 495/3000]  eta: 1:03:17  lr: 0.000027  loss: 0.2833  time: 1.5290  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 495/3000]  eta: 1:03:17  lr: 0.000027  loss: 0.6952  time: 1.5287  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 500/3000]  eta: 1:03:11  lr: 0.000027  loss: 0.4676  time: 1.5463  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 500/3000]  eta: 1:03:10  lr: 0.000027  loss: 0.2583  time: 1.5460  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 505/3000]  eta: 1:03:04  lr: 0.000027  loss: 0.1886  time: 1.5617  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 505/3000]  eta: 1:03:04  lr: 0.000027  loss: 0.6077  time: 1.5615  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 510/3000]  eta: 1:02:57  lr: 0.000027  loss: 0.1783  time: 1.5605  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 510/3000]  eta: 1:02:56  lr: 0.000027  loss: 0.1546  time: 1.5603  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 515/3000]  eta: 1:02:47  lr: 0.000027  loss: 0.4106  time: 1.5205  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 515/3000]  eta: 1:02:47  lr: 0.000027  loss: 0.3660  time: 1.5203  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 520/3000]  eta: 1:02:41  lr: 0.000027  loss: 0.2174  time: 1.5227  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 520/3000]  eta: 1:02:41  lr: 0.000027  loss: 0.7486  time: 1.5225  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 525/3000]  eta: 1:02:35  lr: 0.000027  loss: 0.0989  time: 1.5245  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 525/3000]  eta: 1:02:34  lr: 0.000027  loss: 0.0454  time: 1.5244  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 530/3000]  eta: 1:02:28  lr: 0.000027  loss: 0.5601  time: 1.5267  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 530/3000]  eta: 1:02:27  lr: 0.000027  loss: 0.2301  time: 1.5264  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 535/3000]  eta: 1:02:22  lr: 0.000027  loss: 0.3682  time: 1.5649  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 535/3000]  eta: 1:02:21  lr: 0.000027  loss: 0.6890  time: 1.5647  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 540/3000]  eta: 1:02:14  lr: 0.000027  loss: 0.2421  time: 1.5446  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 540/3000]  eta: 1:02:13  lr: 0.000027  loss: 0.3826  time: 1.5444  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 545/3000]  eta: 1:02:06  lr: 0.000027  loss: 0.3096  time: 1.5382  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 545/3000]  eta: 1:02:05  lr: 0.000027  loss: 0.1099  time: 1.5380  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 550/3000]  eta: 1:01:59  lr: 0.000027  loss: 0.2065  time: 1.5350  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 550/3000]  eta: 1:01:58  lr: 0.000027  loss: 0.3061  time: 1.5345  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 555/3000]  eta: 1:01:52  lr: 0.000027  loss: 0.2049  time: 1.5252  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 555/3000]  eta: 1:01:51  lr: 0.000027  loss: 0.1032  time: 1.5247  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 560/3000]  eta: 1:01:46  lr: 0.000027  loss: 0.3883  time: 1.5444  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 560/3000]  eta: 1:01:45  lr: 0.000027  loss: 0.6913  time: 1.5440  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 565/3000]  eta: 1:01:37  lr: 0.000027  loss: 0.0812  time: 1.5351  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 565/3000]  eta: 1:01:37  lr: 0.000027  loss: 0.2492  time: 1.5346  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 570/3000]  eta: 1:01:29  lr: 0.000027  loss: 0.2183  time: 1.5203  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 570/3000]  eta: 1:01:28  lr: 0.000027  loss: 0.7864  time: 1.5200  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 575/3000]  eta: 1:01:22  lr: 0.000027  loss: 0.4957  time: 1.5201  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 575/3000]  eta: 1:01:21  lr: 0.000027  loss: 0.3611  time: 1.5199  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 580/3000]  eta: 1:01:14  lr: 0.000027  loss: 0.4746  time: 1.5012  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 580/3000]  eta: 1:01:13  lr: 0.000027  loss: 0.4015  time: 1.5009  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 585/3000]  eta: 1:01:05  lr: 0.000027  loss: 0.2886  time: 1.5020  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 585/3000]  eta: 1:01:05  lr: 0.000027  loss: 0.4671  time: 1.5017  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 590/3000]  eta: 1:00:57  lr: 0.000027  loss: 0.1666  time: 1.4969  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 590/3000]  eta: 1:00:56  lr: 0.000027  loss: 0.1618  time: 1.4966  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 595/3000]  eta: 1:00:47  lr: 0.000027  loss: 0.7347  time: 1.4625  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 595/3000]  eta: 1:00:46  lr: 0.000027  loss: 0.0900  time: 1.4623  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 600/3000]  eta: 1:00:38  lr: 0.000027  loss: 0.1808  time: 1.4565  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 600/3000]  eta: 1:00:38  lr: 0.000027  loss: 0.2583  time: 1.4563  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 605/3000]  eta: 1:00:31  lr: 0.000027  loss: 0.3306  time: 1.4620  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 605/3000]  eta: 1:00:30  lr: 0.000027  loss: 0.5246  time: 1.4618  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 610/3000]  eta: 1:00:23  lr: 0.000027  loss: 0.3418  time: 1.4800  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 610/3000]  eta: 1:00:23  lr: 0.000027  loss: 0.4518  time: 1.4798  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 615/3000]  eta: 1:00:15  lr: 0.000027  loss: 0.9233  time: 1.4974  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 615/3000]  eta: 1:00:14  lr: 0.000027  loss: 0.4255  time: 1.4971  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 620/3000]  eta: 1:00:09  lr: 0.000027  loss: 0.1883  time: 1.5239  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 620/3000]  eta: 1:00:08  lr: 0.000027  loss: 0.3010  time: 1.5237  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 625/3000]  eta: 1:00:00  lr: 0.000027  loss: 0.2920  time: 1.5168  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 625/3000]  eta: 1:00:00  lr: 0.000027  loss: 0.3608  time: 1.5166  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 630/3000]  eta: 0:59:53  lr: 0.000027  loss: 0.2819  time: 1.5204  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 630/3000]  eta: 0:59:53  lr: 0.000027  loss: 0.6136  time: 1.5201  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 635/3000]  eta: 0:59:47  lr: 0.000027  loss: 0.8828  time: 1.5433  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 635/3000]  eta: 0:59:46  lr: 0.000027  loss: 0.4629  time: 1.5430  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 640/3000]  eta: 0:59:40  lr: 0.000027  loss: 0.3591  time: 1.5462  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 640/3000]  eta: 0:59:40  lr: 0.000027  loss: 0.2894  time: 1.5459  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 645/3000]  eta: 0:59:34  lr: 0.000027  loss: 0.3616  time: 1.5638  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 645/3000]  eta: 0:59:33  lr: 0.000027  loss: 0.4807  time: 1.5635  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 650/3000]  eta: 0:59:26  lr: 0.000027  loss: 0.4284  time: 1.5596  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 650/3000]  eta: 0:59:25  lr: 0.000027  loss: 0.1554  time: 1.5594  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 655/3000]  eta: 0:59:18  lr: 0.000027  loss: 0.1375  time: 1.5412  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 655/3000]  eta: 0:59:17  lr: 0.000027  loss: 0.2064  time: 1.5409  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 660/3000]  eta: 0:59:11  lr: 0.000027  loss: 0.2153  time: 1.5260  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 660/3000]  eta: 0:59:10  lr: 0.000027  loss: 0.2394  time: 1.5258  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 665/3000]  eta: 0:59:04  lr: 0.000027  loss: 0.1542  time: 1.5246  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 665/3000]  eta: 0:59:03  lr: 0.000027  loss: 0.9302  time: 1.5243  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 670/3000]  eta: 0:58:57  lr: 0.000027  loss: 0.2450  time: 1.5412  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 670/3000]  eta: 0:58:57  lr: 0.000027  loss: 0.4072  time: 1.5409  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 675/3000]  eta: 0:58:49  lr: 0.000027  loss: 0.1495  time: 1.5379  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 675/3000]  eta: 0:58:48  lr: 0.000027  loss: 0.2520  time: 1.5377  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 680/3000]  eta: 0:58:41  lr: 0.000027  loss: 0.1293  time: 1.5306  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 680/3000]  eta: 0:58:41  lr: 0.000027  loss: 0.1368  time: 1.5303  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 685/3000]  eta: 0:58:34  lr: 0.000027  loss: 0.4769  time: 1.5300  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 685/3000]  eta: 0:58:34  lr: 0.000027  loss: 1.1112  time: 1.5297  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 690/3000]  eta: 0:58:27  lr: 0.000027  loss: 0.5064  time: 1.5128  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 690/3000]  eta: 0:58:26  lr: 0.000027  loss: 0.2625  time: 1.5126  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 695/3000]  eta: 0:58:19  lr: 0.000027  loss: 0.7265  time: 1.5153  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 695/3000]  eta: 0:58:18  lr: 0.000027  loss: 0.5889  time: 1.5150  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 700/3000]  eta: 0:58:11  lr: 0.000027  loss: 0.2340  time: 1.5151  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 700/3000]  eta: 0:58:10  lr: 0.000027  loss: 0.1821  time: 1.5149  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 705/3000]  eta: 0:58:03  lr: 0.000027  loss: 0.5463  time: 1.5124  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 705/3000]  eta: 0:58:03  lr: 0.000027  loss: 0.8150  time: 1.5121  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 710/3000]  eta: 0:57:56  lr: 0.000027  loss: 0.9974  time: 1.5070  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 710/3000]  eta: 0:57:55  lr: 0.000027  loss: 0.6093  time: 1.5067  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 715/3000]  eta: 0:57:48  lr: 0.000027  loss: 0.6639  time: 1.5155  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 715/3000]  eta: 0:57:47  lr: 0.000027  loss: 0.2153  time: 1.5153  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 720/3000]  eta: 0:57:41  lr: 0.000027  loss: 0.5932  time: 1.5331  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 720/3000]  eta: 0:57:41  lr: 0.000027  loss: 0.4103  time: 1.5329  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 725/3000]  eta: 0:57:32  lr: 0.000027  loss: 1.1760  time: 1.5031  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 725/3000]  eta: 0:57:32  lr: 0.000027  loss: 0.3244  time: 1.5029  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 730/3000]  eta: 0:57:25  lr: 0.000026  loss: 0.1400  time: 1.5178  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 730/3000]  eta: 0:57:25  lr: 0.000026  loss: 0.1588  time: 1.5176  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 735/3000]  eta: 0:57:17  lr: 0.000026  loss: 0.4066  time: 1.5023  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 735/3000]  eta: 0:57:16  lr: 0.000026  loss: 0.1161  time: 1.5021  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 740/3000]  eta: 0:57:08  lr: 0.000026  loss: 0.4394  time: 1.4723  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 740/3000]  eta: 0:57:08  lr: 0.000026  loss: 0.9415  time: 1.4722  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 745/3000]  eta: 0:57:00  lr: 0.000026  loss: 0.3643  time: 1.4960  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 745/3000]  eta: 0:57:00  lr: 0.000026  loss: 0.5429  time: 1.4958  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 750/3000]  eta: 0:56:54  lr: 0.000026  loss: 0.2782  time: 1.4966  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 750/3000]  eta: 0:56:53  lr: 0.000026  loss: 0.1911  time: 1.4964  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 755/3000]  eta: 0:56:46  lr: 0.000026  loss: 0.5225  time: 1.5103  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 755/3000]  eta: 0:56:45  lr: 0.000026  loss: 0.3578  time: 1.5101  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 760/3000]  eta: 0:56:38  lr: 0.000026  loss: 0.4210  time: 1.5282  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 760/3000]  eta: 0:56:38  lr: 0.000026  loss: 0.3625  time: 1.5279  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 765/3000]  eta: 0:56:30  lr: 0.000026  loss: 0.3463  time: 1.5187  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 765/3000]  eta: 0:56:30  lr: 0.000026  loss: 0.6330  time: 1.5184  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 770/3000]  eta: 0:56:23  lr: 0.000026  loss: 0.2606  time: 1.5220  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 770/3000]  eta: 0:56:23  lr: 0.000026  loss: 0.9530  time: 1.5217  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 775/3000]  eta: 0:56:16  lr: 0.000026  loss: 0.7306  time: 1.5289  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 775/3000]  eta: 0:56:16  lr: 0.000026  loss: 0.7152  time: 1.5287  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 780/3000]  eta: 0:56:09  lr: 0.000026  loss: 0.4277  time: 1.5363  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 780/3000]  eta: 0:56:09  lr: 0.000026  loss: 0.2126  time: 1.5359  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 785/3000]  eta: 0:56:01  lr: 0.000026  loss: 0.6241  time: 1.5409  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 785/3000]  eta: 0:56:01  lr: 0.000026  loss: 0.4791  time: 1.5405  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 790/3000]  eta: 0:55:53  lr: 0.000026  loss: 0.4664  time: 1.5223  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 790/3000]  eta: 0:55:53  lr: 0.000026  loss: 0.5120  time: 1.5220  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 795/3000]  eta: 0:55:46  lr: 0.000026  loss: 0.4207  time: 1.5237  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 795/3000]  eta: 0:55:46  lr: 0.000026  loss: 1.2440  time: 1.5233  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 800/3000]  eta: 0:55:38  lr: 0.000026  loss: 0.3578  time: 1.5077  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 800/3000]  eta: 0:55:38  lr: 0.000026  loss: 0.3031  time: 1.5073  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 805/3000]  eta: 0:55:32  lr: 0.000026  loss: 0.3013  time: 1.5307  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 805/3000]  eta: 0:55:31  lr: 0.000026  loss: 0.5018  time: 1.5305  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 810/3000]  eta: 0:55:23  lr: 0.000026  loss: 0.0814  time: 1.5173  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 810/3000]  eta: 0:55:22  lr: 0.000026  loss: 0.3748  time: 1.5171  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 815/3000]  eta: 0:55:15  lr: 0.000026  loss: 0.2703  time: 1.5052  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 815/3000]  eta: 0:55:15  lr: 0.000026  loss: 0.5559  time: 1.5049  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 820/3000]  eta: 0:55:07  lr: 0.000026  loss: 0.6452  time: 1.4915  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 820/3000]  eta: 0:55:06  lr: 0.000026  loss: 0.2802  time: 1.4912  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 825/3000]  eta: 0:54:59  lr: 0.000026  loss: 0.5014  time: 1.4732  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 825/3000]  eta: 0:54:58  lr: 0.000026  loss: 0.4699  time: 1.4729  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 830/3000]  eta: 0:54:52  lr: 0.000026  loss: 0.7486  time: 1.4985  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 830/3000]  eta: 0:54:51  lr: 0.000026  loss: 0.4103  time: 1.4982  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 835/3000]  eta: 0:54:44  lr: 0.000026  loss: 0.9399  time: 1.5089  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 835/3000]  eta: 0:54:44  lr: 0.000026  loss: 0.7002  time: 1.5087  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 840/3000]  eta: 0:54:37  lr: 0.000026  loss: 0.7847  time: 1.5246  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 840/3000]  eta: 0:54:36  lr: 0.000026  loss: 0.1759  time: 1.5243  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 845/3000]  eta: 0:54:29  lr: 0.000026  loss: 0.1410  time: 1.5286  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 845/3000]  eta: 0:54:29  lr: 0.000026  loss: 0.2669  time: 1.5283  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 850/3000]  eta: 0:54:22  lr: 0.000026  loss: 0.5068  time: 1.5358  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 850/3000]  eta: 0:54:22  lr: 0.000026  loss: 0.3260  time: 1.5355  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 855/3000]  eta: 0:54:16  lr: 0.000026  loss: 0.3433  time: 1.5496  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 855/3000]  eta: 0:54:15  lr: 0.000026  loss: 0.5438  time: 1.5493  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 860/3000]  eta: 0:54:09  lr: 0.000026  loss: 0.3083  time: 1.5643  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 860/3000]  eta: 0:54:08  lr: 0.000026  loss: 0.3304  time: 1.5641  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 865/3000]  eta: 0:54:01  lr: 0.000026  loss: 0.3141  time: 1.5659  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 865/3000]  eta: 0:54:01  lr: 0.000026  loss: 0.0578  time: 1.5657  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 870/3000]  eta: 0:53:53  lr: 0.000026  loss: 0.3429  time: 1.5303  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 870/3000]  eta: 0:53:52  lr: 0.000026  loss: 0.4106  time: 1.5300  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 875/3000]  eta: 0:53:46  lr: 0.000026  loss: 0.4604  time: 1.5274  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 875/3000]  eta: 0:53:45  lr: 0.000026  loss: 0.2295  time: 1.5272  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 880/3000]  eta: 0:53:38  lr: 0.000026  loss: 0.6970  time: 1.5275  data: 0.0000  max mem: 18406Train: data epoch: [8]  [ 880/3000]  eta: 0:53:39  lr: 0.000026  loss: 0.2233  time: 1.5279  data: 0.0000  max mem: 18596

Train: data epoch: [8]  [ 885/3000]  eta: 0:53:32  lr: 0.000026  loss: 0.4639  time: 1.5392  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 885/3000]  eta: 0:53:31  lr: 0.000026  loss: 0.3089  time: 1.5389  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 890/3000]  eta: 0:53:24  lr: 0.000026  loss: 0.2044  time: 1.5594  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 890/3000]  eta: 0:53:24  lr: 0.000026  loss: 0.6550  time: 1.5591  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 895/3000]  eta: 0:53:17  lr: 0.000026  loss: 0.4510  time: 1.5570  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 895/3000]  eta: 0:53:17  lr: 0.000026  loss: 0.2006  time: 1.5566  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 900/3000]  eta: 0:53:10  lr: 0.000026  loss: 0.1627  time: 1.5443  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 900/3000]  eta: 0:53:09  lr: 0.000026  loss: 0.1525  time: 1.5440  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 905/3000]  eta: 0:53:02  lr: 0.000026  loss: 0.5991  time: 1.5294  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 905/3000]  eta: 0:53:01  lr: 0.000026  loss: 0.2919  time: 1.5292  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 910/3000]  eta: 0:52:53  lr: 0.000026  loss: 0.2214  time: 1.5075  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 910/3000]  eta: 0:52:53  lr: 0.000026  loss: 0.2964  time: 1.5072  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 915/3000]  eta: 0:52:45  lr: 0.000026  loss: 0.2260  time: 1.4885  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 915/3000]  eta: 0:52:45  lr: 0.000026  loss: 0.2501  time: 1.4883  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 920/3000]  eta: 0:52:38  lr: 0.000026  loss: 0.8357  time: 1.4892  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 920/3000]  eta: 0:52:37  lr: 0.000026  loss: 0.5241  time: 1.4890  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 925/3000]  eta: 0:52:31  lr: 0.000026  loss: 0.6650  time: 1.5020  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 925/3000]  eta: 0:52:30  lr: 0.000026  loss: 0.6882  time: 1.5019  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 930/3000]  eta: 0:52:24  lr: 0.000026  loss: 0.2076  time: 1.5383  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 930/3000]  eta: 0:52:23  lr: 0.000026  loss: 1.1526  time: 1.5381  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 935/3000]  eta: 0:52:15  lr: 0.000026  loss: 0.2921  time: 1.5231  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 935/3000]  eta: 0:52:15  lr: 0.000026  loss: 0.6876  time: 1.5238  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 940/3000]  eta: 0:52:08  lr: 0.000026  loss: 0.2746  time: 1.5348  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 940/3000]  eta: 0:52:08  lr: 0.000026  loss: 0.2706  time: 1.5346  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 945/3000]  eta: 0:52:00  lr: 0.000026  loss: 0.4866  time: 1.5205  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 945/3000]  eta: 0:52:00  lr: 0.000026  loss: 0.5345  time: 1.5202  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 950/3000]  eta: 0:51:53  lr: 0.000026  loss: 0.4917  time: 1.4998  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 950/3000]  eta: 0:51:52  lr: 0.000026  loss: 0.0980  time: 1.4994  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 955/3000]  eta: 0:51:44  lr: 0.000026  loss: 0.1418  time: 1.5061  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 955/3000]  eta: 0:51:44  lr: 0.000026  loss: 0.4703  time: 1.5049  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 960/3000]  eta: 0:51:37  lr: 0.000026  loss: 0.1085  time: 1.5007  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 960/3000]  eta: 0:51:36  lr: 0.000026  loss: 0.3469  time: 1.5005  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 965/3000]  eta: 0:51:30  lr: 0.000026  loss: 0.3920  time: 1.5196  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 965/3000]  eta: 0:51:30  lr: 0.000026  loss: 0.5608  time: 1.5194  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 970/3000]  eta: 0:51:23  lr: 0.000026  loss: 0.4007  time: 1.5320  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 970/3000]  eta: 0:51:22  lr: 0.000026  loss: 0.3134  time: 1.5318  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 975/3000]  eta: 0:51:16  lr: 0.000026  loss: 0.7457  time: 1.5616  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 975/3000]  eta: 0:51:15  lr: 0.000026  loss: 0.3911  time: 1.5614  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 980/3000]  eta: 0:51:08  lr: 0.000026  loss: 0.4311  time: 1.5542  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 980/3000]  eta: 0:51:08  lr: 0.000026  loss: 0.7160  time: 1.5539  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 985/3000]  eta: 0:51:01  lr: 0.000026  loss: 0.5783  time: 1.5413  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 985/3000]  eta: 0:51:00  lr: 0.000026  loss: 0.6286  time: 1.5411  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 990/3000]  eta: 0:50:52  lr: 0.000026  loss: 0.3831  time: 1.5183  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 990/3000]  eta: 0:50:52  lr: 0.000026  loss: 0.4153  time: 1.5180  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [ 995/3000]  eta: 0:50:44  lr: 0.000026  loss: 0.0926  time: 1.4867  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [ 995/3000]  eta: 0:50:44  lr: 0.000026  loss: 0.1908  time: 1.4864  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1000/3000]  eta: 0:50:36  lr: 0.000026  loss: 0.5375  time: 1.4738  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1000/3000]  eta: 0:50:35  lr: 0.000026  loss: 0.6346  time: 1.4737  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1005/3000]  eta: 0:50:29  lr: 0.000026  loss: 0.2009  time: 1.4738  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1005/3000]  eta: 0:50:28  lr: 0.000026  loss: 0.2839  time: 1.4736  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1010/3000]  eta: 0:50:21  lr: 0.000026  loss: 0.1583  time: 1.4829  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1010/3000]  eta: 0:50:20  lr: 0.000026  loss: 0.1685  time: 1.4826  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1015/3000]  eta: 0:50:13  lr: 0.000026  loss: 0.4104  time: 1.5012  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1015/3000]  eta: 0:50:13  lr: 0.000026  loss: 0.5589  time: 1.5009  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1020/3000]  eta: 0:50:06  lr: 0.000026  loss: 0.3172  time: 1.5299  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1020/3000]  eta: 0:50:05  lr: 0.000026  loss: 0.0723  time: 1.5296  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1025/3000]  eta: 0:49:58  lr: 0.000026  loss: 0.4066  time: 1.5260  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1025/3000]  eta: 0:49:58  lr: 0.000026  loss: 0.4195  time: 1.5256  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1030/3000]  eta: 0:49:50  lr: 0.000026  loss: 0.2103  time: 1.5090  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1030/3000]  eta: 0:49:49  lr: 0.000026  loss: 0.0766  time: 1.5088  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1035/3000]  eta: 0:49:41  lr: 0.000026  loss: 0.3429  time: 1.4786  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1035/3000]  eta: 0:49:41  lr: 0.000026  loss: 1.4118  time: 1.4784  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1040/3000]  eta: 0:49:34  lr: 0.000026  loss: 0.1770  time: 1.4635  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1040/3000]  eta: 0:49:33  lr: 0.000026  loss: 0.6869  time: 1.4633  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1045/3000]  eta: 0:49:26  lr: 0.000026  loss: 0.4381  time: 1.4705  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1045/3000]  eta: 0:49:26  lr: 0.000026  loss: 0.2708  time: 1.4703  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1050/3000]  eta: 0:49:19  lr: 0.000026  loss: 0.3241  time: 1.4973  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1050/3000]  eta: 0:49:18  lr: 0.000026  loss: 0.1627  time: 1.4970  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1055/3000]  eta: 0:49:11  lr: 0.000026  loss: 0.7096  time: 1.5224  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1055/3000]  eta: 0:49:11  lr: 0.000026  loss: 0.5770  time: 1.5222  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1060/3000]  eta: 0:49:04  lr: 0.000026  loss: 0.3050  time: 1.5304  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1060/3000]  eta: 0:49:03  lr: 0.000026  loss: 0.3454  time: 1.5302  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1065/3000]  eta: 0:48:56  lr: 0.000026  loss: 0.1026  time: 1.5258  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1065/3000]  eta: 0:48:56  lr: 0.000026  loss: 0.2754  time: 1.5255  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1070/3000]  eta: 0:48:49  lr: 0.000026  loss: 0.5918  time: 1.5319  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1070/3000]  eta: 0:48:48  lr: 0.000026  loss: 0.3097  time: 1.5317  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1075/3000]  eta: 0:48:41  lr: 0.000026  loss: 0.5105  time: 1.5245  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1075/3000]  eta: 0:48:40  lr: 0.000026  loss: 0.2974  time: 1.5243  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1080/3000]  eta: 0:48:33  lr: 0.000026  loss: 0.4154  time: 1.5191  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1080/3000]  eta: 0:48:33  lr: 0.000026  loss: 0.5853  time: 1.5188  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1085/3000]  eta: 0:48:25  lr: 0.000026  loss: 0.2927  time: 1.4963  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1085/3000]  eta: 0:48:25  lr: 0.000026  loss: 0.9162  time: 1.4961  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1090/3000]  eta: 0:48:17  lr: 0.000026  loss: 0.3905  time: 1.4787  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1090/3000]  eta: 0:48:17  lr: 0.000026  loss: 0.2219  time: 1.4785  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1095/3000]  eta: 0:48:10  lr: 0.000026  loss: 0.6546  time: 1.4973  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1095/3000]  eta: 0:48:09  lr: 0.000026  loss: 0.4070  time: 1.4971  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1100/3000]  eta: 0:48:02  lr: 0.000026  loss: 0.8125  time: 1.4932  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1100/3000]  eta: 0:48:02  lr: 0.000026  loss: 0.3054  time: 1.4930  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1105/3000]  eta: 0:47:54  lr: 0.000026  loss: 0.0878  time: 1.4966  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1105/3000]  eta: 0:47:54  lr: 0.000026  loss: 0.1361  time: 1.4964  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1110/3000]  eta: 0:47:46  lr: 0.000026  loss: 0.2801  time: 1.4982  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1110/3000]  eta: 0:47:46  lr: 0.000026  loss: 0.5668  time: 1.4979  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1115/3000]  eta: 0:47:39  lr: 0.000026  loss: 0.3571  time: 1.4917  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1115/3000]  eta: 0:47:38  lr: 0.000026  loss: 0.3281  time: 1.4915  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1120/3000]  eta: 0:47:31  lr: 0.000026  loss: 0.1339  time: 1.4833  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1120/3000]  eta: 0:47:30  lr: 0.000026  loss: 0.3245  time: 1.4831  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1125/3000]  eta: 0:47:23  lr: 0.000026  loss: 0.3012  time: 1.4915  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1125/3000]  eta: 0:47:22  lr: 0.000026  loss: 0.8371  time: 1.4912  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1130/3000]  eta: 0:47:15  lr: 0.000026  loss: 0.2206  time: 1.4794  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1130/3000]  eta: 0:47:14  lr: 0.000026  loss: 0.5683  time: 1.4792  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1135/3000]  eta: 0:47:06  lr: 0.000026  loss: 0.1898  time: 1.4553  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1135/3000]  eta: 0:47:06  lr: 0.000026  loss: 0.7852  time: 1.4551  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1140/3000]  eta: 0:46:59  lr: 0.000026  loss: 0.4218  time: 1.4793  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1140/3000]  eta: 0:46:59  lr: 0.000026  loss: 0.2130  time: 1.4792  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1145/3000]  eta: 0:46:51  lr: 0.000026  loss: 0.9007  time: 1.4579  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1145/3000]  eta: 0:46:50  lr: 0.000026  loss: 0.3271  time: 1.4577  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1150/3000]  eta: 0:46:44  lr: 0.000026  loss: 0.5520  time: 1.4977  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1150/3000]  eta: 0:46:43  lr: 0.000026  loss: 0.5237  time: 1.4975  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1155/3000]  eta: 0:46:36  lr: 0.000026  loss: 0.1294  time: 1.5263  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1155/3000]  eta: 0:46:36  lr: 0.000026  loss: 0.2571  time: 1.5261  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1160/3000]  eta: 0:46:29  lr: 0.000026  loss: 0.6301  time: 1.5264  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1160/3000]  eta: 0:46:29  lr: 0.000026  loss: 0.2246  time: 1.5261  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1165/3000]  eta: 0:46:22  lr: 0.000026  loss: 0.1686  time: 1.5657  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1165/3000]  eta: 0:46:21  lr: 0.000026  loss: 0.5718  time: 1.5655  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1170/3000]  eta: 0:46:15  lr: 0.000026  loss: 0.7871  time: 1.5623  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1170/3000]  eta: 0:46:14  lr: 0.000026  loss: 0.3379  time: 1.5621  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1175/3000]  eta: 0:46:07  lr: 0.000026  loss: 0.4238  time: 1.5343  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1175/3000]  eta: 0:46:06  lr: 0.000026  loss: 0.2867  time: 1.5341  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1180/3000]  eta: 0:45:59  lr: 0.000026  loss: 0.3918  time: 1.5163  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1180/3000]  eta: 0:45:58  lr: 0.000026  loss: 0.5992  time: 1.5161  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1185/3000]  eta: 0:45:51  lr: 0.000026  loss: 0.1877  time: 1.5052  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1185/3000]  eta: 0:45:51  lr: 0.000026  loss: 0.3917  time: 1.5050  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1190/3000]  eta: 0:45:44  lr: 0.000026  loss: 0.8455  time: 1.5024  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1190/3000]  eta: 0:45:44  lr: 0.000026  loss: 0.8405  time: 1.5022  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1195/3000]  eta: 0:45:36  lr: 0.000026  loss: 0.9468  time: 1.5116  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1195/3000]  eta: 0:45:36  lr: 0.000026  loss: 0.2948  time: 1.5114  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1200/3000]  eta: 0:45:28  lr: 0.000026  loss: 0.2929  time: 1.5046  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1200/3000]  eta: 0:45:28  lr: 0.000026  loss: 0.9703  time: 1.5043  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1205/3000]  eta: 0:45:21  lr: 0.000026  loss: 0.2266  time: 1.5168  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1205/3000]  eta: 0:45:20  lr: 0.000026  loss: 0.1651  time: 1.5165  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1210/3000]  eta: 0:45:13  lr: 0.000026  loss: 0.3430  time: 1.4964  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1210/3000]  eta: 0:45:13  lr: 0.000026  loss: 0.1903  time: 1.4963  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1215/3000]  eta: 0:45:05  lr: 0.000026  loss: 1.0951  time: 1.5017  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1215/3000]  eta: 0:45:05  lr: 0.000026  loss: 0.6134  time: 1.5015  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1220/3000]  eta: 0:44:58  lr: 0.000026  loss: 0.5203  time: 1.5222  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1220/3000]  eta: 0:44:58  lr: 0.000026  loss: 0.4303  time: 1.5221  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1225/3000]  eta: 0:44:50  lr: 0.000026  loss: 0.2309  time: 1.4933  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1225/3000]  eta: 0:44:50  lr: 0.000026  loss: 0.2723  time: 1.4931  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1230/3000]  eta: 0:44:43  lr: 0.000026  loss: 0.0845  time: 1.5085  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1230/3000]  eta: 0:44:42  lr: 0.000026  loss: 0.1208  time: 1.5082  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1235/3000]  eta: 0:44:34  lr: 0.000026  loss: 0.5036  time: 1.4956  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1235/3000]  eta: 0:44:34  lr: 0.000026  loss: 0.1919  time: 1.4953  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1240/3000]  eta: 0:44:27  lr: 0.000026  loss: 0.1435  time: 1.4980  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1240/3000]  eta: 0:44:27  lr: 0.000026  loss: 0.7132  time: 1.4977  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1245/3000]  eta: 0:44:19  lr: 0.000026  loss: 0.2841  time: 1.5028  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1245/3000]  eta: 0:44:19  lr: 0.000026  loss: 0.2227  time: 1.5027  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1250/3000]  eta: 0:44:12  lr: 0.000026  loss: 0.3067  time: 1.4991  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1250/3000]  eta: 0:44:11  lr: 0.000026  loss: 0.8736  time: 1.4989  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1255/3000]  eta: 0:44:04  lr: 0.000026  loss: 0.1704  time: 1.4955  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1255/3000]  eta: 0:44:03  lr: 0.000026  loss: 0.7281  time: 1.4954  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1260/3000]  eta: 0:43:56  lr: 0.000026  loss: 0.4440  time: 1.4791  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1260/3000]  eta: 0:43:55  lr: 0.000026  loss: 0.1356  time: 1.4789  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1265/3000]  eta: 0:43:47  lr: 0.000026  loss: 0.1701  time: 1.4585  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1265/3000]  eta: 0:43:47  lr: 0.000026  loss: 0.2889  time: 1.4582  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1270/3000]  eta: 0:43:40  lr: 0.000026  loss: 0.3779  time: 1.4480  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1270/3000]  eta: 0:43:39  lr: 0.000026  loss: 0.1264  time: 1.4478  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1275/3000]  eta: 0:43:33  lr: 0.000026  loss: 0.4484  time: 1.4871  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1275/3000]  eta: 0:43:32  lr: 0.000026  loss: 0.4108  time: 1.4869  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1280/3000]  eta: 0:43:25  lr: 0.000026  loss: 0.3762  time: 1.5055  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1280/3000]  eta: 0:43:25  lr: 0.000026  loss: 0.5430  time: 1.5053  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1285/3000]  eta: 0:43:18  lr: 0.000026  loss: 0.3059  time: 1.5313  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1285/3000]  eta: 0:43:17  lr: 0.000026  loss: 0.1376  time: 1.5311  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1290/3000]  eta: 0:43:10  lr: 0.000026  loss: 0.1605  time: 1.5444  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1290/3000]  eta: 0:43:10  lr: 0.000026  loss: 0.7289  time: 1.5442  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1295/3000]  eta: 0:43:03  lr: 0.000026  loss: 0.3230  time: 1.5401  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1295/3000]  eta: 0:43:02  lr: 0.000026  loss: 0.3702  time: 1.5399  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1300/3000]  eta: 0:42:55  lr: 0.000026  loss: 0.1643  time: 1.5324  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1300/3000]  eta: 0:42:55  lr: 0.000026  loss: 0.1694  time: 1.5322  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1305/3000]  eta: 0:42:48  lr: 0.000026  loss: 0.7825  time: 1.5375  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1305/3000]  eta: 0:42:47  lr: 0.000026  loss: 0.5867  time: 1.5372  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1310/3000]  eta: 0:42:40  lr: 0.000026  loss: 0.3134  time: 1.5087  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1310/3000]  eta: 0:42:39  lr: 0.000026  loss: 0.2038  time: 1.5085  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1315/3000]  eta: 0:42:32  lr: 0.000026  loss: 0.1647  time: 1.5091  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1315/3000]  eta: 0:42:32  lr: 0.000026  loss: 0.8945  time: 1.5088  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1320/3000]  eta: 0:42:25  lr: 0.000026  loss: 0.3449  time: 1.5131  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1320/3000]  eta: 0:42:25  lr: 0.000026  loss: 0.4280  time: 1.5128  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1325/3000]  eta: 0:42:17  lr: 0.000026  loss: 0.1831  time: 1.5130  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1325/3000]  eta: 0:42:17  lr: 0.000026  loss: 0.2743  time: 1.5128  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1330/3000]  eta: 0:42:10  lr: 0.000026  loss: 0.4228  time: 1.5229  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1330/3000]  eta: 0:42:09  lr: 0.000026  loss: 0.4325  time: 1.5226  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1335/3000]  eta: 0:42:02  lr: 0.000026  loss: 0.2299  time: 1.5127  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1335/3000]  eta: 0:42:02  lr: 0.000026  loss: 0.2995  time: 1.5124  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1340/3000]  eta: 0:41:54  lr: 0.000026  loss: 1.0495  time: 1.4914  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1340/3000]  eta: 0:41:54  lr: 0.000026  loss: 0.4637  time: 1.4912  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1345/3000]  eta: 0:41:47  lr: 0.000026  loss: 0.7229  time: 1.4930  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1345/3000]  eta: 0:41:46  lr: 0.000026  loss: 0.2435  time: 1.4917  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1350/3000]  eta: 0:41:39  lr: 0.000026  loss: 0.4952  time: 1.5084  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1350/3000]  eta: 0:41:39  lr: 0.000026  loss: 0.4901  time: 1.5073  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1355/3000]  eta: 0:41:32  lr: 0.000026  loss: 0.2599  time: 1.5061  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1355/3000]  eta: 0:41:31  lr: 0.000026  loss: 0.5979  time: 1.5049  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1360/3000]  eta: 0:41:24  lr: 0.000026  loss: 0.5944  time: 1.5297  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1360/3000]  eta: 0:41:24  lr: 0.000026  loss: 0.6175  time: 1.5285  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1365/3000]  eta: 0:41:17  lr: 0.000026  loss: 0.2356  time: 1.5329  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1365/3000]  eta: 0:41:16  lr: 0.000026  loss: 0.6805  time: 1.5327  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1370/3000]  eta: 0:41:09  lr: 0.000026  loss: 1.0051  time: 1.5332  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1370/3000]  eta: 0:41:09  lr: 0.000026  loss: 0.3197  time: 1.5329  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1375/3000]  eta: 0:41:01  lr: 0.000026  loss: 0.6220  time: 1.5190  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1375/3000]  eta: 0:41:01  lr: 0.000026  loss: 0.3725  time: 1.5188  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1380/3000]  eta: 0:40:54  lr: 0.000026  loss: 0.3670  time: 1.5031  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1380/3000]  eta: 0:40:53  lr: 0.000026  loss: 0.1905  time: 1.5029  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1385/3000]  eta: 0:40:46  lr: 0.000026  loss: 0.8779  time: 1.5106  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1385/3000]  eta: 0:40:46  lr: 0.000026  loss: 0.1839  time: 1.5103  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1390/3000]  eta: 0:40:38  lr: 0.000026  loss: 0.3760  time: 1.4820  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1390/3000]  eta: 0:40:38  lr: 0.000026  loss: 0.6033  time: 1.4817  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1395/3000]  eta: 0:40:30  lr: 0.000026  loss: 0.7027  time: 1.4684  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1395/3000]  eta: 0:40:30  lr: 0.000026  loss: 0.0809  time: 1.4681  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1400/3000]  eta: 0:40:23  lr: 0.000026  loss: 0.3744  time: 1.4754  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1400/3000]  eta: 0:40:22  lr: 0.000026  loss: 0.7044  time: 1.4751  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1405/3000]  eta: 0:40:14  lr: 0.000026  loss: 0.5062  time: 1.4442  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1405/3000]  eta: 0:40:14  lr: 0.000026  loss: 0.2525  time: 1.4439  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1410/3000]  eta: 0:40:07  lr: 0.000026  loss: 0.3252  time: 1.4807  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1410/3000]  eta: 0:40:07  lr: 0.000026  loss: 0.5820  time: 1.4804  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1415/3000]  eta: 0:39:59  lr: 0.000026  loss: 0.3635  time: 1.4973  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1415/3000]  eta: 0:39:59  lr: 0.000026  loss: 0.9483  time: 1.4971  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1420/3000]  eta: 0:39:52  lr: 0.000026  loss: 0.5769  time: 1.5042  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1420/3000]  eta: 0:39:52  lr: 0.000026  loss: 0.3552  time: 1.5039  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1425/3000]  eta: 0:39:44  lr: 0.000026  loss: 0.5507  time: 1.5223  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1425/3000]  eta: 0:39:44  lr: 0.000026  loss: 0.3227  time: 1.5221  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1430/3000]  eta: 0:39:37  lr: 0.000026  loss: 0.2759  time: 1.5015  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1430/3000]  eta: 0:39:36  lr: 0.000026  loss: 0.3152  time: 1.5013  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1435/3000]  eta: 0:39:29  lr: 0.000026  loss: 0.5312  time: 1.5071  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1435/3000]  eta: 0:39:28  lr: 0.000026  loss: 0.1354  time: 1.5069  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1440/3000]  eta: 0:39:21  lr: 0.000026  loss: 1.1763  time: 1.5012  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1440/3000]  eta: 0:39:21  lr: 0.000026  loss: 0.5070  time: 1.5010  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1445/3000]  eta: 0:39:14  lr: 0.000026  loss: 0.8812  time: 1.5105  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1445/3000]  eta: 0:39:14  lr: 0.000026  loss: 0.4724  time: 1.5102  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1450/3000]  eta: 0:39:06  lr: 0.000026  loss: 0.4207  time: 1.5075  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1450/3000]  eta: 0:39:06  lr: 0.000026  loss: 0.4327  time: 1.5073  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1455/3000]  eta: 0:38:59  lr: 0.000026  loss: 0.6408  time: 1.5255  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1455/3000]  eta: 0:38:58  lr: 0.000026  loss: 0.4286  time: 1.5254  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1460/3000]  eta: 0:38:51  lr: 0.000026  loss: 0.3002  time: 1.5292  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1460/3000]  eta: 0:38:51  lr: 0.000026  loss: 0.5075  time: 1.5289  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1465/3000]  eta: 0:38:44  lr: 0.000026  loss: 0.5082  time: 1.5249  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1465/3000]  eta: 0:38:44  lr: 0.000026  loss: 0.2367  time: 1.5247  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1470/3000]  eta: 0:38:36  lr: 0.000026  loss: 0.0890  time: 1.5204  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1470/3000]  eta: 0:38:36  lr: 0.000026  loss: 0.2582  time: 1.5202  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1475/3000]  eta: 0:38:29  lr: 0.000026  loss: 0.2142  time: 1.5163  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1475/3000]  eta: 0:38:28  lr: 0.000026  loss: 0.3752  time: 1.5161  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1480/3000]  eta: 0:38:21  lr: 0.000026  loss: 0.8792  time: 1.5098  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1480/3000]  eta: 0:38:21  lr: 0.000026  loss: 0.5784  time: 1.5096  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1485/3000]  eta: 0:38:14  lr: 0.000026  loss: 0.3051  time: 1.5125  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1485/3000]  eta: 0:38:13  lr: 0.000026  loss: 0.6751  time: 1.5123  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1490/3000]  eta: 0:38:06  lr: 0.000026  loss: 0.1310  time: 1.5240  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1490/3000]  eta: 0:38:06  lr: 0.000026  loss: 0.2574  time: 1.5238  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1495/3000]  eta: 0:37:59  lr: 0.000026  loss: 0.5985  time: 1.5225  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1495/3000]  eta: 0:37:58  lr: 0.000026  loss: 0.5558  time: 1.5223  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1500/3000]  eta: 0:37:51  lr: 0.000026  loss: 0.4273  time: 1.5189  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1500/3000]  eta: 0:37:50  lr: 0.000026  loss: 0.2830  time: 1.5186  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1505/3000]  eta: 0:37:43  lr: 0.000026  loss: 0.4258  time: 1.5100  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1505/3000]  eta: 0:37:43  lr: 0.000026  loss: 0.6305  time: 1.5098  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1510/3000]  eta: 0:37:35  lr: 0.000026  loss: 0.4646  time: 1.5054  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1510/3000]  eta: 0:37:35  lr: 0.000026  loss: 0.5557  time: 1.5051  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1515/3000]  eta: 0:37:28  lr: 0.000026  loss: 0.6010  time: 1.5081  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1515/3000]  eta: 0:37:28  lr: 0.000026  loss: 1.0102  time: 1.5079  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1520/3000]  eta: 0:37:21  lr: 0.000026  loss: 0.3927  time: 1.5112  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1520/3000]  eta: 0:37:20  lr: 0.000026  loss: 0.3846  time: 1.5110  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1525/3000]  eta: 0:37:13  lr: 0.000026  loss: 0.4615  time: 1.5081  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1525/3000]  eta: 0:37:12  lr: 0.000026  loss: 0.6753  time: 1.5078  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1530/3000]  eta: 0:37:05  lr: 0.000026  loss: 0.2318  time: 1.5194  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1530/3000]  eta: 0:37:05  lr: 0.000026  loss: 0.5633  time: 1.5192  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1535/3000]  eta: 0:36:58  lr: 0.000026  loss: 1.0615  time: 1.5178  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1535/3000]  eta: 0:36:57  lr: 0.000026  loss: 0.6619  time: 1.5175  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1540/3000]  eta: 0:36:50  lr: 0.000026  loss: 0.4023  time: 1.5095  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1540/3000]  eta: 0:36:50  lr: 0.000026  loss: 0.5178  time: 1.5092  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1545/3000]  eta: 0:36:43  lr: 0.000026  loss: 0.1752  time: 1.5191  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1545/3000]  eta: 0:36:42  lr: 0.000026  loss: 0.3368  time: 1.5190  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1550/3000]  eta: 0:36:35  lr: 0.000026  loss: 0.5357  time: 1.5145  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1550/3000]  eta: 0:36:35  lr: 0.000026  loss: 0.7793  time: 1.5144  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1555/3000]  eta: 0:36:28  lr: 0.000026  loss: 0.3325  time: 1.5241  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1555/3000]  eta: 0:36:27  lr: 0.000026  loss: 0.3258  time: 1.5237  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1560/3000]  eta: 0:36:20  lr: 0.000026  loss: 0.6022  time: 1.5352  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1560/3000]  eta: 0:36:20  lr: 0.000026  loss: 0.3893  time: 1.5348  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1565/3000]  eta: 0:36:13  lr: 0.000026  loss: 0.6876  time: 1.5484  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1565/3000]  eta: 0:36:13  lr: 0.000026  loss: 0.4916  time: 1.5479  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1570/3000]  eta: 0:36:05  lr: 0.000026  loss: 0.4274  time: 1.5550  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1570/3000]  eta: 0:36:05  lr: 0.000026  loss: 0.2280  time: 1.5545  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1575/3000]  eta: 0:35:58  lr: 0.000026  loss: 0.3103  time: 1.5262  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1575/3000]  eta: 0:35:57  lr: 0.000026  loss: 0.3542  time: 1.5260  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1580/3000]  eta: 0:35:50  lr: 0.000026  loss: 0.2477  time: 1.5285  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1580/3000]  eta: 0:35:50  lr: 0.000026  loss: 0.2682  time: 1.5282  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1585/3000]  eta: 0:35:42  lr: 0.000026  loss: 0.2810  time: 1.5020  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1585/3000]  eta: 0:35:42  lr: 0.000026  loss: 0.3237  time: 1.5018  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1590/3000]  eta: 0:35:35  lr: 0.000026  loss: 0.2906  time: 1.5084  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1590/3000]  eta: 0:35:35  lr: 0.000026  loss: 0.2623  time: 1.5081  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1595/3000]  eta: 0:35:28  lr: 0.000026  loss: 0.4438  time: 1.5284  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1595/3000]  eta: 0:35:27  lr: 0.000026  loss: 0.6706  time: 1.5280  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1600/3000]  eta: 0:35:20  lr: 0.000026  loss: 0.5455  time: 1.5065  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1600/3000]  eta: 0:35:19  lr: 0.000026  loss: 0.4901  time: 1.5063  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1605/3000]  eta: 0:35:13  lr: 0.000026  loss: 0.9386  time: 1.5332  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1605/3000]  eta: 0:35:12  lr: 0.000026  loss: 0.7396  time: 1.5330  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1610/3000]  eta: 0:35:05  lr: 0.000026  loss: 1.1394  time: 1.5256  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1610/3000]  eta: 0:35:05  lr: 0.000026  loss: 0.4736  time: 1.5254  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1615/3000]  eta: 0:34:57  lr: 0.000026  loss: 0.5127  time: 1.5213  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1615/3000]  eta: 0:34:57  lr: 0.000026  loss: 0.2004  time: 1.5212  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1620/3000]  eta: 0:34:50  lr: 0.000026  loss: 0.2479  time: 1.5320  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1620/3000]  eta: 0:34:49  lr: 0.000026  loss: 0.3276  time: 1.5318  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1625/3000]  eta: 0:34:42  lr: 0.000026  loss: 0.6607  time: 1.4977  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1625/3000]  eta: 0:34:42  lr: 0.000026  loss: 0.2228  time: 1.4974  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1630/3000]  eta: 0:34:34  lr: 0.000026  loss: 0.4494  time: 1.4941  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1630/3000]  eta: 0:34:34  lr: 0.000026  loss: 0.1063  time: 1.4938  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1635/3000]  eta: 0:34:27  lr: 0.000026  loss: 0.3234  time: 1.5033  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1635/3000]  eta: 0:34:27  lr: 0.000026  loss: 0.1610  time: 1.5031  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1640/3000]  eta: 0:34:19  lr: 0.000026  loss: 0.7305  time: 1.4932  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1640/3000]  eta: 0:34:19  lr: 0.000026  loss: 0.2359  time: 1.4929  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1645/3000]  eta: 0:34:12  lr: 0.000026  loss: 0.1621  time: 1.5192  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1645/3000]  eta: 0:34:11  lr: 0.000026  loss: 0.4714  time: 1.5190  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1650/3000]  eta: 0:34:04  lr: 0.000026  loss: 0.6608  time: 1.5191  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1650/3000]  eta: 0:34:04  lr: 0.000026  loss: 0.4177  time: 1.5189  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1655/3000]  eta: 0:33:57  lr: 0.000026  loss: 0.8787  time: 1.5215  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1655/3000]  eta: 0:33:56  lr: 0.000026  loss: 0.5375  time: 1.5212  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1660/3000]  eta: 0:33:49  lr: 0.000026  loss: 0.3987  time: 1.5398  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1660/3000]  eta: 0:33:49  lr: 0.000026  loss: 0.4014  time: 1.5396  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1665/3000]  eta: 0:33:42  lr: 0.000026  loss: 0.4609  time: 1.5372  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1665/3000]  eta: 0:33:41  lr: 0.000026  loss: 0.7004  time: 1.5370  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1670/3000]  eta: 0:33:34  lr: 0.000026  loss: 0.4351  time: 1.5411  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1670/3000]  eta: 0:33:34  lr: 0.000026  loss: 0.4042  time: 1.5409  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1675/3000]  eta: 0:33:27  lr: 0.000026  loss: 0.8270  time: 1.5414  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1675/3000]  eta: 0:33:27  lr: 0.000026  loss: 0.4753  time: 1.5412  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1680/3000]  eta: 0:33:20  lr: 0.000026  loss: 0.5500  time: 1.5562  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1680/3000]  eta: 0:33:19  lr: 0.000026  loss: 0.2029  time: 1.5559  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1685/3000]  eta: 0:33:12  lr: 0.000026  loss: 0.3106  time: 1.5612  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1685/3000]  eta: 0:33:12  lr: 0.000026  loss: 0.5840  time: 1.5609  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1690/3000]  eta: 0:33:05  lr: 0.000026  loss: 0.2874  time: 1.5739  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1690/3000]  eta: 0:33:05  lr: 0.000026  loss: 0.1898  time: 1.5736  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1695/3000]  eta: 0:32:57  lr: 0.000026  loss: 0.3762  time: 1.5611  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1695/3000]  eta: 0:32:57  lr: 0.000026  loss: 0.2115  time: 1.5607  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1700/3000]  eta: 0:32:50  lr: 0.000026  loss: 0.7737  time: 1.5497  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1700/3000]  eta: 0:32:50  lr: 0.000026  loss: 0.3266  time: 1.5495  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1705/3000]  eta: 0:32:42  lr: 0.000026  loss: 0.9882  time: 1.5374  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1705/3000]  eta: 0:32:42  lr: 0.000026  loss: 0.4466  time: 1.5372  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1710/3000]  eta: 0:32:35  lr: 0.000026  loss: 0.5104  time: 1.5299  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1710/3000]  eta: 0:32:34  lr: 0.000026  loss: 0.5852  time: 1.5297  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1715/3000]  eta: 0:32:27  lr: 0.000026  loss: 0.2551  time: 1.5325  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1715/3000]  eta: 0:32:27  lr: 0.000026  loss: 0.3658  time: 1.5323  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1720/3000]  eta: 0:32:20  lr: 0.000026  loss: 0.3504  time: 1.5267  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1720/3000]  eta: 0:32:19  lr: 0.000026  loss: 0.4233  time: 1.5265  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1725/3000]  eta: 0:32:12  lr: 0.000026  loss: 0.4615  time: 1.5255  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1725/3000]  eta: 0:32:12  lr: 0.000026  loss: 0.1705  time: 1.5253  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1730/3000]  eta: 0:32:05  lr: 0.000026  loss: 0.3273  time: 1.5262  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1730/3000]  eta: 0:32:04  lr: 0.000026  loss: 0.0937  time: 1.5260  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1735/3000]  eta: 0:31:57  lr: 0.000026  loss: 0.3579  time: 1.5132  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1735/3000]  eta: 0:31:57  lr: 0.000026  loss: 0.4661  time: 1.5130  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1740/3000]  eta: 0:31:49  lr: 0.000026  loss: 0.2892  time: 1.5037  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1740/3000]  eta: 0:31:49  lr: 0.000026  loss: 0.6418  time: 1.5036  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1745/3000]  eta: 0:31:41  lr: 0.000026  loss: 0.2372  time: 1.4967  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1745/3000]  eta: 0:31:41  lr: 0.000026  loss: 0.2110  time: 1.4966  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1750/3000]  eta: 0:31:34  lr: 0.000026  loss: 0.2186  time: 1.4882  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1750/3000]  eta: 0:31:34  lr: 0.000026  loss: 0.1080  time: 1.4879  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1755/3000]  eta: 0:31:27  lr: 0.000026  loss: 0.4714  time: 1.5152  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1755/3000]  eta: 0:31:26  lr: 0.000026  loss: 0.1432  time: 1.5148  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1760/3000]  eta: 0:31:19  lr: 0.000026  loss: 0.2038  time: 1.5247  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1760/3000]  eta: 0:31:19  lr: 0.000026  loss: 0.6112  time: 1.5244  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1765/3000]  eta: 0:31:11  lr: 0.000026  loss: 0.7042  time: 1.5337  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1765/3000]  eta: 0:31:11  lr: 0.000026  loss: 0.5197  time: 1.5333  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1770/3000]  eta: 0:31:04  lr: 0.000026  loss: 0.8282  time: 1.5351  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1770/3000]  eta: 0:31:04  lr: 0.000026  loss: 0.4847  time: 1.5349  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1775/3000]  eta: 0:30:56  lr: 0.000026  loss: 0.1930  time: 1.5121  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1775/3000]  eta: 0:30:56  lr: 0.000026  loss: 0.2855  time: 1.5119  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1780/3000]  eta: 0:30:49  lr: 0.000026  loss: 0.2563  time: 1.5050  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1780/3000]  eta: 0:30:48  lr: 0.000026  loss: 0.5143  time: 1.5047  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1785/3000]  eta: 0:30:41  lr: 0.000026  loss: 0.6318  time: 1.5142  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1785/3000]  eta: 0:30:41  lr: 0.000026  loss: 0.4418  time: 1.5140  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1790/3000]  eta: 0:30:34  lr: 0.000026  loss: 0.3432  time: 1.5192  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1790/3000]  eta: 0:30:33  lr: 0.000026  loss: 0.5842  time: 1.5189  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1795/3000]  eta: 0:30:26  lr: 0.000026  loss: 0.1714  time: 1.5172  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1795/3000]  eta: 0:30:26  lr: 0.000026  loss: 0.2468  time: 1.5170  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1800/3000]  eta: 0:30:18  lr: 0.000026  loss: 0.3111  time: 1.5327  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1800/3000]  eta: 0:30:18  lr: 0.000026  loss: 0.3917  time: 1.5325  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1805/3000]  eta: 0:30:11  lr: 0.000026  loss: 0.3017  time: 1.5286  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1805/3000]  eta: 0:30:11  lr: 0.000026  loss: 0.2673  time: 1.5285  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1810/3000]  eta: 0:30:04  lr: 0.000026  loss: 0.3946  time: 1.5403  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1810/3000]  eta: 0:30:03  lr: 0.000026  loss: 0.6410  time: 1.5401  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1815/3000]  eta: 0:29:56  lr: 0.000026  loss: 1.0432  time: 1.5584  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1815/3000]  eta: 0:29:56  lr: 0.000026  loss: 0.3279  time: 1.5581  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1820/3000]  eta: 0:29:48  lr: 0.000026  loss: 0.0772  time: 1.5258  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1820/3000]  eta: 0:29:48  lr: 0.000026  loss: 0.6553  time: 1.5255  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1825/3000]  eta: 0:29:41  lr: 0.000026  loss: 0.2312  time: 1.5054  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1825/3000]  eta: 0:29:40  lr: 0.000026  loss: 0.6809  time: 1.5051  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1830/3000]  eta: 0:29:33  lr: 0.000026  loss: 0.2209  time: 1.4749  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1830/3000]  eta: 0:29:32  lr: 0.000026  loss: 0.4927  time: 1.4746  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1835/3000]  eta: 0:29:25  lr: 0.000026  loss: 0.2855  time: 1.4552  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1835/3000]  eta: 0:29:25  lr: 0.000026  loss: 0.2822  time: 1.4549  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1840/3000]  eta: 0:29:17  lr: 0.000026  loss: 0.6931  time: 1.4629  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1840/3000]  eta: 0:29:17  lr: 0.000026  loss: 0.5999  time: 1.4626  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1845/3000]  eta: 0:29:10  lr: 0.000026  loss: 0.4071  time: 1.4809  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1845/3000]  eta: 0:29:09  lr: 0.000026  loss: 1.0537  time: 1.4806  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1850/3000]  eta: 0:29:02  lr: 0.000026  loss: 0.9479  time: 1.4880  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1850/3000]  eta: 0:29:02  lr: 0.000026  loss: 0.4731  time: 1.4877  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1855/3000]  eta: 0:28:55  lr: 0.000026  loss: 0.4174  time: 1.5070  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1855/3000]  eta: 0:28:54  lr: 0.000026  loss: 0.0935  time: 1.5067  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1860/3000]  eta: 0:28:47  lr: 0.000026  loss: 0.5930  time: 1.5471  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1860/3000]  eta: 0:28:47  lr: 0.000026  loss: 0.3824  time: 1.5469  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1865/3000]  eta: 0:28:40  lr: 0.000026  loss: 0.5965  time: 1.5310  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1865/3000]  eta: 0:28:39  lr: 0.000026  loss: 0.6236  time: 1.5308  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1870/3000]  eta: 0:28:32  lr: 0.000026  loss: 0.3561  time: 1.5482  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1870/3000]  eta: 0:28:32  lr: 0.000026  loss: 0.6856  time: 1.5485  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1875/3000]  eta: 0:28:25  lr: 0.000026  loss: 0.2709  time: 1.5299  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1875/3000]  eta: 0:28:24  lr: 0.000026  loss: 0.4993  time: 1.5297  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1880/3000]  eta: 0:28:17  lr: 0.000026  loss: 0.5211  time: 1.5254  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1880/3000]  eta: 0:28:17  lr: 0.000026  loss: 0.2485  time: 1.5253  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1885/3000]  eta: 0:28:10  lr: 0.000026  loss: 0.2212  time: 1.5398  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1885/3000]  eta: 0:28:09  lr: 0.000026  loss: 0.1162  time: 1.5396  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1890/3000]  eta: 0:28:02  lr: 0.000026  loss: 0.3419  time: 1.5272  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1890/3000]  eta: 0:28:02  lr: 0.000026  loss: 0.1221  time: 1.5271  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1895/3000]  eta: 0:27:55  lr: 0.000026  loss: 0.5114  time: 1.5442  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1895/3000]  eta: 0:27:54  lr: 0.000026  loss: 0.2824  time: 1.5440  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1900/3000]  eta: 0:27:47  lr: 0.000026  loss: 0.0937  time: 1.5160  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1900/3000]  eta: 0:27:47  lr: 0.000026  loss: 0.3342  time: 1.5157  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1905/3000]  eta: 0:27:39  lr: 0.000026  loss: 0.3284  time: 1.5000  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1905/3000]  eta: 0:27:39  lr: 0.000026  loss: 0.1937  time: 1.4997  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1910/3000]  eta: 0:27:31  lr: 0.000026  loss: 0.4162  time: 1.4768  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1910/3000]  eta: 0:27:31  lr: 0.000026  loss: 0.1644  time: 1.4766  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1915/3000]  eta: 0:27:24  lr: 0.000026  loss: 0.4489  time: 1.4626  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1915/3000]  eta: 0:27:23  lr: 0.000026  loss: 0.3154  time: 1.4624  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1920/3000]  eta: 0:27:16  lr: 0.000026  loss: 0.4888  time: 1.4761  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1920/3000]  eta: 0:27:16  lr: 0.000026  loss: 0.3078  time: 1.4758  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1925/3000]  eta: 0:27:09  lr: 0.000026  loss: 0.7338  time: 1.5089  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1925/3000]  eta: 0:27:08  lr: 0.000026  loss: 0.4867  time: 1.5087  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1930/3000]  eta: 0:27:01  lr: 0.000026  loss: 0.8793  time: 1.5013  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1930/3000]  eta: 0:27:01  lr: 0.000026  loss: 0.2569  time: 1.5010  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1935/3000]  eta: 0:26:53  lr: 0.000026  loss: 0.2284  time: 1.5081  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1935/3000]  eta: 0:26:53  lr: 0.000026  loss: 0.6527  time: 1.5078  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1940/3000]  eta: 0:26:46  lr: 0.000026  loss: 0.3503  time: 1.5074  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1940/3000]  eta: 0:26:45  lr: 0.000026  loss: 0.4830  time: 1.5072  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1945/3000]  eta: 0:26:38  lr: 0.000026  loss: 0.7195  time: 1.5023  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1945/3000]  eta: 0:26:38  lr: 0.000026  loss: 0.1809  time: 1.5020  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1950/3000]  eta: 0:26:31  lr: 0.000026  loss: 0.2676  time: 1.5389  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1950/3000]  eta: 0:26:30  lr: 0.000026  loss: 0.4971  time: 1.5386  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1955/3000]  eta: 0:26:23  lr: 0.000026  loss: 0.0913  time: 1.5178  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1955/3000]  eta: 0:26:23  lr: 0.000026  loss: 0.7336  time: 1.5176  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1960/3000]  eta: 0:26:15  lr: 0.000026  loss: 0.1447  time: 1.5227  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1960/3000]  eta: 0:26:15  lr: 0.000026  loss: 0.1784  time: 1.5224  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1965/3000]  eta: 0:26:08  lr: 0.000026  loss: 0.3353  time: 1.5209  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1965/3000]  eta: 0:26:08  lr: 0.000026  loss: 0.3830  time: 1.5207  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1970/3000]  eta: 0:26:00  lr: 0.000026  loss: 1.2610  time: 1.5156  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1970/3000]  eta: 0:26:00  lr: 0.000026  loss: 0.3185  time: 1.5153  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1975/3000]  eta: 0:25:53  lr: 0.000026  loss: 0.9133  time: 1.5356  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1975/3000]  eta: 0:25:53  lr: 0.000026  loss: 0.3287  time: 1.5353  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1980/3000]  eta: 0:25:45  lr: 0.000026  loss: 0.4025  time: 1.5357  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1980/3000]  eta: 0:25:45  lr: 0.000026  loss: 0.6428  time: 1.5355  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1985/3000]  eta: 0:25:38  lr: 0.000026  loss: 0.2374  time: 1.5350  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1985/3000]  eta: 0:25:38  lr: 0.000026  loss: 0.5361  time: 1.5347  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1990/3000]  eta: 0:25:30  lr: 0.000026  loss: 0.3074  time: 1.5467  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1990/3000]  eta: 0:25:30  lr: 0.000026  loss: 0.1311  time: 1.5464  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [1995/3000]  eta: 0:25:23  lr: 0.000026  loss: 0.5803  time: 1.5458  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [1995/3000]  eta: 0:25:23  lr: 0.000026  loss: 0.3353  time: 1.5456  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2000/3000]  eta: 0:25:15  lr: 0.000026  loss: 0.4874  time: 1.5390  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2000/3000]  eta: 0:25:15  lr: 0.000026  loss: 0.6240  time: 1.5387  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2005/3000]  eta: 0:25:08  lr: 0.000026  loss: 0.2986  time: 1.5266  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2005/3000]  eta: 0:25:07  lr: 0.000026  loss: 0.7060  time: 1.5263  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2010/3000]  eta: 0:25:00  lr: 0.000026  loss: 0.3708  time: 1.5202  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2010/3000]  eta: 0:25:00  lr: 0.000026  loss: 1.0331  time: 1.5199  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2015/3000]  eta: 0:24:53  lr: 0.000026  loss: 0.4404  time: 1.5261  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2015/3000]  eta: 0:24:52  lr: 0.000026  loss: 0.4502  time: 1.5258  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2020/3000]  eta: 0:24:45  lr: 0.000026  loss: 0.2088  time: 1.5026  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2020/3000]  eta: 0:24:45  lr: 0.000026  loss: 0.2981  time: 1.5024  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2025/3000]  eta: 0:24:37  lr: 0.000026  loss: 0.2015  time: 1.4776  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2025/3000]  eta: 0:24:37  lr: 0.000026  loss: 0.2898  time: 1.4774  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2030/3000]  eta: 0:24:30  lr: 0.000026  loss: 0.8165  time: 1.4873  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2030/3000]  eta: 0:24:29  lr: 0.000026  loss: 0.1837  time: 1.4870  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2035/3000]  eta: 0:24:22  lr: 0.000026  loss: 0.6056  time: 1.4553  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2035/3000]  eta: 0:24:22  lr: 0.000026  loss: 0.3673  time: 1.4550  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2040/3000]  eta: 0:24:14  lr: 0.000026  loss: 0.3892  time: 1.4979  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2040/3000]  eta: 0:24:14  lr: 0.000026  loss: 0.2770  time: 1.4977  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2045/3000]  eta: 0:24:07  lr: 0.000026  loss: 0.6084  time: 1.5325  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2045/3000]  eta: 0:24:07  lr: 0.000026  loss: 0.2573  time: 1.5323  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2050/3000]  eta: 0:23:59  lr: 0.000026  loss: 0.6740  time: 1.4913  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2050/3000]  eta: 0:23:59  lr: 0.000026  loss: 0.1063  time: 1.4911  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2055/3000]  eta: 0:23:51  lr: 0.000026  loss: 0.2923  time: 1.5190  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2055/3000]  eta: 0:23:51  lr: 0.000026  loss: 0.3624  time: 1.5187  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2060/3000]  eta: 0:23:44  lr: 0.000026  loss: 0.1764  time: 1.5106  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2060/3000]  eta: 0:23:44  lr: 0.000026  loss: 0.6459  time: 1.5103  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2065/3000]  eta: 0:23:37  lr: 0.000026  loss: 0.9859  time: 1.5248  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2065/3000]  eta: 0:23:36  lr: 0.000026  loss: 0.2675  time: 1.5246  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2070/3000]  eta: 0:23:29  lr: 0.000026  loss: 0.7587  time: 1.5657  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2070/3000]  eta: 0:23:29  lr: 0.000026  loss: 0.0777  time: 1.5654  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2075/3000]  eta: 0:23:22  lr: 0.000026  loss: 0.1582  time: 1.5561  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2075/3000]  eta: 0:23:21  lr: 0.000026  loss: 0.4695  time: 1.5559  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2080/3000]  eta: 0:23:14  lr: 0.000026  loss: 1.0375  time: 1.5393  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2080/3000]  eta: 0:23:14  lr: 0.000026  loss: 0.3050  time: 1.5390  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2085/3000]  eta: 0:23:06  lr: 0.000026  loss: 0.3686  time: 1.5315  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2085/3000]  eta: 0:23:06  lr: 0.000026  loss: 0.3777  time: 1.5312  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2090/3000]  eta: 0:22:59  lr: 0.000026  loss: 0.1551  time: 1.4967  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2090/3000]  eta: 0:22:58  lr: 0.000026  loss: 0.2601  time: 1.4964  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2095/3000]  eta: 0:22:51  lr: 0.000026  loss: 0.5709  time: 1.5045  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2095/3000]  eta: 0:22:51  lr: 0.000026  loss: 0.4226  time: 1.5042  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2100/3000]  eta: 0:22:44  lr: 0.000026  loss: 0.2261  time: 1.5025  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2100/3000]  eta: 0:22:43  lr: 0.000026  loss: 0.6003  time: 1.5023  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2105/3000]  eta: 0:22:36  lr: 0.000026  loss: 0.2063  time: 1.4879  data: 0.0000  max mem: 18406Train: data epoch: [8]  [2105/3000]  eta: 0:22:36  lr: 0.000026  loss: 0.3194  time: 1.4881  data: 0.0000  max mem: 18596

Train: data epoch: [8]  [2110/3000]  eta: 0:22:28  lr: 0.000026  loss: 0.2128  time: 1.5212  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2110/3000]  eta: 0:22:28  lr: 0.000026  loss: 0.4088  time: 1.5211  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2115/3000]  eta: 0:22:21  lr: 0.000026  loss: 0.3013  time: 1.5176  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2115/3000]  eta: 0:22:21  lr: 0.000026  loss: 0.4651  time: 1.5175  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2120/3000]  eta: 0:22:13  lr: 0.000026  loss: 0.7366  time: 1.5327  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2120/3000]  eta: 0:22:13  lr: 0.000026  loss: 0.3155  time: 1.5325  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2125/3000]  eta: 0:22:06  lr: 0.000026  loss: 0.5747  time: 1.5183  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2125/3000]  eta: 0:22:05  lr: 0.000026  loss: 0.7760  time: 1.5181  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2130/3000]  eta: 0:21:58  lr: 0.000026  loss: 0.1252  time: 1.5100  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2130/3000]  eta: 0:21:58  lr: 0.000026  loss: 0.1286  time: 1.5097  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2135/3000]  eta: 0:21:51  lr: 0.000026  loss: 0.3839  time: 1.5124  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2135/3000]  eta: 0:21:50  lr: 0.000026  loss: 0.6004  time: 1.5121  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2140/3000]  eta: 0:21:43  lr: 0.000026  loss: 0.3496  time: 1.4937  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2140/3000]  eta: 0:21:43  lr: 0.000026  loss: 0.7745  time: 1.4934  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2145/3000]  eta: 0:21:35  lr: 0.000026  loss: 0.4029  time: 1.5132  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2145/3000]  eta: 0:21:35  lr: 0.000026  loss: 0.4992  time: 1.5129  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2150/3000]  eta: 0:21:28  lr: 0.000026  loss: 0.3657  time: 1.5118  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2150/3000]  eta: 0:21:28  lr: 0.000026  loss: 0.3492  time: 1.5116  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2155/3000]  eta: 0:21:20  lr: 0.000026  loss: 0.5045  time: 1.5334  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2155/3000]  eta: 0:21:20  lr: 0.000026  loss: 0.4692  time: 1.5332  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2160/3000]  eta: 0:21:13  lr: 0.000026  loss: 0.4562  time: 1.5372  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2160/3000]  eta: 0:21:12  lr: 0.000026  loss: 0.6095  time: 1.5370  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2165/3000]  eta: 0:21:05  lr: 0.000026  loss: 0.3307  time: 1.5131  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2165/3000]  eta: 0:21:05  lr: 0.000026  loss: 0.2120  time: 1.5130  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2170/3000]  eta: 0:20:57  lr: 0.000026  loss: 0.3482  time: 1.5304  data: 0.0000  max mem: 18406Train: data epoch: [8]  [2170/3000]  eta: 0:20:58  lr: 0.000026  loss: 0.4051  time: 1.5306  data: 0.0000  max mem: 18596

Train: data epoch: [8]  [2175/3000]  eta: 0:20:50  lr: 0.000026  loss: 0.3416  time: 1.5148  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2175/3000]  eta: 0:20:50  lr: 0.000026  loss: 0.6047  time: 1.5146  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2180/3000]  eta: 0:20:42  lr: 0.000026  loss: 0.4617  time: 1.5232  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2180/3000]  eta: 0:20:42  lr: 0.000026  loss: 0.5166  time: 1.5230  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2185/3000]  eta: 0:20:35  lr: 0.000026  loss: 0.5013  time: 1.5375  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2185/3000]  eta: 0:20:35  lr: 0.000026  loss: 0.3973  time: 1.5373  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2190/3000]  eta: 0:20:27  lr: 0.000026  loss: 0.2490  time: 1.5261  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2190/3000]  eta: 0:20:27  lr: 0.000026  loss: 0.4441  time: 1.5259  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2195/3000]  eta: 0:20:20  lr: 0.000026  loss: 0.7324  time: 1.5185  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2195/3000]  eta: 0:20:20  lr: 0.000026  loss: 0.8281  time: 1.5183  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2200/3000]  eta: 0:20:12  lr: 0.000026  loss: 1.0349  time: 1.5259  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2200/3000]  eta: 0:20:12  lr: 0.000026  loss: 0.1733  time: 1.5256  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2205/3000]  eta: 0:20:05  lr: 0.000026  loss: 0.4773  time: 1.5188  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2205/3000]  eta: 0:20:04  lr: 0.000026  loss: 0.3799  time: 1.5186  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2210/3000]  eta: 0:19:57  lr: 0.000026  loss: 0.2867  time: 1.4922  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2210/3000]  eta: 0:19:57  lr: 0.000026  loss: 0.2774  time: 1.4919  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2215/3000]  eta: 0:19:49  lr: 0.000026  loss: 0.2787  time: 1.4943  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2215/3000]  eta: 0:19:49  lr: 0.000026  loss: 0.6246  time: 1.4940  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2220/3000]  eta: 0:19:42  lr: 0.000026  loss: 0.3401  time: 1.4747  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2220/3000]  eta: 0:19:41  lr: 0.000026  loss: 0.5602  time: 1.4745  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2225/3000]  eta: 0:19:34  lr: 0.000026  loss: 0.4144  time: 1.5050  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2225/3000]  eta: 0:19:34  lr: 0.000026  loss: 0.2250  time: 1.5047  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2230/3000]  eta: 0:19:27  lr: 0.000026  loss: 0.7082  time: 1.5281  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2230/3000]  eta: 0:19:26  lr: 0.000026  loss: 0.4315  time: 1.5278  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2235/3000]  eta: 0:19:19  lr: 0.000026  loss: 0.3448  time: 1.5343  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2235/3000]  eta: 0:19:19  lr: 0.000026  loss: 1.1112  time: 1.5341  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2240/3000]  eta: 0:19:12  lr: 0.000026  loss: 0.7442  time: 1.5487  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2240/3000]  eta: 0:19:11  lr: 0.000026  loss: 0.3124  time: 1.5486  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2245/3000]  eta: 0:19:04  lr: 0.000026  loss: 0.5705  time: 1.5232  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2245/3000]  eta: 0:19:04  lr: 0.000026  loss: 0.5816  time: 1.5229  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2250/3000]  eta: 0:18:56  lr: 0.000026  loss: 0.4441  time: 1.5184  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2250/3000]  eta: 0:18:56  lr: 0.000026  loss: 0.9556  time: 1.5181  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2255/3000]  eta: 0:18:49  lr: 0.000026  loss: 0.1963  time: 1.5153  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2255/3000]  eta: 0:18:49  lr: 0.000026  loss: 0.4232  time: 1.5150  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2260/3000]  eta: 0:18:41  lr: 0.000026  loss: 0.1563  time: 1.5046  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2260/3000]  eta: 0:18:41  lr: 0.000026  loss: 0.4720  time: 1.5043  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2265/3000]  eta: 0:18:34  lr: 0.000026  loss: 0.4075  time: 1.5195  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2265/3000]  eta: 0:18:33  lr: 0.000026  loss: 0.1712  time: 1.5193  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2270/3000]  eta: 0:18:26  lr: 0.000026  loss: 0.6778  time: 1.5147  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2270/3000]  eta: 0:18:26  lr: 0.000026  loss: 0.0933  time: 1.5145  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2275/3000]  eta: 0:18:18  lr: 0.000026  loss: 0.2737  time: 1.4724  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2275/3000]  eta: 0:18:18  lr: 0.000026  loss: 0.5968  time: 1.4722  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2280/3000]  eta: 0:18:11  lr: 0.000026  loss: 0.5022  time: 1.4813  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2280/3000]  eta: 0:18:10  lr: 0.000026  loss: 0.4824  time: 1.4811  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2285/3000]  eta: 0:18:03  lr: 0.000026  loss: 0.3030  time: 1.4770  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2285/3000]  eta: 0:18:03  lr: 0.000026  loss: 0.3291  time: 1.4768  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2290/3000]  eta: 0:17:55  lr: 0.000026  loss: 0.5242  time: 1.4726  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2290/3000]  eta: 0:17:55  lr: 0.000026  loss: 0.3445  time: 1.4724  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2295/3000]  eta: 0:17:48  lr: 0.000026  loss: 0.2832  time: 1.5221  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2295/3000]  eta: 0:17:48  lr: 0.000026  loss: 0.2088  time: 1.5219  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2300/3000]  eta: 0:17:40  lr: 0.000026  loss: 0.3195  time: 1.5078  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2300/3000]  eta: 0:17:40  lr: 0.000026  loss: 0.6140  time: 1.5076  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2305/3000]  eta: 0:17:33  lr: 0.000026  loss: 0.2478  time: 1.4940  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2305/3000]  eta: 0:17:32  lr: 0.000026  loss: 0.1859  time: 1.4938  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2310/3000]  eta: 0:17:25  lr: 0.000026  loss: 0.2424  time: 1.5137  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2310/3000]  eta: 0:17:25  lr: 0.000026  loss: 0.6226  time: 1.5135  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2315/3000]  eta: 0:17:18  lr: 0.000026  loss: 0.5055  time: 1.5061  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2315/3000]  eta: 0:17:17  lr: 0.000026  loss: 0.7840  time: 1.5059  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2320/3000]  eta: 0:17:10  lr: 0.000026  loss: 0.1063  time: 1.5304  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2320/3000]  eta: 0:17:10  lr: 0.000026  loss: 0.8771  time: 1.5302  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2325/3000]  eta: 0:17:02  lr: 0.000026  loss: 0.2085  time: 1.5515  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2325/3000]  eta: 0:17:02  lr: 0.000026  loss: 0.7228  time: 1.5513  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2330/3000]  eta: 0:16:55  lr: 0.000026  loss: 0.4064  time: 1.5586  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2330/3000]  eta: 0:16:55  lr: 0.000026  loss: 0.0959  time: 1.5583  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2335/3000]  eta: 0:16:47  lr: 0.000026  loss: 0.5617  time: 1.5589  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2335/3000]  eta: 0:16:47  lr: 0.000026  loss: 0.2270  time: 1.5587  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2340/3000]  eta: 0:16:40  lr: 0.000026  loss: 0.7479  time: 1.5303  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2340/3000]  eta: 0:16:40  lr: 0.000026  loss: 0.8943  time: 1.5301  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2345/3000]  eta: 0:16:32  lr: 0.000026  loss: 0.6885  time: 1.5398  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2345/3000]  eta: 0:16:32  lr: 0.000026  loss: 0.7525  time: 1.5397  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2350/3000]  eta: 0:16:25  lr: 0.000026  loss: 0.8879  time: 1.5133  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2350/3000]  eta: 0:16:25  lr: 0.000026  loss: 0.2610  time: 1.5131  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2355/3000]  eta: 0:16:17  lr: 0.000026  loss: 0.1863  time: 1.5127  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2355/3000]  eta: 0:16:17  lr: 0.000026  loss: 0.3872  time: 1.5126  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2360/3000]  eta: 0:16:10  lr: 0.000026  loss: 0.2993  time: 1.5203  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2360/3000]  eta: 0:16:09  lr: 0.000026  loss: 0.1758  time: 1.5200  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2365/3000]  eta: 0:16:02  lr: 0.000026  loss: 0.9803  time: 1.4923  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2365/3000]  eta: 0:16:02  lr: 0.000026  loss: 0.3613  time: 1.4920  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2370/3000]  eta: 0:15:54  lr: 0.000026  loss: 0.6202  time: 1.4915  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2370/3000]  eta: 0:15:54  lr: 0.000026  loss: 0.5369  time: 1.4912  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2375/3000]  eta: 0:15:47  lr: 0.000026  loss: 0.2046  time: 1.4673  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2375/3000]  eta: 0:15:46  lr: 0.000026  loss: 0.4113  time: 1.4670  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2380/3000]  eta: 0:15:39  lr: 0.000026  loss: 0.4270  time: 1.4644  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2380/3000]  eta: 0:15:39  lr: 0.000026  loss: 0.6459  time: 1.4642  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2385/3000]  eta: 0:15:31  lr: 0.000026  loss: 1.1206  time: 1.4801  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2385/3000]  eta: 0:15:31  lr: 0.000026  loss: 0.7004  time: 1.4798  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2390/3000]  eta: 0:15:24  lr: 0.000026  loss: 0.3112  time: 1.4806  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2390/3000]  eta: 0:15:24  lr: 0.000026  loss: 0.6562  time: 1.4804  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2395/3000]  eta: 0:15:16  lr: 0.000026  loss: 0.7356  time: 1.4929  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2395/3000]  eta: 0:15:16  lr: 0.000026  loss: 0.4320  time: 1.4927  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2400/3000]  eta: 0:15:09  lr: 0.000026  loss: 0.5272  time: 1.5098  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2400/3000]  eta: 0:15:08  lr: 0.000026  loss: 0.1342  time: 1.5096  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2405/3000]  eta: 0:15:01  lr: 0.000026  loss: 0.2435  time: 1.4921  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2405/3000]  eta: 0:15:01  lr: 0.000026  loss: 0.6390  time: 1.4919  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2410/3000]  eta: 0:14:53  lr: 0.000026  loss: 0.3378  time: 1.5153  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2410/3000]  eta: 0:14:53  lr: 0.000026  loss: 0.3277  time: 1.5151  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2415/3000]  eta: 0:14:46  lr: 0.000026  loss: 0.1468  time: 1.5303  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2415/3000]  eta: 0:14:46  lr: 0.000026  loss: 0.2200  time: 1.5301  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2420/3000]  eta: 0:14:38  lr: 0.000026  loss: 0.1928  time: 1.5251  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2420/3000]  eta: 0:14:38  lr: 0.000026  loss: 0.6242  time: 1.5249  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2425/3000]  eta: 0:14:31  lr: 0.000026  loss: 0.4068  time: 1.5275  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2425/3000]  eta: 0:14:31  lr: 0.000026  loss: 0.2317  time: 1.5273  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2430/3000]  eta: 0:14:23  lr: 0.000026  loss: 0.5415  time: 1.5244  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2430/3000]  eta: 0:14:23  lr: 0.000026  loss: 0.1521  time: 1.5242  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2435/3000]  eta: 0:14:16  lr: 0.000026  loss: 0.8163  time: 1.5231  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2435/3000]  eta: 0:14:16  lr: 0.000026  loss: 0.2242  time: 1.5228  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2440/3000]  eta: 0:14:08  lr: 0.000026  loss: 0.8616  time: 1.5292  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2440/3000]  eta: 0:14:08  lr: 0.000026  loss: 0.4022  time: 1.5289  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2445/3000]  eta: 0:14:01  lr: 0.000026  loss: 0.3050  time: 1.5507  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2445/3000]  eta: 0:14:00  lr: 0.000026  loss: 0.2658  time: 1.5505  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2450/3000]  eta: 0:13:53  lr: 0.000026  loss: 0.5059  time: 1.5538  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2450/3000]  eta: 0:13:53  lr: 0.000026  loss: 0.1236  time: 1.5535  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2455/3000]  eta: 0:13:45  lr: 0.000026  loss: 0.5491  time: 1.5465  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2455/3000]  eta: 0:13:45  lr: 0.000026  loss: 0.4783  time: 1.5462  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2460/3000]  eta: 0:13:38  lr: 0.000026  loss: 0.3123  time: 1.5276  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2460/3000]  eta: 0:13:38  lr: 0.000026  loss: 0.3247  time: 1.5273  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2465/3000]  eta: 0:13:30  lr: 0.000026  loss: 0.6131  time: 1.5245  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2465/3000]  eta: 0:13:30  lr: 0.000026  loss: 0.5129  time: 1.5242  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2470/3000]  eta: 0:13:23  lr: 0.000026  loss: 0.3187  time: 1.5247  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2470/3000]  eta: 0:13:23  lr: 0.000026  loss: 0.4905  time: 1.5244  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2475/3000]  eta: 0:13:15  lr: 0.000026  loss: 0.8959  time: 1.5343  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2475/3000]  eta: 0:13:15  lr: 0.000026  loss: 0.3825  time: 1.5341  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2480/3000]  eta: 0:13:08  lr: 0.000026  loss: 0.4710  time: 1.5392  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2480/3000]  eta: 0:13:08  lr: 0.000026  loss: 0.4912  time: 1.5390  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2485/3000]  eta: 0:13:00  lr: 0.000026  loss: 0.1273  time: 1.5310  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2485/3000]  eta: 0:13:00  lr: 0.000026  loss: 0.4330  time: 1.5308  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2490/3000]  eta: 0:12:53  lr: 0.000026  loss: 0.5175  time: 1.5228  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2490/3000]  eta: 0:12:52  lr: 0.000026  loss: 0.3674  time: 1.5226  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2495/3000]  eta: 0:12:45  lr: 0.000026  loss: 0.6363  time: 1.5269  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2495/3000]  eta: 0:12:45  lr: 0.000026  loss: 0.6526  time: 1.5267  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2500/3000]  eta: 0:12:37  lr: 0.000026  loss: 0.0987  time: 1.5343  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2500/3000]  eta: 0:12:37  lr: 0.000026  loss: 0.2176  time: 1.5342  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2505/3000]  eta: 0:12:30  lr: 0.000026  loss: 0.0470  time: 1.5487  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2505/3000]  eta: 0:12:30  lr: 0.000026  loss: 0.4202  time: 1.5485  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2510/3000]  eta: 0:12:22  lr: 0.000026  loss: 0.4544  time: 1.5425  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2510/3000]  eta: 0:12:22  lr: 0.000026  loss: 0.5912  time: 1.5423  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2515/3000]  eta: 0:12:15  lr: 0.000026  loss: 0.4664  time: 1.5504  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2515/3000]  eta: 0:12:15  lr: 0.000026  loss: 0.5166  time: 1.5502  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2520/3000]  eta: 0:12:07  lr: 0.000026  loss: 0.5497  time: 1.5574  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2520/3000]  eta: 0:12:07  lr: 0.000026  loss: 0.3443  time: 1.5572  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2525/3000]  eta: 0:12:00  lr: 0.000026  loss: 0.6176  time: 1.5419  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2525/3000]  eta: 0:12:00  lr: 0.000026  loss: 0.4609  time: 1.5416  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2530/3000]  eta: 0:11:52  lr: 0.000026  loss: 0.2832  time: 1.5333  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2530/3000]  eta: 0:11:52  lr: 0.000026  loss: 0.4518  time: 1.5330  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2535/3000]  eta: 0:11:44  lr: 0.000026  loss: 0.4045  time: 1.5020  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2535/3000]  eta: 0:11:44  lr: 0.000026  loss: 0.1867  time: 1.5017  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2540/3000]  eta: 0:11:37  lr: 0.000026  loss: 1.0404  time: 1.4842  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2540/3000]  eta: 0:11:37  lr: 0.000026  loss: 0.4822  time: 1.4839  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2545/3000]  eta: 0:11:29  lr: 0.000026  loss: 0.3965  time: 1.4875  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2545/3000]  eta: 0:11:29  lr: 0.000026  loss: 0.3661  time: 1.4873  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2550/3000]  eta: 0:11:22  lr: 0.000026  loss: 0.2677  time: 1.4904  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2550/3000]  eta: 0:11:22  lr: 0.000026  loss: 0.4951  time: 1.4902  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2555/3000]  eta: 0:11:14  lr: 0.000026  loss: 0.3674  time: 1.4937  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2555/3000]  eta: 0:11:14  lr: 0.000026  loss: 0.1564  time: 1.4935  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2560/3000]  eta: 0:11:06  lr: 0.000026  loss: 0.2369  time: 1.4989  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2560/3000]  eta: 0:11:06  lr: 0.000026  loss: 0.3770  time: 1.4987  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2565/3000]  eta: 0:10:59  lr: 0.000026  loss: 0.3634  time: 1.4955  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2565/3000]  eta: 0:10:59  lr: 0.000026  loss: 0.3398  time: 1.4942  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2570/3000]  eta: 0:10:51  lr: 0.000026  loss: 0.1161  time: 1.4955  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2570/3000]  eta: 0:10:51  lr: 0.000026  loss: 0.4472  time: 1.4941  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2575/3000]  eta: 0:10:44  lr: 0.000026  loss: 0.4714  time: 1.5006  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2575/3000]  eta: 0:10:44  lr: 0.000026  loss: 0.3268  time: 1.4992  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2580/3000]  eta: 0:10:36  lr: 0.000026  loss: 0.7697  time: 1.4934  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2580/3000]  eta: 0:10:36  lr: 0.000026  loss: 0.5836  time: 1.4920  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2585/3000]  eta: 0:10:29  lr: 0.000026  loss: 0.1224  time: 1.5080  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2585/3000]  eta: 0:10:28  lr: 0.000026  loss: 0.4973  time: 1.5077  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2590/3000]  eta: 0:10:21  lr: 0.000026  loss: 0.2103  time: 1.5070  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2590/3000]  eta: 0:10:21  lr: 0.000026  loss: 0.3194  time: 1.5068  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2595/3000]  eta: 0:10:13  lr: 0.000026  loss: 0.8918  time: 1.4973  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2595/3000]  eta: 0:10:13  lr: 0.000026  loss: 0.1467  time: 1.4971  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2600/3000]  eta: 0:10:06  lr: 0.000026  loss: 0.7521  time: 1.5055  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2600/3000]  eta: 0:10:06  lr: 0.000026  loss: 1.0383  time: 1.5052  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2605/3000]  eta: 0:09:58  lr: 0.000026  loss: 0.4484  time: 1.5062  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2605/3000]  eta: 0:09:58  lr: 0.000026  loss: 0.2508  time: 1.5059  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2610/3000]  eta: 0:09:51  lr: 0.000026  loss: 0.2691  time: 1.5258  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2610/3000]  eta: 0:09:51  lr: 0.000026  loss: 0.4472  time: 1.5255  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2615/3000]  eta: 0:09:43  lr: 0.000026  loss: 0.6442  time: 1.5201  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2615/3000]  eta: 0:09:43  lr: 0.000026  loss: 0.1981  time: 1.5198  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2620/3000]  eta: 0:09:35  lr: 0.000026  loss: 0.2160  time: 1.5285  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2620/3000]  eta: 0:09:35  lr: 0.000026  loss: 0.1174  time: 1.5282  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2625/3000]  eta: 0:09:28  lr: 0.000026  loss: 1.0722  time: 1.5195  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2625/3000]  eta: 0:09:28  lr: 0.000026  loss: 0.7458  time: 1.5193  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2630/3000]  eta: 0:09:20  lr: 0.000026  loss: 0.1228  time: 1.5121  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2630/3000]  eta: 0:09:20  lr: 0.000026  loss: 0.5648  time: 1.5119  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2635/3000]  eta: 0:09:13  lr: 0.000026  loss: 0.4311  time: 1.5140  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2635/3000]  eta: 0:09:13  lr: 0.000026  loss: 0.3106  time: 1.5138  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2640/3000]  eta: 0:09:05  lr: 0.000026  loss: 0.9607  time: 1.5093  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2640/3000]  eta: 0:09:05  lr: 0.000026  loss: 0.1193  time: 1.5091  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2645/3000]  eta: 0:08:58  lr: 0.000026  loss: 0.0807  time: 1.5063  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2645/3000]  eta: 0:08:57  lr: 0.000026  loss: 0.5174  time: 1.5061  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2650/3000]  eta: 0:08:50  lr: 0.000026  loss: 0.2995  time: 1.5058  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2650/3000]  eta: 0:08:50  lr: 0.000026  loss: 0.2237  time: 1.5055  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2655/3000]  eta: 0:08:42  lr: 0.000026  loss: 0.4969  time: 1.5323  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2655/3000]  eta: 0:08:42  lr: 0.000026  loss: 0.1318  time: 1.5321  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2660/3000]  eta: 0:08:35  lr: 0.000026  loss: 0.6152  time: 1.5342  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2660/3000]  eta: 0:08:35  lr: 0.000026  loss: 0.2870  time: 1.5339  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2665/3000]  eta: 0:08:27  lr: 0.000026  loss: 0.5170  time: 1.5191  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2665/3000]  eta: 0:08:27  lr: 0.000026  loss: 0.3385  time: 1.5188  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2670/3000]  eta: 0:08:20  lr: 0.000026  loss: 0.5613  time: 1.5101  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2670/3000]  eta: 0:08:20  lr: 0.000026  loss: 0.3052  time: 1.5099  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2675/3000]  eta: 0:08:12  lr: 0.000026  loss: 0.3628  time: 1.5188  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2675/3000]  eta: 0:08:12  lr: 0.000026  loss: 0.6266  time: 1.5186  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2680/3000]  eta: 0:08:04  lr: 0.000026  loss: 0.3121  time: 1.5002  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2680/3000]  eta: 0:08:04  lr: 0.000026  loss: 0.2783  time: 1.4998  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2685/3000]  eta: 0:07:57  lr: 0.000026  loss: 0.1629  time: 1.5067  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2685/3000]  eta: 0:07:57  lr: 0.000026  loss: 0.5248  time: 1.5065  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2690/3000]  eta: 0:07:49  lr: 0.000026  loss: 0.9382  time: 1.4924  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2690/3000]  eta: 0:07:49  lr: 0.000026  loss: 0.2161  time: 1.4922  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2695/3000]  eta: 0:07:42  lr: 0.000026  loss: 0.1853  time: 1.4743  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2695/3000]  eta: 0:07:42  lr: 0.000026  loss: 0.5264  time: 1.4741  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2700/3000]  eta: 0:07:34  lr: 0.000026  loss: 0.5788  time: 1.4815  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2700/3000]  eta: 0:07:34  lr: 0.000026  loss: 0.2949  time: 1.4813  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2705/3000]  eta: 0:07:27  lr: 0.000026  loss: 0.2999  time: 1.4795  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2705/3000]  eta: 0:07:26  lr: 0.000026  loss: 0.2628  time: 1.4792  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2710/3000]  eta: 0:07:19  lr: 0.000026  loss: 0.3619  time: 1.5024  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2710/3000]  eta: 0:07:19  lr: 0.000026  loss: 0.4525  time: 1.5021  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2715/3000]  eta: 0:07:11  lr: 0.000026  loss: 0.1874  time: 1.4893  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2715/3000]  eta: 0:07:11  lr: 0.000026  loss: 0.3814  time: 1.4890  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2720/3000]  eta: 0:07:04  lr: 0.000026  loss: 0.8609  time: 1.5012  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2720/3000]  eta: 0:07:04  lr: 0.000026  loss: 0.4275  time: 1.5009  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2725/3000]  eta: 0:06:56  lr: 0.000026  loss: 0.2515  time: 1.5113  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2725/3000]  eta: 0:06:56  lr: 0.000026  loss: 0.1732  time: 1.5110  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2730/3000]  eta: 0:06:49  lr: 0.000026  loss: 0.1325  time: 1.5065  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2730/3000]  eta: 0:06:49  lr: 0.000026  loss: 0.1598  time: 1.5062  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2735/3000]  eta: 0:06:41  lr: 0.000026  loss: 0.4567  time: 1.5245  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2735/3000]  eta: 0:06:41  lr: 0.000026  loss: 0.3933  time: 1.5243  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2740/3000]  eta: 0:06:33  lr: 0.000026  loss: 0.4455  time: 1.5296  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2740/3000]  eta: 0:06:33  lr: 0.000026  loss: 0.0855  time: 1.5293  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2745/3000]  eta: 0:06:26  lr: 0.000026  loss: 0.4033  time: 1.5474  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2745/3000]  eta: 0:06:26  lr: 0.000026  loss: 0.5551  time: 1.5471  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2750/3000]  eta: 0:06:18  lr: 0.000026  loss: 0.3907  time: 1.5504  data: 0.0000  max mem: 18406Train: data epoch: [8]  [2750/3000]  eta: 0:06:18  lr: 0.000026  loss: 0.1738  time: 1.5507  data: 0.0000  max mem: 18596

Train: data epoch: [8]  [2755/3000]  eta: 0:06:11  lr: 0.000026  loss: 0.4388  time: 1.5516  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2755/3000]  eta: 0:06:11  lr: 0.000026  loss: 0.3796  time: 1.5512  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2760/3000]  eta: 0:06:03  lr: 0.000026  loss: 0.7936  time: 1.5467  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2760/3000]  eta: 0:06:03  lr: 0.000026  loss: 0.3080  time: 1.5464  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2765/3000]  eta: 0:05:56  lr: 0.000026  loss: 0.2586  time: 1.5045  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2765/3000]  eta: 0:05:56  lr: 0.000026  loss: 0.5009  time: 1.5042  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2770/3000]  eta: 0:05:48  lr: 0.000026  loss: 0.3290  time: 1.5034  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2770/3000]  eta: 0:05:48  lr: 0.000026  loss: 1.1939  time: 1.5031  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2775/3000]  eta: 0:05:40  lr: 0.000026  loss: 0.6362  time: 1.5173  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2775/3000]  eta: 0:05:40  lr: 0.000026  loss: 0.1040  time: 1.5171  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2780/3000]  eta: 0:05:33  lr: 0.000026  loss: 0.4027  time: 1.5119  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2780/3000]  eta: 0:05:33  lr: 0.000026  loss: 0.5217  time: 1.5117  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2785/3000]  eta: 0:05:25  lr: 0.000026  loss: 0.4240  time: 1.5395  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2785/3000]  eta: 0:05:25  lr: 0.000026  loss: 1.2270  time: 1.5393  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2790/3000]  eta: 0:05:18  lr: 0.000026  loss: 0.3988  time: 1.5441  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2790/3000]  eta: 0:05:18  lr: 0.000026  loss: 0.1099  time: 1.5440  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2795/3000]  eta: 0:05:10  lr: 0.000026  loss: 0.4424  time: 1.5166  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2795/3000]  eta: 0:05:10  lr: 0.000026  loss: 0.0925  time: 1.5160  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2800/3000]  eta: 0:05:03  lr: 0.000026  loss: 0.8783  time: 1.5180  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2800/3000]  eta: 0:05:03  lr: 0.000026  loss: 0.0657  time: 1.5175  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2805/3000]  eta: 0:04:55  lr: 0.000026  loss: 0.2155  time: 1.5055  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2805/3000]  eta: 0:04:55  lr: 0.000026  loss: 0.6266  time: 1.5049  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2810/3000]  eta: 0:04:47  lr: 0.000026  loss: 0.5989  time: 1.5079  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2810/3000]  eta: 0:04:47  lr: 0.000026  loss: 0.3513  time: 1.5073  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2815/3000]  eta: 0:04:40  lr: 0.000026  loss: 0.5486  time: 1.5277  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2815/3000]  eta: 0:04:40  lr: 0.000026  loss: 0.7315  time: 1.5275  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2820/3000]  eta: 0:04:32  lr: 0.000026  loss: 0.3071  time: 1.5142  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2820/3000]  eta: 0:04:32  lr: 0.000026  loss: 0.5834  time: 1.5140  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2825/3000]  eta: 0:04:25  lr: 0.000026  loss: 0.1913  time: 1.5180  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2825/3000]  eta: 0:04:25  lr: 0.000026  loss: 0.5933  time: 1.5177  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2830/3000]  eta: 0:04:17  lr: 0.000026  loss: 0.3948  time: 1.5170  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2830/3000]  eta: 0:04:17  lr: 0.000026  loss: 0.0596  time: 1.5167  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2835/3000]  eta: 0:04:10  lr: 0.000026  loss: 0.2459  time: 1.4959  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2835/3000]  eta: 0:04:10  lr: 0.000026  loss: 0.7010  time: 1.4957  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2840/3000]  eta: 0:04:02  lr: 0.000026  loss: 0.1269  time: 1.5192  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2840/3000]  eta: 0:04:02  lr: 0.000026  loss: 0.3753  time: 1.5190  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2845/3000]  eta: 0:03:54  lr: 0.000026  loss: 0.3982  time: 1.5056  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2845/3000]  eta: 0:03:54  lr: 0.000026  loss: 1.0687  time: 1.5053  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2850/3000]  eta: 0:03:47  lr: 0.000026  loss: 0.3775  time: 1.4937  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2850/3000]  eta: 0:03:47  lr: 0.000026  loss: 0.5044  time: 1.4935  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2855/3000]  eta: 0:03:39  lr: 0.000026  loss: 0.0474  time: 1.5027  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2855/3000]  eta: 0:03:39  lr: 0.000026  loss: 0.0950  time: 1.5025  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2860/3000]  eta: 0:03:32  lr: 0.000026  loss: 0.3664  time: 1.4919  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2860/3000]  eta: 0:03:32  lr: 0.000026  loss: 0.6702  time: 1.4916  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2865/3000]  eta: 0:03:24  lr: 0.000026  loss: 0.3974  time: 1.5074  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2865/3000]  eta: 0:03:24  lr: 0.000026  loss: 0.5105  time: 1.5071  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2870/3000]  eta: 0:03:16  lr: 0.000026  loss: 0.0985  time: 1.5083  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2870/3000]  eta: 0:03:16  lr: 0.000026  loss: 0.2131  time: 1.5081  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2875/3000]  eta: 0:03:09  lr: 0.000026  loss: 0.6445  time: 1.5147  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2875/3000]  eta: 0:03:09  lr: 0.000026  loss: 0.2762  time: 1.5144  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2880/3000]  eta: 0:03:01  lr: 0.000026  loss: 0.2943  time: 1.5235  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2880/3000]  eta: 0:03:01  lr: 0.000026  loss: 0.4182  time: 1.5233  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2885/3000]  eta: 0:02:54  lr: 0.000026  loss: 0.8457  time: 1.5295  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2885/3000]  eta: 0:02:54  lr: 0.000026  loss: 0.3031  time: 1.5293  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2890/3000]  eta: 0:02:46  lr: 0.000026  loss: 0.2146  time: 1.5430  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2890/3000]  eta: 0:02:46  lr: 0.000026  loss: 0.1045  time: 1.5427  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2895/3000]  eta: 0:02:39  lr: 0.000026  loss: 0.5104  time: 1.5392  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2895/3000]  eta: 0:02:39  lr: 0.000026  loss: 0.3566  time: 1.5390  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2900/3000]  eta: 0:02:31  lr: 0.000026  loss: 0.5712  time: 1.5312  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2900/3000]  eta: 0:02:31  lr: 0.000026  loss: 0.6549  time: 1.5310  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2905/3000]  eta: 0:02:23  lr: 0.000026  loss: 0.4992  time: 1.5335  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2905/3000]  eta: 0:02:23  lr: 0.000026  loss: 0.5783  time: 1.5333  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2910/3000]  eta: 0:02:16  lr: 0.000026  loss: 0.2414  time: 1.5260  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2910/3000]  eta: 0:02:16  lr: 0.000026  loss: 0.5206  time: 1.5259  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2915/3000]  eta: 0:02:08  lr: 0.000026  loss: 0.7149  time: 1.5303  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2915/3000]  eta: 0:02:08  lr: 0.000026  loss: 0.3038  time: 1.5300  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2920/3000]  eta: 0:02:01  lr: 0.000026  loss: 0.4458  time: 1.5358  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2920/3000]  eta: 0:02:01  lr: 0.000026  loss: 0.5782  time: 1.5355  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2925/3000]  eta: 0:01:53  lr: 0.000026  loss: 0.5874  time: 1.5315  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2925/3000]  eta: 0:01:53  lr: 0.000026  loss: 0.9577  time: 1.5312  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2930/3000]  eta: 0:01:46  lr: 0.000026  loss: 0.1097  time: 1.5366  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2930/3000]  eta: 0:01:46  lr: 0.000026  loss: 0.5284  time: 1.5364  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2935/3000]  eta: 0:01:38  lr: 0.000026  loss: 0.4740  time: 1.5433  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2935/3000]  eta: 0:01:38  lr: 0.000026  loss: 0.3741  time: 1.5431  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2940/3000]  eta: 0:01:30  lr: 0.000026  loss: 0.9091  time: 1.5538  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2940/3000]  eta: 0:01:30  lr: 0.000026  loss: 0.2571  time: 1.5536  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2945/3000]  eta: 0:01:23  lr: 0.000026  loss: 1.1292  time: 1.5377  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2945/3000]  eta: 0:01:23  lr: 0.000026  loss: 0.4799  time: 1.5375  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2950/3000]  eta: 0:01:15  lr: 0.000026  loss: 0.2013  time: 1.5166  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2950/3000]  eta: 0:01:15  lr: 0.000026  loss: 0.2633  time: 1.5163  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2955/3000]  eta: 0:01:08  lr: 0.000026  loss: 0.6071  time: 1.5196  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2955/3000]  eta: 0:01:08  lr: 0.000026  loss: 0.8934  time: 1.5193  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2960/3000]  eta: 0:01:00  lr: 0.000026  loss: 0.2142  time: 1.4922  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2960/3000]  eta: 0:01:00  lr: 0.000026  loss: 0.4553  time: 1.4919  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2965/3000]  eta: 0:00:53  lr: 0.000026  loss: 0.1702  time: 1.5135  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2965/3000]  eta: 0:00:53  lr: 0.000026  loss: 0.0888  time: 1.5133  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2970/3000]  eta: 0:00:45  lr: 0.000026  loss: 0.1198  time: 1.5335  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2970/3000]  eta: 0:00:45  lr: 0.000026  loss: 0.2999  time: 1.5333  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2975/3000]  eta: 0:00:37  lr: 0.000026  loss: 0.6115  time: 1.5184  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2975/3000]  eta: 0:00:37  lr: 0.000026  loss: 0.3420  time: 1.5182  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2980/3000]  eta: 0:00:30  lr: 0.000026  loss: 0.1842  time: 1.5531  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2980/3000]  eta: 0:00:30  lr: 0.000026  loss: 0.3178  time: 1.5528  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2985/3000]  eta: 0:00:22  lr: 0.000026  loss: 0.2135  time: 1.5355  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2985/3000]  eta: 0:00:22  lr: 0.000026  loss: 0.1265  time: 1.5353  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2990/3000]  eta: 0:00:15  lr: 0.000026  loss: 1.0108  time: 1.5191  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2990/3000]  eta: 0:00:15  lr: 0.000026  loss: 0.1278  time: 1.5189  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2995/3000]  eta: 0:00:07  lr: 0.000026  loss: 0.7614  time: 1.5282  data: 0.0000  max mem: 18596
Train: data epoch: [8]  [2995/3000]  eta: 0:00:07  lr: 0.000026  loss: 0.2945  time: 1.5280  data: 0.0000  max mem: 18406
Train: data epoch: [8]  [2999/3000]  eta: 0:00:01  lr: 0.000026  loss: 0.3828  time: 1.5072  data: 0.0000  max mem: 18596
Train: data epoch: [8] Total time: 1:15:48 (1.5161 s / it)
Train: data epoch: [8]  [2999/3000]  eta: 0:00:01  lr: 0.000026  loss: 0.7150  time: 1.5070  data: 0.0000  max mem: 18406
Train: data epoch: [8] Total time: 1:15:48 (1.5161 s / it)
2025-01-19 11:02:20,169 [INFO] Averaged stats: lr: 0.0000  loss: 0.4242
2025-01-19 11:02:20,175 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [8]  [0/1]  eta: 0:00:00    time: 0.7995  data: 0.4994  max mem: 18406
Eval: data epoch: [8] Total time: 0:00:00 (0.9003 s / it)
Eval: data epoch: [8]  [0/1]  eta: 0:00:00    time: 0.9544  data: 0.6635  max mem: 18596
Eval: data epoch: [8] Total time: 0:00:01 (1.0855 s / it)
2025-01-19 11:02:21,286 [INFO] Saving checkpoint at epoch 8 to outputs_stage1_only/202501182338/checkpoint_8.pth.
2025-01-19 11:02:23,658 [INFO] Training Phase
2025-01-19 11:02:23,665 [INFO] Start training epoch 9, 3000 iters per inner epoch.
Train: data epoch: [9]  [   0/3000]  eta: 1:11:07  lr: 0.000026  loss: 0.5591  time: 1.4224  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [   0/3000]  eta: 1:11:06  lr: 0.000026  loss: 0.3537  time: 1.4223  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [   5/3000]  eta: 1:14:54  lr: 0.000026  loss: 0.8760  time: 1.5008  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [   5/3000]  eta: 1:14:54  lr: 0.000026  loss: 0.1494  time: 1.5006  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [  10/3000]  eta: 1:13:32  lr: 0.000026  loss: 0.3932  time: 1.4758  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [  10/3000]  eta: 1:13:31  lr: 0.000026  loss: 0.3923  time: 1.4755  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [  15/3000]  eta: 1:12:43  lr: 0.000026  loss: 0.4674  time: 1.4618  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [  15/3000]  eta: 1:12:42  lr: 0.000026  loss: 0.1226  time: 1.4615  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [  20/3000]  eta: 1:13:09  lr: 0.000026  loss: 0.2469  time: 1.4754  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [  20/3000]  eta: 1:13:08  lr: 0.000026  loss: 0.3268  time: 1.4752  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [  25/3000]  eta: 1:13:27  lr: 0.000026  loss: 0.8112  time: 1.4757  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [  25/3000]  eta: 1:13:26  lr: 0.000026  loss: 0.4699  time: 1.4754  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [  30/3000]  eta: 1:13:23  lr: 0.000026  loss: 0.2806  time: 1.4867  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [  30/3000]  eta: 1:13:23  lr: 0.000026  loss: 0.0667  time: 1.4864  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [  35/3000]  eta: 1:13:33  lr: 0.000026  loss: 0.5796  time: 1.5103  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [  35/3000]  eta: 1:13:33  lr: 0.000026  loss: 0.6342  time: 1.5100  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [  40/3000]  eta: 1:14:01  lr: 0.000026  loss: 0.1754  time: 1.5292  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [  40/3000]  eta: 1:14:00  lr: 0.000026  loss: 0.4681  time: 1.5289  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [  45/3000]  eta: 1:14:09  lr: 0.000026  loss: 0.8971  time: 1.5373  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [  45/3000]  eta: 1:14:08  lr: 0.000026  loss: 0.1948  time: 1.5371  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [  50/3000]  eta: 1:14:11  lr: 0.000026  loss: 0.5862  time: 1.5496  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [  50/3000]  eta: 1:14:10  lr: 0.000026  loss: 0.2095  time: 1.5494  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [  55/3000]  eta: 1:14:09  lr: 0.000026  loss: 0.5228  time: 1.5510  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [  55/3000]  eta: 1:14:09  lr: 0.000026  loss: 0.5137  time: 1.5508  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [  60/3000]  eta: 1:13:57  lr: 0.000026  loss: 0.3216  time: 1.5279  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [  60/3000]  eta: 1:13:56  lr: 0.000026  loss: 0.2693  time: 1.5276  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [  65/3000]  eta: 1:13:55  lr: 0.000026  loss: 0.8325  time: 1.5235  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [  65/3000]  eta: 1:13:54  lr: 0.000026  loss: 0.4458  time: 1.5233  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [  70/3000]  eta: 1:13:51  lr: 0.000026  loss: 0.6859  time: 1.5216  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [  70/3000]  eta: 1:13:51  lr: 0.000026  loss: 0.1023  time: 1.5213  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [  75/3000]  eta: 1:13:32  lr: 0.000026  loss: 0.2407  time: 1.5018  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [  75/3000]  eta: 1:13:31  lr: 0.000026  loss: 0.5938  time: 1.5015  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [  80/3000]  eta: 1:13:26  lr: 0.000026  loss: 0.2963  time: 1.5080  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [  80/3000]  eta: 1:13:25  lr: 0.000026  loss: 0.7403  time: 1.5078  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [  85/3000]  eta: 1:13:30  lr: 0.000026  loss: 0.2504  time: 1.5186  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [  85/3000]  eta: 1:13:29  lr: 0.000026  loss: 0.4620  time: 1.5183  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [  90/3000]  eta: 1:13:24  lr: 0.000026  loss: 0.2695  time: 1.5176  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [  90/3000]  eta: 1:13:23  lr: 0.000026  loss: 0.0830  time: 1.5173  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [  95/3000]  eta: 1:13:11  lr: 0.000026  loss: 0.4939  time: 1.5231  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [  95/3000]  eta: 1:13:10  lr: 0.000026  loss: 0.3602  time: 1.5229  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 100/3000]  eta: 1:13:02  lr: 0.000026  loss: 0.0545  time: 1.5207  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 100/3000]  eta: 1:13:02  lr: 0.000026  loss: 0.3931  time: 1.5204  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 105/3000]  eta: 1:12:51  lr: 0.000026  loss: 0.3028  time: 1.4969  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 105/3000]  eta: 1:12:50  lr: 0.000026  loss: 0.1989  time: 1.4965  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 110/3000]  eta: 1:12:40  lr: 0.000026  loss: 0.1257  time: 1.4863  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 110/3000]  eta: 1:12:39  lr: 0.000026  loss: 0.1141  time: 1.4859  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 115/3000]  eta: 1:12:40  lr: 0.000026  loss: 0.3228  time: 1.5097  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 115/3000]  eta: 1:12:39  lr: 0.000026  loss: 0.7670  time: 1.5093  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 120/3000]  eta: 1:12:30  lr: 0.000026  loss: 0.5606  time: 1.5056  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 120/3000]  eta: 1:12:29  lr: 0.000026  loss: 0.3083  time: 1.5053  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 125/3000]  eta: 1:12:27  lr: 0.000026  loss: 0.0951  time: 1.5250  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 125/3000]  eta: 1:12:26  lr: 0.000026  loss: 0.2568  time: 1.5247  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 130/3000]  eta: 1:12:17  lr: 0.000026  loss: 0.4124  time: 1.5261  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 130/3000]  eta: 1:12:16  lr: 0.000026  loss: 0.2480  time: 1.5259  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 135/3000]  eta: 1:12:14  lr: 0.000026  loss: 0.5895  time: 1.5214  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 135/3000]  eta: 1:12:13  lr: 0.000026  loss: 0.7873  time: 1.5211  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 140/3000]  eta: 1:12:01  lr: 0.000026  loss: 0.5481  time: 1.5139  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 140/3000]  eta: 1:12:00  lr: 0.000026  loss: 0.9398  time: 1.5138  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 145/3000]  eta: 1:11:52  lr: 0.000026  loss: 0.0703  time: 1.4992  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 145/3000]  eta: 1:11:51  lr: 0.000026  loss: 0.2410  time: 1.4989  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 150/3000]  eta: 1:11:47  lr: 0.000026  loss: 0.3597  time: 1.5108  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 150/3000]  eta: 1:11:46  lr: 0.000026  loss: 0.2509  time: 1.5105  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 155/3000]  eta: 1:11:36  lr: 0.000026  loss: 0.3607  time: 1.4919  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 155/3000]  eta: 1:11:35  lr: 0.000026  loss: 0.6067  time: 1.4918  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 160/3000]  eta: 1:11:30  lr: 0.000026  loss: 0.3272  time: 1.5100  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 160/3000]  eta: 1:11:29  lr: 0.000026  loss: 0.8232  time: 1.5098  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 165/3000]  eta: 1:11:13  lr: 0.000026  loss: 0.2554  time: 1.4865  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 165/3000]  eta: 1:11:13  lr: 0.000026  loss: 0.2928  time: 1.4862  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 170/3000]  eta: 1:11:06  lr: 0.000026  loss: 0.5108  time: 1.4808  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 170/3000]  eta: 1:11:06  lr: 0.000026  loss: 0.2107  time: 1.4806  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 175/3000]  eta: 1:10:59  lr: 0.000026  loss: 0.2262  time: 1.4912  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 175/3000]  eta: 1:10:59  lr: 0.000026  loss: 0.2771  time: 1.4910  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 180/3000]  eta: 1:10:57  lr: 0.000026  loss: 0.4512  time: 1.5017  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 180/3000]  eta: 1:10:56  lr: 0.000026  loss: 0.7483  time: 1.5015  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 185/3000]  eta: 1:10:49  lr: 0.000026  loss: 0.2807  time: 1.5267  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 185/3000]  eta: 1:10:48  lr: 0.000026  loss: 0.7892  time: 1.5266  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 190/3000]  eta: 1:10:40  lr: 0.000026  loss: 0.5819  time: 1.5220  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 190/3000]  eta: 1:10:40  lr: 0.000026  loss: 0.2565  time: 1.5218  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 195/3000]  eta: 1:10:35  lr: 0.000026  loss: 0.1536  time: 1.5295  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 195/3000]  eta: 1:10:35  lr: 0.000026  loss: 0.1210  time: 1.5292  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 200/3000]  eta: 1:10:25  lr: 0.000026  loss: 0.4759  time: 1.5044  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 200/3000]  eta: 1:10:25  lr: 0.000026  loss: 0.8334  time: 1.5042  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 205/3000]  eta: 1:10:17  lr: 0.000026  loss: 0.2615  time: 1.5021  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 205/3000]  eta: 1:10:16  lr: 0.000026  loss: 0.1404  time: 1.5018  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 210/3000]  eta: 1:10:07  lr: 0.000026  loss: 0.4273  time: 1.4960  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 210/3000]  eta: 1:10:06  lr: 0.000026  loss: 0.1164  time: 1.4956  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 215/3000]  eta: 1:09:58  lr: 0.000026  loss: 0.6843  time: 1.4825  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 215/3000]  eta: 1:09:57  lr: 0.000026  loss: 0.0639  time: 1.4823  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 220/3000]  eta: 1:09:52  lr: 0.000026  loss: 0.2448  time: 1.4967  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 220/3000]  eta: 1:09:51  lr: 0.000026  loss: 0.1825  time: 1.4964  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 225/3000]  eta: 1:09:46  lr: 0.000026  loss: 0.3273  time: 1.5063  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 225/3000]  eta: 1:09:45  lr: 0.000026  loss: 0.2852  time: 1.5060  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 230/3000]  eta: 1:09:41  lr: 0.000026  loss: 0.2219  time: 1.5258  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 230/3000]  eta: 1:09:40  lr: 0.000026  loss: 0.6543  time: 1.5256  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 235/3000]  eta: 1:09:28  lr: 0.000026  loss: 0.1371  time: 1.5096  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 235/3000]  eta: 1:09:28  lr: 0.000026  loss: 0.3388  time: 1.5094  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 240/3000]  eta: 1:09:23  lr: 0.000026  loss: 0.5767  time: 1.5130  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 240/3000]  eta: 1:09:22  lr: 0.000026  loss: 0.3764  time: 1.5128  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 245/3000]  eta: 1:09:19  lr: 0.000026  loss: 0.1934  time: 1.5205  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 245/3000]  eta: 1:09:18  lr: 0.000026  loss: 0.2807  time: 1.5202  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 250/3000]  eta: 1:09:15  lr: 0.000026  loss: 1.2789  time: 1.5282  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 250/3000]  eta: 1:09:14  lr: 0.000026  loss: 0.2886  time: 1.5280  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 255/3000]  eta: 1:09:11  lr: 0.000026  loss: 0.1219  time: 1.5651  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 255/3000]  eta: 1:09:10  lr: 0.000026  loss: 0.5879  time: 1.5648  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 260/3000]  eta: 1:09:05  lr: 0.000026  loss: 0.4070  time: 1.5651  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 260/3000]  eta: 1:09:04  lr: 0.000026  loss: 0.3330  time: 1.5648  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 265/3000]  eta: 1:08:57  lr: 0.000026  loss: 0.4005  time: 1.5504  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 265/3000]  eta: 1:08:56  lr: 0.000026  loss: 0.4041  time: 1.5501  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 270/3000]  eta: 1:08:53  lr: 0.000026  loss: 0.4555  time: 1.5537  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 270/3000]  eta: 1:08:52  lr: 0.000026  loss: 0.2067  time: 1.5534  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 275/3000]  eta: 1:08:51  lr: 0.000026  loss: 0.5509  time: 1.5639  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 275/3000]  eta: 1:08:50  lr: 0.000026  loss: 0.6419  time: 1.5636  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 280/3000]  eta: 1:08:43  lr: 0.000026  loss: 0.0972  time: 1.5586  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 280/3000]  eta: 1:08:43  lr: 0.000026  loss: 0.3071  time: 1.5583  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 285/3000]  eta: 1:08:31  lr: 0.000026  loss: 0.5457  time: 1.5357  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 285/3000]  eta: 1:08:30  lr: 0.000026  loss: 0.4559  time: 1.5354  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 290/3000]  eta: 1:08:26  lr: 0.000026  loss: 0.3865  time: 1.5291  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 290/3000]  eta: 1:08:25  lr: 0.000026  loss: 0.0967  time: 1.5289  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 295/3000]  eta: 1:08:19  lr: 0.000026  loss: 0.5023  time: 1.5110  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 295/3000]  eta: 1:08:19  lr: 0.000026  loss: 0.2942  time: 1.5108  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 300/3000]  eta: 1:08:13  lr: 0.000026  loss: 0.6223  time: 1.5146  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 300/3000]  eta: 1:08:12  lr: 0.000026  loss: 0.3724  time: 1.5143  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 305/3000]  eta: 1:08:05  lr: 0.000026  loss: 0.3289  time: 1.5391  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 305/3000]  eta: 1:08:04  lr: 0.000026  loss: 0.5125  time: 1.5388  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 310/3000]  eta: 1:08:00  lr: 0.000026  loss: 0.4577  time: 1.5425  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 310/3000]  eta: 1:07:59  lr: 0.000026  loss: 0.4917  time: 1.5423  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 315/3000]  eta: 1:07:54  lr: 0.000026  loss: 0.3564  time: 1.5427  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 315/3000]  eta: 1:07:53  lr: 0.000026  loss: 0.1101  time: 1.5424  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 320/3000]  eta: 1:07:47  lr: 0.000026  loss: 0.2250  time: 1.5449  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 320/3000]  eta: 1:07:47  lr: 0.000026  loss: 0.2860  time: 1.5446  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 325/3000]  eta: 1:07:38  lr: 0.000026  loss: 0.7651  time: 1.5378  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 325/3000]  eta: 1:07:37  lr: 0.000026  loss: 0.1542  time: 1.5375  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 330/3000]  eta: 1:07:31  lr: 0.000026  loss: 0.4509  time: 1.5242  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 330/3000]  eta: 1:07:30  lr: 0.000026  loss: 0.7033  time: 1.5239  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 335/3000]  eta: 1:07:25  lr: 0.000026  loss: 0.2015  time: 1.5292  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 335/3000]  eta: 1:07:24  lr: 0.000026  loss: 0.1814  time: 1.5290  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 340/3000]  eta: 1:07:18  lr: 0.000026  loss: 0.5678  time: 1.5234  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 340/3000]  eta: 1:07:17  lr: 0.000026  loss: 0.4922  time: 1.5231  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 345/3000]  eta: 1:07:12  lr: 0.000026  loss: 0.2324  time: 1.5419  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 345/3000]  eta: 1:07:11  lr: 0.000026  loss: 0.6000  time: 1.5417  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 350/3000]  eta: 1:07:06  lr: 0.000026  loss: 0.4580  time: 1.5552  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 350/3000]  eta: 1:07:06  lr: 0.000026  loss: 0.9966  time: 1.5550  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 355/3000]  eta: 1:06:58  lr: 0.000026  loss: 0.2799  time: 1.5398  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 355/3000]  eta: 1:06:57  lr: 0.000026  loss: 0.1904  time: 1.5395  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 360/3000]  eta: 1:06:50  lr: 0.000026  loss: 0.1766  time: 1.5362  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 360/3000]  eta: 1:06:49  lr: 0.000026  loss: 0.1932  time: 1.5359  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 365/3000]  eta: 1:06:42  lr: 0.000026  loss: 0.2817  time: 1.5208  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 365/3000]  eta: 1:06:41  lr: 0.000026  loss: 0.3570  time: 1.5215  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 370/3000]  eta: 1:06:34  lr: 0.000026  loss: 0.3417  time: 1.5085  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 370/3000]  eta: 1:06:34  lr: 0.000026  loss: 0.6763  time: 1.5081  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 375/3000]  eta: 1:06:27  lr: 0.000026  loss: 0.9602  time: 1.5115  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 375/3000]  eta: 1:06:26  lr: 0.000026  loss: 0.3658  time: 1.5113  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 380/3000]  eta: 1:06:20  lr: 0.000026  loss: 0.6116  time: 1.5186  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 380/3000]  eta: 1:06:19  lr: 0.000026  loss: 0.8863  time: 1.5183  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 385/3000]  eta: 1:06:11  lr: 0.000026  loss: 1.0654  time: 1.5181  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 385/3000]  eta: 1:06:10  lr: 0.000026  loss: 0.3589  time: 1.5169  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 390/3000]  eta: 1:06:04  lr: 0.000026  loss: 0.1782  time: 1.5204  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 390/3000]  eta: 1:06:03  lr: 0.000026  loss: 0.3104  time: 1.5202  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 395/3000]  eta: 1:05:55  lr: 0.000026  loss: 0.0788  time: 1.5069  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 395/3000]  eta: 1:05:54  lr: 0.000026  loss: 0.5648  time: 1.5066  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 400/3000]  eta: 1:05:48  lr: 0.000026  loss: 0.2273  time: 1.5097  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 400/3000]  eta: 1:05:47  lr: 0.000026  loss: 0.3433  time: 1.5095  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 405/3000]  eta: 1:05:43  lr: 0.000026  loss: 0.4603  time: 1.5370  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 405/3000]  eta: 1:05:42  lr: 0.000026  loss: 0.2573  time: 1.5367  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 410/3000]  eta: 1:05:36  lr: 0.000026  loss: 0.0782  time: 1.5329  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 410/3000]  eta: 1:05:35  lr: 0.000026  loss: 0.2534  time: 1.5326  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 415/3000]  eta: 1:05:30  lr: 0.000026  loss: 0.3107  time: 1.5616  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 415/3000]  eta: 1:05:29  lr: 0.000026  loss: 0.4491  time: 1.5613  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 420/3000]  eta: 1:05:23  lr: 0.000026  loss: 0.7399  time: 1.5650  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 420/3000]  eta: 1:05:23  lr: 0.000026  loss: 0.6891  time: 1.5647  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 425/3000]  eta: 1:05:16  lr: 0.000026  loss: 0.4603  time: 1.5461  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 425/3000]  eta: 1:05:15  lr: 0.000026  loss: 0.4848  time: 1.5459  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 430/3000]  eta: 1:05:07  lr: 0.000026  loss: 0.1631  time: 1.5348  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 430/3000]  eta: 1:05:06  lr: 0.000026  loss: 0.7116  time: 1.5345  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 435/3000]  eta: 1:05:00  lr: 0.000026  loss: 0.1377  time: 1.5246  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 435/3000]  eta: 1:04:59  lr: 0.000026  loss: 0.6522  time: 1.5243  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 440/3000]  eta: 1:04:54  lr: 0.000026  loss: 0.2137  time: 1.5274  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 440/3000]  eta: 1:04:53  lr: 0.000026  loss: 0.2647  time: 1.5272  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 445/3000]  eta: 1:04:47  lr: 0.000026  loss: 0.7193  time: 1.5334  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 445/3000]  eta: 1:04:46  lr: 0.000026  loss: 0.7174  time: 1.5331  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 450/3000]  eta: 1:04:39  lr: 0.000026  loss: 0.4283  time: 1.5452  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 450/3000]  eta: 1:04:39  lr: 0.000026  loss: 0.2538  time: 1.5449  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 455/3000]  eta: 1:04:30  lr: 0.000026  loss: 0.4129  time: 1.5240  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 455/3000]  eta: 1:04:29  lr: 0.000026  loss: 0.4457  time: 1.5238  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 460/3000]  eta: 1:04:22  lr: 0.000026  loss: 0.1498  time: 1.5067  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 460/3000]  eta: 1:04:21  lr: 0.000026  loss: 0.3072  time: 1.5065  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 465/3000]  eta: 1:04:14  lr: 0.000026  loss: 0.3815  time: 1.4957  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 465/3000]  eta: 1:04:13  lr: 0.000026  loss: 0.1215  time: 1.4954  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 470/3000]  eta: 1:04:06  lr: 0.000026  loss: 0.6085  time: 1.4926  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 470/3000]  eta: 1:04:05  lr: 0.000026  loss: 0.3686  time: 1.4923  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 475/3000]  eta: 1:03:59  lr: 0.000026  loss: 0.0575  time: 1.5143  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 475/3000]  eta: 1:03:58  lr: 0.000026  loss: 0.4705  time: 1.5141  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 480/3000]  eta: 1:03:51  lr: 0.000026  loss: 0.2573  time: 1.5189  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 480/3000]  eta: 1:03:50  lr: 0.000026  loss: 0.2843  time: 1.5187  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 485/3000]  eta: 1:03:44  lr: 0.000026  loss: 0.5111  time: 1.5278  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 485/3000]  eta: 1:03:43  lr: 0.000026  loss: 0.7414  time: 1.5276  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 490/3000]  eta: 1:03:37  lr: 0.000026  loss: 0.4822  time: 1.5363  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 490/3000]  eta: 1:03:36  lr: 0.000026  loss: 0.4144  time: 1.5360  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 495/3000]  eta: 1:03:30  lr: 0.000026  loss: 0.6499  time: 1.5376  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 495/3000]  eta: 1:03:29  lr: 0.000026  loss: 0.2188  time: 1.5374  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 500/3000]  eta: 1:03:23  lr: 0.000026  loss: 0.0538  time: 1.5436  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 500/3000]  eta: 1:03:22  lr: 0.000026  loss: 0.7787  time: 1.5434  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 505/3000]  eta: 1:03:15  lr: 0.000026  loss: 0.1546  time: 1.5332  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 505/3000]  eta: 1:03:14  lr: 0.000026  loss: 0.2707  time: 1.5329  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 510/3000]  eta: 1:03:08  lr: 0.000026  loss: 0.4348  time: 1.5391  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 510/3000]  eta: 1:03:08  lr: 0.000026  loss: 0.3186  time: 1.5389  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 515/3000]  eta: 1:02:59  lr: 0.000026  loss: 0.2177  time: 1.5184  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 515/3000]  eta: 1:02:59  lr: 0.000026  loss: 0.2074  time: 1.5182  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 520/3000]  eta: 1:02:51  lr: 0.000026  loss: 0.5288  time: 1.5017  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 520/3000]  eta: 1:02:50  lr: 0.000026  loss: 0.2276  time: 1.5015  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 525/3000]  eta: 1:02:41  lr: 0.000026  loss: 0.5276  time: 1.4879  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 525/3000]  eta: 1:02:41  lr: 0.000026  loss: 0.2288  time: 1.4877  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 530/3000]  eta: 1:02:33  lr: 0.000026  loss: 0.2954  time: 1.4716  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 530/3000]  eta: 1:02:33  lr: 0.000026  loss: 0.8105  time: 1.4714  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 535/3000]  eta: 1:02:26  lr: 0.000026  loss: 0.6198  time: 1.4904  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 535/3000]  eta: 1:02:25  lr: 0.000026  loss: 0.1004  time: 1.4901  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 540/3000]  eta: 1:02:19  lr: 0.000026  loss: 0.5221  time: 1.5105  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 540/3000]  eta: 1:02:19  lr: 0.000026  loss: 0.5077  time: 1.5102  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 545/3000]  eta: 1:02:12  lr: 0.000026  loss: 0.2722  time: 1.5277  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 545/3000]  eta: 1:02:11  lr: 0.000026  loss: 0.5801  time: 1.5274  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 550/3000]  eta: 1:02:05  lr: 0.000026  loss: 0.6827  time: 1.5429  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 550/3000]  eta: 1:02:04  lr: 0.000026  loss: 0.6578  time: 1.5426  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 555/3000]  eta: 1:01:56  lr: 0.000026  loss: 0.4728  time: 1.5297  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 555/3000]  eta: 1:01:56  lr: 0.000026  loss: 0.6926  time: 1.5296  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 560/3000]  eta: 1:01:49  lr: 0.000026  loss: 0.2436  time: 1.5253  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 560/3000]  eta: 1:01:49  lr: 0.000026  loss: 0.4014  time: 1.5251  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 565/3000]  eta: 1:01:41  lr: 0.000026  loss: 0.2567  time: 1.5212  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 565/3000]  eta: 1:01:41  lr: 0.000026  loss: 0.2674  time: 1.5211  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 570/3000]  eta: 1:01:34  lr: 0.000026  loss: 0.2520  time: 1.5148  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 570/3000]  eta: 1:01:33  lr: 0.000026  loss: 0.6108  time: 1.5146  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 575/3000]  eta: 1:01:25  lr: 0.000026  loss: 0.0714  time: 1.5050  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 575/3000]  eta: 1:01:24  lr: 0.000026  loss: 0.2974  time: 1.5048  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 580/3000]  eta: 1:01:17  lr: 0.000026  loss: 0.3197  time: 1.4961  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 580/3000]  eta: 1:01:16  lr: 0.000026  loss: 0.2973  time: 1.4959  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 585/3000]  eta: 1:01:09  lr: 0.000026  loss: 0.4410  time: 1.5037  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 585/3000]  eta: 1:01:09  lr: 0.000026  loss: 0.4308  time: 1.5034  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 590/3000]  eta: 1:01:02  lr: 0.000026  loss: 0.4867  time: 1.5013  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 590/3000]  eta: 1:01:01  lr: 0.000026  loss: 0.3515  time: 1.5011  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 595/3000]  eta: 1:00:55  lr: 0.000026  loss: 0.2369  time: 1.5250  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 595/3000]  eta: 1:00:54  lr: 0.000026  loss: 0.5347  time: 1.5248  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 600/3000]  eta: 1:00:46  lr: 0.000026  loss: 0.1164  time: 1.5073  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 600/3000]  eta: 1:00:45  lr: 0.000026  loss: 0.1689  time: 1.5070  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 605/3000]  eta: 1:00:38  lr: 0.000026  loss: 0.3257  time: 1.5102  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 605/3000]  eta: 1:00:38  lr: 0.000026  loss: 0.2363  time: 1.5099  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 610/3000]  eta: 1:00:30  lr: 0.000026  loss: 0.1196  time: 1.5020  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 610/3000]  eta: 1:00:30  lr: 0.000026  loss: 0.4355  time: 1.5017  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 615/3000]  eta: 1:00:24  lr: 0.000026  loss: 0.2087  time: 1.5167  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 615/3000]  eta: 1:00:24  lr: 0.000026  loss: 0.3798  time: 1.5165  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 620/3000]  eta: 1:00:17  lr: 0.000026  loss: 0.4328  time: 1.5372  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 620/3000]  eta: 1:00:16  lr: 0.000026  loss: 0.1002  time: 1.5369  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 625/3000]  eta: 1:00:08  lr: 0.000026  loss: 0.1368  time: 1.5158  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 625/3000]  eta: 1:00:07  lr: 0.000026  loss: 0.2425  time: 1.5156  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 630/3000]  eta: 1:00:02  lr: 0.000026  loss: 0.2571  time: 1.5412  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 630/3000]  eta: 1:00:01  lr: 0.000026  loss: 0.2040  time: 1.5410  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 635/3000]  eta: 0:59:55  lr: 0.000026  loss: 0.7241  time: 1.5416  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 635/3000]  eta: 0:59:55  lr: 0.000026  loss: 0.8119  time: 1.5413  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 640/3000]  eta: 0:59:48  lr: 0.000026  loss: 0.2896  time: 1.5512  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 640/3000]  eta: 0:59:48  lr: 0.000026  loss: 0.7332  time: 1.5511  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 645/3000]  eta: 0:59:41  lr: 0.000026  loss: 0.5798  time: 1.5707  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 645/3000]  eta: 0:59:40  lr: 0.000026  loss: 0.2799  time: 1.5704  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 650/3000]  eta: 0:59:32  lr: 0.000026  loss: 0.4001  time: 1.5343  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 650/3000]  eta: 0:59:32  lr: 0.000026  loss: 0.0334  time: 1.5341  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 655/3000]  eta: 0:59:25  lr: 0.000026  loss: 0.7848  time: 1.5239  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 655/3000]  eta: 0:59:25  lr: 0.000026  loss: 0.8575  time: 1.5236  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 660/3000]  eta: 0:59:18  lr: 0.000026  loss: 0.6560  time: 1.5265  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 660/3000]  eta: 0:59:18  lr: 0.000026  loss: 0.5188  time: 1.5262  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 665/3000]  eta: 0:59:10  lr: 0.000026  loss: 0.7768  time: 1.5136  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 665/3000]  eta: 0:59:10  lr: 0.000026  loss: 0.9343  time: 1.5134  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 670/3000]  eta: 0:59:04  lr: 0.000026  loss: 0.3008  time: 1.5496  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 670/3000]  eta: 0:59:03  lr: 0.000026  loss: 0.2394  time: 1.5492  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 675/3000]  eta: 0:58:56  lr: 0.000026  loss: 0.4429  time: 1.5365  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 675/3000]  eta: 0:58:55  lr: 0.000026  loss: 0.2351  time: 1.5363  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 680/3000]  eta: 0:58:49  lr: 0.000026  loss: 0.3492  time: 1.5294  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 680/3000]  eta: 0:58:48  lr: 0.000026  loss: 0.4693  time: 1.5291  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 685/3000]  eta: 0:58:42  lr: 0.000026  loss: 0.1543  time: 1.5495  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 685/3000]  eta: 0:58:41  lr: 0.000026  loss: 0.5721  time: 1.5492  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 690/3000]  eta: 0:58:34  lr: 0.000026  loss: 1.1044  time: 1.5365  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 690/3000]  eta: 0:58:34  lr: 0.000026  loss: 0.3824  time: 1.5362  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 695/3000]  eta: 0:58:27  lr: 0.000026  loss: 0.4631  time: 1.5497  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 695/3000]  eta: 0:58:27  lr: 0.000026  loss: 0.1389  time: 1.5495  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 700/3000]  eta: 0:58:21  lr: 0.000026  loss: 0.3858  time: 1.5610  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 700/3000]  eta: 0:58:20  lr: 0.000026  loss: 0.6873  time: 1.5608  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 705/3000]  eta: 0:58:13  lr: 0.000026  loss: 0.8511  time: 1.5481  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 705/3000]  eta: 0:58:12  lr: 0.000026  loss: 0.7605  time: 1.5478  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 710/3000]  eta: 0:58:05  lr: 0.000026  loss: 0.2964  time: 1.5415  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 710/3000]  eta: 0:58:05  lr: 0.000026  loss: 0.3864  time: 1.5412  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 715/3000]  eta: 0:57:58  lr: 0.000026  loss: 0.4832  time: 1.5362  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 715/3000]  eta: 0:57:57  lr: 0.000026  loss: 0.5956  time: 1.5360  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 720/3000]  eta: 0:57:51  lr: 0.000026  loss: 0.2255  time: 1.5284  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 720/3000]  eta: 0:57:50  lr: 0.000026  loss: 0.4811  time: 1.5282  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 725/3000]  eta: 0:57:43  lr: 0.000026  loss: 0.3679  time: 1.5233  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 725/3000]  eta: 0:57:42  lr: 0.000026  loss: 0.6330  time: 1.5230  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 730/3000]  eta: 0:57:33  lr: 0.000026  loss: 1.0100  time: 1.4945  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 730/3000]  eta: 0:57:33  lr: 0.000026  loss: 0.2816  time: 1.4943  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 735/3000]  eta: 0:57:26  lr: 0.000026  loss: 0.2396  time: 1.4989  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 735/3000]  eta: 0:57:25  lr: 0.000026  loss: 0.1635  time: 1.4987  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 740/3000]  eta: 0:57:18  lr: 0.000026  loss: 0.2004  time: 1.4919  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 740/3000]  eta: 0:57:18  lr: 0.000026  loss: 1.2814  time: 1.4916  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 745/3000]  eta: 0:57:11  lr: 0.000026  loss: 0.2005  time: 1.5110  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 745/3000]  eta: 0:57:11  lr: 0.000026  loss: 0.2322  time: 1.5108  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 750/3000]  eta: 0:57:04  lr: 0.000026  loss: 0.4721  time: 1.5459  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 750/3000]  eta: 0:57:04  lr: 0.000026  loss: 0.3682  time: 1.5456  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 755/3000]  eta: 0:56:57  lr: 0.000026  loss: 0.5650  time: 1.5469  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 755/3000]  eta: 0:56:56  lr: 0.000026  loss: 0.0851  time: 1.5466  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 760/3000]  eta: 0:56:50  lr: 0.000026  loss: 0.2251  time: 1.5495  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 760/3000]  eta: 0:56:49  lr: 0.000026  loss: 0.7255  time: 1.5493  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 765/3000]  eta: 0:56:43  lr: 0.000026  loss: 0.2452  time: 1.5531  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 765/3000]  eta: 0:56:42  lr: 0.000026  loss: 1.4291  time: 1.5528  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 770/3000]  eta: 0:56:35  lr: 0.000026  loss: 0.1592  time: 1.5519  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 770/3000]  eta: 0:56:35  lr: 0.000026  loss: 0.2550  time: 1.5516  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 775/3000]  eta: 0:56:29  lr: 0.000026  loss: 0.3780  time: 1.5558  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 775/3000]  eta: 0:56:28  lr: 0.000026  loss: 1.1021  time: 1.5555  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 780/3000]  eta: 0:56:21  lr: 0.000026  loss: 0.2422  time: 1.5566  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 780/3000]  eta: 0:56:21  lr: 0.000026  loss: 0.5198  time: 1.5563  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 785/3000]  eta: 0:56:13  lr: 0.000026  loss: 0.6767  time: 1.5341  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 785/3000]  eta: 0:56:12  lr: 0.000026  loss: 0.2453  time: 1.5338  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 790/3000]  eta: 0:56:06  lr: 0.000026  loss: 0.2296  time: 1.5389  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 790/3000]  eta: 0:56:05  lr: 0.000026  loss: 0.5058  time: 1.5387  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 795/3000]  eta: 0:55:59  lr: 0.000026  loss: 0.7626  time: 1.5329  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 795/3000]  eta: 0:55:58  lr: 0.000026  loss: 0.3028  time: 1.5326  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 800/3000]  eta: 0:55:50  lr: 0.000026  loss: 0.2358  time: 1.5200  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 800/3000]  eta: 0:55:50  lr: 0.000026  loss: 0.3579  time: 1.5198  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 805/3000]  eta: 0:55:43  lr: 0.000026  loss: 0.1627  time: 1.5242  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 805/3000]  eta: 0:55:42  lr: 0.000026  loss: 0.1813  time: 1.5241  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 810/3000]  eta: 0:55:35  lr: 0.000026  loss: 0.4878  time: 1.5073  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 810/3000]  eta: 0:55:34  lr: 0.000026  loss: 0.8601  time: 1.5070  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 815/3000]  eta: 0:55:27  lr: 0.000026  loss: 0.3209  time: 1.5000  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 815/3000]  eta: 0:55:26  lr: 0.000026  loss: 0.2562  time: 1.4997  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 820/3000]  eta: 0:55:19  lr: 0.000026  loss: 0.8531  time: 1.5056  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 820/3000]  eta: 0:55:19  lr: 0.000026  loss: 0.1908  time: 1.5053  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 825/3000]  eta: 0:55:11  lr: 0.000026  loss: 0.2205  time: 1.5081  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 825/3000]  eta: 0:55:11  lr: 0.000026  loss: 0.2912  time: 1.5078  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 830/3000]  eta: 0:55:04  lr: 0.000026  loss: 0.3577  time: 1.5148  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 830/3000]  eta: 0:55:03  lr: 0.000026  loss: 0.1932  time: 1.5145  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 835/3000]  eta: 0:54:57  lr: 0.000026  loss: 0.8112  time: 1.5322  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 835/3000]  eta: 0:54:56  lr: 0.000026  loss: 0.1991  time: 1.5320  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 840/3000]  eta: 0:54:48  lr: 0.000026  loss: 0.3038  time: 1.5075  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 840/3000]  eta: 0:54:47  lr: 0.000026  loss: 0.3118  time: 1.5073  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 845/3000]  eta: 0:54:40  lr: 0.000026  loss: 0.3388  time: 1.5069  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 845/3000]  eta: 0:54:40  lr: 0.000026  loss: 0.0842  time: 1.5067  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 850/3000]  eta: 0:54:32  lr: 0.000026  loss: 0.5790  time: 1.4974  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 850/3000]  eta: 0:54:31  lr: 0.000026  loss: 0.2720  time: 1.4971  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 855/3000]  eta: 0:54:24  lr: 0.000026  loss: 0.6597  time: 1.4768  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 855/3000]  eta: 0:54:24  lr: 0.000026  loss: 0.3355  time: 1.4765  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 860/3000]  eta: 0:54:15  lr: 0.000026  loss: 0.5560  time: 1.4821  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 860/3000]  eta: 0:54:15  lr: 0.000026  loss: 0.4231  time: 1.4819  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 865/3000]  eta: 0:54:08  lr: 0.000026  loss: 0.2070  time: 1.4843  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 865/3000]  eta: 0:54:07  lr: 0.000026  loss: 0.7293  time: 1.4841  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 870/3000]  eta: 0:54:00  lr: 0.000026  loss: 0.3889  time: 1.4974  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 870/3000]  eta: 0:54:00  lr: 0.000026  loss: 0.2521  time: 1.4971  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 875/3000]  eta: 0:53:53  lr: 0.000026  loss: 0.4105  time: 1.5000  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 875/3000]  eta: 0:53:52  lr: 0.000026  loss: 0.3748  time: 1.4998  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 880/3000]  eta: 0:53:46  lr: 0.000026  loss: 0.4332  time: 1.5336  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 880/3000]  eta: 0:53:45  lr: 0.000026  loss: 0.6048  time: 1.5334  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 885/3000]  eta: 0:53:38  lr: 0.000026  loss: 0.3690  time: 1.5349  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 885/3000]  eta: 0:53:37  lr: 0.000026  loss: 0.7039  time: 1.5347  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 890/3000]  eta: 0:53:31  lr: 0.000026  loss: 0.5354  time: 1.5436  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 890/3000]  eta: 0:53:30  lr: 0.000026  loss: 0.7074  time: 1.5434  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 895/3000]  eta: 0:53:23  lr: 0.000026  loss: 0.3730  time: 1.5423  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 895/3000]  eta: 0:53:23  lr: 0.000026  loss: 0.6217  time: 1.5420  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 900/3000]  eta: 0:53:15  lr: 0.000026  loss: 0.4063  time: 1.5218  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 900/3000]  eta: 0:53:15  lr: 0.000026  loss: 0.0724  time: 1.5216  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 905/3000]  eta: 0:53:07  lr: 0.000026  loss: 0.6673  time: 1.5033  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 905/3000]  eta: 0:53:06  lr: 0.000026  loss: 1.0213  time: 1.5030  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 910/3000]  eta: 0:52:59  lr: 0.000026  loss: 0.6297  time: 1.4880  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 910/3000]  eta: 0:52:58  lr: 0.000026  loss: 0.3285  time: 1.4876  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 915/3000]  eta: 0:52:51  lr: 0.000026  loss: 0.6654  time: 1.4839  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 915/3000]  eta: 0:52:50  lr: 0.000026  loss: 0.3020  time: 1.4837  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 920/3000]  eta: 0:52:43  lr: 0.000026  loss: 0.3594  time: 1.4950  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 920/3000]  eta: 0:52:43  lr: 0.000026  loss: 0.3875  time: 1.4946  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 925/3000]  eta: 0:52:36  lr: 0.000026  loss: 0.4530  time: 1.5078  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 925/3000]  eta: 0:52:35  lr: 0.000026  loss: 0.5470  time: 1.5075  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 930/3000]  eta: 0:52:26  lr: 0.000026  loss: 0.2805  time: 1.4706  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 930/3000]  eta: 0:52:26  lr: 0.000026  loss: 0.1906  time: 1.4703  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 935/3000]  eta: 0:52:18  lr: 0.000026  loss: 0.7176  time: 1.4719  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 935/3000]  eta: 0:52:18  lr: 0.000026  loss: 0.4875  time: 1.4716  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 940/3000]  eta: 0:52:11  lr: 0.000026  loss: 0.8063  time: 1.4798  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 940/3000]  eta: 0:52:11  lr: 0.000026  loss: 0.1725  time: 1.4795  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 945/3000]  eta: 0:52:03  lr: 0.000026  loss: 0.4906  time: 1.4725  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 945/3000]  eta: 0:52:03  lr: 0.000026  loss: 0.6734  time: 1.4722  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 950/3000]  eta: 0:51:55  lr: 0.000026  loss: 0.1563  time: 1.5068  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 950/3000]  eta: 0:51:55  lr: 0.000026  loss: 0.1971  time: 1.5066  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 955/3000]  eta: 0:51:48  lr: 0.000026  loss: 0.1790  time: 1.5106  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 955/3000]  eta: 0:51:47  lr: 0.000026  loss: 0.2497  time: 1.5103  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 960/3000]  eta: 0:51:40  lr: 0.000026  loss: 0.4453  time: 1.5049  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 960/3000]  eta: 0:51:40  lr: 0.000026  loss: 0.6167  time: 1.5047  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 965/3000]  eta: 0:51:33  lr: 0.000026  loss: 0.4592  time: 1.5186  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 965/3000]  eta: 0:51:32  lr: 0.000026  loss: 0.2209  time: 1.5183  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 970/3000]  eta: 0:51:25  lr: 0.000026  loss: 0.1317  time: 1.5179  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 970/3000]  eta: 0:51:24  lr: 0.000026  loss: 0.1014  time: 1.5177  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 975/3000]  eta: 0:51:17  lr: 0.000026  loss: 0.3972  time: 1.5263  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 975/3000]  eta: 0:51:17  lr: 0.000026  loss: 1.0226  time: 1.5261  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 980/3000]  eta: 0:51:09  lr: 0.000026  loss: 0.2419  time: 1.5105  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 980/3000]  eta: 0:51:09  lr: 0.000026  loss: 0.9956  time: 1.5102  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 985/3000]  eta: 0:51:01  lr: 0.000026  loss: 0.3118  time: 1.4918  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 985/3000]  eta: 0:51:01  lr: 0.000026  loss: 0.6004  time: 1.4915  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 990/3000]  eta: 0:50:53  lr: 0.000026  loss: 0.3684  time: 1.4926  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 990/3000]  eta: 0:50:53  lr: 0.000026  loss: 0.1452  time: 1.4923  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [ 995/3000]  eta: 0:50:46  lr: 0.000026  loss: 0.2558  time: 1.4924  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [ 995/3000]  eta: 0:50:45  lr: 0.000026  loss: 0.2588  time: 1.4922  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1000/3000]  eta: 0:50:38  lr: 0.000026  loss: 0.4153  time: 1.4993  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1000/3000]  eta: 0:50:38  lr: 0.000026  loss: 0.5160  time: 1.4990  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1005/3000]  eta: 0:50:31  lr: 0.000026  loss: 0.3962  time: 1.5165  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1005/3000]  eta: 0:50:30  lr: 0.000026  loss: 1.1459  time: 1.5161  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1010/3000]  eta: 0:50:23  lr: 0.000026  loss: 0.5350  time: 1.5100  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1010/3000]  eta: 0:50:22  lr: 0.000026  loss: 0.3802  time: 1.5097  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1015/3000]  eta: 0:50:15  lr: 0.000026  loss: 0.5088  time: 1.5036  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1015/3000]  eta: 0:50:14  lr: 0.000026  loss: 0.4353  time: 1.5032  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1020/3000]  eta: 0:50:08  lr: 0.000026  loss: 0.4678  time: 1.5165  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1020/3000]  eta: 0:50:07  lr: 0.000026  loss: 0.1225  time: 1.5162  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1025/3000]  eta: 0:50:00  lr: 0.000026  loss: 0.5707  time: 1.5198  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1025/3000]  eta: 0:50:00  lr: 0.000026  loss: 0.6596  time: 1.5195  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1030/3000]  eta: 0:49:52  lr: 0.000026  loss: 0.1598  time: 1.5154  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1030/3000]  eta: 0:49:51  lr: 0.000026  loss: 0.8992  time: 1.5152  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1035/3000]  eta: 0:49:44  lr: 0.000026  loss: 0.5426  time: 1.5174  data: 0.0000  max mem: 18406Train: data epoch: [9]  [1035/3000]  eta: 0:49:45  lr: 0.000026  loss: 0.3850  time: 1.5177  data: 0.0000  max mem: 18596

Train: data epoch: [9]  [1040/3000]  eta: 0:49:37  lr: 0.000026  loss: 0.4114  time: 1.5034  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1040/3000]  eta: 0:49:36  lr: 0.000026  loss: 0.4529  time: 1.5031  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1045/3000]  eta: 0:49:29  lr: 0.000026  loss: 0.2318  time: 1.4864  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1045/3000]  eta: 0:49:28  lr: 0.000026  loss: 0.2359  time: 1.4862  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1050/3000]  eta: 0:49:21  lr: 0.000026  loss: 0.6879  time: 1.5108  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1050/3000]  eta: 0:49:21  lr: 0.000026  loss: 0.4402  time: 1.5105  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1055/3000]  eta: 0:49:14  lr: 0.000026  loss: 0.3124  time: 1.5064  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1055/3000]  eta: 0:49:13  lr: 0.000026  loss: 0.3245  time: 1.5062  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1060/3000]  eta: 0:49:06  lr: 0.000026  loss: 0.4837  time: 1.4969  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1060/3000]  eta: 0:49:05  lr: 0.000026  loss: 0.2640  time: 1.4966  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1065/3000]  eta: 0:48:58  lr: 0.000026  loss: 0.4776  time: 1.5007  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1065/3000]  eta: 0:48:57  lr: 0.000026  loss: 0.3876  time: 1.5004  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1070/3000]  eta: 0:48:50  lr: 0.000026  loss: 0.5996  time: 1.4898  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1070/3000]  eta: 0:48:49  lr: 0.000026  loss: 0.5501  time: 1.4896  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1075/3000]  eta: 0:48:43  lr: 0.000026  loss: 0.7395  time: 1.5086  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1075/3000]  eta: 0:48:42  lr: 0.000026  loss: 0.2465  time: 1.5083  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1080/3000]  eta: 0:48:35  lr: 0.000026  loss: 0.3817  time: 1.5246  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1080/3000]  eta: 0:48:35  lr: 0.000026  loss: 0.5587  time: 1.5244  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1085/3000]  eta: 0:48:27  lr: 0.000026  loss: 0.2057  time: 1.5211  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1085/3000]  eta: 0:48:27  lr: 0.000026  loss: 0.1781  time: 1.5208  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1090/3000]  eta: 0:48:19  lr: 0.000026  loss: 0.2142  time: 1.5034  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1090/3000]  eta: 0:48:19  lr: 0.000026  loss: 0.4137  time: 1.5031  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1095/3000]  eta: 0:48:11  lr: 0.000026  loss: 0.3113  time: 1.4762  data: 0.0000  max mem: 18596Train: data epoch: [9]  [1095/3000]  eta: 0:48:11  lr: 0.000026  loss: 0.1896  time: 1.4759  data: 0.0000  max mem: 18406

Train: data epoch: [9]  [1100/3000]  eta: 0:48:03  lr: 0.000026  loss: 0.4376  time: 1.4630  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1100/3000]  eta: 0:48:03  lr: 0.000026  loss: 0.4980  time: 1.4627  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1105/3000]  eta: 0:47:56  lr: 0.000026  loss: 0.1513  time: 1.4872  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1105/3000]  eta: 0:47:55  lr: 0.000026  loss: 0.6769  time: 1.4870  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1110/3000]  eta: 0:47:48  lr: 0.000026  loss: 1.2376  time: 1.4985  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1110/3000]  eta: 0:47:47  lr: 0.000026  loss: 0.2833  time: 1.4983  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1115/3000]  eta: 0:47:40  lr: 0.000026  loss: 0.2962  time: 1.4952  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1115/3000]  eta: 0:47:39  lr: 0.000026  loss: 0.4941  time: 1.4950  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1120/3000]  eta: 0:47:32  lr: 0.000026  loss: 0.3248  time: 1.5018  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1120/3000]  eta: 0:47:32  lr: 0.000026  loss: 0.6382  time: 1.5016  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1125/3000]  eta: 0:47:25  lr: 0.000026  loss: 0.6729  time: 1.4941  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1125/3000]  eta: 0:47:24  lr: 0.000026  loss: 0.1904  time: 1.4938  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1130/3000]  eta: 0:47:17  lr: 0.000026  loss: 1.0229  time: 1.5084  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1130/3000]  eta: 0:47:17  lr: 0.000026  loss: 0.1764  time: 1.5081  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1135/3000]  eta: 0:47:10  lr: 0.000026  loss: 0.5633  time: 1.5269  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1135/3000]  eta: 0:47:09  lr: 0.000026  loss: 0.5154  time: 1.5267  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1140/3000]  eta: 0:47:03  lr: 0.000026  loss: 0.2007  time: 1.5449  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1140/3000]  eta: 0:47:02  lr: 0.000026  loss: 0.1074  time: 1.5447  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1145/3000]  eta: 0:46:55  lr: 0.000026  loss: 0.4452  time: 1.5379  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1145/3000]  eta: 0:46:55  lr: 0.000026  loss: 0.1292  time: 1.5376  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1150/3000]  eta: 0:46:47  lr: 0.000026  loss: 0.0862  time: 1.5290  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1150/3000]  eta: 0:46:47  lr: 0.000026  loss: 0.6476  time: 1.5287  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1155/3000]  eta: 0:46:39  lr: 0.000026  loss: 0.3425  time: 1.5092  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1155/3000]  eta: 0:46:39  lr: 0.000026  loss: 0.7264  time: 1.5090  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1160/3000]  eta: 0:46:31  lr: 0.000026  loss: 0.6040  time: 1.4897  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1160/3000]  eta: 0:46:31  lr: 0.000026  loss: 0.2933  time: 1.4894  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1165/3000]  eta: 0:46:24  lr: 0.000026  loss: 0.3719  time: 1.4945  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1165/3000]  eta: 0:46:24  lr: 0.000026  loss: 0.2980  time: 1.4943  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1170/3000]  eta: 0:46:17  lr: 0.000026  loss: 0.3149  time: 1.5067  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1170/3000]  eta: 0:46:16  lr: 0.000026  loss: 0.5493  time: 1.5064  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1175/3000]  eta: 0:46:09  lr: 0.000026  loss: 0.3796  time: 1.5099  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1175/3000]  eta: 0:46:08  lr: 0.000026  loss: 0.5379  time: 1.5086  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1180/3000]  eta: 0:46:01  lr: 0.000026  loss: 0.2637  time: 1.5235  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1180/3000]  eta: 0:46:01  lr: 0.000026  loss: 0.4746  time: 1.5222  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1185/3000]  eta: 0:45:54  lr: 0.000026  loss: 0.1632  time: 1.5316  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1185/3000]  eta: 0:45:54  lr: 0.000026  loss: 0.9265  time: 1.5304  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1190/3000]  eta: 0:45:46  lr: 0.000026  loss: 0.5433  time: 1.5084  data: 0.0000  max mem: 18406Train: data epoch: [9]  [1190/3000]  eta: 0:45:46  lr: 0.000026  loss: 0.2028  time: 1.5097  data: 0.0000  max mem: 18596

Train: data epoch: [9]  [1195/3000]  eta: 0:45:38  lr: 0.000026  loss: 0.3398  time: 1.5166  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1195/3000]  eta: 0:45:38  lr: 0.000026  loss: 0.3307  time: 1.5164  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1200/3000]  eta: 0:45:31  lr: 0.000026  loss: 1.2525  time: 1.5101  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1200/3000]  eta: 0:45:30  lr: 0.000026  loss: 0.3635  time: 1.5098  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1205/3000]  eta: 0:45:23  lr: 0.000026  loss: 0.6674  time: 1.5084  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1205/3000]  eta: 0:45:23  lr: 0.000026  loss: 0.5013  time: 1.5081  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1210/3000]  eta: 0:45:16  lr: 0.000026  loss: 0.2502  time: 1.5306  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1210/3000]  eta: 0:45:16  lr: 0.000026  loss: 0.2012  time: 1.5304  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1215/3000]  eta: 0:45:09  lr: 0.000026  loss: 0.1198  time: 1.5388  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1215/3000]  eta: 0:45:08  lr: 0.000026  loss: 0.4007  time: 1.5386  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1220/3000]  eta: 0:45:00  lr: 0.000026  loss: 0.3365  time: 1.5189  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1220/3000]  eta: 0:45:00  lr: 0.000026  loss: 0.4219  time: 1.5187  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1225/3000]  eta: 0:44:52  lr: 0.000026  loss: 0.3534  time: 1.4824  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1225/3000]  eta: 0:44:52  lr: 0.000026  loss: 0.1868  time: 1.4822  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1230/3000]  eta: 0:44:44  lr: 0.000026  loss: 0.4602  time: 1.4643  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1230/3000]  eta: 0:44:44  lr: 0.000026  loss: 0.1561  time: 1.4640  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1235/3000]  eta: 0:44:36  lr: 0.000026  loss: 0.3905  time: 1.4527  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1235/3000]  eta: 0:44:36  lr: 0.000026  loss: 0.4981  time: 1.4524  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1240/3000]  eta: 0:44:29  lr: 0.000026  loss: 0.4409  time: 1.4755  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1240/3000]  eta: 0:44:28  lr: 0.000026  loss: 0.3721  time: 1.4753  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1245/3000]  eta: 0:44:22  lr: 0.000026  loss: 0.4164  time: 1.5136  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1245/3000]  eta: 0:44:21  lr: 0.000026  loss: 0.6040  time: 1.5134  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1250/3000]  eta: 0:44:14  lr: 0.000026  loss: 0.3498  time: 1.5208  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1250/3000]  eta: 0:44:13  lr: 0.000026  loss: 0.4709  time: 1.5206  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1255/3000]  eta: 0:44:07  lr: 0.000026  loss: 0.3744  time: 1.5465  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1255/3000]  eta: 0:44:06  lr: 0.000026  loss: 0.1120  time: 1.5463  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1260/3000]  eta: 0:43:59  lr: 0.000026  loss: 1.1062  time: 1.5422  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1260/3000]  eta: 0:43:59  lr: 0.000026  loss: 0.7079  time: 1.5420  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1265/3000]  eta: 0:43:52  lr: 0.000026  loss: 0.1688  time: 1.5329  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1265/3000]  eta: 0:43:51  lr: 0.000026  loss: 0.2589  time: 1.5326  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1270/3000]  eta: 0:43:44  lr: 0.000026  loss: 0.4917  time: 1.5190  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1270/3000]  eta: 0:43:43  lr: 0.000026  loss: 1.1905  time: 1.5187  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1275/3000]  eta: 0:43:36  lr: 0.000026  loss: 0.4548  time: 1.4849  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1275/3000]  eta: 0:43:35  lr: 0.000026  loss: 0.1689  time: 1.4848  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1280/3000]  eta: 0:43:28  lr: 0.000026  loss: 0.4054  time: 1.4875  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1280/3000]  eta: 0:43:28  lr: 0.000026  loss: 0.6754  time: 1.4872  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1285/3000]  eta: 0:43:21  lr: 0.000026  loss: 0.3867  time: 1.4937  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1285/3000]  eta: 0:43:20  lr: 0.000026  loss: 0.8686  time: 1.4934  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1290/3000]  eta: 0:43:13  lr: 0.000026  loss: 0.2418  time: 1.4932  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1290/3000]  eta: 0:43:12  lr: 0.000026  loss: 0.6436  time: 1.4930  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1295/3000]  eta: 0:43:06  lr: 0.000026  loss: 0.1395  time: 1.5277  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1295/3000]  eta: 0:43:05  lr: 0.000026  loss: 0.4535  time: 1.5274  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1300/3000]  eta: 0:42:58  lr: 0.000026  loss: 0.3620  time: 1.5300  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1300/3000]  eta: 0:42:58  lr: 0.000026  loss: 0.4947  time: 1.5298  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1305/3000]  eta: 0:42:51  lr: 0.000026  loss: 0.2444  time: 1.5403  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1305/3000]  eta: 0:42:51  lr: 0.000026  loss: 0.5326  time: 1.5400  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1310/3000]  eta: 0:42:44  lr: 0.000026  loss: 0.6435  time: 1.5624  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1310/3000]  eta: 0:42:43  lr: 0.000026  loss: 0.3745  time: 1.5621  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1315/3000]  eta: 0:42:36  lr: 0.000026  loss: 0.3752  time: 1.5286  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1315/3000]  eta: 0:42:35  lr: 0.000026  loss: 0.3797  time: 1.5284  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1320/3000]  eta: 0:42:28  lr: 0.000025  loss: 0.2187  time: 1.5215  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1320/3000]  eta: 0:42:27  lr: 0.000025  loss: 0.8265  time: 1.5213  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1325/3000]  eta: 0:42:20  lr: 0.000025  loss: 0.2002  time: 1.4947  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1325/3000]  eta: 0:42:20  lr: 0.000025  loss: 0.1918  time: 1.4945  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1330/3000]  eta: 0:42:13  lr: 0.000025  loss: 0.9658  time: 1.4994  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1330/3000]  eta: 0:42:12  lr: 0.000025  loss: 0.1844  time: 1.4991  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1335/3000]  eta: 0:42:05  lr: 0.000025  loss: 0.3070  time: 1.5182  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1335/3000]  eta: 0:42:05  lr: 0.000025  loss: 0.1205  time: 1.5179  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1340/3000]  eta: 0:41:57  lr: 0.000025  loss: 0.3181  time: 1.5106  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1340/3000]  eta: 0:41:57  lr: 0.000025  loss: 0.3592  time: 1.5103  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1345/3000]  eta: 0:41:50  lr: 0.000025  loss: 0.3680  time: 1.5338  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1345/3000]  eta: 0:41:50  lr: 0.000025  loss: 0.1178  time: 1.5335  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1350/3000]  eta: 0:41:43  lr: 0.000025  loss: 0.5768  time: 1.5310  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1350/3000]  eta: 0:41:42  lr: 0.000025  loss: 0.3168  time: 1.5307  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1355/3000]  eta: 0:41:35  lr: 0.000025  loss: 0.2369  time: 1.5322  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1355/3000]  eta: 0:41:35  lr: 0.000025  loss: 0.3731  time: 1.5320  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1360/3000]  eta: 0:41:28  lr: 0.000025  loss: 0.5023  time: 1.5417  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1360/3000]  eta: 0:41:27  lr: 0.000025  loss: 0.3632  time: 1.5415  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1365/3000]  eta: 0:41:20  lr: 0.000025  loss: 0.3295  time: 1.5264  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1365/3000]  eta: 0:41:20  lr: 0.000025  loss: 0.3044  time: 1.5262  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1370/3000]  eta: 0:41:13  lr: 0.000025  loss: 0.1588  time: 1.5207  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1370/3000]  eta: 0:41:12  lr: 0.000025  loss: 0.5859  time: 1.5204  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1375/3000]  eta: 0:41:05  lr: 0.000025  loss: 0.2551  time: 1.5037  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1375/3000]  eta: 0:41:04  lr: 0.000025  loss: 0.1547  time: 1.5034  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1380/3000]  eta: 0:40:57  lr: 0.000025  loss: 0.1673  time: 1.5081  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1380/3000]  eta: 0:40:57  lr: 0.000025  loss: 0.6603  time: 1.5078  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1385/3000]  eta: 0:40:50  lr: 0.000025  loss: 0.3416  time: 1.5269  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1385/3000]  eta: 0:40:50  lr: 0.000025  loss: 0.4315  time: 1.5266  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1390/3000]  eta: 0:40:42  lr: 0.000025  loss: 0.2669  time: 1.5137  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1390/3000]  eta: 0:40:42  lr: 0.000025  loss: 0.1939  time: 1.5134  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1395/3000]  eta: 0:40:34  lr: 0.000025  loss: 0.5805  time: 1.5236  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1395/3000]  eta: 0:40:34  lr: 0.000025  loss: 0.0555  time: 1.5234  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1400/3000]  eta: 0:40:27  lr: 0.000025  loss: 0.1477  time: 1.5190  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1400/3000]  eta: 0:40:26  lr: 0.000025  loss: 0.4819  time: 1.5187  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1405/3000]  eta: 0:40:19  lr: 0.000025  loss: 0.4705  time: 1.5043  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1405/3000]  eta: 0:40:19  lr: 0.000025  loss: 0.5853  time: 1.5041  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1410/3000]  eta: 0:40:12  lr: 0.000025  loss: 0.7142  time: 1.5100  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1410/3000]  eta: 0:40:11  lr: 0.000025  loss: 0.0940  time: 1.5097  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1415/3000]  eta: 0:40:04  lr: 0.000025  loss: 0.7074  time: 1.5125  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1415/3000]  eta: 0:40:04  lr: 0.000025  loss: 0.2696  time: 1.5123  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1420/3000]  eta: 0:39:56  lr: 0.000025  loss: 0.3714  time: 1.5077  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1420/3000]  eta: 0:39:56  lr: 0.000025  loss: 0.2847  time: 1.5074  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1425/3000]  eta: 0:39:48  lr: 0.000025  loss: 0.1699  time: 1.4820  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1425/3000]  eta: 0:39:48  lr: 0.000025  loss: 0.3754  time: 1.4818  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1430/3000]  eta: 0:39:41  lr: 0.000025  loss: 0.2281  time: 1.4880  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1430/3000]  eta: 0:39:40  lr: 0.000025  loss: 0.3781  time: 1.4879  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1435/3000]  eta: 0:39:33  lr: 0.000025  loss: 0.3577  time: 1.4792  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1435/3000]  eta: 0:39:32  lr: 0.000025  loss: 0.7256  time: 1.4789  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1440/3000]  eta: 0:39:25  lr: 0.000025  loss: 0.2455  time: 1.4933  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1440/3000]  eta: 0:39:25  lr: 0.000025  loss: 0.2816  time: 1.4931  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1445/3000]  eta: 0:39:17  lr: 0.000025  loss: 0.2727  time: 1.4743  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1445/3000]  eta: 0:39:17  lr: 0.000025  loss: 0.5558  time: 1.4741  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1450/3000]  eta: 0:39:09  lr: 0.000025  loss: 0.3548  time: 1.4663  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1450/3000]  eta: 0:39:09  lr: 0.000025  loss: 0.5397  time: 1.4661  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1455/3000]  eta: 0:39:02  lr: 0.000025  loss: 1.1895  time: 1.4800  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1455/3000]  eta: 0:39:01  lr: 0.000025  loss: 0.8872  time: 1.4799  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1460/3000]  eta: 0:38:54  lr: 0.000025  loss: 0.1748  time: 1.4822  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1460/3000]  eta: 0:38:54  lr: 0.000025  loss: 0.5877  time: 1.4820  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1465/3000]  eta: 0:38:47  lr: 0.000025  loss: 0.3638  time: 1.5217  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1465/3000]  eta: 0:38:46  lr: 0.000025  loss: 0.2360  time: 1.5215  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1470/3000]  eta: 0:38:40  lr: 0.000025  loss: 0.4023  time: 1.5487  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1470/3000]  eta: 0:38:39  lr: 0.000025  loss: 0.3939  time: 1.5485  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1475/3000]  eta: 0:38:32  lr: 0.000025  loss: 0.5546  time: 1.5198  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1475/3000]  eta: 0:38:31  lr: 0.000025  loss: 0.1910  time: 1.5195  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1480/3000]  eta: 0:38:24  lr: 0.000025  loss: 0.3651  time: 1.5138  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1480/3000]  eta: 0:38:24  lr: 0.000025  loss: 0.1598  time: 1.5135  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1485/3000]  eta: 0:38:17  lr: 0.000025  loss: 0.6924  time: 1.5213  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1485/3000]  eta: 0:38:16  lr: 0.000025  loss: 0.2052  time: 1.5211  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1490/3000]  eta: 0:38:09  lr: 0.000025  loss: 0.0479  time: 1.5065  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1490/3000]  eta: 0:38:09  lr: 0.000025  loss: 0.2992  time: 1.5063  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1495/3000]  eta: 0:38:01  lr: 0.000025  loss: 0.1253  time: 1.5238  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1495/3000]  eta: 0:38:01  lr: 0.000025  loss: 0.1344  time: 1.5236  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1500/3000]  eta: 0:37:54  lr: 0.000025  loss: 0.2224  time: 1.5252  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1500/3000]  eta: 0:37:53  lr: 0.000025  loss: 0.8092  time: 1.5250  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1505/3000]  eta: 0:37:46  lr: 0.000025  loss: 0.7874  time: 1.5193  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1505/3000]  eta: 0:37:46  lr: 0.000025  loss: 0.6240  time: 1.5191  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1510/3000]  eta: 0:37:38  lr: 0.000025  loss: 0.1320  time: 1.4976  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1510/3000]  eta: 0:37:38  lr: 0.000025  loss: 0.2163  time: 1.4974  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1515/3000]  eta: 0:37:31  lr: 0.000025  loss: 0.0972  time: 1.4934  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1515/3000]  eta: 0:37:30  lr: 0.000025  loss: 0.2729  time: 1.4931  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1520/3000]  eta: 0:37:23  lr: 0.000025  loss: 0.7301  time: 1.4841  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1520/3000]  eta: 0:37:23  lr: 0.000025  loss: 0.3880  time: 1.4838  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1525/3000]  eta: 0:37:15  lr: 0.000025  loss: 0.3065  time: 1.4787  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1525/3000]  eta: 0:37:15  lr: 0.000025  loss: 0.3120  time: 1.4784  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1530/3000]  eta: 0:37:08  lr: 0.000025  loss: 0.4551  time: 1.4968  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1530/3000]  eta: 0:37:07  lr: 0.000025  loss: 0.9356  time: 1.4966  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1535/3000]  eta: 0:37:00  lr: 0.000025  loss: 0.6295  time: 1.5102  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1535/3000]  eta: 0:37:00  lr: 0.000025  loss: 0.2005  time: 1.5100  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1540/3000]  eta: 0:36:53  lr: 0.000025  loss: 0.5326  time: 1.5263  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1540/3000]  eta: 0:36:52  lr: 0.000025  loss: 0.5612  time: 1.5261  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1545/3000]  eta: 0:36:45  lr: 0.000025  loss: 0.4257  time: 1.5439  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1545/3000]  eta: 0:36:45  lr: 0.000025  loss: 0.2459  time: 1.5436  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1550/3000]  eta: 0:36:38  lr: 0.000025  loss: 0.3264  time: 1.5326  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1550/3000]  eta: 0:36:37  lr: 0.000025  loss: 0.2618  time: 1.5324  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1555/3000]  eta: 0:36:30  lr: 0.000025  loss: 0.2716  time: 1.5265  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1555/3000]  eta: 0:36:30  lr: 0.000025  loss: 0.2996  time: 1.5262  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1560/3000]  eta: 0:36:23  lr: 0.000025  loss: 0.1884  time: 1.5197  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1560/3000]  eta: 0:36:22  lr: 0.000025  loss: 0.5412  time: 1.5194  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1565/3000]  eta: 0:36:15  lr: 0.000025  loss: 0.1800  time: 1.4977  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1565/3000]  eta: 0:36:14  lr: 0.000025  loss: 0.3104  time: 1.4974  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1570/3000]  eta: 0:36:07  lr: 0.000025  loss: 0.1821  time: 1.5069  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1570/3000]  eta: 0:36:07  lr: 0.000025  loss: 0.5918  time: 1.5068  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1575/3000]  eta: 0:36:00  lr: 0.000025  loss: 1.4779  time: 1.5061  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1575/3000]  eta: 0:35:59  lr: 0.000025  loss: 1.7571  time: 1.5059  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1580/3000]  eta: 0:35:52  lr: 0.000025  loss: 0.6666  time: 1.5068  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1580/3000]  eta: 0:35:52  lr: 0.000025  loss: 0.4362  time: 1.5066  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1585/3000]  eta: 0:35:45  lr: 0.000025  loss: 0.4226  time: 1.5176  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1585/3000]  eta: 0:35:44  lr: 0.000025  loss: 0.3715  time: 1.5174  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1590/3000]  eta: 0:35:37  lr: 0.000025  loss: 0.1853  time: 1.5086  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1590/3000]  eta: 0:35:36  lr: 0.000025  loss: 0.2772  time: 1.5083  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1595/3000]  eta: 0:35:29  lr: 0.000025  loss: 0.4592  time: 1.4988  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1595/3000]  eta: 0:35:29  lr: 0.000025  loss: 0.6318  time: 1.4985  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1600/3000]  eta: 0:35:21  lr: 0.000025  loss: 0.2313  time: 1.4700  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1600/3000]  eta: 0:35:21  lr: 0.000025  loss: 0.3619  time: 1.4698  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1605/3000]  eta: 0:35:13  lr: 0.000025  loss: 0.9170  time: 1.4691  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1605/3000]  eta: 0:35:13  lr: 0.000025  loss: 0.3262  time: 1.4689  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1610/3000]  eta: 0:35:06  lr: 0.000025  loss: 0.1351  time: 1.4728  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1610/3000]  eta: 0:35:05  lr: 0.000025  loss: 0.3897  time: 1.4726  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1615/3000]  eta: 0:34:58  lr: 0.000025  loss: 0.3631  time: 1.4910  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1615/3000]  eta: 0:34:58  lr: 0.000025  loss: 0.3717  time: 1.4908  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1620/3000]  eta: 0:34:50  lr: 0.000025  loss: 0.0407  time: 1.5038  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1620/3000]  eta: 0:34:50  lr: 0.000025  loss: 0.1806  time: 1.5035  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1625/3000]  eta: 0:34:43  lr: 0.000025  loss: 0.2997  time: 1.5044  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1625/3000]  eta: 0:34:43  lr: 0.000025  loss: 0.1456  time: 1.5041  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1630/3000]  eta: 0:34:35  lr: 0.000025  loss: 0.2495  time: 1.5001  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1630/3000]  eta: 0:34:35  lr: 0.000025  loss: 0.1383  time: 1.4999  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1635/3000]  eta: 0:34:27  lr: 0.000025  loss: 0.1652  time: 1.4745  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1635/3000]  eta: 0:34:27  lr: 0.000025  loss: 0.1969  time: 1.4743  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1640/3000]  eta: 0:34:19  lr: 0.000025  loss: 0.2703  time: 1.4738  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1640/3000]  eta: 0:34:19  lr: 0.000025  loss: 0.4037  time: 1.4736  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1645/3000]  eta: 0:34:12  lr: 0.000025  loss: 0.9813  time: 1.4806  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1645/3000]  eta: 0:34:12  lr: 0.000025  loss: 0.4264  time: 1.4804  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1650/3000]  eta: 0:34:05  lr: 0.000025  loss: 0.1056  time: 1.5039  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1650/3000]  eta: 0:34:04  lr: 0.000025  loss: 0.5045  time: 1.5037  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1655/3000]  eta: 0:33:57  lr: 0.000025  loss: 0.2212  time: 1.5315  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1655/3000]  eta: 0:33:57  lr: 0.000025  loss: 0.3128  time: 1.5312  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1660/3000]  eta: 0:33:49  lr: 0.000025  loss: 0.5763  time: 1.5281  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1660/3000]  eta: 0:33:49  lr: 0.000025  loss: 0.5593  time: 1.5280  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1665/3000]  eta: 0:33:42  lr: 0.000025  loss: 0.2205  time: 1.5387  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1665/3000]  eta: 0:33:42  lr: 0.000025  loss: 0.3370  time: 1.5384  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1670/3000]  eta: 0:33:34  lr: 0.000025  loss: 0.4827  time: 1.5153  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1670/3000]  eta: 0:33:34  lr: 0.000025  loss: 0.4982  time: 1.5151  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1675/3000]  eta: 0:33:27  lr: 0.000025  loss: 0.2366  time: 1.5100  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1675/3000]  eta: 0:33:26  lr: 0.000025  loss: 0.5867  time: 1.5098  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1680/3000]  eta: 0:33:19  lr: 0.000025  loss: 0.1484  time: 1.5202  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1680/3000]  eta: 0:33:19  lr: 0.000025  loss: 0.3106  time: 1.5199  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1685/3000]  eta: 0:33:12  lr: 0.000025  loss: 0.5592  time: 1.5171  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1685/3000]  eta: 0:33:11  lr: 0.000025  loss: 0.2399  time: 1.5169  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1690/3000]  eta: 0:33:04  lr: 0.000025  loss: 0.8408  time: 1.5382  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1690/3000]  eta: 0:33:04  lr: 0.000025  loss: 0.3557  time: 1.5379  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1695/3000]  eta: 0:32:57  lr: 0.000025  loss: 0.3411  time: 1.5571  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1695/3000]  eta: 0:32:57  lr: 0.000025  loss: 0.5691  time: 1.5568  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1700/3000]  eta: 0:32:50  lr: 0.000025  loss: 0.9806  time: 1.5690  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1700/3000]  eta: 0:32:49  lr: 0.000025  loss: 0.6786  time: 1.5687  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1705/3000]  eta: 0:32:42  lr: 0.000025  loss: 0.3400  time: 1.5596  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1705/3000]  eta: 0:32:42  lr: 0.000025  loss: 0.4398  time: 1.5593  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1710/3000]  eta: 0:32:35  lr: 0.000025  loss: 0.7735  time: 1.5528  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1710/3000]  eta: 0:32:34  lr: 0.000025  loss: 0.6614  time: 1.5526  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1715/3000]  eta: 0:32:27  lr: 0.000025  loss: 0.3395  time: 1.5330  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1715/3000]  eta: 0:32:27  lr: 0.000025  loss: 0.2217  time: 1.5328  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1720/3000]  eta: 0:32:20  lr: 0.000025  loss: 0.4237  time: 1.5270  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1720/3000]  eta: 0:32:19  lr: 0.000025  loss: 0.2587  time: 1.5268  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1725/3000]  eta: 0:32:12  lr: 0.000025  loss: 0.3491  time: 1.5199  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1725/3000]  eta: 0:32:12  lr: 0.000025  loss: 0.3442  time: 1.5198  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1730/3000]  eta: 0:32:04  lr: 0.000025  loss: 0.2005  time: 1.5211  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1730/3000]  eta: 0:32:04  lr: 0.000025  loss: 0.4495  time: 1.5204  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1735/3000]  eta: 0:31:57  lr: 0.000025  loss: 0.2628  time: 1.5254  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1735/3000]  eta: 0:31:57  lr: 0.000025  loss: 0.2675  time: 1.5247  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1740/3000]  eta: 0:31:50  lr: 0.000025  loss: 0.5309  time: 1.5366  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1740/3000]  eta: 0:31:49  lr: 0.000025  loss: 0.1745  time: 1.5359  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1745/3000]  eta: 0:31:42  lr: 0.000025  loss: 0.3306  time: 1.5262  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1745/3000]  eta: 0:31:41  lr: 0.000025  loss: 0.6079  time: 1.5253  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1750/3000]  eta: 0:31:34  lr: 0.000025  loss: 0.3474  time: 1.5042  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1750/3000]  eta: 0:31:34  lr: 0.000025  loss: 0.3542  time: 1.5040  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1755/3000]  eta: 0:31:26  lr: 0.000025  loss: 0.4443  time: 1.4964  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1755/3000]  eta: 0:31:26  lr: 0.000025  loss: 0.2759  time: 1.4961  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1760/3000]  eta: 0:31:19  lr: 0.000025  loss: 0.3084  time: 1.4686  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1760/3000]  eta: 0:31:18  lr: 0.000025  loss: 0.3448  time: 1.4683  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1765/3000]  eta: 0:31:11  lr: 0.000025  loss: 0.1608  time: 1.4750  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1765/3000]  eta: 0:31:11  lr: 0.000025  loss: 0.1447  time: 1.4747  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1770/3000]  eta: 0:31:03  lr: 0.000025  loss: 0.3004  time: 1.4818  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1770/3000]  eta: 0:31:03  lr: 0.000025  loss: 0.3043  time: 1.4816  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1775/3000]  eta: 0:30:55  lr: 0.000025  loss: 0.8154  time: 1.4569  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1775/3000]  eta: 0:30:55  lr: 0.000025  loss: 0.2767  time: 1.4574  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1780/3000]  eta: 0:30:48  lr: 0.000025  loss: 0.3644  time: 1.4950  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1780/3000]  eta: 0:30:48  lr: 0.000025  loss: 0.2605  time: 1.4947  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1785/3000]  eta: 0:30:40  lr: 0.000025  loss: 0.2255  time: 1.4934  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1785/3000]  eta: 0:30:40  lr: 0.000025  loss: 0.4624  time: 1.4932  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1790/3000]  eta: 0:30:33  lr: 0.000025  loss: 0.2698  time: 1.5077  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1790/3000]  eta: 0:30:32  lr: 0.000025  loss: 0.8870  time: 1.5075  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1795/3000]  eta: 0:30:25  lr: 0.000025  loss: 0.6316  time: 1.5450  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1795/3000]  eta: 0:30:25  lr: 0.000025  loss: 0.1701  time: 1.5448  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1800/3000]  eta: 0:30:18  lr: 0.000025  loss: 0.2714  time: 1.5108  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1800/3000]  eta: 0:30:17  lr: 0.000025  loss: 0.5158  time: 1.5106  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1805/3000]  eta: 0:30:10  lr: 0.000025  loss: 0.4172  time: 1.4959  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1805/3000]  eta: 0:30:09  lr: 0.000025  loss: 0.1080  time: 1.4955  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1810/3000]  eta: 0:30:02  lr: 0.000025  loss: 0.4521  time: 1.4814  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1810/3000]  eta: 0:30:02  lr: 0.000025  loss: 0.4545  time: 1.4812  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1815/3000]  eta: 0:29:55  lr: 0.000025  loss: 0.2018  time: 1.4789  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1815/3000]  eta: 0:29:54  lr: 0.000025  loss: 0.7745  time: 1.4787  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1820/3000]  eta: 0:29:47  lr: 0.000025  loss: 0.1095  time: 1.4835  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1820/3000]  eta: 0:29:47  lr: 0.000025  loss: 0.5407  time: 1.4833  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1825/3000]  eta: 0:29:39  lr: 0.000025  loss: 0.2782  time: 1.5161  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1825/3000]  eta: 0:29:39  lr: 0.000025  loss: 0.6036  time: 1.5159  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1830/3000]  eta: 0:29:32  lr: 0.000025  loss: 0.3763  time: 1.5103  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1830/3000]  eta: 0:29:31  lr: 0.000025  loss: 0.8064  time: 1.5101  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1835/3000]  eta: 0:29:24  lr: 0.000025  loss: 0.2528  time: 1.5032  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1835/3000]  eta: 0:29:24  lr: 0.000025  loss: 0.4073  time: 1.5029  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1840/3000]  eta: 0:29:17  lr: 0.000025  loss: 1.0981  time: 1.5214  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1840/3000]  eta: 0:29:16  lr: 0.000025  loss: 0.3985  time: 1.5212  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1845/3000]  eta: 0:29:09  lr: 0.000025  loss: 0.7899  time: 1.5149  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1845/3000]  eta: 0:29:09  lr: 0.000025  loss: 0.1557  time: 1.5148  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1850/3000]  eta: 0:29:02  lr: 0.000025  loss: 0.0895  time: 1.5452  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1850/3000]  eta: 0:29:01  lr: 0.000025  loss: 0.5493  time: 1.5449  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1855/3000]  eta: 0:28:54  lr: 0.000025  loss: 0.4306  time: 1.5637  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1855/3000]  eta: 0:28:54  lr: 0.000025  loss: 0.5455  time: 1.5636  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1860/3000]  eta: 0:28:47  lr: 0.000025  loss: 0.1335  time: 1.5585  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1860/3000]  eta: 0:28:47  lr: 0.000025  loss: 0.5436  time: 1.5583  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1865/3000]  eta: 0:28:39  lr: 0.000025  loss: 0.2822  time: 1.5531  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1865/3000]  eta: 0:28:39  lr: 0.000025  loss: 0.3176  time: 1.5528  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1870/3000]  eta: 0:28:32  lr: 0.000025  loss: 0.3185  time: 1.5303  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1870/3000]  eta: 0:28:31  lr: 0.000025  loss: 0.8614  time: 1.5301  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1875/3000]  eta: 0:28:24  lr: 0.000025  loss: 0.3069  time: 1.5121  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1875/3000]  eta: 0:28:24  lr: 0.000025  loss: 0.2262  time: 1.5119  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1880/3000]  eta: 0:28:17  lr: 0.000025  loss: 0.3556  time: 1.5055  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1880/3000]  eta: 0:28:16  lr: 0.000025  loss: 0.6174  time: 1.5052  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1885/3000]  eta: 0:28:09  lr: 0.000025  loss: 0.2844  time: 1.5166  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1885/3000]  eta: 0:28:09  lr: 0.000025  loss: 0.3947  time: 1.5164  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1890/3000]  eta: 0:28:02  lr: 0.000025  loss: 0.4800  time: 1.5415  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1890/3000]  eta: 0:28:01  lr: 0.000025  loss: 0.1220  time: 1.5412  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1895/3000]  eta: 0:27:54  lr: 0.000025  loss: 0.3149  time: 1.5457  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1895/3000]  eta: 0:27:54  lr: 0.000025  loss: 0.2418  time: 1.5454  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1900/3000]  eta: 0:27:46  lr: 0.000025  loss: 0.4328  time: 1.5230  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1900/3000]  eta: 0:27:46  lr: 0.000025  loss: 0.4400  time: 1.5227  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1905/3000]  eta: 0:27:39  lr: 0.000025  loss: 0.1732  time: 1.4945  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1905/3000]  eta: 0:27:38  lr: 0.000025  loss: 1.0731  time: 1.4942  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1910/3000]  eta: 0:27:31  lr: 0.000025  loss: 0.5091  time: 1.4632  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1910/3000]  eta: 0:27:30  lr: 0.000025  loss: 0.7603  time: 1.4630  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1915/3000]  eta: 0:27:23  lr: 0.000025  loss: 0.3921  time: 1.4233  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1915/3000]  eta: 0:27:22  lr: 0.000025  loss: 0.2199  time: 1.4230  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1920/3000]  eta: 0:27:15  lr: 0.000025  loss: 0.1783  time: 1.4206  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1920/3000]  eta: 0:27:15  lr: 0.000025  loss: 0.1631  time: 1.4204  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1925/3000]  eta: 0:27:07  lr: 0.000025  loss: 0.7715  time: 1.4324  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1925/3000]  eta: 0:27:07  lr: 0.000025  loss: 0.6002  time: 1.4322  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1930/3000]  eta: 0:27:00  lr: 0.000025  loss: 0.2876  time: 1.4598  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1930/3000]  eta: 0:27:00  lr: 0.000025  loss: 0.5807  time: 1.4595  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1935/3000]  eta: 0:26:52  lr: 0.000025  loss: 0.4099  time: 1.4771  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1935/3000]  eta: 0:26:52  lr: 0.000025  loss: 0.3664  time: 1.4768  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1940/3000]  eta: 0:26:45  lr: 0.000025  loss: 0.9094  time: 1.5032  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1940/3000]  eta: 0:26:44  lr: 0.000025  loss: 0.2161  time: 1.5029  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1945/3000]  eta: 0:26:37  lr: 0.000025  loss: 1.4383  time: 1.5141  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1945/3000]  eta: 0:26:37  lr: 0.000025  loss: 0.7597  time: 1.5138  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1950/3000]  eta: 0:26:29  lr: 0.000025  loss: 0.3225  time: 1.4824  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1950/3000]  eta: 0:26:29  lr: 0.000025  loss: 0.9439  time: 1.4822  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1955/3000]  eta: 0:26:22  lr: 0.000025  loss: 1.0414  time: 1.4988  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1955/3000]  eta: 0:26:21  lr: 0.000025  loss: 0.3442  time: 1.4985  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1960/3000]  eta: 0:26:14  lr: 0.000025  loss: 0.3644  time: 1.5085  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1960/3000]  eta: 0:26:14  lr: 0.000025  loss: 0.4113  time: 1.5082  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1965/3000]  eta: 0:26:07  lr: 0.000025  loss: 0.3465  time: 1.4960  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1965/3000]  eta: 0:26:06  lr: 0.000025  loss: 0.3480  time: 1.4957  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1970/3000]  eta: 0:25:59  lr: 0.000025  loss: 0.1358  time: 1.5096  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1970/3000]  eta: 0:25:59  lr: 0.000025  loss: 0.8361  time: 1.5094  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1975/3000]  eta: 0:25:52  lr: 0.000025  loss: 0.6428  time: 1.5285  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1975/3000]  eta: 0:25:51  lr: 0.000025  loss: 0.2770  time: 1.5283  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1980/3000]  eta: 0:25:44  lr: 0.000025  loss: 0.2847  time: 1.5216  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1980/3000]  eta: 0:25:44  lr: 0.000025  loss: 0.3251  time: 1.5214  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1985/3000]  eta: 0:25:36  lr: 0.000025  loss: 0.1334  time: 1.5256  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1985/3000]  eta: 0:25:36  lr: 0.000025  loss: 0.0663  time: 1.5255  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1990/3000]  eta: 0:25:29  lr: 0.000025  loss: 0.7687  time: 1.5356  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1990/3000]  eta: 0:25:29  lr: 0.000025  loss: 0.4576  time: 1.5344  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [1995/3000]  eta: 0:25:21  lr: 0.000025  loss: 0.1780  time: 1.5367  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [1995/3000]  eta: 0:25:21  lr: 0.000025  loss: 0.5622  time: 1.5355  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2000/3000]  eta: 0:25:14  lr: 0.000025  loss: 0.4545  time: 1.5373  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2000/3000]  eta: 0:25:14  lr: 0.000025  loss: 0.1868  time: 1.5360  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2005/3000]  eta: 0:25:06  lr: 0.000025  loss: 0.2053  time: 1.5403  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2005/3000]  eta: 0:25:06  lr: 0.000025  loss: 0.8801  time: 1.5391  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2010/3000]  eta: 0:24:59  lr: 0.000025  loss: 0.2493  time: 1.5470  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2010/3000]  eta: 0:24:59  lr: 0.000025  loss: 0.3350  time: 1.5468  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2015/3000]  eta: 0:24:51  lr: 0.000025  loss: 0.1885  time: 1.5277  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2015/3000]  eta: 0:24:51  lr: 0.000025  loss: 0.4006  time: 1.5275  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2020/3000]  eta: 0:24:44  lr: 0.000025  loss: 0.3118  time: 1.5212  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2020/3000]  eta: 0:24:43  lr: 0.000025  loss: 0.4195  time: 1.5210  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2025/3000]  eta: 0:24:36  lr: 0.000025  loss: 0.2780  time: 1.5197  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2025/3000]  eta: 0:24:36  lr: 0.000025  loss: 1.0653  time: 1.5194  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2030/3000]  eta: 0:24:29  lr: 0.000025  loss: 0.8226  time: 1.5119  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2030/3000]  eta: 0:24:28  lr: 0.000025  loss: 0.5286  time: 1.5116  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2035/3000]  eta: 0:24:21  lr: 0.000025  loss: 0.4630  time: 1.5299  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2035/3000]  eta: 0:24:21  lr: 0.000025  loss: 0.2326  time: 1.5296  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2040/3000]  eta: 0:24:14  lr: 0.000025  loss: 0.4141  time: 1.5441  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2040/3000]  eta: 0:24:13  lr: 0.000025  loss: 0.3297  time: 1.5438  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2045/3000]  eta: 0:24:06  lr: 0.000025  loss: 0.5761  time: 1.5402  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2045/3000]  eta: 0:24:06  lr: 0.000025  loss: 0.5185  time: 1.5400  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2050/3000]  eta: 0:23:59  lr: 0.000025  loss: 0.5622  time: 1.5452  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2050/3000]  eta: 0:23:58  lr: 0.000025  loss: 0.9916  time: 1.5449  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2055/3000]  eta: 0:23:51  lr: 0.000025  loss: 0.6420  time: 1.5296  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2055/3000]  eta: 0:23:51  lr: 0.000025  loss: 0.4296  time: 1.5293  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2060/3000]  eta: 0:23:43  lr: 0.000025  loss: 0.0683  time: 1.5080  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2060/3000]  eta: 0:23:43  lr: 0.000025  loss: 0.2314  time: 1.5077  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2065/3000]  eta: 0:23:36  lr: 0.000025  loss: 0.1154  time: 1.5186  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2065/3000]  eta: 0:23:36  lr: 0.000025  loss: 0.2541  time: 1.5183  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2070/3000]  eta: 0:23:28  lr: 0.000025  loss: 0.1857  time: 1.4985  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2070/3000]  eta: 0:23:28  lr: 0.000025  loss: 0.2635  time: 1.4983  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2075/3000]  eta: 0:23:21  lr: 0.000025  loss: 0.2246  time: 1.5106  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2075/3000]  eta: 0:23:20  lr: 0.000025  loss: 0.4414  time: 1.5104  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2080/3000]  eta: 0:23:13  lr: 0.000025  loss: 0.2720  time: 1.5362  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2080/3000]  eta: 0:23:13  lr: 0.000025  loss: 0.3524  time: 1.5360  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2085/3000]  eta: 0:23:06  lr: 0.000025  loss: 0.2412  time: 1.5250  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2085/3000]  eta: 0:23:05  lr: 0.000025  loss: 0.3491  time: 1.5248  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2090/3000]  eta: 0:22:58  lr: 0.000025  loss: 0.5299  time: 1.5453  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2090/3000]  eta: 0:22:58  lr: 0.000025  loss: 0.7324  time: 1.5451  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2095/3000]  eta: 0:22:50  lr: 0.000025  loss: 0.7900  time: 1.5297  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2095/3000]  eta: 0:22:50  lr: 0.000025  loss: 0.0487  time: 1.5294  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2100/3000]  eta: 0:22:43  lr: 0.000025  loss: 0.3490  time: 1.5260  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2100/3000]  eta: 0:22:43  lr: 0.000025  loss: 0.9555  time: 1.5258  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2105/3000]  eta: 0:22:35  lr: 0.000025  loss: 0.4373  time: 1.5309  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2105/3000]  eta: 0:22:35  lr: 0.000025  loss: 0.3290  time: 1.5306  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2110/3000]  eta: 0:22:28  lr: 0.000025  loss: 0.6684  time: 1.5020  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2110/3000]  eta: 0:22:27  lr: 0.000025  loss: 0.1521  time: 1.5018  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2115/3000]  eta: 0:22:20  lr: 0.000025  loss: 0.2150  time: 1.4945  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2115/3000]  eta: 0:22:20  lr: 0.000025  loss: 0.1412  time: 1.4942  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2120/3000]  eta: 0:22:12  lr: 0.000025  loss: 0.3927  time: 1.4847  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2120/3000]  eta: 0:22:12  lr: 0.000025  loss: 0.4246  time: 1.4845  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2125/3000]  eta: 0:22:05  lr: 0.000025  loss: 0.8151  time: 1.4822  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2125/3000]  eta: 0:22:05  lr: 0.000025  loss: 0.0922  time: 1.4819  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2130/3000]  eta: 0:21:57  lr: 0.000025  loss: 0.5434  time: 1.4839  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2130/3000]  eta: 0:21:57  lr: 0.000025  loss: 0.1606  time: 1.4837  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2135/3000]  eta: 0:21:50  lr: 0.000025  loss: 0.3088  time: 1.5047  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2135/3000]  eta: 0:21:49  lr: 0.000025  loss: 0.2958  time: 1.5045  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2140/3000]  eta: 0:21:42  lr: 0.000025  loss: 0.2716  time: 1.5174  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2140/3000]  eta: 0:21:42  lr: 0.000025  loss: 0.2542  time: 1.5171  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2145/3000]  eta: 0:21:35  lr: 0.000025  loss: 0.3317  time: 1.5352  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2145/3000]  eta: 0:21:34  lr: 0.000025  loss: 0.2826  time: 1.5350  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2150/3000]  eta: 0:21:27  lr: 0.000025  loss: 0.7708  time: 1.5718  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2150/3000]  eta: 0:21:27  lr: 0.000025  loss: 1.5624  time: 1.5716  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2155/3000]  eta: 0:21:20  lr: 0.000025  loss: 0.4456  time: 1.5605  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2155/3000]  eta: 0:21:19  lr: 0.000025  loss: 0.4293  time: 1.5602  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2160/3000]  eta: 0:21:12  lr: 0.000025  loss: 0.1990  time: 1.5543  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2160/3000]  eta: 0:21:12  lr: 0.000025  loss: 0.3362  time: 1.5541  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2165/3000]  eta: 0:21:05  lr: 0.000025  loss: 0.4739  time: 1.5484  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2165/3000]  eta: 0:21:04  lr: 0.000025  loss: 0.4527  time: 1.5482  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2170/3000]  eta: 0:20:57  lr: 0.000025  loss: 0.3402  time: 1.5340  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2170/3000]  eta: 0:20:57  lr: 0.000025  loss: 0.2678  time: 1.5337  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2175/3000]  eta: 0:20:50  lr: 0.000025  loss: 0.1611  time: 1.5415  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2175/3000]  eta: 0:20:49  lr: 0.000025  loss: 0.1053  time: 1.5413  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2180/3000]  eta: 0:20:42  lr: 0.000025  loss: 0.6446  time: 1.5353  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2180/3000]  eta: 0:20:42  lr: 0.000025  loss: 0.5094  time: 1.5351  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2185/3000]  eta: 0:20:34  lr: 0.000025  loss: 0.2460  time: 1.5158  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2185/3000]  eta: 0:20:34  lr: 0.000025  loss: 0.2052  time: 1.5156  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2190/3000]  eta: 0:20:27  lr: 0.000025  loss: 1.1068  time: 1.5243  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2190/3000]  eta: 0:20:27  lr: 0.000025  loss: 0.1305  time: 1.5241  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2195/3000]  eta: 0:20:19  lr: 0.000025  loss: 0.4092  time: 1.4878  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2195/3000]  eta: 0:20:19  lr: 0.000025  loss: 0.2025  time: 1.4876  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2200/3000]  eta: 0:20:12  lr: 0.000025  loss: 0.3121  time: 1.4860  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2200/3000]  eta: 0:20:11  lr: 0.000025  loss: 0.2351  time: 1.4858  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2205/3000]  eta: 0:20:04  lr: 0.000025  loss: 0.4660  time: 1.4895  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2205/3000]  eta: 0:20:04  lr: 0.000025  loss: 0.4600  time: 1.4893  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2210/3000]  eta: 0:19:56  lr: 0.000025  loss: 0.4046  time: 1.4722  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2210/3000]  eta: 0:19:56  lr: 0.000025  loss: 0.4711  time: 1.4720  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2215/3000]  eta: 0:19:49  lr: 0.000025  loss: 0.0542  time: 1.4994  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2215/3000]  eta: 0:19:48  lr: 0.000025  loss: 0.2250  time: 1.4992  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2220/3000]  eta: 0:19:41  lr: 0.000025  loss: 0.2062  time: 1.5124  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2220/3000]  eta: 0:19:41  lr: 0.000025  loss: 0.3197  time: 1.5122  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2225/3000]  eta: 0:19:34  lr: 0.000025  loss: 0.6076  time: 1.5169  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2225/3000]  eta: 0:19:33  lr: 0.000025  loss: 0.6139  time: 1.5166  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2230/3000]  eta: 0:19:26  lr: 0.000025  loss: 0.3187  time: 1.5378  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2230/3000]  eta: 0:19:26  lr: 0.000025  loss: 0.0816  time: 1.5375  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2235/3000]  eta: 0:19:19  lr: 0.000025  loss: 0.3840  time: 1.5441  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2235/3000]  eta: 0:19:18  lr: 0.000025  loss: 0.4639  time: 1.5438  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2240/3000]  eta: 0:19:11  lr: 0.000025  loss: 0.2683  time: 1.5154  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2240/3000]  eta: 0:19:11  lr: 0.000025  loss: 0.1825  time: 1.5152  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2245/3000]  eta: 0:19:03  lr: 0.000025  loss: 0.6613  time: 1.5291  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2245/3000]  eta: 0:19:03  lr: 0.000025  loss: 0.2932  time: 1.5289  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2250/3000]  eta: 0:18:56  lr: 0.000025  loss: 0.5752  time: 1.5200  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2250/3000]  eta: 0:18:56  lr: 0.000025  loss: 0.4927  time: 1.5198  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2255/3000]  eta: 0:18:48  lr: 0.000025  loss: 0.3802  time: 1.5278  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2255/3000]  eta: 0:18:48  lr: 0.000025  loss: 0.3552  time: 1.5276  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2260/3000]  eta: 0:18:41  lr: 0.000025  loss: 0.8052  time: 1.5315  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2260/3000]  eta: 0:18:41  lr: 0.000025  loss: 0.2021  time: 1.5313  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2265/3000]  eta: 0:18:33  lr: 0.000025  loss: 0.4347  time: 1.5116  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2265/3000]  eta: 0:18:33  lr: 0.000025  loss: 0.0769  time: 1.5115  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2270/3000]  eta: 0:18:25  lr: 0.000025  loss: 0.3546  time: 1.5033  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2270/3000]  eta: 0:18:25  lr: 0.000025  loss: 0.7987  time: 1.5031  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2275/3000]  eta: 0:18:18  lr: 0.000025  loss: 0.3276  time: 1.5011  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2275/3000]  eta: 0:18:18  lr: 0.000025  loss: 0.0880  time: 1.5009  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2280/3000]  eta: 0:18:10  lr: 0.000025  loss: 0.3334  time: 1.5159  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2280/3000]  eta: 0:18:10  lr: 0.000025  loss: 0.2041  time: 1.5156  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2285/3000]  eta: 0:18:03  lr: 0.000025  loss: 0.3851  time: 1.5183  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2285/3000]  eta: 0:18:03  lr: 0.000025  loss: 1.2825  time: 1.5180  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2290/3000]  eta: 0:17:55  lr: 0.000025  loss: 0.7907  time: 1.5275  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2290/3000]  eta: 0:17:55  lr: 0.000025  loss: 0.1459  time: 1.5272  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2295/3000]  eta: 0:17:48  lr: 0.000025  loss: 1.2943  time: 1.4927  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2295/3000]  eta: 0:17:47  lr: 0.000025  loss: 0.3420  time: 1.4926  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2300/3000]  eta: 0:17:40  lr: 0.000025  loss: 0.0922  time: 1.4859  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2300/3000]  eta: 0:17:40  lr: 0.000025  loss: 0.1897  time: 1.4857  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2305/3000]  eta: 0:17:32  lr: 0.000025  loss: 0.4077  time: 1.5098  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2305/3000]  eta: 0:17:32  lr: 0.000025  loss: 0.5644  time: 1.5096  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2310/3000]  eta: 0:17:25  lr: 0.000025  loss: 0.3755  time: 1.5049  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2310/3000]  eta: 0:17:25  lr: 0.000025  loss: 0.9969  time: 1.5047  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2315/3000]  eta: 0:17:17  lr: 0.000025  loss: 0.4659  time: 1.5270  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2315/3000]  eta: 0:17:17  lr: 0.000025  loss: 0.2256  time: 1.5268  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2320/3000]  eta: 0:17:10  lr: 0.000025  loss: 0.1565  time: 1.5319  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2320/3000]  eta: 0:17:10  lr: 0.000025  loss: 0.5861  time: 1.5316  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2325/3000]  eta: 0:17:02  lr: 0.000025  loss: 0.5283  time: 1.5332  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2325/3000]  eta: 0:17:02  lr: 0.000025  loss: 0.6532  time: 1.5329  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2330/3000]  eta: 0:16:55  lr: 0.000025  loss: 0.1329  time: 1.5309  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2330/3000]  eta: 0:16:55  lr: 0.000025  loss: 0.4309  time: 1.5307  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2335/3000]  eta: 0:16:47  lr: 0.000025  loss: 0.1855  time: 1.5091  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2335/3000]  eta: 0:16:47  lr: 0.000025  loss: 0.5442  time: 1.5088  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2340/3000]  eta: 0:16:39  lr: 0.000025  loss: 0.1820  time: 1.5185  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2340/3000]  eta: 0:16:39  lr: 0.000025  loss: 0.3775  time: 1.5183  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2345/3000]  eta: 0:16:32  lr: 0.000025  loss: 0.4052  time: 1.5004  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2345/3000]  eta: 0:16:32  lr: 0.000025  loss: 0.3079  time: 1.5001  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2350/3000]  eta: 0:16:24  lr: 0.000025  loss: 0.8101  time: 1.5065  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2350/3000]  eta: 0:16:24  lr: 0.000025  loss: 0.1471  time: 1.5062  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2355/3000]  eta: 0:16:17  lr: 0.000025  loss: 0.9222  time: 1.5399  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2355/3000]  eta: 0:16:17  lr: 0.000025  loss: 0.7578  time: 1.5397  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2360/3000]  eta: 0:16:09  lr: 0.000025  loss: 0.2102  time: 1.5265  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2360/3000]  eta: 0:16:09  lr: 0.000025  loss: 0.4377  time: 1.5262  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2365/3000]  eta: 0:16:02  lr: 0.000025  loss: 0.4339  time: 1.5457  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2365/3000]  eta: 0:16:02  lr: 0.000025  loss: 0.8168  time: 1.5455  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2370/3000]  eta: 0:15:54  lr: 0.000025  loss: 0.1871  time: 1.5316  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2370/3000]  eta: 0:15:54  lr: 0.000025  loss: 0.2533  time: 1.5314  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2375/3000]  eta: 0:15:47  lr: 0.000025  loss: 0.1269  time: 1.5214  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2375/3000]  eta: 0:15:46  lr: 0.000025  loss: 0.3703  time: 1.5212  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2380/3000]  eta: 0:15:39  lr: 0.000025  loss: 0.6055  time: 1.5319  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2380/3000]  eta: 0:15:39  lr: 0.000025  loss: 0.7207  time: 1.5317  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2385/3000]  eta: 0:15:31  lr: 0.000025  loss: 0.2804  time: 1.5274  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2385/3000]  eta: 0:15:31  lr: 0.000025  loss: 0.1550  time: 1.5272  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2390/3000]  eta: 0:15:24  lr: 0.000025  loss: 1.3286  time: 1.5099  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2390/3000]  eta: 0:15:24  lr: 0.000025  loss: 0.8429  time: 1.5096  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2395/3000]  eta: 0:15:16  lr: 0.000025  loss: 0.9169  time: 1.5181  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2395/3000]  eta: 0:15:16  lr: 0.000025  loss: 0.2924  time: 1.5178  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2400/3000]  eta: 0:15:09  lr: 0.000025  loss: 0.6892  time: 1.5159  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2400/3000]  eta: 0:15:09  lr: 0.000025  loss: 0.4124  time: 1.5156  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2405/3000]  eta: 0:15:01  lr: 0.000025  loss: 0.3687  time: 1.5086  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2405/3000]  eta: 0:15:01  lr: 0.000025  loss: 0.9063  time: 1.5084  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2410/3000]  eta: 0:14:54  lr: 0.000025  loss: 0.7513  time: 1.5382  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2410/3000]  eta: 0:14:53  lr: 0.000025  loss: 0.3079  time: 1.5380  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2415/3000]  eta: 0:14:46  lr: 0.000025  loss: 0.1412  time: 1.5316  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2415/3000]  eta: 0:14:46  lr: 0.000025  loss: 0.2466  time: 1.5313  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2420/3000]  eta: 0:14:38  lr: 0.000025  loss: 0.1543  time: 1.5257  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2420/3000]  eta: 0:14:38  lr: 0.000025  loss: 0.2053  time: 1.5254  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2425/3000]  eta: 0:14:31  lr: 0.000025  loss: 0.2824  time: 1.5018  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2425/3000]  eta: 0:14:31  lr: 0.000025  loss: 0.4346  time: 1.5016  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2430/3000]  eta: 0:14:23  lr: 0.000025  loss: 0.5919  time: 1.4905  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2430/3000]  eta: 0:14:23  lr: 0.000025  loss: 0.9504  time: 1.4902  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2435/3000]  eta: 0:14:16  lr: 0.000025  loss: 0.2601  time: 1.4940  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2435/3000]  eta: 0:14:15  lr: 0.000025  loss: 0.5349  time: 1.4937  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2440/3000]  eta: 0:14:08  lr: 0.000025  loss: 0.2752  time: 1.4831  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2440/3000]  eta: 0:14:08  lr: 0.000025  loss: 0.1766  time: 1.4829  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2445/3000]  eta: 0:14:00  lr: 0.000025  loss: 0.2681  time: 1.4759  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2445/3000]  eta: 0:14:00  lr: 0.000025  loss: 0.3342  time: 1.4757  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2450/3000]  eta: 0:13:53  lr: 0.000025  loss: 0.2409  time: 1.4855  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2450/3000]  eta: 0:13:53  lr: 0.000025  loss: 0.2347  time: 1.4852  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2455/3000]  eta: 0:13:45  lr: 0.000025  loss: 0.5385  time: 1.4723  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2455/3000]  eta: 0:13:45  lr: 0.000025  loss: 0.2963  time: 1.4721  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2460/3000]  eta: 0:13:38  lr: 0.000025  loss: 0.2230  time: 1.4754  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2460/3000]  eta: 0:13:37  lr: 0.000025  loss: 0.7950  time: 1.4752  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2465/3000]  eta: 0:13:30  lr: 0.000025  loss: 0.1595  time: 1.5047  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2465/3000]  eta: 0:13:30  lr: 0.000025  loss: 1.2551  time: 1.5044  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2470/3000]  eta: 0:13:22  lr: 0.000025  loss: 0.5206  time: 1.4788  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2470/3000]  eta: 0:13:22  lr: 0.000025  loss: 0.2479  time: 1.4786  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2475/3000]  eta: 0:13:15  lr: 0.000025  loss: 0.3667  time: 1.4893  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2475/3000]  eta: 0:13:15  lr: 0.000025  loss: 0.2499  time: 1.4891  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2480/3000]  eta: 0:13:07  lr: 0.000025  loss: 0.5745  time: 1.5059  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2480/3000]  eta: 0:13:07  lr: 0.000025  loss: 1.2369  time: 1.5056  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2485/3000]  eta: 0:13:00  lr: 0.000025  loss: 0.3143  time: 1.5005  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2485/3000]  eta: 0:12:59  lr: 0.000025  loss: 0.3631  time: 1.5002  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2490/3000]  eta: 0:12:52  lr: 0.000025  loss: 0.4051  time: 1.5374  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2490/3000]  eta: 0:12:52  lr: 0.000025  loss: 0.6453  time: 1.5372  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2495/3000]  eta: 0:12:45  lr: 0.000025  loss: 0.3539  time: 1.5540  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2495/3000]  eta: 0:12:44  lr: 0.000025  loss: 0.3811  time: 1.5538  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2500/3000]  eta: 0:12:37  lr: 0.000025  loss: 0.2209  time: 1.5361  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2500/3000]  eta: 0:12:37  lr: 0.000025  loss: 0.2721  time: 1.5359  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2505/3000]  eta: 0:12:29  lr: 0.000025  loss: 0.3290  time: 1.5431  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2505/3000]  eta: 0:12:29  lr: 0.000025  loss: 0.3710  time: 1.5429  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2510/3000]  eta: 0:12:22  lr: 0.000025  loss: 0.1339  time: 1.5345  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2510/3000]  eta: 0:12:22  lr: 0.000025  loss: 0.4048  time: 1.5343  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2515/3000]  eta: 0:12:14  lr: 0.000025  loss: 0.2758  time: 1.5316  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2515/3000]  eta: 0:12:14  lr: 0.000025  loss: 0.5621  time: 1.5314  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2520/3000]  eta: 0:12:07  lr: 0.000025  loss: 0.2965  time: 1.5332  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2520/3000]  eta: 0:12:07  lr: 0.000025  loss: 0.6603  time: 1.5330  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2525/3000]  eta: 0:11:59  lr: 0.000025  loss: 0.4140  time: 1.5148  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2525/3000]  eta: 0:11:59  lr: 0.000025  loss: 0.3849  time: 1.5145  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2530/3000]  eta: 0:11:51  lr: 0.000025  loss: 0.8250  time: 1.4804  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2530/3000]  eta: 0:11:51  lr: 0.000025  loss: 0.3750  time: 1.4802  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2535/3000]  eta: 0:11:44  lr: 0.000025  loss: 0.2475  time: 1.4747  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2535/3000]  eta: 0:11:44  lr: 0.000025  loss: 0.1817  time: 1.4745  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2540/3000]  eta: 0:11:36  lr: 0.000025  loss: 0.3491  time: 1.4754  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2540/3000]  eta: 0:11:36  lr: 0.000025  loss: 0.2803  time: 1.4752  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2545/3000]  eta: 0:11:29  lr: 0.000025  loss: 0.4345  time: 1.4791  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2545/3000]  eta: 0:11:29  lr: 0.000025  loss: 0.4570  time: 1.4788  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2550/3000]  eta: 0:11:21  lr: 0.000025  loss: 0.0967  time: 1.5201  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2550/3000]  eta: 0:11:21  lr: 0.000025  loss: 0.7119  time: 1.5198  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2555/3000]  eta: 0:11:14  lr: 0.000025  loss: 0.4755  time: 1.5029  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2555/3000]  eta: 0:11:13  lr: 0.000025  loss: 0.2635  time: 1.5026  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2560/3000]  eta: 0:11:06  lr: 0.000025  loss: 0.1476  time: 1.4954  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2560/3000]  eta: 0:11:06  lr: 0.000025  loss: 0.2209  time: 1.4952  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2565/3000]  eta: 0:10:58  lr: 0.000025  loss: 0.2810  time: 1.4921  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2565/3000]  eta: 0:10:58  lr: 0.000025  loss: 1.0215  time: 1.4919  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2570/3000]  eta: 0:10:51  lr: 0.000025  loss: 0.2103  time: 1.4830  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2570/3000]  eta: 0:10:51  lr: 0.000025  loss: 0.4573  time: 1.4828  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2575/3000]  eta: 0:10:43  lr: 0.000025  loss: 0.4745  time: 1.5077  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2575/3000]  eta: 0:10:43  lr: 0.000025  loss: 0.3946  time: 1.5075  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2580/3000]  eta: 0:10:36  lr: 0.000025  loss: 0.2728  time: 1.5174  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2580/3000]  eta: 0:10:36  lr: 0.000025  loss: 0.3211  time: 1.5172  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2585/3000]  eta: 0:10:28  lr: 0.000025  loss: 0.3715  time: 1.5312  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2585/3000]  eta: 0:10:28  lr: 0.000025  loss: 0.3126  time: 1.5309  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2590/3000]  eta: 0:10:21  lr: 0.000025  loss: 0.2039  time: 1.5290  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2590/3000]  eta: 0:10:20  lr: 0.000025  loss: 0.5117  time: 1.5287  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2595/3000]  eta: 0:10:13  lr: 0.000025  loss: 0.2607  time: 1.5247  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2595/3000]  eta: 0:10:13  lr: 0.000025  loss: 0.1849  time: 1.5244  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2600/3000]  eta: 0:10:05  lr: 0.000025  loss: 0.3551  time: 1.5282  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2600/3000]  eta: 0:10:05  lr: 0.000025  loss: 0.5373  time: 1.5279  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2605/3000]  eta: 0:09:58  lr: 0.000025  loss: 0.2936  time: 1.5136  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2605/3000]  eta: 0:09:58  lr: 0.000025  loss: 0.1546  time: 1.5134  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2610/3000]  eta: 0:09:50  lr: 0.000025  loss: 0.1785  time: 1.5257  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2610/3000]  eta: 0:09:50  lr: 0.000025  loss: 0.6769  time: 1.5255  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2615/3000]  eta: 0:09:43  lr: 0.000025  loss: 0.2063  time: 1.4976  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2615/3000]  eta: 0:09:43  lr: 0.000025  loss: 0.1155  time: 1.4974  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2620/3000]  eta: 0:09:35  lr: 0.000025  loss: 0.6749  time: 1.4784  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2620/3000]  eta: 0:09:35  lr: 0.000025  loss: 1.0777  time: 1.4782  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2625/3000]  eta: 0:09:27  lr: 0.000025  loss: 0.2728  time: 1.4972  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2625/3000]  eta: 0:09:27  lr: 0.000025  loss: 0.6259  time: 1.4970  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2630/3000]  eta: 0:09:20  lr: 0.000025  loss: 0.1330  time: 1.4633  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2630/3000]  eta: 0:09:20  lr: 0.000025  loss: 0.4794  time: 1.4629  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2635/3000]  eta: 0:09:12  lr: 0.000025  loss: 0.3234  time: 1.4821  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2635/3000]  eta: 0:09:12  lr: 0.000025  loss: 0.5181  time: 1.4818  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2640/3000]  eta: 0:09:05  lr: 0.000025  loss: 0.9124  time: 1.4992  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2640/3000]  eta: 0:09:05  lr: 0.000025  loss: 0.1814  time: 1.4989  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2645/3000]  eta: 0:08:57  lr: 0.000025  loss: 0.5362  time: 1.4940  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2645/3000]  eta: 0:08:57  lr: 0.000025  loss: 0.4990  time: 1.4938  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2650/3000]  eta: 0:08:49  lr: 0.000025  loss: 0.3928  time: 1.4977  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2650/3000]  eta: 0:08:49  lr: 0.000025  loss: 0.4556  time: 1.4976  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2655/3000]  eta: 0:08:42  lr: 0.000025  loss: 0.6990  time: 1.4898  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2655/3000]  eta: 0:08:42  lr: 0.000025  loss: 0.3109  time: 1.4896  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2660/3000]  eta: 0:08:34  lr: 0.000025  loss: 0.4208  time: 1.4993  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2660/3000]  eta: 0:08:34  lr: 0.000025  loss: 0.3556  time: 1.4992  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2665/3000]  eta: 0:08:27  lr: 0.000025  loss: 0.4021  time: 1.4912  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2665/3000]  eta: 0:08:27  lr: 0.000025  loss: 0.6451  time: 1.4909  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2670/3000]  eta: 0:08:19  lr: 0.000025  loss: 0.0837  time: 1.4892  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2670/3000]  eta: 0:08:19  lr: 0.000025  loss: 0.0994  time: 1.4889  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2675/3000]  eta: 0:08:12  lr: 0.000025  loss: 0.4808  time: 1.4873  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2675/3000]  eta: 0:08:11  lr: 0.000025  loss: 0.4523  time: 1.4871  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2680/3000]  eta: 0:08:04  lr: 0.000025  loss: 0.3286  time: 1.4833  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2680/3000]  eta: 0:08:04  lr: 0.000025  loss: 0.5893  time: 1.4830  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2685/3000]  eta: 0:07:56  lr: 0.000025  loss: 0.4900  time: 1.4915  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2685/3000]  eta: 0:07:56  lr: 0.000025  loss: 0.2855  time: 1.4913  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2690/3000]  eta: 0:07:49  lr: 0.000025  loss: 0.4162  time: 1.4979  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2690/3000]  eta: 0:07:49  lr: 0.000025  loss: 0.1254  time: 1.4977  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2695/3000]  eta: 0:07:41  lr: 0.000025  loss: 0.4422  time: 1.4939  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2695/3000]  eta: 0:07:41  lr: 0.000025  loss: 0.4613  time: 1.4936  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2700/3000]  eta: 0:07:34  lr: 0.000025  loss: 0.3283  time: 1.5002  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2700/3000]  eta: 0:07:34  lr: 0.000025  loss: 1.2907  time: 1.5000  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2705/3000]  eta: 0:07:26  lr: 0.000025  loss: 0.1740  time: 1.5096  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2705/3000]  eta: 0:07:26  lr: 0.000025  loss: 0.4834  time: 1.5095  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2710/3000]  eta: 0:07:19  lr: 0.000025  loss: 0.3158  time: 1.5247  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2710/3000]  eta: 0:07:18  lr: 0.000025  loss: 1.1182  time: 1.5245  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2715/3000]  eta: 0:07:11  lr: 0.000025  loss: 0.2536  time: 1.5281  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2715/3000]  eta: 0:07:11  lr: 0.000025  loss: 0.5443  time: 1.5279  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2720/3000]  eta: 0:07:03  lr: 0.000025  loss: 0.3201  time: 1.5405  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2720/3000]  eta: 0:07:03  lr: 0.000025  loss: 0.3200  time: 1.5404  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2725/3000]  eta: 0:06:56  lr: 0.000025  loss: 0.2033  time: 1.5325  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2725/3000]  eta: 0:06:56  lr: 0.000025  loss: 0.3104  time: 1.5322  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2730/3000]  eta: 0:06:48  lr: 0.000025  loss: 0.6273  time: 1.5244  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2730/3000]  eta: 0:06:48  lr: 0.000025  loss: 0.8139  time: 1.5242  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2735/3000]  eta: 0:06:41  lr: 0.000025  loss: 0.1475  time: 1.5388  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2735/3000]  eta: 0:06:41  lr: 0.000025  loss: 0.3790  time: 1.5386  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2740/3000]  eta: 0:06:33  lr: 0.000025  loss: 0.2466  time: 1.5315  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2740/3000]  eta: 0:06:33  lr: 0.000025  loss: 0.2497  time: 1.5312  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2745/3000]  eta: 0:06:26  lr: 0.000025  loss: 0.2476  time: 1.5133  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2745/3000]  eta: 0:06:26  lr: 0.000025  loss: 0.3714  time: 1.5131  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2750/3000]  eta: 0:06:18  lr: 0.000025  loss: 0.1542  time: 1.5258  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2750/3000]  eta: 0:06:18  lr: 0.000025  loss: 0.3701  time: 1.5255  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2755/3000]  eta: 0:06:10  lr: 0.000025  loss: 0.4836  time: 1.5275  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2755/3000]  eta: 0:06:10  lr: 0.000025  loss: 0.4729  time: 1.5273  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2760/3000]  eta: 0:06:03  lr: 0.000025  loss: 0.2064  time: 1.5067  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2760/3000]  eta: 0:06:03  lr: 0.000025  loss: 0.5300  time: 1.5065  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2765/3000]  eta: 0:05:55  lr: 0.000025  loss: 0.3142  time: 1.5155  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2765/3000]  eta: 0:05:55  lr: 0.000025  loss: 0.4152  time: 1.5153  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2770/3000]  eta: 0:05:48  lr: 0.000025  loss: 0.9220  time: 1.5093  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2770/3000]  eta: 0:05:48  lr: 0.000025  loss: 0.3002  time: 1.5091  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2775/3000]  eta: 0:05:40  lr: 0.000025  loss: 0.3575  time: 1.4968  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2775/3000]  eta: 0:05:40  lr: 0.000025  loss: 0.3687  time: 1.4966  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2780/3000]  eta: 0:05:33  lr: 0.000025  loss: 0.2108  time: 1.4979  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2780/3000]  eta: 0:05:33  lr: 0.000025  loss: 0.3988  time: 1.4977  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2785/3000]  eta: 0:05:25  lr: 0.000025  loss: 0.2282  time: 1.5035  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2785/3000]  eta: 0:05:25  lr: 0.000025  loss: 0.4996  time: 1.5032  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2790/3000]  eta: 0:05:17  lr: 0.000025  loss: 0.6642  time: 1.5128  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2790/3000]  eta: 0:05:17  lr: 0.000025  loss: 0.2927  time: 1.5125  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2795/3000]  eta: 0:05:10  lr: 0.000025  loss: 0.5340  time: 1.5193  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2795/3000]  eta: 0:05:10  lr: 0.000025  loss: 0.3811  time: 1.5191  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2800/3000]  eta: 0:05:02  lr: 0.000025  loss: 0.5082  time: 1.5401  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2800/3000]  eta: 0:05:02  lr: 0.000025  loss: 0.1755  time: 1.5398  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2805/3000]  eta: 0:04:55  lr: 0.000025  loss: 0.3898  time: 1.5483  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2805/3000]  eta: 0:04:55  lr: 0.000025  loss: 0.4948  time: 1.5482  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2810/3000]  eta: 0:04:47  lr: 0.000025  loss: 0.4292  time: 1.5546  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2810/3000]  eta: 0:04:47  lr: 0.000025  loss: 0.6074  time: 1.5543  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2815/3000]  eta: 0:04:40  lr: 0.000025  loss: 0.1178  time: 1.5561  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2815/3000]  eta: 0:04:40  lr: 0.000025  loss: 0.3816  time: 1.5558  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2820/3000]  eta: 0:04:32  lr: 0.000025  loss: 0.5830  time: 1.5497  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2820/3000]  eta: 0:04:32  lr: 0.000025  loss: 0.5342  time: 1.5494  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2825/3000]  eta: 0:04:25  lr: 0.000025  loss: 0.5595  time: 1.5379  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2825/3000]  eta: 0:04:24  lr: 0.000025  loss: 0.5415  time: 1.5376  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2830/3000]  eta: 0:04:17  lr: 0.000025  loss: 0.2798  time: 1.5321  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2830/3000]  eta: 0:04:17  lr: 0.000025  loss: 0.2577  time: 1.5319  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2835/3000]  eta: 0:04:09  lr: 0.000025  loss: 0.5491  time: 1.5325  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2835/3000]  eta: 0:04:09  lr: 0.000025  loss: 0.4747  time: 1.5322  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2840/3000]  eta: 0:04:02  lr: 0.000025  loss: 0.4524  time: 1.5148  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2840/3000]  eta: 0:04:02  lr: 0.000025  loss: 0.4297  time: 1.5146  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2845/3000]  eta: 0:03:54  lr: 0.000025  loss: 0.1388  time: 1.5362  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2845/3000]  eta: 0:03:54  lr: 0.000025  loss: 0.4211  time: 1.5360  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2850/3000]  eta: 0:03:47  lr: 0.000025  loss: 0.7440  time: 1.5420  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2850/3000]  eta: 0:03:47  lr: 0.000025  loss: 0.1950  time: 1.5418  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2855/3000]  eta: 0:03:39  lr: 0.000025  loss: 1.3362  time: 1.5559  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2855/3000]  eta: 0:03:39  lr: 0.000025  loss: 0.3693  time: 1.5557  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2860/3000]  eta: 0:03:32  lr: 0.000025  loss: 0.4598  time: 1.5733  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2860/3000]  eta: 0:03:32  lr: 0.000025  loss: 0.0707  time: 1.5731  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2865/3000]  eta: 0:03:24  lr: 0.000025  loss: 0.3407  time: 1.5705  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2865/3000]  eta: 0:03:24  lr: 0.000025  loss: 0.4894  time: 1.5703  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2870/3000]  eta: 0:03:16  lr: 0.000025  loss: 0.4315  time: 1.5691  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2870/3000]  eta: 0:03:16  lr: 0.000025  loss: 0.4432  time: 1.5688  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2875/3000]  eta: 0:03:09  lr: 0.000025  loss: 0.1662  time: 1.5494  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2875/3000]  eta: 0:03:09  lr: 0.000025  loss: 0.3017  time: 1.5492  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2880/3000]  eta: 0:03:01  lr: 0.000025  loss: 0.7672  time: 1.5419  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2880/3000]  eta: 0:03:01  lr: 0.000025  loss: 0.4488  time: 1.5418  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2885/3000]  eta: 0:02:54  lr: 0.000025  loss: 0.4765  time: 1.5107  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2885/3000]  eta: 0:02:54  lr: 0.000025  loss: 0.2565  time: 1.5104  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2890/3000]  eta: 0:02:46  lr: 0.000025  loss: 0.9869  time: 1.4930  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2890/3000]  eta: 0:02:46  lr: 0.000025  loss: 0.1170  time: 1.4927  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2895/3000]  eta: 0:02:39  lr: 0.000025  loss: 0.3195  time: 1.5002  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2895/3000]  eta: 0:02:39  lr: 0.000025  loss: 0.2745  time: 1.4999  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2900/3000]  eta: 0:02:31  lr: 0.000025  loss: 1.6719  time: 1.5022  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2900/3000]  eta: 0:02:31  lr: 0.000025  loss: 0.9721  time: 1.5019  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2905/3000]  eta: 0:02:23  lr: 0.000025  loss: 0.1348  time: 1.5232  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2905/3000]  eta: 0:02:23  lr: 0.000025  loss: 0.5652  time: 1.5230  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2910/3000]  eta: 0:02:16  lr: 0.000025  loss: 0.2498  time: 1.5278  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2910/3000]  eta: 0:02:16  lr: 0.000025  loss: 0.0774  time: 1.5276  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2915/3000]  eta: 0:02:08  lr: 0.000025  loss: 0.2542  time: 1.5007  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2915/3000]  eta: 0:02:08  lr: 0.000025  loss: 0.8871  time: 1.5005  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2920/3000]  eta: 0:02:01  lr: 0.000025  loss: 0.1524  time: 1.4782  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2920/3000]  eta: 0:02:01  lr: 0.000025  loss: 0.2527  time: 1.4780  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2925/3000]  eta: 0:01:53  lr: 0.000025  loss: 0.7804  time: 1.4754  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2925/3000]  eta: 0:01:53  lr: 0.000025  loss: 0.2591  time: 1.4752  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2930/3000]  eta: 0:01:46  lr: 0.000025  loss: 0.3219  time: 1.4681  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2930/3000]  eta: 0:01:46  lr: 0.000025  loss: 0.5754  time: 1.4680  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2935/3000]  eta: 0:01:38  lr: 0.000025  loss: 0.2720  time: 1.4854  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2935/3000]  eta: 0:01:38  lr: 0.000025  loss: 0.3722  time: 1.4853  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2940/3000]  eta: 0:01:30  lr: 0.000025  loss: 0.4020  time: 1.4962  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2940/3000]  eta: 0:01:30  lr: 0.000025  loss: 0.4696  time: 1.4960  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2945/3000]  eta: 0:01:23  lr: 0.000025  loss: 0.3762  time: 1.4974  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2945/3000]  eta: 0:01:23  lr: 0.000025  loss: 0.4172  time: 1.4971  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2950/3000]  eta: 0:01:15  lr: 0.000025  loss: 0.1796  time: 1.5147  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2950/3000]  eta: 0:01:15  lr: 0.000025  loss: 0.2185  time: 1.5145  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2955/3000]  eta: 0:01:08  lr: 0.000025  loss: 0.3466  time: 1.5233  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2955/3000]  eta: 0:01:08  lr: 0.000025  loss: 0.2883  time: 1.5230  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2960/3000]  eta: 0:01:00  lr: 0.000025  loss: 0.2593  time: 1.5203  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2960/3000]  eta: 0:01:00  lr: 0.000025  loss: 0.2157  time: 1.5201  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2965/3000]  eta: 0:00:53  lr: 0.000025  loss: 0.6781  time: 1.5115  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2965/3000]  eta: 0:00:53  lr: 0.000025  loss: 0.1864  time: 1.5113  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2970/3000]  eta: 0:00:45  lr: 0.000025  loss: 0.3926  time: 1.5154  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2970/3000]  eta: 0:00:45  lr: 0.000025  loss: 0.3564  time: 1.5151  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2975/3000]  eta: 0:00:37  lr: 0.000025  loss: 0.9799  time: 1.5307  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2975/3000]  eta: 0:00:37  lr: 0.000025  loss: 0.6409  time: 1.5304  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2980/3000]  eta: 0:00:30  lr: 0.000025  loss: 0.2662  time: 1.5230  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2980/3000]  eta: 0:00:30  lr: 0.000025  loss: 0.2972  time: 1.5228  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2985/3000]  eta: 0:00:22  lr: 0.000025  loss: 0.3136  time: 1.5317  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2985/3000]  eta: 0:00:22  lr: 0.000025  loss: 0.6793  time: 1.5314  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2990/3000]  eta: 0:00:15  lr: 0.000025  loss: 0.3286  time: 1.5157  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2990/3000]  eta: 0:00:15  lr: 0.000025  loss: 0.3347  time: 1.5155  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2995/3000]  eta: 0:00:07  lr: 0.000025  loss: 0.1995  time: 1.5057  data: 0.0000  max mem: 18596
Train: data epoch: [9]  [2995/3000]  eta: 0:00:07  lr: 0.000025  loss: 0.2090  time: 1.5054  data: 0.0000  max mem: 18406
Train: data epoch: [9]  [2999/3000]  eta: 0:00:01  lr: 0.000025  loss: 0.2966  time: 1.5131  data: 0.0000  max mem: 18596
Train: data epoch: [9] Total time: 1:15:44 (1.5149 s / it)
Train: data epoch: [9]  [2999/3000]  eta: 0:00:01  lr: 0.000025  loss: 0.1859  time: 1.5128  data: 0.0000  max mem: 18406
Train: data epoch: [9] Total time: 1:15:44 (1.5149 s / it)
2025-01-19 12:18:08,388 [INFO] Averaged stats: lr: 0.0000  loss: 0.4191
2025-01-19 12:18:08,394 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [9]  [0/1]  eta: 0:00:00    time: 0.7788  data: 0.4921  max mem: 18406
Eval: data epoch: [9] Total time: 0:00:00 (0.8856 s / it)
Eval: data epoch: [9]  [0/1]  eta: 0:00:00    time: 0.9303  data: 0.6459  max mem: 18596
Eval: data epoch: [9] Total time: 0:00:01 (1.0756 s / it)
2025-01-19 12:18:09,495 [INFO] Saving checkpoint at epoch 9 to outputs_stage1_only/202501182338/checkpoint_9.pth.
2025-01-19 12:18:11,887 [INFO] Training Phase
2025-01-19 12:18:11,895 [INFO] Start training epoch 10, 3000 iters per inner epoch.
Train: data epoch: [10]  [   0/3000]  eta: 1:18:44  lr: 0.000025  loss: 0.1651  time: 1.5750  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [   0/3000]  eta: 1:18:42  lr: 0.000025  loss: 1.4171  time: 1.5742  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [   5/3000]  eta: 1:15:58  lr: 0.000025  loss: 0.6139  time: 1.5221  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [   5/3000]  eta: 1:15:57  lr: 0.000025  loss: 0.1392  time: 1.5217  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [  10/3000]  eta: 1:15:52  lr: 0.000025  loss: 0.7110  time: 1.5225  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [  10/3000]  eta: 1:15:51  lr: 0.000025  loss: 0.2446  time: 1.5222  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [  15/3000]  eta: 1:16:44  lr: 0.000025  loss: 0.6838  time: 1.5424  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [  15/3000]  eta: 1:16:43  lr: 0.000025  loss: 0.1903  time: 1.5422  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [  20/3000]  eta: 1:17:12  lr: 0.000025  loss: 0.3770  time: 1.5535  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [  20/3000]  eta: 1:17:11  lr: 0.000025  loss: 0.4594  time: 1.5533  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [  25/3000]  eta: 1:16:56  lr: 0.000025  loss: 0.7642  time: 1.5607  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [  25/3000]  eta: 1:16:55  lr: 0.000025  loss: 0.4413  time: 1.5605  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [  30/3000]  eta: 1:16:21  lr: 0.000025  loss: 0.1118  time: 1.5538  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [  30/3000]  eta: 1:16:21  lr: 0.000025  loss: 0.6814  time: 1.5536  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [  35/3000]  eta: 1:16:06  lr: 0.000025  loss: 0.3527  time: 1.5385  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [  35/3000]  eta: 1:16:06  lr: 0.000025  loss: 0.3954  time: 1.5383  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [  40/3000]  eta: 1:15:59  lr: 0.000025  loss: 0.6273  time: 1.5254  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [  40/3000]  eta: 1:15:58  lr: 0.000025  loss: 0.2376  time: 1.5251  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [  45/3000]  eta: 1:15:25  lr: 0.000025  loss: 0.3339  time: 1.5048  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [  45/3000]  eta: 1:15:24  lr: 0.000025  loss: 0.1427  time: 1.5046  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [  50/3000]  eta: 1:15:29  lr: 0.000025  loss: 0.3693  time: 1.5239  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [  50/3000]  eta: 1:15:28  lr: 0.000025  loss: 0.6874  time: 1.5237  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [  55/3000]  eta: 1:15:18  lr: 0.000025  loss: 0.4225  time: 1.5234  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [  55/3000]  eta: 1:15:17  lr: 0.000025  loss: 0.2462  time: 1.5231  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [  60/3000]  eta: 1:15:10  lr: 0.000025  loss: 0.6612  time: 1.5215  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [  60/3000]  eta: 1:15:09  lr: 0.000025  loss: 0.5692  time: 1.5213  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [  65/3000]  eta: 1:15:09  lr: 0.000025  loss: 0.1139  time: 1.5483  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [  65/3000]  eta: 1:15:08  lr: 0.000025  loss: 0.4557  time: 1.5481  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [  70/3000]  eta: 1:15:03  lr: 0.000025  loss: 0.0945  time: 1.5408  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [  70/3000]  eta: 1:15:02  lr: 0.000025  loss: 0.0987  time: 1.5406  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [  75/3000]  eta: 1:14:35  lr: 0.000025  loss: 0.3467  time: 1.5181  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [  75/3000]  eta: 1:14:34  lr: 0.000025  loss: 0.2646  time: 1.5179  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [  80/3000]  eta: 1:14:23  lr: 0.000025  loss: 0.3689  time: 1.5120  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [  80/3000]  eta: 1:14:22  lr: 0.000025  loss: 0.2865  time: 1.5117  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [  85/3000]  eta: 1:14:25  lr: 0.000025  loss: 0.2903  time: 1.5167  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [  85/3000]  eta: 1:14:24  lr: 0.000025  loss: 0.4770  time: 1.5165  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [  90/3000]  eta: 1:14:20  lr: 0.000025  loss: 0.3899  time: 1.5178  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [  90/3000]  eta: 1:14:19  lr: 0.000025  loss: 0.5369  time: 1.5175  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [  95/3000]  eta: 1:14:20  lr: 0.000025  loss: 0.1314  time: 1.5563  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [  95/3000]  eta: 1:14:19  lr: 0.000025  loss: 0.8377  time: 1.5561  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 100/3000]  eta: 1:14:11  lr: 0.000025  loss: 0.0957  time: 1.5606  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 100/3000]  eta: 1:14:10  lr: 0.000025  loss: 0.3132  time: 1.5604  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 105/3000]  eta: 1:14:01  lr: 0.000025  loss: 0.1875  time: 1.5443  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 105/3000]  eta: 1:14:00  lr: 0.000025  loss: 0.2705  time: 1.5441  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 110/3000]  eta: 1:13:44  lr: 0.000025  loss: 0.7123  time: 1.5236  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 110/3000]  eta: 1:13:43  lr: 0.000025  loss: 0.3093  time: 1.5234  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 115/3000]  eta: 1:13:36  lr: 0.000025  loss: 0.2159  time: 1.5089  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 115/3000]  eta: 1:13:35  lr: 0.000025  loss: 0.0798  time: 1.5087  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 120/3000]  eta: 1:13:30  lr: 0.000025  loss: 0.3506  time: 1.5136  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 120/3000]  eta: 1:13:29  lr: 0.000025  loss: 0.2217  time: 1.5128  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 125/3000]  eta: 1:13:26  lr: 0.000025  loss: 0.5462  time: 1.5237  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 125/3000]  eta: 1:13:25  lr: 0.000025  loss: 0.4011  time: 1.5228  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 130/3000]  eta: 1:13:24  lr: 0.000025  loss: 0.1989  time: 1.5546  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 130/3000]  eta: 1:13:23  lr: 0.000025  loss: 0.9461  time: 1.5538  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 135/3000]  eta: 1:13:20  lr: 0.000025  loss: 0.2927  time: 1.5660  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 135/3000]  eta: 1:13:19  lr: 0.000025  loss: 0.9867  time: 1.5652  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 140/3000]  eta: 1:13:13  lr: 0.000025  loss: 0.2895  time: 1.5642  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 140/3000]  eta: 1:13:12  lr: 0.000025  loss: 0.3883  time: 1.5640  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 145/3000]  eta: 1:13:09  lr: 0.000025  loss: 0.7669  time: 1.5687  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 145/3000]  eta: 1:13:08  lr: 0.000025  loss: 0.5896  time: 1.5685  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 150/3000]  eta: 1:13:06  lr: 0.000025  loss: 0.5565  time: 1.5688  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 150/3000]  eta: 1:13:05  lr: 0.000025  loss: 0.6604  time: 1.5686  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 155/3000]  eta: 1:12:47  lr: 0.000025  loss: 0.4166  time: 1.5282  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 155/3000]  eta: 1:12:46  lr: 0.000025  loss: 0.5463  time: 1.5280  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 160/3000]  eta: 1:12:44  lr: 0.000025  loss: 0.2112  time: 1.5405  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 160/3000]  eta: 1:12:43  lr: 0.000025  loss: 0.7027  time: 1.5403  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 165/3000]  eta: 1:12:36  lr: 0.000025  loss: 0.3268  time: 1.5315  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 165/3000]  eta: 1:12:35  lr: 0.000025  loss: 0.2486  time: 1.5312  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 170/3000]  eta: 1:12:21  lr: 0.000025  loss: 0.5891  time: 1.4967  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 170/3000]  eta: 1:12:20  lr: 0.000025  loss: 0.3552  time: 1.4965  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 175/3000]  eta: 1:12:14  lr: 0.000025  loss: 0.8718  time: 1.5289  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 175/3000]  eta: 1:12:13  lr: 0.000025  loss: 0.3895  time: 1.5287  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 180/3000]  eta: 1:12:04  lr: 0.000025  loss: 0.0801  time: 1.5095  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 180/3000]  eta: 1:12:04  lr: 0.000025  loss: 0.2363  time: 1.5092  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 185/3000]  eta: 1:11:55  lr: 0.000025  loss: 0.1390  time: 1.5003  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 185/3000]  eta: 1:11:54  lr: 0.000025  loss: 0.2372  time: 1.5001  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 190/3000]  eta: 1:11:42  lr: 0.000025  loss: 0.5389  time: 1.5057  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 190/3000]  eta: 1:11:41  lr: 0.000025  loss: 0.1205  time: 1.5055  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 195/3000]  eta: 1:11:36  lr: 0.000025  loss: 0.8056  time: 1.5095  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 195/3000]  eta: 1:11:35  lr: 0.000025  loss: 0.8584  time: 1.5091  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 200/3000]  eta: 1:11:31  lr: 0.000025  loss: 0.3294  time: 1.5232  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 200/3000]  eta: 1:11:30  lr: 0.000025  loss: 0.6232  time: 1.5229  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 205/3000]  eta: 1:11:22  lr: 0.000025  loss: 0.4797  time: 1.5274  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 205/3000]  eta: 1:11:22  lr: 0.000025  loss: 0.3422  time: 1.5272  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 210/3000]  eta: 1:11:10  lr: 0.000025  loss: 0.2099  time: 1.5261  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 210/3000]  eta: 1:11:09  lr: 0.000025  loss: 0.3343  time: 1.5258  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 215/3000]  eta: 1:10:59  lr: 0.000025  loss: 0.2896  time: 1.5067  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 215/3000]  eta: 1:10:58  lr: 0.000025  loss: 0.0632  time: 1.5065  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 220/3000]  eta: 1:10:54  lr: 0.000025  loss: 0.2224  time: 1.5079  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 220/3000]  eta: 1:10:53  lr: 0.000025  loss: 0.4422  time: 1.5077  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 225/3000]  eta: 1:10:44  lr: 0.000025  loss: 0.3545  time: 1.4991  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 225/3000]  eta: 1:10:43  lr: 0.000025  loss: 0.1338  time: 1.4989  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 230/3000]  eta: 1:10:36  lr: 0.000025  loss: 0.4553  time: 1.5177  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 230/3000]  eta: 1:10:36  lr: 0.000025  loss: 0.0749  time: 1.5175  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 235/3000]  eta: 1:10:28  lr: 0.000025  loss: 0.4201  time: 1.5275  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 235/3000]  eta: 1:10:27  lr: 0.000025  loss: 0.5726  time: 1.5273  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 240/3000]  eta: 1:10:23  lr: 0.000025  loss: 0.7845  time: 1.5270  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 240/3000]  eta: 1:10:22  lr: 0.000025  loss: 0.1076  time: 1.5267  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 245/3000]  eta: 1:10:14  lr: 0.000025  loss: 0.2156  time: 1.5342  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 245/3000]  eta: 1:10:13  lr: 0.000025  loss: 0.4451  time: 1.5339  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 250/3000]  eta: 1:10:05  lr: 0.000025  loss: 0.4944  time: 1.5258  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 250/3000]  eta: 1:10:04  lr: 0.000025  loss: 0.3112  time: 1.5255  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 255/3000]  eta: 1:09:53  lr: 0.000025  loss: 0.0972  time: 1.5068  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 255/3000]  eta: 1:09:52  lr: 0.000025  loss: 0.1102  time: 1.5066  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 260/3000]  eta: 1:09:45  lr: 0.000025  loss: 0.2348  time: 1.4973  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 260/3000]  eta: 1:09:44  lr: 0.000025  loss: 0.3614  time: 1.4971  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 265/3000]  eta: 1:09:38  lr: 0.000025  loss: 0.2848  time: 1.5045  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 265/3000]  eta: 1:09:38  lr: 0.000025  loss: 0.3465  time: 1.5043  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 270/3000]  eta: 1:09:28  lr: 0.000025  loss: 0.2501  time: 1.4995  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 270/3000]  eta: 1:09:28  lr: 0.000025  loss: 0.4864  time: 1.4993  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 275/3000]  eta: 1:09:21  lr: 0.000025  loss: 0.4342  time: 1.5222  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 275/3000]  eta: 1:09:20  lr: 0.000025  loss: 0.2366  time: 1.5211  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 280/3000]  eta: 1:09:15  lr: 0.000025  loss: 0.3389  time: 1.5302  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 280/3000]  eta: 1:09:14  lr: 0.000025  loss: 0.3841  time: 1.5292  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 285/3000]  eta: 1:09:06  lr: 0.000025  loss: 0.1961  time: 1.5188  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 285/3000]  eta: 1:09:05  lr: 0.000025  loss: 0.4022  time: 1.5179  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 290/3000]  eta: 1:09:01  lr: 0.000025  loss: 1.1511  time: 1.5418  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 290/3000]  eta: 1:09:00  lr: 0.000025  loss: 0.5654  time: 1.5408  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 295/3000]  eta: 1:08:51  lr: 0.000025  loss: 0.3267  time: 1.5285  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 295/3000]  eta: 1:08:50  lr: 0.000025  loss: 0.3222  time: 1.5283  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 300/3000]  eta: 1:08:43  lr: 0.000025  loss: 0.5884  time: 1.5202  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 300/3000]  eta: 1:08:42  lr: 0.000025  loss: 0.3348  time: 1.5200  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 305/3000]  eta: 1:08:35  lr: 0.000025  loss: 0.1792  time: 1.5265  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 305/3000]  eta: 1:08:34  lr: 0.000025  loss: 0.7187  time: 1.5262  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 310/3000]  eta: 1:08:28  lr: 0.000025  loss: 0.4878  time: 1.5135  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 310/3000]  eta: 1:08:27  lr: 0.000025  loss: 1.1541  time: 1.5132  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 315/3000]  eta: 1:08:21  lr: 0.000025  loss: 0.5463  time: 1.5349  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 315/3000]  eta: 1:08:21  lr: 0.000025  loss: 0.3354  time: 1.5346  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 320/3000]  eta: 1:08:15  lr: 0.000025  loss: 0.4193  time: 1.5439  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 320/3000]  eta: 1:08:14  lr: 0.000025  loss: 0.5333  time: 1.5436  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 325/3000]  eta: 1:08:07  lr: 0.000025  loss: 0.6074  time: 1.5435  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 325/3000]  eta: 1:08:07  lr: 0.000025  loss: 0.1819  time: 1.5432  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 330/3000]  eta: 1:08:03  lr: 0.000025  loss: 0.1059  time: 1.5637  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 330/3000]  eta: 1:08:02  lr: 0.000025  loss: 0.2921  time: 1.5635  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 335/3000]  eta: 1:07:55  lr: 0.000025  loss: 0.3330  time: 1.5529  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 335/3000]  eta: 1:07:54  lr: 0.000025  loss: 0.2819  time: 1.5526  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 340/3000]  eta: 1:07:49  lr: 0.000025  loss: 0.9335  time: 1.5533  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 340/3000]  eta: 1:07:48  lr: 0.000025  loss: 0.4553  time: 1.5530  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 345/3000]  eta: 1:07:37  lr: 0.000025  loss: 0.4362  time: 1.5301  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 345/3000]  eta: 1:07:36  lr: 0.000025  loss: 0.3497  time: 1.5298  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 350/3000]  eta: 1:07:31  lr: 0.000025  loss: 0.5995  time: 1.5182  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 350/3000]  eta: 1:07:30  lr: 0.000025  loss: 0.6896  time: 1.5179  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 355/3000]  eta: 1:07:21  lr: 0.000025  loss: 0.5796  time: 1.5094  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 355/3000]  eta: 1:07:21  lr: 0.000025  loss: 0.4735  time: 1.5092  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 360/3000]  eta: 1:07:14  lr: 0.000025  loss: 0.4911  time: 1.5020  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 360/3000]  eta: 1:07:13  lr: 0.000025  loss: 0.5916  time: 1.5017  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 365/3000]  eta: 1:07:07  lr: 0.000025  loss: 1.0966  time: 1.5304  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 365/3000]  eta: 1:07:06  lr: 0.000025  loss: 0.7635  time: 1.5302  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 370/3000]  eta: 1:06:59  lr: 0.000025  loss: 0.4423  time: 1.5226  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 370/3000]  eta: 1:06:58  lr: 0.000025  loss: 0.3851  time: 1.5224  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 375/3000]  eta: 1:06:50  lr: 0.000025  loss: 0.2434  time: 1.5237  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 375/3000]  eta: 1:06:49  lr: 0.000025  loss: 0.8993  time: 1.5235  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 380/3000]  eta: 1:06:42  lr: 0.000025  loss: 0.3989  time: 1.5142  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 380/3000]  eta: 1:06:41  lr: 0.000025  loss: 0.3755  time: 1.5139  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 385/3000]  eta: 1:06:33  lr: 0.000025  loss: 0.1032  time: 1.5006  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 385/3000]  eta: 1:06:32  lr: 0.000025  loss: 0.1055  time: 1.5003  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 390/3000]  eta: 1:06:22  lr: 0.000025  loss: 0.0838  time: 1.4799  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 390/3000]  eta: 1:06:21  lr: 0.000025  loss: 0.2414  time: 1.4796  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 395/3000]  eta: 1:06:14  lr: 0.000025  loss: 0.1250  time: 1.4866  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 395/3000]  eta: 1:06:13  lr: 0.000025  loss: 0.9356  time: 1.4863  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 400/3000]  eta: 1:06:06  lr: 0.000025  loss: 0.0970  time: 1.4859  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 400/3000]  eta: 1:06:05  lr: 0.000025  loss: 0.0897  time: 1.4857  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 405/3000]  eta: 1:05:59  lr: 0.000025  loss: 0.4627  time: 1.5026  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 405/3000]  eta: 1:05:58  lr: 0.000025  loss: 0.3559  time: 1.5024  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 410/3000]  eta: 1:05:51  lr: 0.000025  loss: 0.3383  time: 1.5194  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 410/3000]  eta: 1:05:50  lr: 0.000025  loss: 0.1790  time: 1.5192  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 415/3000]  eta: 1:05:44  lr: 0.000025  loss: 0.8618  time: 1.5272  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 415/3000]  eta: 1:05:43  lr: 0.000025  loss: 0.7005  time: 1.5269  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 420/3000]  eta: 1:05:34  lr: 0.000025  loss: 0.3400  time: 1.5159  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 420/3000]  eta: 1:05:33  lr: 0.000025  loss: 0.3858  time: 1.5157  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 425/3000]  eta: 1:05:26  lr: 0.000025  loss: 0.3599  time: 1.5063  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 425/3000]  eta: 1:05:25  lr: 0.000025  loss: 0.2646  time: 1.5061  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 430/3000]  eta: 1:05:15  lr: 0.000025  loss: 0.5460  time: 1.4850  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 430/3000]  eta: 1:05:15  lr: 0.000025  loss: 0.3020  time: 1.4848  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 435/3000]  eta: 1:05:07  lr: 0.000025  loss: 0.2713  time: 1.4752  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 435/3000]  eta: 1:05:07  lr: 0.000025  loss: 0.1890  time: 1.4750  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 440/3000]  eta: 1:04:58  lr: 0.000025  loss: 0.6570  time: 1.4813  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 440/3000]  eta: 1:04:58  lr: 0.000025  loss: 0.7615  time: 1.4811  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 445/3000]  eta: 1:04:51  lr: 0.000025  loss: 0.4477  time: 1.4890  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 445/3000]  eta: 1:04:51  lr: 0.000025  loss: 0.6443  time: 1.4888  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 450/3000]  eta: 1:04:44  lr: 0.000025  loss: 0.3274  time: 1.5118  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 450/3000]  eta: 1:04:43  lr: 0.000025  loss: 0.8269  time: 1.5116  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 455/3000]  eta: 1:04:37  lr: 0.000025  loss: 0.9405  time: 1.5245  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 455/3000]  eta: 1:04:36  lr: 0.000025  loss: 0.6303  time: 1.5242  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 460/3000]  eta: 1:04:30  lr: 0.000025  loss: 1.1913  time: 1.5388  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 460/3000]  eta: 1:04:29  lr: 0.000025  loss: 0.4356  time: 1.5386  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 465/3000]  eta: 1:04:22  lr: 0.000025  loss: 0.1753  time: 1.5359  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 465/3000]  eta: 1:04:22  lr: 0.000025  loss: 0.7560  time: 1.5357  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 470/3000]  eta: 1:04:14  lr: 0.000025  loss: 0.2758  time: 1.5341  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 470/3000]  eta: 1:04:14  lr: 0.000025  loss: 0.4351  time: 1.5338  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 475/3000]  eta: 1:04:07  lr: 0.000025  loss: 0.5624  time: 1.5296  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 475/3000]  eta: 1:04:06  lr: 0.000025  loss: 0.1557  time: 1.5293  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 480/3000]  eta: 1:03:57  lr: 0.000025  loss: 0.2698  time: 1.5070  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 480/3000]  eta: 1:03:57  lr: 0.000025  loss: 0.4377  time: 1.5067  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 485/3000]  eta: 1:03:46  lr: 0.000025  loss: 0.1944  time: 1.4678  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 485/3000]  eta: 1:03:45  lr: 0.000025  loss: 0.2786  time: 1.4676  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 490/3000]  eta: 1:03:36  lr: 0.000025  loss: 0.5149  time: 1.4452  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 490/3000]  eta: 1:03:35  lr: 0.000025  loss: 0.3820  time: 1.4449  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 495/3000]  eta: 1:03:28  lr: 0.000025  loss: 0.5465  time: 1.4377  data: 0.0000  max mem: 18596Train: data epoch: [10]  [ 495/3000]  eta: 1:03:27  lr: 0.000025  loss: 0.2476  time: 1.4375  data: 0.0000  max mem: 18406

Train: data epoch: [10]  [ 500/3000]  eta: 1:03:22  lr: 0.000025  loss: 0.4098  time: 1.4773  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 500/3000]  eta: 1:03:22  lr: 0.000025  loss: 0.5847  time: 1.4771  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 505/3000]  eta: 1:03:16  lr: 0.000025  loss: 0.5347  time: 1.5228  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 505/3000]  eta: 1:03:15  lr: 0.000025  loss: 0.6415  time: 1.5225  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 510/3000]  eta: 1:03:08  lr: 0.000025  loss: 0.7075  time: 1.5500  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 510/3000]  eta: 1:03:08  lr: 0.000025  loss: 0.2191  time: 1.5498  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 515/3000]  eta: 1:02:59  lr: 0.000025  loss: 0.6643  time: 1.5341  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 515/3000]  eta: 1:02:58  lr: 0.000025  loss: 0.2366  time: 1.5339  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 520/3000]  eta: 1:02:52  lr: 0.000025  loss: 0.3240  time: 1.5261  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 520/3000]  eta: 1:02:52  lr: 0.000025  loss: 0.1670  time: 1.5258  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 525/3000]  eta: 1:02:45  lr: 0.000025  loss: 0.1510  time: 1.5139  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 525/3000]  eta: 1:02:44  lr: 0.000025  loss: 0.5170  time: 1.5136  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 530/3000]  eta: 1:02:37  lr: 0.000025  loss: 0.4892  time: 1.5101  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 530/3000]  eta: 1:02:36  lr: 0.000025  loss: 0.4912  time: 1.5098  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 535/3000]  eta: 1:02:29  lr: 0.000025  loss: 0.4929  time: 1.5307  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 535/3000]  eta: 1:02:29  lr: 0.000025  loss: 0.4317  time: 1.5305  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 540/3000]  eta: 1:02:21  lr: 0.000025  loss: 0.7097  time: 1.5129  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 540/3000]  eta: 1:02:21  lr: 0.000025  loss: 0.7753  time: 1.5126  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 545/3000]  eta: 1:02:14  lr: 0.000025  loss: 0.3323  time: 1.5167  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 545/3000]  eta: 1:02:13  lr: 0.000025  loss: 0.5651  time: 1.5164  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 550/3000]  eta: 1:02:08  lr: 0.000025  loss: 0.1293  time: 1.5350  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 550/3000]  eta: 1:02:07  lr: 0.000025  loss: 0.5639  time: 1.5348  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 555/3000]  eta: 1:01:59  lr: 0.000025  loss: 0.3131  time: 1.5161  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 555/3000]  eta: 1:01:58  lr: 0.000025  loss: 0.0523  time: 1.5159  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 560/3000]  eta: 1:01:52  lr: 0.000025  loss: 0.1426  time: 1.5320  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 560/3000]  eta: 1:01:51  lr: 0.000025  loss: 0.2235  time: 1.5318  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 565/3000]  eta: 1:01:44  lr: 0.000025  loss: 0.4312  time: 1.5319  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 565/3000]  eta: 1:01:44  lr: 0.000025  loss: 0.2037  time: 1.5317  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 570/3000]  eta: 1:01:36  lr: 0.000025  loss: 0.1665  time: 1.5126  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 570/3000]  eta: 1:01:36  lr: 0.000025  loss: 0.3241  time: 1.5122  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 575/3000]  eta: 1:01:30  lr: 0.000025  loss: 0.3818  time: 1.5478  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 575/3000]  eta: 1:01:30  lr: 0.000025  loss: 0.5921  time: 1.5474  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 580/3000]  eta: 1:01:24  lr: 0.000025  loss: 0.4294  time: 1.5509  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 580/3000]  eta: 1:01:23  lr: 0.000025  loss: 0.6601  time: 1.5505  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 585/3000]  eta: 1:01:16  lr: 0.000025  loss: 0.4783  time: 1.5466  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 585/3000]  eta: 1:01:15  lr: 0.000025  loss: 0.2854  time: 1.5462  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 590/3000]  eta: 1:01:07  lr: 0.000025  loss: 0.5992  time: 1.5322  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 590/3000]  eta: 1:01:06  lr: 0.000025  loss: 0.1813  time: 1.5319  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 595/3000]  eta: 1:00:57  lr: 0.000025  loss: 0.1925  time: 1.4902  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 595/3000]  eta: 1:00:57  lr: 0.000025  loss: 0.4815  time: 1.4899  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 600/3000]  eta: 1:00:49  lr: 0.000025  loss: 0.2313  time: 1.4735  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 600/3000]  eta: 1:00:49  lr: 0.000025  loss: 0.2875  time: 1.4732  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 605/3000]  eta: 1:00:41  lr: 0.000025  loss: 0.2783  time: 1.4697  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 605/3000]  eta: 1:00:41  lr: 0.000025  loss: 0.0673  time: 1.4695  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 610/3000]  eta: 1:00:35  lr: 0.000025  loss: 0.5688  time: 1.5061  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 610/3000]  eta: 1:00:35  lr: 0.000025  loss: 0.6861  time: 1.5059  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 615/3000]  eta: 1:00:29  lr: 0.000025  loss: 0.3523  time: 1.5453  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 615/3000]  eta: 1:00:28  lr: 0.000025  loss: 0.5561  time: 1.5450  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 620/3000]  eta: 1:00:22  lr: 0.000025  loss: 0.2253  time: 1.5565  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 620/3000]  eta: 1:00:21  lr: 0.000025  loss: 0.1824  time: 1.5563  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 625/3000]  eta: 1:00:14  lr: 0.000025  loss: 0.7862  time: 1.5677  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 625/3000]  eta: 1:00:14  lr: 0.000025  loss: 0.2263  time: 1.5673  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 630/3000]  eta: 1:00:06  lr: 0.000025  loss: 0.8066  time: 1.5428  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 630/3000]  eta: 1:00:06  lr: 0.000025  loss: 0.6126  time: 1.5425  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 635/3000]  eta: 0:59:58  lr: 0.000025  loss: 0.2551  time: 1.5128  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 635/3000]  eta: 0:59:57  lr: 0.000025  loss: 0.3107  time: 1.5126  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 640/3000]  eta: 0:59:51  lr: 0.000025  loss: 0.1989  time: 1.5225  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 640/3000]  eta: 0:59:51  lr: 0.000025  loss: 0.5083  time: 1.5223  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 645/3000]  eta: 0:59:43  lr: 0.000025  loss: 0.2153  time: 1.5069  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 645/3000]  eta: 0:59:42  lr: 0.000025  loss: 0.5224  time: 1.5066  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 650/3000]  eta: 0:59:35  lr: 0.000025  loss: 0.4145  time: 1.5151  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 650/3000]  eta: 0:59:35  lr: 0.000025  loss: 0.1668  time: 1.5148  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 655/3000]  eta: 0:59:28  lr: 0.000025  loss: 0.2100  time: 1.5344  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 655/3000]  eta: 0:59:28  lr: 0.000025  loss: 0.2906  time: 1.5341  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 660/3000]  eta: 0:59:20  lr: 0.000025  loss: 0.4852  time: 1.5073  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 660/3000]  eta: 0:59:19  lr: 0.000025  loss: 0.5702  time: 1.5069  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 665/3000]  eta: 0:59:09  lr: 0.000025  loss: 0.1470  time: 1.4769  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 665/3000]  eta: 0:59:09  lr: 0.000025  loss: 0.0503  time: 1.4765  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 670/3000]  eta: 0:59:01  lr: 0.000025  loss: 0.1847  time: 1.4650  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 670/3000]  eta: 0:59:00  lr: 0.000025  loss: 0.1582  time: 1.4648  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 675/3000]  eta: 0:58:53  lr: 0.000025  loss: 0.2661  time: 1.4468  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 675/3000]  eta: 0:58:52  lr: 0.000025  loss: 0.3096  time: 1.4465  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 680/3000]  eta: 0:58:45  lr: 0.000025  loss: 0.9711  time: 1.4505  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 680/3000]  eta: 0:58:44  lr: 0.000025  loss: 0.5534  time: 1.4503  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 685/3000]  eta: 0:58:38  lr: 0.000025  loss: 0.5641  time: 1.5039  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 685/3000]  eta: 0:58:37  lr: 0.000025  loss: 0.3179  time: 1.5037  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 690/3000]  eta: 0:58:28  lr: 0.000025  loss: 0.1553  time: 1.4861  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 690/3000]  eta: 0:58:28  lr: 0.000025  loss: 0.2500  time: 1.4858  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 695/3000]  eta: 0:58:21  lr: 0.000025  loss: 0.1653  time: 1.5053  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 695/3000]  eta: 0:58:21  lr: 0.000025  loss: 0.2265  time: 1.5051  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 700/3000]  eta: 0:58:14  lr: 0.000025  loss: 0.2084  time: 1.5171  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 700/3000]  eta: 0:58:13  lr: 0.000025  loss: 0.8241  time: 1.5168  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 705/3000]  eta: 0:58:07  lr: 0.000025  loss: 0.4257  time: 1.5194  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 705/3000]  eta: 0:58:07  lr: 0.000025  loss: 0.5389  time: 1.5191  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 710/3000]  eta: 0:58:00  lr: 0.000025  loss: 0.3871  time: 1.5507  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 710/3000]  eta: 0:57:59  lr: 0.000025  loss: 0.6828  time: 1.5504  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 715/3000]  eta: 0:57:51  lr: 0.000025  loss: 0.8799  time: 1.5375  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 715/3000]  eta: 0:57:52  lr: 0.000025  loss: 0.6347  time: 1.5379  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 720/3000]  eta: 0:57:46  lr: 0.000025  loss: 0.8229  time: 1.5487  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 720/3000]  eta: 0:57:45  lr: 0.000025  loss: 0.2825  time: 1.5484  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 725/3000]  eta: 0:57:37  lr: 0.000025  loss: 0.3916  time: 1.5204  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 725/3000]  eta: 0:57:36  lr: 0.000025  loss: 1.0989  time: 1.5202  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 730/3000]  eta: 0:57:30  lr: 0.000025  loss: 0.6758  time: 1.5268  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 730/3000]  eta: 0:57:29  lr: 0.000025  loss: 0.4951  time: 1.5265  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 735/3000]  eta: 0:57:23  lr: 0.000025  loss: 0.3774  time: 1.5429  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 735/3000]  eta: 0:57:22  lr: 0.000025  loss: 0.8378  time: 1.5429  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 740/3000]  eta: 0:57:16  lr: 0.000025  loss: 0.5110  time: 1.5303  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 740/3000]  eta: 0:57:15  lr: 0.000025  loss: 0.4119  time: 1.5301  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 745/3000]  eta: 0:57:07  lr: 0.000025  loss: 0.1900  time: 1.5252  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 745/3000]  eta: 0:57:06  lr: 0.000025  loss: 0.2032  time: 1.5250  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 750/3000]  eta: 0:57:00  lr: 0.000025  loss: 0.5562  time: 1.5204  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 750/3000]  eta: 0:56:59  lr: 0.000025  loss: 0.3710  time: 1.5202  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 755/3000]  eta: 0:56:52  lr: 0.000025  loss: 1.0696  time: 1.5160  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 755/3000]  eta: 0:56:52  lr: 0.000025  loss: 0.3360  time: 1.5158  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 760/3000]  eta: 0:56:44  lr: 0.000025  loss: 0.7631  time: 1.4998  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 760/3000]  eta: 0:56:43  lr: 0.000025  loss: 0.5619  time: 1.4996  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 765/3000]  eta: 0:56:36  lr: 0.000025  loss: 0.1699  time: 1.5136  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 765/3000]  eta: 0:56:36  lr: 0.000025  loss: 0.6096  time: 1.5133  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 770/3000]  eta: 0:56:30  lr: 0.000025  loss: 0.3966  time: 1.5262  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 770/3000]  eta: 0:56:29  lr: 0.000025  loss: 0.3964  time: 1.5260  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 775/3000]  eta: 0:56:22  lr: 0.000025  loss: 0.2197  time: 1.5131  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 775/3000]  eta: 0:56:21  lr: 0.000025  loss: 0.1851  time: 1.5129  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 780/3000]  eta: 0:56:14  lr: 0.000025  loss: 0.2999  time: 1.5309  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 780/3000]  eta: 0:56:14  lr: 0.000025  loss: 0.5471  time: 1.5306  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 785/3000]  eta: 0:56:07  lr: 0.000025  loss: 0.8316  time: 1.5372  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 785/3000]  eta: 0:56:06  lr: 0.000025  loss: 0.2470  time: 1.5370  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 790/3000]  eta: 0:56:00  lr: 0.000025  loss: 0.4627  time: 1.5284  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 790/3000]  eta: 0:55:59  lr: 0.000025  loss: 0.2773  time: 1.5281  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 795/3000]  eta: 0:55:52  lr: 0.000025  loss: 0.1026  time: 1.5279  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 795/3000]  eta: 0:55:51  lr: 0.000025  loss: 0.1311  time: 1.5276  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 800/3000]  eta: 0:55:45  lr: 0.000025  loss: 0.6230  time: 1.5338  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 800/3000]  eta: 0:55:44  lr: 0.000025  loss: 0.2404  time: 1.5335  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 805/3000]  eta: 0:55:38  lr: 0.000025  loss: 0.3650  time: 1.5437  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 805/3000]  eta: 0:55:37  lr: 0.000025  loss: 0.4378  time: 1.5434  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 810/3000]  eta: 0:55:31  lr: 0.000025  loss: 0.5159  time: 1.5448  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 810/3000]  eta: 0:55:30  lr: 0.000025  loss: 0.9280  time: 1.5446  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 815/3000]  eta: 0:55:24  lr: 0.000025  loss: 0.4414  time: 1.5664  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 815/3000]  eta: 0:55:23  lr: 0.000025  loss: 0.4073  time: 1.5662  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 820/3000]  eta: 0:55:15  lr: 0.000025  loss: 0.4044  time: 1.5376  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 820/3000]  eta: 0:55:15  lr: 0.000025  loss: 0.4619  time: 1.5374  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 825/3000]  eta: 0:55:07  lr: 0.000025  loss: 0.0828  time: 1.5140  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 825/3000]  eta: 0:55:06  lr: 0.000025  loss: 0.1903  time: 1.5137  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 830/3000]  eta: 0:54:58  lr: 0.000025  loss: 0.5642  time: 1.4851  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 830/3000]  eta: 0:54:58  lr: 0.000025  loss: 0.3494  time: 1.4849  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 835/3000]  eta: 0:54:49  lr: 0.000025  loss: 0.8833  time: 1.4479  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 835/3000]  eta: 0:54:49  lr: 0.000025  loss: 0.2907  time: 1.4477  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 840/3000]  eta: 0:54:42  lr: 0.000025  loss: 0.6009  time: 1.4738  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 840/3000]  eta: 0:54:42  lr: 0.000025  loss: 0.4258  time: 1.4735  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 845/3000]  eta: 0:54:34  lr: 0.000025  loss: 0.8335  time: 1.4696  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 845/3000]  eta: 0:54:33  lr: 0.000025  loss: 0.4307  time: 1.4693  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 850/3000]  eta: 0:54:27  lr: 0.000025  loss: 0.0720  time: 1.5003  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 850/3000]  eta: 0:54:26  lr: 0.000025  loss: 0.4193  time: 1.5001  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 855/3000]  eta: 0:54:20  lr: 0.000025  loss: 0.5065  time: 1.5408  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 855/3000]  eta: 0:54:20  lr: 0.000025  loss: 0.1208  time: 1.5405  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 860/3000]  eta: 0:54:13  lr: 0.000025  loss: 0.2152  time: 1.5359  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 860/3000]  eta: 0:54:12  lr: 0.000025  loss: 0.7644  time: 1.5357  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 865/3000]  eta: 0:54:05  lr: 0.000025  loss: 0.9089  time: 1.5507  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 865/3000]  eta: 0:54:04  lr: 0.000025  loss: 0.2058  time: 1.5504  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 870/3000]  eta: 0:53:58  lr: 0.000025  loss: 1.2508  time: 1.5421  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 870/3000]  eta: 0:53:57  lr: 0.000025  loss: 0.3827  time: 1.5419  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 875/3000]  eta: 0:53:49  lr: 0.000025  loss: 0.3705  time: 1.5008  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 875/3000]  eta: 0:53:48  lr: 0.000025  loss: 0.4992  time: 1.5007  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 880/3000]  eta: 0:53:41  lr: 0.000025  loss: 1.0183  time: 1.4853  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 880/3000]  eta: 0:53:40  lr: 0.000025  loss: 0.1910  time: 1.4851  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 885/3000]  eta: 0:53:33  lr: 0.000025  loss: 0.1817  time: 1.4842  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 885/3000]  eta: 0:53:32  lr: 0.000025  loss: 0.3694  time: 1.4840  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 890/3000]  eta: 0:53:26  lr: 0.000025  loss: 0.1532  time: 1.5010  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 890/3000]  eta: 0:53:26  lr: 0.000025  loss: 0.4481  time: 1.5006  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 895/3000]  eta: 0:53:18  lr: 0.000025  loss: 0.6961  time: 1.5141  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 895/3000]  eta: 0:53:18  lr: 0.000025  loss: 0.4863  time: 1.5138  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 900/3000]  eta: 0:53:11  lr: 0.000025  loss: 0.1551  time: 1.5284  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 900/3000]  eta: 0:53:10  lr: 0.000025  loss: 0.5378  time: 1.5281  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 905/3000]  eta: 0:53:03  lr: 0.000025  loss: 0.2453  time: 1.5333  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 905/3000]  eta: 0:53:03  lr: 0.000025  loss: 0.2372  time: 1.5330  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 910/3000]  eta: 0:52:56  lr: 0.000025  loss: 0.2397  time: 1.5139  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 910/3000]  eta: 0:52:55  lr: 0.000025  loss: 0.5703  time: 1.5137  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 915/3000]  eta: 0:52:47  lr: 0.000025  loss: 0.5279  time: 1.5055  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 915/3000]  eta: 0:52:47  lr: 0.000025  loss: 0.2777  time: 1.5052  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 920/3000]  eta: 0:52:40  lr: 0.000025  loss: 0.6638  time: 1.5186  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 920/3000]  eta: 0:52:40  lr: 0.000025  loss: 0.2844  time: 1.5184  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 925/3000]  eta: 0:52:32  lr: 0.000025  loss: 0.5004  time: 1.5117  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 925/3000]  eta: 0:52:32  lr: 0.000025  loss: 0.2752  time: 1.5114  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 930/3000]  eta: 0:52:25  lr: 0.000025  loss: 0.5812  time: 1.5216  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 930/3000]  eta: 0:52:25  lr: 0.000025  loss: 0.3187  time: 1.5213  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 935/3000]  eta: 0:52:19  lr: 0.000025  loss: 0.5029  time: 1.5591  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 935/3000]  eta: 0:52:18  lr: 0.000025  loss: 0.3210  time: 1.5588  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 940/3000]  eta: 0:52:11  lr: 0.000025  loss: 0.9185  time: 1.5365  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 940/3000]  eta: 0:52:10  lr: 0.000025  loss: 0.7909  time: 1.5362  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 945/3000]  eta: 0:52:03  lr: 0.000025  loss: 0.3560  time: 1.5350  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 945/3000]  eta: 0:52:02  lr: 0.000025  loss: 0.5579  time: 1.5348  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 950/3000]  eta: 0:51:55  lr: 0.000025  loss: 0.4183  time: 1.5285  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 950/3000]  eta: 0:51:55  lr: 0.000025  loss: 0.2051  time: 1.5283  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 955/3000]  eta: 0:51:48  lr: 0.000025  loss: 0.1396  time: 1.5092  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 955/3000]  eta: 0:51:47  lr: 0.000025  loss: 1.2457  time: 1.5089  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 960/3000]  eta: 0:51:40  lr: 0.000025  loss: 0.6822  time: 1.5232  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 960/3000]  eta: 0:51:40  lr: 0.000025  loss: 0.2303  time: 1.5229  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 965/3000]  eta: 0:51:32  lr: 0.000025  loss: 0.4184  time: 1.5082  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 965/3000]  eta: 0:51:31  lr: 0.000025  loss: 0.1875  time: 1.5080  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 970/3000]  eta: 0:51:24  lr: 0.000025  loss: 0.1415  time: 1.5055  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 970/3000]  eta: 0:51:24  lr: 0.000025  loss: 0.8331  time: 1.5052  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 975/3000]  eta: 0:51:17  lr: 0.000025  loss: 0.2374  time: 1.5150  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 975/3000]  eta: 0:51:16  lr: 0.000025  loss: 0.2126  time: 1.5147  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 980/3000]  eta: 0:51:08  lr: 0.000025  loss: 0.1241  time: 1.4689  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 980/3000]  eta: 0:51:07  lr: 0.000025  loss: 0.5773  time: 1.4686  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 985/3000]  eta: 0:51:00  lr: 0.000025  loss: 0.5766  time: 1.4866  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 985/3000]  eta: 0:51:00  lr: 0.000025  loss: 0.2458  time: 1.4864  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 990/3000]  eta: 0:50:53  lr: 0.000025  loss: 0.7611  time: 1.4869  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 990/3000]  eta: 0:50:52  lr: 0.000025  loss: 0.2919  time: 1.4867  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [ 995/3000]  eta: 0:50:45  lr: 0.000025  loss: 0.4022  time: 1.4737  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [ 995/3000]  eta: 0:50:44  lr: 0.000025  loss: 0.1947  time: 1.4734  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1000/3000]  eta: 0:50:36  lr: 0.000025  loss: 0.3143  time: 1.4937  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1000/3000]  eta: 0:50:36  lr: 0.000025  loss: 0.3418  time: 1.4935  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1005/3000]  eta: 0:50:29  lr: 0.000025  loss: 0.5525  time: 1.5009  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1005/3000]  eta: 0:50:28  lr: 0.000025  loss: 0.6990  time: 1.5006  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1010/3000]  eta: 0:50:22  lr: 0.000025  loss: 0.2796  time: 1.5119  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1010/3000]  eta: 0:50:21  lr: 0.000025  loss: 0.2485  time: 1.5116  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1015/3000]  eta: 0:50:14  lr: 0.000025  loss: 0.2281  time: 1.4994  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1015/3000]  eta: 0:50:13  lr: 0.000025  loss: 0.0726  time: 1.4992  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1020/3000]  eta: 0:50:06  lr: 0.000025  loss: 0.2302  time: 1.5225  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1020/3000]  eta: 0:50:06  lr: 0.000025  loss: 0.7936  time: 1.5222  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1025/3000]  eta: 0:49:58  lr: 0.000025  loss: 0.2285  time: 1.5098  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1025/3000]  eta: 0:49:58  lr: 0.000025  loss: 0.2928  time: 1.5095  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1030/3000]  eta: 0:49:50  lr: 0.000025  loss: 0.2303  time: 1.4871  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1030/3000]  eta: 0:49:50  lr: 0.000025  loss: 0.3307  time: 1.4869  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1035/3000]  eta: 0:49:43  lr: 0.000025  loss: 0.2151  time: 1.5031  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1035/3000]  eta: 0:49:42  lr: 0.000025  loss: 0.1587  time: 1.5028  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1040/3000]  eta: 0:49:34  lr: 0.000025  loss: 0.4462  time: 1.4859  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1040/3000]  eta: 0:49:35  lr: 0.000025  loss: 0.8261  time: 1.4874  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1045/3000]  eta: 0:49:27  lr: 0.000025  loss: 0.0974  time: 1.4949  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1045/3000]  eta: 0:49:26  lr: 0.000025  loss: 0.5701  time: 1.4938  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1050/3000]  eta: 0:49:20  lr: 0.000025  loss: 0.6038  time: 1.5194  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1050/3000]  eta: 0:49:19  lr: 0.000025  loss: 0.2945  time: 1.5181  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1055/3000]  eta: 0:49:13  lr: 0.000025  loss: 0.2778  time: 1.5233  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1055/3000]  eta: 0:49:12  lr: 0.000025  loss: 0.1175  time: 1.5221  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1060/3000]  eta: 0:49:05  lr: 0.000025  loss: 0.6276  time: 1.5286  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1060/3000]  eta: 0:49:04  lr: 0.000025  loss: 0.8215  time: 1.5286  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1065/3000]  eta: 0:48:57  lr: 0.000025  loss: 0.4381  time: 1.5256  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1065/3000]  eta: 0:48:56  lr: 0.000025  loss: 0.7970  time: 1.5253  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1070/3000]  eta: 0:48:50  lr: 0.000025  loss: 0.4581  time: 1.5199  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1070/3000]  eta: 0:48:49  lr: 0.000025  loss: 0.9145  time: 1.5197  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1075/3000]  eta: 0:48:42  lr: 0.000025  loss: 0.7101  time: 1.5145  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1075/3000]  eta: 0:48:41  lr: 0.000025  loss: 0.7468  time: 1.5143  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1080/3000]  eta: 0:48:34  lr: 0.000025  loss: 1.5229  time: 1.4946  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1080/3000]  eta: 0:48:33  lr: 0.000025  loss: 0.4144  time: 1.4943  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1085/3000]  eta: 0:48:26  lr: 0.000025  loss: 0.5594  time: 1.5012  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1085/3000]  eta: 0:48:25  lr: 0.000025  loss: 0.2159  time: 1.5010  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1090/3000]  eta: 0:48:18  lr: 0.000025  loss: 0.6679  time: 1.4878  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1090/3000]  eta: 0:48:18  lr: 0.000025  loss: 0.4949  time: 1.4876  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1095/3000]  eta: 0:48:10  lr: 0.000025  loss: 0.1930  time: 1.4627  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1095/3000]  eta: 0:48:09  lr: 0.000025  loss: 0.1998  time: 1.4624  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1100/3000]  eta: 0:48:03  lr: 0.000025  loss: 0.1682  time: 1.5080  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1100/3000]  eta: 0:48:02  lr: 0.000025  loss: 0.3572  time: 1.5077  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1105/3000]  eta: 0:47:56  lr: 0.000025  loss: 0.3987  time: 1.5241  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1105/3000]  eta: 0:47:55  lr: 0.000025  loss: 0.9847  time: 1.5238  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1110/3000]  eta: 0:47:49  lr: 0.000025  loss: 0.5177  time: 1.5422  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1110/3000]  eta: 0:47:48  lr: 0.000025  loss: 0.3915  time: 1.5419  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1115/3000]  eta: 0:47:41  lr: 0.000025  loss: 0.4041  time: 1.5722  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1115/3000]  eta: 0:47:41  lr: 0.000025  loss: 0.5485  time: 1.5720  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1120/3000]  eta: 0:47:34  lr: 0.000025  loss: 0.4749  time: 1.5712  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1120/3000]  eta: 0:47:34  lr: 0.000025  loss: 0.3806  time: 1.5710  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1125/3000]  eta: 0:47:26  lr: 0.000025  loss: 0.3257  time: 1.5401  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1125/3000]  eta: 0:47:26  lr: 0.000025  loss: 0.1498  time: 1.5398  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1130/3000]  eta: 0:47:18  lr: 0.000025  loss: 0.8008  time: 1.5158  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1130/3000]  eta: 0:47:18  lr: 0.000025  loss: 0.3403  time: 1.5155  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1135/3000]  eta: 0:47:11  lr: 0.000025  loss: 0.1716  time: 1.5271  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1135/3000]  eta: 0:47:11  lr: 0.000025  loss: 0.7129  time: 1.5268  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1140/3000]  eta: 0:47:04  lr: 0.000025  loss: 0.2360  time: 1.5124  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1140/3000]  eta: 0:47:03  lr: 0.000025  loss: 0.2488  time: 1.5122  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1145/3000]  eta: 0:46:56  lr: 0.000025  loss: 0.1451  time: 1.5182  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1145/3000]  eta: 0:46:55  lr: 0.000025  loss: 0.7617  time: 1.5180  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1150/3000]  eta: 0:46:48  lr: 0.000025  loss: 0.2749  time: 1.5250  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1150/3000]  eta: 0:46:48  lr: 0.000025  loss: 0.7231  time: 1.5248  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1155/3000]  eta: 0:46:41  lr: 0.000025  loss: 0.1947  time: 1.5189  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1155/3000]  eta: 0:46:40  lr: 0.000025  loss: 0.4782  time: 1.5187  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1160/3000]  eta: 0:46:33  lr: 0.000025  loss: 0.3198  time: 1.5093  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1160/3000]  eta: 0:46:33  lr: 0.000025  loss: 0.2220  time: 1.5090  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1165/3000]  eta: 0:46:26  lr: 0.000025  loss: 0.1952  time: 1.5176  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1165/3000]  eta: 0:46:25  lr: 0.000025  loss: 0.4698  time: 1.5174  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1170/3000]  eta: 0:46:18  lr: 0.000025  loss: 0.3005  time: 1.5228  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1170/3000]  eta: 0:46:17  lr: 0.000025  loss: 0.2659  time: 1.5225  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1175/3000]  eta: 0:46:11  lr: 0.000025  loss: 0.2788  time: 1.5233  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1175/3000]  eta: 0:46:10  lr: 0.000025  loss: 0.5898  time: 1.5230  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1180/3000]  eta: 0:46:03  lr: 0.000025  loss: 0.0558  time: 1.5324  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1180/3000]  eta: 0:46:03  lr: 0.000025  loss: 0.4577  time: 1.5321  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1185/3000]  eta: 0:45:56  lr: 0.000025  loss: 0.7587  time: 1.5324  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1185/3000]  eta: 0:45:55  lr: 0.000025  loss: 0.4730  time: 1.5321  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1190/3000]  eta: 0:45:48  lr: 0.000025  loss: 0.5566  time: 1.5326  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1190/3000]  eta: 0:45:48  lr: 0.000025  loss: 0.2219  time: 1.5323  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1195/3000]  eta: 0:45:41  lr: 0.000025  loss: 0.3240  time: 1.5423  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1195/3000]  eta: 0:45:40  lr: 0.000025  loss: 0.4264  time: 1.5420  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1200/3000]  eta: 0:45:33  lr: 0.000025  loss: 0.3986  time: 1.5326  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1200/3000]  eta: 0:45:33  lr: 0.000025  loss: 0.6958  time: 1.5324  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1205/3000]  eta: 0:45:26  lr: 0.000025  loss: 0.3499  time: 1.5331  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1205/3000]  eta: 0:45:25  lr: 0.000025  loss: 0.4370  time: 1.5329  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1210/3000]  eta: 0:45:18  lr: 0.000025  loss: 0.7404  time: 1.5289  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1210/3000]  eta: 0:45:17  lr: 0.000025  loss: 0.3439  time: 1.5286  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1215/3000]  eta: 0:45:10  lr: 0.000025  loss: 0.5716  time: 1.4845  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1215/3000]  eta: 0:45:09  lr: 0.000025  loss: 0.4691  time: 1.4842  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1220/3000]  eta: 0:45:02  lr: 0.000025  loss: 0.9826  time: 1.5044  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1220/3000]  eta: 0:45:02  lr: 0.000025  loss: 0.5674  time: 1.5041  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1225/3000]  eta: 0:44:55  lr: 0.000025  loss: 0.3880  time: 1.5033  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1225/3000]  eta: 0:44:54  lr: 0.000025  loss: 0.3222  time: 1.5031  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1230/3000]  eta: 0:44:47  lr: 0.000025  loss: 0.3535  time: 1.5078  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1230/3000]  eta: 0:44:47  lr: 0.000025  loss: 0.7635  time: 1.5075  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1235/3000]  eta: 0:44:40  lr: 0.000025  loss: 0.1576  time: 1.5486  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1235/3000]  eta: 0:44:40  lr: 0.000025  loss: 0.2627  time: 1.5483  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1240/3000]  eta: 0:44:33  lr: 0.000025  loss: 0.1851  time: 1.5505  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1240/3000]  eta: 0:44:32  lr: 0.000025  loss: 0.4802  time: 1.5502  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1245/3000]  eta: 0:44:25  lr: 0.000025  loss: 0.2391  time: 1.5474  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1245/3000]  eta: 0:44:25  lr: 0.000025  loss: 0.2843  time: 1.5471  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1250/3000]  eta: 0:44:17  lr: 0.000025  loss: 0.4705  time: 1.5258  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1250/3000]  eta: 0:44:17  lr: 0.000025  loss: 0.2936  time: 1.5255  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1255/3000]  eta: 0:44:09  lr: 0.000025  loss: 0.1904  time: 1.5062  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1255/3000]  eta: 0:44:09  lr: 0.000025  loss: 0.3344  time: 1.5059  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1260/3000]  eta: 0:44:02  lr: 0.000025  loss: 0.3260  time: 1.5011  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1260/3000]  eta: 0:44:02  lr: 0.000025  loss: 0.1709  time: 1.5008  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1265/3000]  eta: 0:43:54  lr: 0.000025  loss: 0.3215  time: 1.4844  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1265/3000]  eta: 0:43:53  lr: 0.000025  loss: 0.1936  time: 1.4842  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1270/3000]  eta: 0:43:47  lr: 0.000025  loss: 0.2656  time: 1.5090  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1270/3000]  eta: 0:43:46  lr: 0.000025  loss: 0.3155  time: 1.5088  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1275/3000]  eta: 0:43:39  lr: 0.000025  loss: 0.2087  time: 1.5113  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1275/3000]  eta: 0:43:38  lr: 0.000025  loss: 0.4409  time: 1.5111  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1280/3000]  eta: 0:43:31  lr: 0.000025  loss: 0.5770  time: 1.5069  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1280/3000]  eta: 0:43:31  lr: 0.000025  loss: 0.3178  time: 1.5066  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1285/3000]  eta: 0:43:23  lr: 0.000025  loss: 0.0731  time: 1.5019  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1285/3000]  eta: 0:43:23  lr: 0.000025  loss: 0.2881  time: 1.5016  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1290/3000]  eta: 0:43:16  lr: 0.000025  loss: 0.0943  time: 1.5007  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1290/3000]  eta: 0:43:15  lr: 0.000025  loss: 0.2527  time: 1.5005  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1295/3000]  eta: 0:43:08  lr: 0.000025  loss: 0.3480  time: 1.5095  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1295/3000]  eta: 0:43:08  lr: 0.000025  loss: 0.0706  time: 1.5092  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1300/3000]  eta: 0:43:00  lr: 0.000025  loss: 0.3935  time: 1.4930  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1300/3000]  eta: 0:43:00  lr: 0.000025  loss: 0.1073  time: 1.4927  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1305/3000]  eta: 0:42:53  lr: 0.000025  loss: 0.7521  time: 1.5164  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1305/3000]  eta: 0:42:52  lr: 0.000025  loss: 0.3361  time: 1.5162  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1310/3000]  eta: 0:42:45  lr: 0.000025  loss: 0.4753  time: 1.5238  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1310/3000]  eta: 0:42:45  lr: 0.000025  loss: 1.2212  time: 1.5236  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1315/3000]  eta: 0:42:38  lr: 0.000025  loss: 0.3700  time: 1.5219  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1315/3000]  eta: 0:42:37  lr: 0.000025  loss: 0.1376  time: 1.5217  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1320/3000]  eta: 0:42:31  lr: 0.000025  loss: 0.4519  time: 1.5459  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1320/3000]  eta: 0:42:30  lr: 0.000025  loss: 0.4443  time: 1.5456  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1325/3000]  eta: 0:42:23  lr: 0.000025  loss: 0.5381  time: 1.5632  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1325/3000]  eta: 0:42:23  lr: 0.000025  loss: 0.2721  time: 1.5629  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1330/3000]  eta: 0:42:16  lr: 0.000025  loss: 0.2258  time: 1.5628  data: 0.0000  max mem: 18406Train: data epoch: [10]  [1330/3000]  eta: 0:42:16  lr: 0.000025  loss: 0.3756  time: 1.5631  data: 0.0000  max mem: 18596

Train: data epoch: [10]  [1335/3000]  eta: 0:42:08  lr: 0.000025  loss: 0.6598  time: 1.5513  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1335/3000]  eta: 0:42:08  lr: 0.000025  loss: 0.7065  time: 1.5510  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1340/3000]  eta: 0:42:01  lr: 0.000025  loss: 0.3507  time: 1.5307  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1340/3000]  eta: 0:42:00  lr: 0.000025  loss: 0.3785  time: 1.5305  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1345/3000]  eta: 0:41:53  lr: 0.000025  loss: 0.5570  time: 1.5137  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1345/3000]  eta: 0:41:53  lr: 0.000025  loss: 0.2322  time: 1.5134  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1350/3000]  eta: 0:41:45  lr: 0.000025  loss: 0.3649  time: 1.4976  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1350/3000]  eta: 0:41:45  lr: 0.000025  loss: 0.2902  time: 1.4974  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1355/3000]  eta: 0:41:38  lr: 0.000025  loss: 0.3480  time: 1.5262  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1355/3000]  eta: 0:41:38  lr: 0.000025  loss: 1.1173  time: 1.5260  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1360/3000]  eta: 0:41:31  lr: 0.000025  loss: 0.5096  time: 1.5468  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1360/3000]  eta: 0:41:30  lr: 0.000025  loss: 0.1099  time: 1.5465  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1365/3000]  eta: 0:41:23  lr: 0.000025  loss: 0.0717  time: 1.5546  data: 0.0000  max mem: 18406Train: data epoch: [10]  [1365/3000]  eta: 0:41:23  lr: 0.000025  loss: 0.2665  time: 1.5549  data: 0.0000  max mem: 18596

Train: data epoch: [10]  [1370/3000]  eta: 0:41:16  lr: 0.000025  loss: 0.3126  time: 1.5569  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1370/3000]  eta: 0:41:15  lr: 0.000025  loss: 0.8168  time: 1.5566  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1375/3000]  eta: 0:41:08  lr: 0.000025  loss: 0.1258  time: 1.5206  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1375/3000]  eta: 0:41:07  lr: 0.000025  loss: 0.5996  time: 1.5204  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1380/3000]  eta: 0:41:00  lr: 0.000025  loss: 0.3070  time: 1.5171  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1380/3000]  eta: 0:41:00  lr: 0.000025  loss: 0.6405  time: 1.5169  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1385/3000]  eta: 0:40:53  lr: 0.000025  loss: 0.3361  time: 1.5138  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1385/3000]  eta: 0:40:53  lr: 0.000025  loss: 0.1314  time: 1.5136  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1390/3000]  eta: 0:40:45  lr: 0.000025  loss: 0.6762  time: 1.5207  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1390/3000]  eta: 0:40:45  lr: 0.000025  loss: 0.2575  time: 1.5205  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1395/3000]  eta: 0:40:38  lr: 0.000025  loss: 0.3009  time: 1.5458  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1395/3000]  eta: 0:40:38  lr: 0.000025  loss: 0.1921  time: 1.5455  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1400/3000]  eta: 0:40:31  lr: 0.000025  loss: 0.9510  time: 1.5399  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1400/3000]  eta: 0:40:30  lr: 0.000025  loss: 0.4303  time: 1.5395  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1405/3000]  eta: 0:40:23  lr: 0.000025  loss: 0.5001  time: 1.5405  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1405/3000]  eta: 0:40:23  lr: 0.000025  loss: 0.5935  time: 1.5402  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1410/3000]  eta: 0:40:16  lr: 0.000025  loss: 0.5178  time: 1.5545  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1410/3000]  eta: 0:40:15  lr: 0.000025  loss: 0.2961  time: 1.5542  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1415/3000]  eta: 0:40:09  lr: 0.000025  loss: 0.2613  time: 1.5648  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1415/3000]  eta: 0:40:08  lr: 0.000025  loss: 0.1510  time: 1.5645  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1420/3000]  eta: 0:40:01  lr: 0.000025  loss: 0.3345  time: 1.5666  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1420/3000]  eta: 0:40:01  lr: 0.000025  loss: 0.2494  time: 1.5664  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1425/3000]  eta: 0:39:54  lr: 0.000025  loss: 0.5154  time: 1.5638  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1425/3000]  eta: 0:39:53  lr: 0.000025  loss: 0.1517  time: 1.5635  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1430/3000]  eta: 0:39:46  lr: 0.000025  loss: 1.2984  time: 1.5592  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1430/3000]  eta: 0:39:46  lr: 0.000025  loss: 0.2796  time: 1.5590  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1435/3000]  eta: 0:39:39  lr: 0.000025  loss: 0.7159  time: 1.5671  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1435/3000]  eta: 0:39:39  lr: 0.000025  loss: 0.5933  time: 1.5669  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1440/3000]  eta: 0:39:32  lr: 0.000025  loss: 0.1546  time: 1.5687  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1440/3000]  eta: 0:39:31  lr: 0.000025  loss: 0.1779  time: 1.5684  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1445/3000]  eta: 0:39:24  lr: 0.000025  loss: 0.7143  time: 1.5589  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1445/3000]  eta: 0:39:24  lr: 0.000025  loss: 0.4127  time: 1.5586  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1450/3000]  eta: 0:39:17  lr: 0.000025  loss: 0.5775  time: 1.5548  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1450/3000]  eta: 0:39:16  lr: 0.000025  loss: 0.4009  time: 1.5545  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1455/3000]  eta: 0:39:09  lr: 0.000025  loss: 0.5713  time: 1.5245  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1455/3000]  eta: 0:39:09  lr: 0.000025  loss: 0.4659  time: 1.5242  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1460/3000]  eta: 0:39:01  lr: 0.000025  loss: 0.3987  time: 1.5206  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1460/3000]  eta: 0:39:01  lr: 0.000025  loss: 0.2607  time: 1.5203  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1465/3000]  eta: 0:38:53  lr: 0.000025  loss: 0.2101  time: 1.5067  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1465/3000]  eta: 0:38:53  lr: 0.000025  loss: 0.1431  time: 1.5064  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1470/3000]  eta: 0:38:46  lr: 0.000025  loss: 0.3588  time: 1.4899  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1470/3000]  eta: 0:38:45  lr: 0.000025  loss: 0.4480  time: 1.4896  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1475/3000]  eta: 0:38:38  lr: 0.000025  loss: 0.3346  time: 1.4996  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1475/3000]  eta: 0:38:38  lr: 0.000025  loss: 0.0599  time: 1.4990  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1480/3000]  eta: 0:38:31  lr: 0.000025  loss: 0.1924  time: 1.5044  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1480/3000]  eta: 0:38:30  lr: 0.000025  loss: 0.2197  time: 1.5039  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1485/3000]  eta: 0:38:23  lr: 0.000025  loss: 0.2480  time: 1.5328  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1485/3000]  eta: 0:38:23  lr: 0.000025  loss: 0.5511  time: 1.5323  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1490/3000]  eta: 0:38:16  lr: 0.000025  loss: 0.4746  time: 1.5412  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1490/3000]  eta: 0:38:15  lr: 0.000025  loss: 0.0687  time: 1.5406  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1495/3000]  eta: 0:38:07  lr: 0.000025  loss: 0.2953  time: 1.5281  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1495/3000]  eta: 0:38:08  lr: 0.000025  loss: 0.6935  time: 1.5285  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1500/3000]  eta: 0:38:00  lr: 0.000025  loss: 0.2150  time: 1.5243  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1500/3000]  eta: 0:38:00  lr: 0.000025  loss: 0.2815  time: 1.5241  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1505/3000]  eta: 0:37:52  lr: 0.000025  loss: 0.1972  time: 1.4977  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1505/3000]  eta: 0:37:52  lr: 0.000025  loss: 0.1044  time: 1.4975  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1510/3000]  eta: 0:37:45  lr: 0.000025  loss: 0.2271  time: 1.5070  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1510/3000]  eta: 0:37:45  lr: 0.000025  loss: 0.1994  time: 1.5069  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1515/3000]  eta: 0:37:37  lr: 0.000025  loss: 0.5277  time: 1.4971  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1515/3000]  eta: 0:37:37  lr: 0.000025  loss: 0.6567  time: 1.4970  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1520/3000]  eta: 0:37:29  lr: 0.000025  loss: 0.5147  time: 1.4811  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1520/3000]  eta: 0:37:29  lr: 0.000025  loss: 0.4047  time: 1.4808  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1525/3000]  eta: 0:37:21  lr: 0.000025  loss: 0.2895  time: 1.4961  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1525/3000]  eta: 0:37:21  lr: 0.000025  loss: 0.1825  time: 1.4957  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1530/3000]  eta: 0:37:14  lr: 0.000025  loss: 0.2732  time: 1.4700  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1530/3000]  eta: 0:37:13  lr: 0.000025  loss: 0.5158  time: 1.4696  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1535/3000]  eta: 0:37:06  lr: 0.000025  loss: 0.1511  time: 1.4804  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1535/3000]  eta: 0:37:05  lr: 0.000025  loss: 0.5592  time: 1.4802  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1540/3000]  eta: 0:36:58  lr: 0.000025  loss: 0.2019  time: 1.4903  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1540/3000]  eta: 0:36:58  lr: 0.000025  loss: 0.4997  time: 1.4900  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1545/3000]  eta: 0:36:51  lr: 0.000025  loss: 0.3196  time: 1.4886  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1545/3000]  eta: 0:36:50  lr: 0.000025  loss: 0.0956  time: 1.4884  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1550/3000]  eta: 0:36:43  lr: 0.000025  loss: 0.4407  time: 1.5179  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1550/3000]  eta: 0:36:43  lr: 0.000025  loss: 0.7833  time: 1.5178  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1555/3000]  eta: 0:36:35  lr: 0.000025  loss: 0.1491  time: 1.5118  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1555/3000]  eta: 0:36:35  lr: 0.000025  loss: 0.2398  time: 1.5116  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1560/3000]  eta: 0:36:28  lr: 0.000025  loss: 0.2915  time: 1.5082  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1560/3000]  eta: 0:36:27  lr: 0.000025  loss: 0.4360  time: 1.5080  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1565/3000]  eta: 0:36:20  lr: 0.000025  loss: 1.1045  time: 1.4882  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1565/3000]  eta: 0:36:19  lr: 0.000025  loss: 0.5866  time: 1.4880  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1570/3000]  eta: 0:36:12  lr: 0.000025  loss: 0.3854  time: 1.4771  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1570/3000]  eta: 0:36:12  lr: 0.000025  loss: 0.3118  time: 1.4768  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1575/3000]  eta: 0:36:04  lr: 0.000025  loss: 0.2774  time: 1.4939  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1575/3000]  eta: 0:36:04  lr: 0.000025  loss: 0.1145  time: 1.4935  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1580/3000]  eta: 0:35:57  lr: 0.000025  loss: 0.6566  time: 1.5087  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1580/3000]  eta: 0:35:57  lr: 0.000025  loss: 0.3023  time: 1.5084  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1585/3000]  eta: 0:35:49  lr: 0.000025  loss: 0.1038  time: 1.5271  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1585/3000]  eta: 0:35:49  lr: 0.000025  loss: 0.0804  time: 1.5269  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1590/3000]  eta: 0:35:42  lr: 0.000025  loss: 0.6566  time: 1.5165  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1590/3000]  eta: 0:35:41  lr: 0.000025  loss: 0.2412  time: 1.5162  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1595/3000]  eta: 0:35:34  lr: 0.000025  loss: 0.5375  time: 1.5166  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1595/3000]  eta: 0:35:34  lr: 0.000025  loss: 0.2669  time: 1.5164  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1600/3000]  eta: 0:35:26  lr: 0.000025  loss: 0.3158  time: 1.5077  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1600/3000]  eta: 0:35:26  lr: 0.000025  loss: 0.6589  time: 1.5075  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1605/3000]  eta: 0:35:19  lr: 0.000025  loss: 0.3015  time: 1.5179  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1605/3000]  eta: 0:35:18  lr: 0.000025  loss: 0.1920  time: 1.5176  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1610/3000]  eta: 0:35:11  lr: 0.000025  loss: 0.3968  time: 1.5320  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1610/3000]  eta: 0:35:11  lr: 0.000025  loss: 0.1778  time: 1.5318  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1615/3000]  eta: 0:35:04  lr: 0.000025  loss: 0.3147  time: 1.5266  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1615/3000]  eta: 0:35:03  lr: 0.000025  loss: 0.5602  time: 1.5263  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1620/3000]  eta: 0:34:56  lr: 0.000025  loss: 0.2862  time: 1.5184  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1620/3000]  eta: 0:34:56  lr: 0.000025  loss: 0.3431  time: 1.5181  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1625/3000]  eta: 0:34:49  lr: 0.000025  loss: 0.2242  time: 1.5262  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1625/3000]  eta: 0:34:48  lr: 0.000025  loss: 0.1869  time: 1.5259  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1630/3000]  eta: 0:34:41  lr: 0.000024  loss: 0.1675  time: 1.5237  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1630/3000]  eta: 0:34:41  lr: 0.000024  loss: 0.7076  time: 1.5233  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1635/3000]  eta: 0:34:34  lr: 0.000024  loss: 0.7352  time: 1.5337  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1635/3000]  eta: 0:34:33  lr: 0.000024  loss: 0.3842  time: 1.5333  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1640/3000]  eta: 0:34:26  lr: 0.000024  loss: 0.2817  time: 1.5526  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1640/3000]  eta: 0:34:26  lr: 0.000024  loss: 0.5953  time: 1.5524  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1645/3000]  eta: 0:34:19  lr: 0.000024  loss: 0.6452  time: 1.5549  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1645/3000]  eta: 0:34:18  lr: 0.000024  loss: 0.8961  time: 1.5546  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1650/3000]  eta: 0:34:11  lr: 0.000024  loss: 0.1114  time: 1.5345  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1650/3000]  eta: 0:34:11  lr: 0.000024  loss: 0.2262  time: 1.5343  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1655/3000]  eta: 0:34:03  lr: 0.000024  loss: 0.4956  time: 1.5128  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1655/3000]  eta: 0:34:03  lr: 0.000024  loss: 0.6122  time: 1.5125  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1660/3000]  eta: 0:33:56  lr: 0.000024  loss: 0.2480  time: 1.5022  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1660/3000]  eta: 0:33:55  lr: 0.000024  loss: 0.5814  time: 1.5020  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1665/3000]  eta: 0:33:48  lr: 0.000024  loss: 0.1972  time: 1.4999  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1665/3000]  eta: 0:33:48  lr: 0.000024  loss: 0.0863  time: 1.4997  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1670/3000]  eta: 0:33:41  lr: 0.000024  loss: 0.4503  time: 1.5315  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1670/3000]  eta: 0:33:40  lr: 0.000024  loss: 0.1186  time: 1.5313  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1675/3000]  eta: 0:33:33  lr: 0.000024  loss: 0.3343  time: 1.5363  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1675/3000]  eta: 0:33:33  lr: 0.000024  loss: 0.9839  time: 1.5361  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1680/3000]  eta: 0:33:25  lr: 0.000024  loss: 0.2332  time: 1.5406  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1680/3000]  eta: 0:33:25  lr: 0.000024  loss: 0.5641  time: 1.5404  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1685/3000]  eta: 0:33:18  lr: 0.000024  loss: 0.3170  time: 1.5298  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1685/3000]  eta: 0:33:17  lr: 0.000024  loss: 0.3204  time: 1.5296  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1690/3000]  eta: 0:33:11  lr: 0.000024  loss: 1.0126  time: 1.5311  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1690/3000]  eta: 0:33:10  lr: 0.000024  loss: 0.2992  time: 1.5309  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1695/3000]  eta: 0:33:03  lr: 0.000024  loss: 0.6369  time: 1.5446  data: 0.0000  max mem: 18596Train: data epoch: [10]  [1695/3000]  eta: 0:33:03  lr: 0.000024  loss: 0.3260  time: 1.5443  data: 0.0000  max mem: 18406

Train: data epoch: [10]  [1700/3000]  eta: 0:32:55  lr: 0.000024  loss: 0.2066  time: 1.5367  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1700/3000]  eta: 0:32:55  lr: 0.000024  loss: 0.4927  time: 1.5365  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1705/3000]  eta: 0:32:48  lr: 0.000024  loss: 0.4724  time: 1.5504  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1705/3000]  eta: 0:32:48  lr: 0.000024  loss: 0.1766  time: 1.5502  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1710/3000]  eta: 0:32:40  lr: 0.000024  loss: 0.2146  time: 1.5201  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1710/3000]  eta: 0:32:40  lr: 0.000024  loss: 0.5835  time: 1.5199  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1715/3000]  eta: 0:32:32  lr: 0.000024  loss: 1.0297  time: 1.5029  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1715/3000]  eta: 0:32:32  lr: 0.000024  loss: 0.7227  time: 1.5028  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1720/3000]  eta: 0:32:25  lr: 0.000024  loss: 0.3137  time: 1.5052  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1720/3000]  eta: 0:32:24  lr: 0.000024  loss: 0.5872  time: 1.5049  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1725/3000]  eta: 0:32:17  lr: 0.000024  loss: 0.2443  time: 1.4829  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1725/3000]  eta: 0:32:17  lr: 0.000024  loss: 0.4119  time: 1.4826  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1730/3000]  eta: 0:32:10  lr: 0.000024  loss: 0.3088  time: 1.5044  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1730/3000]  eta: 0:32:09  lr: 0.000024  loss: 0.4223  time: 1.5041  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1735/3000]  eta: 0:32:01  lr: 0.000024  loss: 0.2708  time: 1.4867  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1735/3000]  eta: 0:32:01  lr: 0.000024  loss: 0.6207  time: 1.4863  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1740/3000]  eta: 0:31:54  lr: 0.000024  loss: 0.7257  time: 1.4752  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1740/3000]  eta: 0:31:53  lr: 0.000024  loss: 0.6534  time: 1.4750  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1745/3000]  eta: 0:31:46  lr: 0.000024  loss: 0.0968  time: 1.4824  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1745/3000]  eta: 0:31:46  lr: 0.000024  loss: 0.3522  time: 1.4822  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1750/3000]  eta: 0:31:39  lr: 0.000024  loss: 0.2428  time: 1.4799  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1750/3000]  eta: 0:31:38  lr: 0.000024  loss: 0.1564  time: 1.4796  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1755/3000]  eta: 0:31:31  lr: 0.000024  loss: 0.3568  time: 1.5279  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1755/3000]  eta: 0:31:31  lr: 0.000024  loss: 0.3787  time: 1.5276  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1760/3000]  eta: 0:31:23  lr: 0.000024  loss: 0.1911  time: 1.5290  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1760/3000]  eta: 0:31:23  lr: 0.000024  loss: 0.4869  time: 1.5288  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1765/3000]  eta: 0:31:16  lr: 0.000024  loss: 0.3339  time: 1.5336  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1765/3000]  eta: 0:31:16  lr: 0.000024  loss: 0.4746  time: 1.5333  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1770/3000]  eta: 0:31:08  lr: 0.000024  loss: 0.1396  time: 1.5368  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1770/3000]  eta: 0:31:08  lr: 0.000024  loss: 0.4042  time: 1.5366  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1775/3000]  eta: 0:31:01  lr: 0.000024  loss: 0.6700  time: 1.5162  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1775/3000]  eta: 0:31:00  lr: 0.000024  loss: 0.3980  time: 1.5159  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1780/3000]  eta: 0:30:53  lr: 0.000024  loss: 0.5135  time: 1.5341  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1780/3000]  eta: 0:30:53  lr: 0.000024  loss: 0.1836  time: 1.5339  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1785/3000]  eta: 0:30:46  lr: 0.000024  loss: 0.4090  time: 1.5332  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1785/3000]  eta: 0:30:45  lr: 0.000024  loss: 0.4301  time: 1.5329  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1790/3000]  eta: 0:30:38  lr: 0.000024  loss: 0.0752  time: 1.5220  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1790/3000]  eta: 0:30:38  lr: 0.000024  loss: 0.2137  time: 1.5217  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1795/3000]  eta: 0:30:31  lr: 0.000024  loss: 0.2394  time: 1.5313  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1795/3000]  eta: 0:30:30  lr: 0.000024  loss: 0.1820  time: 1.5310  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1800/3000]  eta: 0:30:23  lr: 0.000024  loss: 0.4549  time: 1.5296  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1800/3000]  eta: 0:30:23  lr: 0.000024  loss: 0.6053  time: 1.5293  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1805/3000]  eta: 0:30:16  lr: 0.000024  loss: 0.1552  time: 1.5382  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1805/3000]  eta: 0:30:15  lr: 0.000024  loss: 0.2057  time: 1.5379  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1810/3000]  eta: 0:30:08  lr: 0.000024  loss: 0.4606  time: 1.5375  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1810/3000]  eta: 0:30:08  lr: 0.000024  loss: 0.3779  time: 1.5373  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1815/3000]  eta: 0:30:00  lr: 0.000024  loss: 1.0336  time: 1.5364  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1815/3000]  eta: 0:30:00  lr: 0.000024  loss: 0.4131  time: 1.5361  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1820/3000]  eta: 0:29:53  lr: 0.000024  loss: 0.3718  time: 1.5250  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1820/3000]  eta: 0:29:52  lr: 0.000024  loss: 0.4695  time: 1.5248  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1825/3000]  eta: 0:29:45  lr: 0.000024  loss: 0.2250  time: 1.5178  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1825/3000]  eta: 0:29:45  lr: 0.000024  loss: 0.1511  time: 1.5176  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1830/3000]  eta: 0:29:38  lr: 0.000024  loss: 0.3489  time: 1.5222  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1830/3000]  eta: 0:29:37  lr: 0.000024  loss: 0.4588  time: 1.5219  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1835/3000]  eta: 0:29:30  lr: 0.000024  loss: 0.3943  time: 1.5190  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1835/3000]  eta: 0:29:30  lr: 0.000024  loss: 0.3205  time: 1.5188  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1840/3000]  eta: 0:29:22  lr: 0.000024  loss: 0.1961  time: 1.5173  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1840/3000]  eta: 0:29:22  lr: 0.000024  loss: 0.5770  time: 1.5181  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1845/3000]  eta: 0:29:15  lr: 0.000024  loss: 0.6676  time: 1.5137  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1845/3000]  eta: 0:29:14  lr: 0.000024  loss: 0.7176  time: 1.5134  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1850/3000]  eta: 0:29:07  lr: 0.000024  loss: 0.2594  time: 1.5053  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1850/3000]  eta: 0:29:07  lr: 0.000024  loss: 0.1918  time: 1.5050  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1855/3000]  eta: 0:28:59  lr: 0.000024  loss: 0.3450  time: 1.5018  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1855/3000]  eta: 0:28:59  lr: 0.000024  loss: 0.8363  time: 1.5015  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1860/3000]  eta: 0:28:52  lr: 0.000024  loss: 0.6831  time: 1.5138  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1860/3000]  eta: 0:28:51  lr: 0.000024  loss: 0.1230  time: 1.5125  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1865/3000]  eta: 0:28:44  lr: 0.000024  loss: 0.3881  time: 1.5318  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1865/3000]  eta: 0:28:44  lr: 0.000024  loss: 0.2769  time: 1.5316  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1870/3000]  eta: 0:28:37  lr: 0.000024  loss: 0.2582  time: 1.5205  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1870/3000]  eta: 0:28:36  lr: 0.000024  loss: 0.1402  time: 1.5202  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1875/3000]  eta: 0:28:29  lr: 0.000024  loss: 0.1510  time: 1.5027  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1875/3000]  eta: 0:28:28  lr: 0.000024  loss: 0.4889  time: 1.5025  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1880/3000]  eta: 0:28:21  lr: 0.000024  loss: 0.1225  time: 1.4966  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1880/3000]  eta: 0:28:21  lr: 0.000024  loss: 0.6939  time: 1.4963  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1885/3000]  eta: 0:28:14  lr: 0.000024  loss: 0.1833  time: 1.4801  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1885/3000]  eta: 0:28:13  lr: 0.000024  loss: 0.4933  time: 1.4798  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1890/3000]  eta: 0:28:06  lr: 0.000024  loss: 0.2241  time: 1.5040  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1890/3000]  eta: 0:28:06  lr: 0.000024  loss: 0.1365  time: 1.5038  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1895/3000]  eta: 0:27:58  lr: 0.000024  loss: 0.3849  time: 1.5216  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1895/3000]  eta: 0:27:58  lr: 0.000024  loss: 0.3441  time: 1.5212  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1900/3000]  eta: 0:27:51  lr: 0.000024  loss: 0.0607  time: 1.5069  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1900/3000]  eta: 0:27:50  lr: 0.000024  loss: 0.2140  time: 1.5066  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1905/3000]  eta: 0:27:43  lr: 0.000024  loss: 0.4574  time: 1.5119  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1905/3000]  eta: 0:27:43  lr: 0.000024  loss: 0.0939  time: 1.5117  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1910/3000]  eta: 0:27:36  lr: 0.000024  loss: 0.3326  time: 1.5127  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1910/3000]  eta: 0:27:35  lr: 0.000024  loss: 0.5138  time: 1.5125  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1915/3000]  eta: 0:27:28  lr: 0.000024  loss: 0.6821  time: 1.4936  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1915/3000]  eta: 0:27:27  lr: 0.000024  loss: 0.6925  time: 1.4934  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1920/3000]  eta: 0:27:20  lr: 0.000024  loss: 0.1526  time: 1.5251  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1920/3000]  eta: 0:27:20  lr: 0.000024  loss: 0.1271  time: 1.5249  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1925/3000]  eta: 0:27:13  lr: 0.000024  loss: 0.6139  time: 1.5170  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1925/3000]  eta: 0:27:12  lr: 0.000024  loss: 0.3912  time: 1.5166  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1930/3000]  eta: 0:27:05  lr: 0.000024  loss: 0.3487  time: 1.4929  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1930/3000]  eta: 0:27:05  lr: 0.000024  loss: 0.3161  time: 1.4925  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1935/3000]  eta: 0:26:57  lr: 0.000024  loss: 0.3418  time: 1.5067  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1935/3000]  eta: 0:26:57  lr: 0.000024  loss: 0.5689  time: 1.5062  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1940/3000]  eta: 0:26:50  lr: 0.000024  loss: 0.2622  time: 1.4915  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1940/3000]  eta: 0:26:49  lr: 0.000024  loss: 0.7707  time: 1.4911  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1945/3000]  eta: 0:26:42  lr: 0.000024  loss: 0.2968  time: 1.4947  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1945/3000]  eta: 0:26:42  lr: 0.000024  loss: 0.4545  time: 1.4945  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1950/3000]  eta: 0:26:35  lr: 0.000024  loss: 0.1362  time: 1.5295  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1950/3000]  eta: 0:26:34  lr: 0.000024  loss: 0.4205  time: 1.5292  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1955/3000]  eta: 0:26:27  lr: 0.000024  loss: 0.3761  time: 1.5224  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1955/3000]  eta: 0:26:27  lr: 0.000024  loss: 0.3317  time: 1.5222  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1960/3000]  eta: 0:26:19  lr: 0.000024  loss: 0.3025  time: 1.5176  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1960/3000]  eta: 0:26:19  lr: 0.000024  loss: 0.5506  time: 1.5172  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1965/3000]  eta: 0:26:12  lr: 0.000024  loss: 0.3332  time: 1.5274  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1965/3000]  eta: 0:26:11  lr: 0.000024  loss: 0.5250  time: 1.5270  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1970/3000]  eta: 0:26:04  lr: 0.000024  loss: 0.0604  time: 1.5144  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1970/3000]  eta: 0:26:04  lr: 0.000024  loss: 0.4134  time: 1.5142  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1975/3000]  eta: 0:25:57  lr: 0.000024  loss: 0.1263  time: 1.5240  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1975/3000]  eta: 0:25:56  lr: 0.000024  loss: 0.2108  time: 1.5237  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1980/3000]  eta: 0:25:49  lr: 0.000024  loss: 0.4049  time: 1.5292  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1980/3000]  eta: 0:25:49  lr: 0.000024  loss: 0.4862  time: 1.5290  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1985/3000]  eta: 0:25:41  lr: 0.000024  loss: 0.1335  time: 1.5200  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1985/3000]  eta: 0:25:41  lr: 0.000024  loss: 0.8853  time: 1.5198  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1990/3000]  eta: 0:25:34  lr: 0.000024  loss: 0.5673  time: 1.5241  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1990/3000]  eta: 0:25:34  lr: 0.000024  loss: 0.9115  time: 1.5239  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [1995/3000]  eta: 0:25:26  lr: 0.000024  loss: 0.3490  time: 1.5182  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [1995/3000]  eta: 0:25:26  lr: 0.000024  loss: 0.6643  time: 1.5180  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2000/3000]  eta: 0:25:19  lr: 0.000024  loss: 0.5004  time: 1.5261  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2000/3000]  eta: 0:25:18  lr: 0.000024  loss: 0.3291  time: 1.5259  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2005/3000]  eta: 0:25:11  lr: 0.000024  loss: 0.1799  time: 1.5277  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2005/3000]  eta: 0:25:11  lr: 0.000024  loss: 0.0820  time: 1.5275  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2010/3000]  eta: 0:25:04  lr: 0.000024  loss: 0.1052  time: 1.5303  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2010/3000]  eta: 0:25:03  lr: 0.000024  loss: 0.4755  time: 1.5301  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2015/3000]  eta: 0:24:56  lr: 0.000024  loss: 0.4533  time: 1.5476  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2015/3000]  eta: 0:24:56  lr: 0.000024  loss: 0.5325  time: 1.5475  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2020/3000]  eta: 0:24:49  lr: 0.000024  loss: 0.2136  time: 1.5468  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2020/3000]  eta: 0:24:48  lr: 0.000024  loss: 0.2653  time: 1.5465  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2025/3000]  eta: 0:24:41  lr: 0.000024  loss: 0.0933  time: 1.5523  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2025/3000]  eta: 0:24:41  lr: 0.000024  loss: 0.4056  time: 1.5520  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2030/3000]  eta: 0:24:33  lr: 0.000024  loss: 0.5485  time: 1.5513  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2030/3000]  eta: 0:24:33  lr: 0.000024  loss: 0.5825  time: 1.5510  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2035/3000]  eta: 0:24:26  lr: 0.000024  loss: 0.9498  time: 1.5463  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2035/3000]  eta: 0:24:26  lr: 0.000024  loss: 0.7779  time: 1.5460  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2040/3000]  eta: 0:24:18  lr: 0.000024  loss: 0.6285  time: 1.5338  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2040/3000]  eta: 0:24:18  lr: 0.000024  loss: 0.3916  time: 1.5336  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2045/3000]  eta: 0:24:11  lr: 0.000024  loss: 1.4970  time: 1.5142  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2045/3000]  eta: 0:24:10  lr: 0.000024  loss: 0.3021  time: 1.5139  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2050/3000]  eta: 0:24:03  lr: 0.000024  loss: 0.1453  time: 1.4881  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2050/3000]  eta: 0:24:03  lr: 0.000024  loss: 0.5826  time: 1.4879  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2055/3000]  eta: 0:23:55  lr: 0.000024  loss: 0.2399  time: 1.4906  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2055/3000]  eta: 0:23:55  lr: 0.000024  loss: 0.4234  time: 1.4903  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2060/3000]  eta: 0:23:48  lr: 0.000024  loss: 0.5869  time: 1.5075  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2060/3000]  eta: 0:23:47  lr: 0.000024  loss: 0.8826  time: 1.5073  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2065/3000]  eta: 0:23:40  lr: 0.000024  loss: 0.2760  time: 1.5380  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2065/3000]  eta: 0:23:40  lr: 0.000024  loss: 0.4894  time: 1.5378  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2070/3000]  eta: 0:23:33  lr: 0.000024  loss: 0.3198  time: 1.5533  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2070/3000]  eta: 0:23:32  lr: 0.000024  loss: 0.3076  time: 1.5530  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2075/3000]  eta: 0:23:25  lr: 0.000024  loss: 0.5086  time: 1.5638  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2075/3000]  eta: 0:23:25  lr: 0.000024  loss: 0.3302  time: 1.5636  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2080/3000]  eta: 0:23:18  lr: 0.000024  loss: 1.4182  time: 1.5603  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2080/3000]  eta: 0:23:17  lr: 0.000024  loss: 0.3710  time: 1.5600  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2085/3000]  eta: 0:23:10  lr: 0.000024  loss: 0.2534  time: 1.5309  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2085/3000]  eta: 0:23:10  lr: 0.000024  loss: 1.1434  time: 1.5306  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2090/3000]  eta: 0:23:02  lr: 0.000024  loss: 0.1684  time: 1.5182  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2090/3000]  eta: 0:23:02  lr: 0.000024  loss: 0.9104  time: 1.5180  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2095/3000]  eta: 0:22:55  lr: 0.000024  loss: 0.1986  time: 1.4941  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2095/3000]  eta: 0:22:54  lr: 0.000024  loss: 0.5102  time: 1.4938  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2100/3000]  eta: 0:22:47  lr: 0.000024  loss: 0.3698  time: 1.4758  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2100/3000]  eta: 0:22:47  lr: 0.000024  loss: 0.1404  time: 1.4755  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2105/3000]  eta: 0:22:39  lr: 0.000024  loss: 0.0628  time: 1.4881  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2105/3000]  eta: 0:22:39  lr: 0.000024  loss: 0.4768  time: 1.4878  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2110/3000]  eta: 0:22:32  lr: 0.000024  loss: 0.4030  time: 1.5066  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2110/3000]  eta: 0:22:32  lr: 0.000024  loss: 0.5291  time: 1.5062  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2115/3000]  eta: 0:22:24  lr: 0.000024  loss: 0.2694  time: 1.5317  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2115/3000]  eta: 0:22:24  lr: 0.000024  loss: 0.4520  time: 1.5314  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2120/3000]  eta: 0:22:17  lr: 0.000024  loss: 0.2342  time: 1.5243  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2120/3000]  eta: 0:22:16  lr: 0.000024  loss: 0.3218  time: 1.5239  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2125/3000]  eta: 0:22:09  lr: 0.000024  loss: 0.0904  time: 1.5254  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2125/3000]  eta: 0:22:09  lr: 0.000024  loss: 0.4847  time: 1.5252  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2130/3000]  eta: 0:22:01  lr: 0.000024  loss: 0.1384  time: 1.5269  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2130/3000]  eta: 0:22:01  lr: 0.000024  loss: 0.1823  time: 1.5266  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2135/3000]  eta: 0:21:54  lr: 0.000024  loss: 0.3426  time: 1.4921  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2135/3000]  eta: 0:21:54  lr: 0.000024  loss: 0.7001  time: 1.4919  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2140/3000]  eta: 0:21:46  lr: 0.000024  loss: 0.5324  time: 1.5082  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2140/3000]  eta: 0:21:46  lr: 0.000024  loss: 0.3991  time: 1.5079  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2145/3000]  eta: 0:21:39  lr: 0.000024  loss: 0.6905  time: 1.5021  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2145/3000]  eta: 0:21:38  lr: 0.000024  loss: 0.2116  time: 1.5018  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2150/3000]  eta: 0:21:31  lr: 0.000024  loss: 0.0667  time: 1.4903  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2150/3000]  eta: 0:21:31  lr: 0.000024  loss: 0.2760  time: 1.4901  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2155/3000]  eta: 0:21:23  lr: 0.000024  loss: 0.1025  time: 1.5198  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2155/3000]  eta: 0:21:23  lr: 0.000024  loss: 0.4056  time: 1.5195  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2160/3000]  eta: 0:21:16  lr: 0.000024  loss: 0.1819  time: 1.5300  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2160/3000]  eta: 0:21:16  lr: 0.000024  loss: 0.6366  time: 1.5298  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2165/3000]  eta: 0:21:08  lr: 0.000024  loss: 0.1116  time: 1.5398  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2165/3000]  eta: 0:21:08  lr: 0.000024  loss: 0.4933  time: 1.5395  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2170/3000]  eta: 0:21:01  lr: 0.000024  loss: 0.3325  time: 1.5518  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2170/3000]  eta: 0:21:00  lr: 0.000024  loss: 0.5771  time: 1.5516  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2175/3000]  eta: 0:20:53  lr: 0.000024  loss: 0.3738  time: 1.5470  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2175/3000]  eta: 0:20:53  lr: 0.000024  loss: 1.0449  time: 1.5468  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2180/3000]  eta: 0:20:46  lr: 0.000024  loss: 0.4236  time: 1.5399  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2180/3000]  eta: 0:20:45  lr: 0.000024  loss: 0.1317  time: 1.5397  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2185/3000]  eta: 0:20:38  lr: 0.000024  loss: 0.3760  time: 1.5352  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2185/3000]  eta: 0:20:38  lr: 0.000024  loss: 0.1549  time: 1.5350  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2190/3000]  eta: 0:20:31  lr: 0.000024  loss: 0.1780  time: 1.5458  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2190/3000]  eta: 0:20:30  lr: 0.000024  loss: 0.6517  time: 1.5456  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2195/3000]  eta: 0:20:23  lr: 0.000024  loss: 0.4622  time: 1.5461  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2195/3000]  eta: 0:20:23  lr: 0.000024  loss: 0.1223  time: 1.5459  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2200/3000]  eta: 0:20:15  lr: 0.000024  loss: 0.3208  time: 1.5479  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2200/3000]  eta: 0:20:15  lr: 0.000024  loss: 0.5319  time: 1.5476  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2205/3000]  eta: 0:20:08  lr: 0.000024  loss: 0.1120  time: 1.5344  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2205/3000]  eta: 0:20:08  lr: 0.000024  loss: 0.5948  time: 1.5342  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2210/3000]  eta: 0:20:00  lr: 0.000024  loss: 0.1957  time: 1.5128  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2210/3000]  eta: 0:20:00  lr: 0.000024  loss: 0.2841  time: 1.5125  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2215/3000]  eta: 0:19:52  lr: 0.000024  loss: 0.2601  time: 1.5046  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2215/3000]  eta: 0:19:52  lr: 0.000024  loss: 0.1784  time: 1.5043  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2220/3000]  eta: 0:19:45  lr: 0.000024  loss: 0.1204  time: 1.5135  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2220/3000]  eta: 0:19:45  lr: 0.000024  loss: 0.3971  time: 1.5131  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2225/3000]  eta: 0:19:37  lr: 0.000024  loss: 0.2255  time: 1.5399  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2225/3000]  eta: 0:19:37  lr: 0.000024  loss: 0.3344  time: 1.5394  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2230/3000]  eta: 0:19:30  lr: 0.000024  loss: 0.6130  time: 1.5433  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2230/3000]  eta: 0:19:30  lr: 0.000024  loss: 0.4823  time: 1.5428  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2235/3000]  eta: 0:19:22  lr: 0.000024  loss: 0.2634  time: 1.5463  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2235/3000]  eta: 0:19:22  lr: 0.000024  loss: 0.1978  time: 1.5458  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2240/3000]  eta: 0:19:15  lr: 0.000024  loss: 0.3669  time: 1.5435  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2240/3000]  eta: 0:19:15  lr: 0.000024  loss: 0.5040  time: 1.5432  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2245/3000]  eta: 0:19:07  lr: 0.000024  loss: 0.5473  time: 1.5214  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2245/3000]  eta: 0:19:07  lr: 0.000024  loss: 0.8426  time: 1.5211  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2250/3000]  eta: 0:19:00  lr: 0.000024  loss: 0.2766  time: 1.5353  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2250/3000]  eta: 0:18:59  lr: 0.000024  loss: 0.3916  time: 1.5340  data: 0.0000  max mem: 18406
Train: data epoch: [10]  [2255/3000]  eta: 0:18:52  lr: 0.000024  loss: 0.5352  time: 1.5452  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2255/3000]  eta: 0:18:52  lr: 0.000024  loss: 0.6200  time: 1.5439  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2260/3000]  eta: 0:18:44  lr: 0.000024  loss: 0.0785  time: 1.5276  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2260/3000]  eta: 0:18:44  lr: 0.000024  loss: 0.2736  time: 1.5263  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2265/3000]  eta: 0:18:37  lr: 0.000024  loss: 0.3505  time: 1.5392  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2265/3000]  eta: 0:18:37  lr: 0.000024  loss: 0.4955  time: 1.5380  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2270/3000]  eta: 0:18:29  lr: 0.000024  loss: 0.2531  time: 1.5402  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2270/3000]  eta: 0:18:29  lr: 0.000024  loss: 0.1049  time: 1.5400  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2275/3000]  eta: 0:18:22  lr: 0.000024  loss: 0.7057  time: 1.5494  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2275/3000]  eta: 0:18:22  lr: 0.000024  loss: 0.7019  time: 1.5492  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2280/3000]  eta: 0:18:14  lr: 0.000024  loss: 0.3976  time: 1.5506  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2280/3000]  eta: 0:18:14  lr: 0.000024  loss: 0.1040  time: 1.5503  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2285/3000]  eta: 0:18:06  lr: 0.000024  loss: 0.3321  time: 1.5281  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2285/3000]  eta: 0:18:06  lr: 0.000024  loss: 0.1333  time: 1.5279  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2290/3000]  eta: 0:17:59  lr: 0.000024  loss: 0.2106  time: 1.5165  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2290/3000]  eta: 0:17:59  lr: 0.000024  loss: 0.8645  time: 1.5162  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2295/3000]  eta: 0:17:51  lr: 0.000024  loss: 0.3010  time: 1.4937  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2295/3000]  eta: 0:17:51  lr: 0.000024  loss: 0.8411  time: 1.4934  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2300/3000]  eta: 0:17:44  lr: 0.000024  loss: 0.7834  time: 1.4885  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2300/3000]  eta: 0:17:43  lr: 0.000024  loss: 1.0368  time: 1.4882  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2305/3000]  eta: 0:17:36  lr: 0.000024  loss: 0.6301  time: 1.5197  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2305/3000]  eta: 0:17:36  lr: 0.000024  loss: 0.3168  time: 1.5194  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2310/3000]  eta: 0:17:28  lr: 0.000024  loss: 1.0117  time: 1.5203  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2310/3000]  eta: 0:17:28  lr: 0.000024  loss: 0.1364  time: 1.5200  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2315/3000]  eta: 0:17:21  lr: 0.000024  loss: 0.3438  time: 1.5079  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2315/3000]  eta: 0:17:21  lr: 0.000024  loss: 0.3700  time: 1.5077  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2320/3000]  eta: 0:17:13  lr: 0.000024  loss: 0.1337  time: 1.5127  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2320/3000]  eta: 0:17:13  lr: 0.000024  loss: 0.0742  time: 1.5125  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2325/3000]  eta: 0:17:06  lr: 0.000024  loss: 0.1939  time: 1.5046  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2325/3000]  eta: 0:17:05  lr: 0.000024  loss: 0.3753  time: 1.5043  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2330/3000]  eta: 0:16:58  lr: 0.000024  loss: 0.2508  time: 1.4959  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2330/3000]  eta: 0:16:58  lr: 0.000024  loss: 0.4108  time: 1.4957  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2335/3000]  eta: 0:16:50  lr: 0.000024  loss: 0.6570  time: 1.4980  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2335/3000]  eta: 0:16:50  lr: 0.000024  loss: 0.1316  time: 1.4978  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2340/3000]  eta: 0:16:43  lr: 0.000024  loss: 0.2762  time: 1.5057  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2340/3000]  eta: 0:16:42  lr: 0.000024  loss: 0.3658  time: 1.5054  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2345/3000]  eta: 0:16:35  lr: 0.000024  loss: 0.8375  time: 1.5099  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2345/3000]  eta: 0:16:35  lr: 0.000024  loss: 0.3169  time: 1.5097  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2350/3000]  eta: 0:16:28  lr: 0.000024  loss: 0.3823  time: 1.5206  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2350/3000]  eta: 0:16:27  lr: 0.000024  loss: 0.4187  time: 1.5203  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2355/3000]  eta: 0:16:20  lr: 0.000024  loss: 0.0748  time: 1.5420  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2355/3000]  eta: 0:16:20  lr: 0.000024  loss: 0.5566  time: 1.5418  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2360/3000]  eta: 0:16:12  lr: 0.000024  loss: 0.5703  time: 1.5329  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2360/3000]  eta: 0:16:12  lr: 0.000024  loss: 0.6640  time: 1.5326  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2365/3000]  eta: 0:16:05  lr: 0.000024  loss: 0.5095  time: 1.5426  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2365/3000]  eta: 0:16:05  lr: 0.000024  loss: 0.3811  time: 1.5423  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2370/3000]  eta: 0:15:57  lr: 0.000024  loss: 0.3593  time: 1.5181  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2370/3000]  eta: 0:15:57  lr: 0.000024  loss: 0.1574  time: 1.5178  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2375/3000]  eta: 0:15:49  lr: 0.000024  loss: 0.3535  time: 1.5075  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2375/3000]  eta: 0:15:49  lr: 0.000024  loss: 0.0996  time: 1.5072  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2380/3000]  eta: 0:15:42  lr: 0.000024  loss: 0.3274  time: 1.5179  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2380/3000]  eta: 0:15:42  lr: 0.000024  loss: 0.4424  time: 1.5177  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2385/3000]  eta: 0:15:34  lr: 0.000024  loss: 0.3256  time: 1.5179  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2385/3000]  eta: 0:15:34  lr: 0.000024  loss: 0.4509  time: 1.5177  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2390/3000]  eta: 0:15:27  lr: 0.000024  loss: 1.1244  time: 1.5454  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2390/3000]  eta: 0:15:27  lr: 0.000024  loss: 0.4145  time: 1.5452  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2395/3000]  eta: 0:15:19  lr: 0.000024  loss: 0.4035  time: 1.5484  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2395/3000]  eta: 0:15:19  lr: 0.000024  loss: 0.4238  time: 1.5481  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2400/3000]  eta: 0:15:12  lr: 0.000024  loss: 0.3767  time: 1.5345  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2400/3000]  eta: 0:15:11  lr: 0.000024  loss: 0.5738  time: 1.5342  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2405/3000]  eta: 0:15:04  lr: 0.000024  loss: 0.4530  time: 1.5248  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2405/3000]  eta: 0:15:04  lr: 0.000024  loss: 0.2229  time: 1.5245  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2410/3000]  eta: 0:14:56  lr: 0.000024  loss: 0.3885  time: 1.5347  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2410/3000]  eta: 0:14:56  lr: 0.000024  loss: 0.2373  time: 1.5344  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2415/3000]  eta: 0:14:49  lr: 0.000024  loss: 0.9156  time: 1.5476  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2415/3000]  eta: 0:14:49  lr: 0.000024  loss: 0.2331  time: 1.5474  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2420/3000]  eta: 0:14:41  lr: 0.000024  loss: 0.4166  time: 1.5593  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2420/3000]  eta: 0:14:41  lr: 0.000024  loss: 0.3344  time: 1.5590  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2425/3000]  eta: 0:14:34  lr: 0.000024  loss: 0.3697  time: 1.5492  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2425/3000]  eta: 0:14:34  lr: 0.000024  loss: 0.3400  time: 1.5489  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2430/3000]  eta: 0:14:26  lr: 0.000024  loss: 0.3905  time: 1.5244  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2430/3000]  eta: 0:14:26  lr: 0.000024  loss: 0.3611  time: 1.5243  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2435/3000]  eta: 0:14:19  lr: 0.000024  loss: 0.3688  time: 1.5115  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2435/3000]  eta: 0:14:18  lr: 0.000024  loss: 0.4710  time: 1.5112  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2440/3000]  eta: 0:14:11  lr: 0.000024  loss: 0.4305  time: 1.5174  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2440/3000]  eta: 0:14:11  lr: 0.000024  loss: 0.6012  time: 1.5171  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2445/3000]  eta: 0:14:03  lr: 0.000024  loss: 0.3875  time: 1.5140  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2445/3000]  eta: 0:14:03  lr: 0.000024  loss: 0.4145  time: 1.5138  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2450/3000]  eta: 0:13:56  lr: 0.000024  loss: 0.5321  time: 1.5234  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2450/3000]  eta: 0:13:56  lr: 0.000024  loss: 0.0769  time: 1.5231  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2455/3000]  eta: 0:13:48  lr: 0.000024  loss: 0.1866  time: 1.5271  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2455/3000]  eta: 0:13:48  lr: 0.000024  loss: 0.2934  time: 1.5269  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2460/3000]  eta: 0:13:41  lr: 0.000024  loss: 0.5249  time: 1.5164  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2460/3000]  eta: 0:13:40  lr: 0.000024  loss: 0.3769  time: 1.5162  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2465/3000]  eta: 0:13:33  lr: 0.000024  loss: 0.4836  time: 1.5162  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2465/3000]  eta: 0:13:33  lr: 0.000024  loss: 0.5340  time: 1.5159  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2470/3000]  eta: 0:13:25  lr: 0.000024  loss: 0.4297  time: 1.5223  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2470/3000]  eta: 0:13:25  lr: 0.000024  loss: 0.1401  time: 1.5221  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2475/3000]  eta: 0:13:18  lr: 0.000024  loss: 0.2740  time: 1.5017  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2475/3000]  eta: 0:13:17  lr: 0.000024  loss: 0.2391  time: 1.5015  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2480/3000]  eta: 0:13:10  lr: 0.000024  loss: 0.3660  time: 1.5227  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2480/3000]  eta: 0:13:10  lr: 0.000024  loss: 0.2586  time: 1.5224  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2485/3000]  eta: 0:13:03  lr: 0.000024  loss: 0.1943  time: 1.5290  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2485/3000]  eta: 0:13:02  lr: 0.000024  loss: 0.3873  time: 1.5288  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2490/3000]  eta: 0:12:55  lr: 0.000024  loss: 0.1412  time: 1.5255  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2490/3000]  eta: 0:12:55  lr: 0.000024  loss: 0.3428  time: 1.5252  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2495/3000]  eta: 0:12:47  lr: 0.000024  loss: 0.4892  time: 1.5080  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2495/3000]  eta: 0:12:47  lr: 0.000024  loss: 0.5580  time: 1.5078  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2500/3000]  eta: 0:12:40  lr: 0.000024  loss: 0.9276  time: 1.5069  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2500/3000]  eta: 0:12:40  lr: 0.000024  loss: 0.2336  time: 1.5067  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2505/3000]  eta: 0:12:32  lr: 0.000024  loss: 0.2912  time: 1.4934  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2505/3000]  eta: 0:12:32  lr: 0.000024  loss: 0.2803  time: 1.4931  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2510/3000]  eta: 0:12:24  lr: 0.000024  loss: 0.3322  time: 1.4676  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2510/3000]  eta: 0:12:24  lr: 0.000024  loss: 0.3794  time: 1.4673  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2515/3000]  eta: 0:12:17  lr: 0.000024  loss: 0.1213  time: 1.4775  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2515/3000]  eta: 0:12:16  lr: 0.000024  loss: 0.3107  time: 1.4773  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2520/3000]  eta: 0:12:09  lr: 0.000024  loss: 0.3051  time: 1.4421  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2520/3000]  eta: 0:12:09  lr: 0.000024  loss: 0.5794  time: 1.4418  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2525/3000]  eta: 0:12:01  lr: 0.000024  loss: 0.4204  time: 1.4414  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2525/3000]  eta: 0:12:01  lr: 0.000024  loss: 0.2217  time: 1.4411  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2530/3000]  eta: 0:11:54  lr: 0.000024  loss: 0.8871  time: 1.4506  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2530/3000]  eta: 0:11:54  lr: 0.000024  loss: 0.0718  time: 1.4504  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2535/3000]  eta: 0:11:46  lr: 0.000024  loss: 0.2341  time: 1.4723  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2535/3000]  eta: 0:11:46  lr: 0.000024  loss: 0.4623  time: 1.4720  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2540/3000]  eta: 0:11:39  lr: 0.000024  loss: 0.4852  time: 1.5093  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2540/3000]  eta: 0:11:38  lr: 0.000024  loss: 0.4765  time: 1.5091  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2545/3000]  eta: 0:11:31  lr: 0.000024  loss: 0.3025  time: 1.5148  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2545/3000]  eta: 0:11:31  lr: 0.000024  loss: 0.1793  time: 1.5146  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2550/3000]  eta: 0:11:23  lr: 0.000024  loss: 0.1506  time: 1.5332  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2550/3000]  eta: 0:11:23  lr: 0.000024  loss: 0.3595  time: 1.5329  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2555/3000]  eta: 0:11:16  lr: 0.000024  loss: 0.1472  time: 1.5225  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2555/3000]  eta: 0:11:16  lr: 0.000024  loss: 0.4576  time: 1.5222  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2560/3000]  eta: 0:11:08  lr: 0.000024  loss: 0.5346  time: 1.5116  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2560/3000]  eta: 0:11:08  lr: 0.000024  loss: 0.1421  time: 1.5113  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2565/3000]  eta: 0:11:01  lr: 0.000024  loss: 0.2450  time: 1.5238  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2565/3000]  eta: 0:11:00  lr: 0.000024  loss: 0.4116  time: 1.5237  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2570/3000]  eta: 0:10:53  lr: 0.000024  loss: 0.1933  time: 1.5330  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2570/3000]  eta: 0:10:53  lr: 0.000024  loss: 0.4231  time: 1.5328  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2575/3000]  eta: 0:10:45  lr: 0.000024  loss: 0.1717  time: 1.5419  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2575/3000]  eta: 0:10:45  lr: 0.000024  loss: 0.4669  time: 1.5416  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2580/3000]  eta: 0:10:38  lr: 0.000024  loss: 0.4067  time: 1.5500  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2580/3000]  eta: 0:10:38  lr: 0.000024  loss: 1.0459  time: 1.5497  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2585/3000]  eta: 0:10:30  lr: 0.000024  loss: 0.8542  time: 1.5325  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2585/3000]  eta: 0:10:30  lr: 0.000024  loss: 0.1801  time: 1.5321  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2590/3000]  eta: 0:10:23  lr: 0.000024  loss: 0.2754  time: 1.5070  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2590/3000]  eta: 0:10:22  lr: 0.000024  loss: 0.2326  time: 1.5067  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2595/3000]  eta: 0:10:15  lr: 0.000024  loss: 0.6772  time: 1.5117  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2595/3000]  eta: 0:10:15  lr: 0.000024  loss: 0.3652  time: 1.5114  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2600/3000]  eta: 0:10:07  lr: 0.000024  loss: 0.4173  time: 1.4800  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2600/3000]  eta: 0:10:07  lr: 0.000024  loss: 0.2770  time: 1.4798  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2605/3000]  eta: 0:10:00  lr: 0.000024  loss: 0.2548  time: 1.4807  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2605/3000]  eta: 0:10:00  lr: 0.000024  loss: 0.6714  time: 1.4804  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2610/3000]  eta: 0:09:52  lr: 0.000024  loss: 0.1684  time: 1.4944  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2610/3000]  eta: 0:09:52  lr: 0.000024  loss: 0.2316  time: 1.4941  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2615/3000]  eta: 0:09:45  lr: 0.000024  loss: 0.3724  time: 1.5041  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2615/3000]  eta: 0:09:44  lr: 0.000024  loss: 0.2163  time: 1.5038  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2620/3000]  eta: 0:09:37  lr: 0.000024  loss: 0.5784  time: 1.5193  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2620/3000]  eta: 0:09:37  lr: 0.000024  loss: 0.3467  time: 1.5190  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2625/3000]  eta: 0:09:29  lr: 0.000024  loss: 0.2806  time: 1.5166  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2625/3000]  eta: 0:09:29  lr: 0.000024  loss: 0.3429  time: 1.5163  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2630/3000]  eta: 0:09:22  lr: 0.000024  loss: 0.0636  time: 1.5128  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2630/3000]  eta: 0:09:22  lr: 0.000024  loss: 0.4624  time: 1.5126  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2635/3000]  eta: 0:09:14  lr: 0.000024  loss: 0.3002  time: 1.5160  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2635/3000]  eta: 0:09:14  lr: 0.000024  loss: 0.3394  time: 1.5157  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2640/3000]  eta: 0:09:06  lr: 0.000024  loss: 0.3190  time: 1.5121  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2640/3000]  eta: 0:09:06  lr: 0.000024  loss: 0.0843  time: 1.5118  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2645/3000]  eta: 0:08:59  lr: 0.000024  loss: 0.7069  time: 1.5245  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2645/3000]  eta: 0:08:59  lr: 0.000024  loss: 0.4286  time: 1.5242  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2650/3000]  eta: 0:08:51  lr: 0.000024  loss: 0.7063  time: 1.5275  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2650/3000]  eta: 0:08:51  lr: 0.000024  loss: 0.3457  time: 1.5271  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2655/3000]  eta: 0:08:44  lr: 0.000024  loss: 0.2239  time: 1.5005  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2655/3000]  eta: 0:08:44  lr: 0.000024  loss: 0.1468  time: 1.5002  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2660/3000]  eta: 0:08:36  lr: 0.000024  loss: 0.4713  time: 1.5169  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2660/3000]  eta: 0:08:36  lr: 0.000024  loss: 0.0992  time: 1.5167  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2665/3000]  eta: 0:08:29  lr: 0.000024  loss: 0.1119  time: 1.5358  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2665/3000]  eta: 0:08:28  lr: 0.000024  loss: 0.3363  time: 1.5356  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2670/3000]  eta: 0:08:21  lr: 0.000024  loss: 0.3661  time: 1.5356  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2670/3000]  eta: 0:08:21  lr: 0.000024  loss: 0.3391  time: 1.5354  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2675/3000]  eta: 0:08:13  lr: 0.000024  loss: 0.2607  time: 1.5531  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2675/3000]  eta: 0:08:13  lr: 0.000024  loss: 0.5340  time: 1.5529  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2680/3000]  eta: 0:08:06  lr: 0.000024  loss: 0.1435  time: 1.5442  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2680/3000]  eta: 0:08:06  lr: 0.000024  loss: 0.2212  time: 1.5440  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2685/3000]  eta: 0:07:58  lr: 0.000024  loss: 0.5525  time: 1.5035  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2685/3000]  eta: 0:07:58  lr: 0.000024  loss: 0.6108  time: 1.5032  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2690/3000]  eta: 0:07:50  lr: 0.000024  loss: 0.2928  time: 1.4905  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2690/3000]  eta: 0:07:50  lr: 0.000024  loss: 0.2487  time: 1.4902  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2695/3000]  eta: 0:07:43  lr: 0.000024  loss: 0.3223  time: 1.5027  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2695/3000]  eta: 0:07:43  lr: 0.000024  loss: 0.7937  time: 1.5025  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2700/3000]  eta: 0:07:35  lr: 0.000024  loss: 0.4664  time: 1.5090  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2700/3000]  eta: 0:07:35  lr: 0.000024  loss: 0.1502  time: 1.5088  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2705/3000]  eta: 0:07:28  lr: 0.000024  loss: 0.1822  time: 1.5283  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2705/3000]  eta: 0:07:28  lr: 0.000024  loss: 0.0808  time: 1.5281  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2710/3000]  eta: 0:07:20  lr: 0.000024  loss: 0.0634  time: 1.5377  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2710/3000]  eta: 0:07:20  lr: 0.000024  loss: 0.5068  time: 1.5375  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2715/3000]  eta: 0:07:13  lr: 0.000024  loss: 0.0533  time: 1.5285  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2715/3000]  eta: 0:07:12  lr: 0.000024  loss: 0.6443  time: 1.5282  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2720/3000]  eta: 0:07:05  lr: 0.000024  loss: 0.6541  time: 1.5273  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2720/3000]  eta: 0:07:05  lr: 0.000024  loss: 0.5419  time: 1.5270  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2725/3000]  eta: 0:06:57  lr: 0.000024  loss: 0.2723  time: 1.5473  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2725/3000]  eta: 0:06:57  lr: 0.000024  loss: 0.9558  time: 1.5471  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2730/3000]  eta: 0:06:50  lr: 0.000024  loss: 0.3274  time: 1.5441  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2730/3000]  eta: 0:06:50  lr: 0.000024  loss: 0.1140  time: 1.5438  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2735/3000]  eta: 0:06:42  lr: 0.000024  loss: 0.5303  time: 1.5295  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2735/3000]  eta: 0:06:42  lr: 0.000024  loss: 0.3164  time: 1.5293  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2740/3000]  eta: 0:06:35  lr: 0.000024  loss: 0.2798  time: 1.5303  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2740/3000]  eta: 0:06:35  lr: 0.000024  loss: 0.4029  time: 1.5301  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2745/3000]  eta: 0:06:27  lr: 0.000024  loss: 0.4241  time: 1.5049  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2745/3000]  eta: 0:06:27  lr: 0.000024  loss: 0.4359  time: 1.5047  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2750/3000]  eta: 0:06:19  lr: 0.000024  loss: 0.4402  time: 1.5077  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2750/3000]  eta: 0:06:19  lr: 0.000024  loss: 0.1874  time: 1.5075  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2755/3000]  eta: 0:06:12  lr: 0.000024  loss: 0.4802  time: 1.5265  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2755/3000]  eta: 0:06:12  lr: 0.000024  loss: 0.5536  time: 1.5263  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2760/3000]  eta: 0:06:04  lr: 0.000024  loss: 0.5970  time: 1.5105  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2760/3000]  eta: 0:06:04  lr: 0.000024  loss: 0.1172  time: 1.5102  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2765/3000]  eta: 0:05:57  lr: 0.000024  loss: 0.3019  time: 1.5099  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2765/3000]  eta: 0:05:57  lr: 0.000024  loss: 0.1234  time: 1.5096  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2770/3000]  eta: 0:05:49  lr: 0.000024  loss: 1.1209  time: 1.5135  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2770/3000]  eta: 0:05:49  lr: 0.000024  loss: 0.1720  time: 1.5132  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2775/3000]  eta: 0:05:41  lr: 0.000024  loss: 0.9662  time: 1.4909  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2775/3000]  eta: 0:05:41  lr: 0.000024  loss: 1.0204  time: 1.4906  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2780/3000]  eta: 0:05:34  lr: 0.000024  loss: 0.4521  time: 1.4720  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2780/3000]  eta: 0:05:34  lr: 0.000024  loss: 0.1985  time: 1.4716  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2785/3000]  eta: 0:05:26  lr: 0.000024  loss: 0.7506  time: 1.4731  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2785/3000]  eta: 0:05:26  lr: 0.000024  loss: 0.1298  time: 1.4728  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2790/3000]  eta: 0:05:19  lr: 0.000024  loss: 0.2208  time: 1.4779  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2790/3000]  eta: 0:05:18  lr: 0.000024  loss: 0.3344  time: 1.4777  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2795/3000]  eta: 0:05:11  lr: 0.000024  loss: 0.6234  time: 1.5021  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2795/3000]  eta: 0:05:11  lr: 0.000024  loss: 0.2265  time: 1.5018  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2800/3000]  eta: 0:05:03  lr: 0.000024  loss: 0.0985  time: 1.5303  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2800/3000]  eta: 0:05:03  lr: 0.000024  loss: 0.6628  time: 1.5301  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2805/3000]  eta: 0:04:56  lr: 0.000024  loss: 0.1259  time: 1.5427  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2805/3000]  eta: 0:04:56  lr: 0.000024  loss: 0.2563  time: 1.5425  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2810/3000]  eta: 0:04:48  lr: 0.000024  loss: 0.9525  time: 1.5368  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2810/3000]  eta: 0:04:48  lr: 0.000024  loss: 0.2303  time: 1.5364  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2815/3000]  eta: 0:04:41  lr: 0.000024  loss: 0.4268  time: 1.5204  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2815/3000]  eta: 0:04:41  lr: 0.000024  loss: 0.2256  time: 1.5202  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2820/3000]  eta: 0:04:33  lr: 0.000024  loss: 0.4850  time: 1.5252  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2820/3000]  eta: 0:04:33  lr: 0.000024  loss: 0.6705  time: 1.5249  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2825/3000]  eta: 0:04:25  lr: 0.000024  loss: 0.6753  time: 1.5383  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2825/3000]  eta: 0:04:25  lr: 0.000024  loss: 0.2977  time: 1.5380  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2830/3000]  eta: 0:04:18  lr: 0.000024  loss: 0.6763  time: 1.5499  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2830/3000]  eta: 0:04:18  lr: 0.000024  loss: 0.4385  time: 1.5497  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2835/3000]  eta: 0:04:10  lr: 0.000024  loss: 0.8647  time: 1.5592  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2835/3000]  eta: 0:04:10  lr: 0.000024  loss: 0.5007  time: 1.5590  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2840/3000]  eta: 0:04:03  lr: 0.000024  loss: 0.2357  time: 1.5322  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2840/3000]  eta: 0:04:03  lr: 0.000024  loss: 0.3291  time: 1.5319  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2845/3000]  eta: 0:03:55  lr: 0.000024  loss: 0.3562  time: 1.5154  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2845/3000]  eta: 0:03:55  lr: 0.000024  loss: 0.3755  time: 1.5151  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2850/3000]  eta: 0:03:47  lr: 0.000024  loss: 0.1259  time: 1.5170  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2850/3000]  eta: 0:03:47  lr: 0.000024  loss: 0.7915  time: 1.5168  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2855/3000]  eta: 0:03:40  lr: 0.000024  loss: 0.1901  time: 1.5132  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2855/3000]  eta: 0:03:40  lr: 0.000024  loss: 0.4796  time: 1.5127  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2860/3000]  eta: 0:03:32  lr: 0.000024  loss: 0.2357  time: 1.5397  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2860/3000]  eta: 0:03:32  lr: 0.000024  loss: 0.5445  time: 1.5393  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2865/3000]  eta: 0:03:25  lr: 0.000024  loss: 0.5014  time: 1.5295  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2865/3000]  eta: 0:03:25  lr: 0.000024  loss: 0.5366  time: 1.5290  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2870/3000]  eta: 0:03:17  lr: 0.000024  loss: 0.1789  time: 1.4941  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2870/3000]  eta: 0:03:17  lr: 0.000024  loss: 0.1322  time: 1.4936  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2875/3000]  eta: 0:03:09  lr: 0.000024  loss: 0.1500  time: 1.4676  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2875/3000]  eta: 0:03:09  lr: 0.000024  loss: 0.3244  time: 1.4673  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2880/3000]  eta: 0:03:02  lr: 0.000024  loss: 0.1581  time: 1.4698  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2880/3000]  eta: 0:03:02  lr: 0.000024  loss: 0.6293  time: 1.4693  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2885/3000]  eta: 0:02:54  lr: 0.000024  loss: 0.3451  time: 1.4896  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2885/3000]  eta: 0:02:54  lr: 0.000024  loss: 0.5786  time: 1.4893  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2890/3000]  eta: 0:02:47  lr: 0.000024  loss: 0.2412  time: 1.5198  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2890/3000]  eta: 0:02:47  lr: 0.000024  loss: 0.7243  time: 1.5195  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2895/3000]  eta: 0:02:39  lr: 0.000024  loss: 0.5241  time: 1.5523  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2895/3000]  eta: 0:02:39  lr: 0.000024  loss: 0.3209  time: 1.5519  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2900/3000]  eta: 0:02:31  lr: 0.000024  loss: 0.4920  time: 1.5498  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2900/3000]  eta: 0:02:31  lr: 0.000024  loss: 0.5472  time: 1.5495  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2905/3000]  eta: 0:02:24  lr: 0.000024  loss: 0.3686  time: 1.5349  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2905/3000]  eta: 0:02:24  lr: 0.000024  loss: 0.5742  time: 1.5346  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2910/3000]  eta: 0:02:16  lr: 0.000024  loss: 0.1488  time: 1.5052  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2910/3000]  eta: 0:02:16  lr: 0.000024  loss: 0.4330  time: 1.5049  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2915/3000]  eta: 0:02:09  lr: 0.000024  loss: 0.4430  time: 1.4826  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2915/3000]  eta: 0:02:09  lr: 0.000024  loss: 0.1981  time: 1.4824  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2920/3000]  eta: 0:02:01  lr: 0.000024  loss: 0.4121  time: 1.4633  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2920/3000]  eta: 0:02:01  lr: 0.000024  loss: 0.3329  time: 1.4631  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2925/3000]  eta: 0:01:53  lr: 0.000024  loss: 0.9172  time: 1.4593  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2925/3000]  eta: 0:01:53  lr: 0.000024  loss: 0.4021  time: 1.4591  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2930/3000]  eta: 0:01:46  lr: 0.000024  loss: 0.4229  time: 1.4794  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2930/3000]  eta: 0:01:46  lr: 0.000024  loss: 0.2745  time: 1.4792  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2935/3000]  eta: 0:01:38  lr: 0.000024  loss: 0.1411  time: 1.4805  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2935/3000]  eta: 0:01:38  lr: 0.000024  loss: 0.3505  time: 1.4800  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2940/3000]  eta: 0:01:31  lr: 0.000024  loss: 0.6321  time: 1.4931  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2940/3000]  eta: 0:01:31  lr: 0.000024  loss: 0.1869  time: 1.4926  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2945/3000]  eta: 0:01:23  lr: 0.000024  loss: 0.3224  time: 1.5033  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2945/3000]  eta: 0:01:23  lr: 0.000024  loss: 0.4158  time: 1.5028  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2950/3000]  eta: 0:01:15  lr: 0.000024  loss: 0.3820  time: 1.5119  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2950/3000]  eta: 0:01:15  lr: 0.000024  loss: 0.2482  time: 1.5113  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2955/3000]  eta: 0:01:08  lr: 0.000024  loss: 0.4193  time: 1.5167  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2955/3000]  eta: 0:01:08  lr: 0.000024  loss: 0.3468  time: 1.5165  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2960/3000]  eta: 0:01:00  lr: 0.000024  loss: 0.3340  time: 1.5254  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2960/3000]  eta: 0:01:00  lr: 0.000024  loss: 0.3807  time: 1.5251  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2965/3000]  eta: 0:00:53  lr: 0.000024  loss: 0.6160  time: 1.5259  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2965/3000]  eta: 0:00:53  lr: 0.000024  loss: 0.2064  time: 1.5256  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2970/3000]  eta: 0:00:45  lr: 0.000024  loss: 0.1822  time: 1.5237  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2970/3000]  eta: 0:00:45  lr: 0.000024  loss: 1.1490  time: 1.5234  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2975/3000]  eta: 0:00:37  lr: 0.000024  loss: 0.9534  time: 1.5184  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2975/3000]  eta: 0:00:37  lr: 0.000024  loss: 0.2747  time: 1.5180  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2980/3000]  eta: 0:00:30  lr: 0.000024  loss: 0.3651  time: 1.5323  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2980/3000]  eta: 0:00:30  lr: 0.000024  loss: 0.9199  time: 1.5320  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2985/3000]  eta: 0:00:22  lr: 0.000024  loss: 0.3368  time: 1.5245  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2985/3000]  eta: 0:00:22  lr: 0.000024  loss: 0.2523  time: 1.5241  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2990/3000]  eta: 0:00:15  lr: 0.000024  loss: 0.6989  time: 1.5061  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2990/3000]  eta: 0:00:15  lr: 0.000024  loss: 0.1382  time: 1.5059  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2995/3000]  eta: 0:00:07  lr: 0.000024  loss: 0.4642  time: 1.5408  data: 0.0000  max mem: 18596
Train: data epoch: [10]  [2995/3000]  eta: 0:00:07  lr: 0.000024  loss: 0.2317  time: 1.5406  data: 0.0000  max mem: 18430
Train: data epoch: [10]  [2999/3000]  eta: 0:00:01  lr: 0.000024  loss: 0.1246  time: 1.5257  data: 0.0000  max mem: 18596
Train: data epoch: [10] Total time: 1:15:57 (1.5192 s / it)
Train: data epoch: [10]  [2999/3000]  eta: 0:00:01  lr: 0.000024  loss: 0.2982  time: 1.5254  data: 0.0000  max mem: 18430
Train: data epoch: [10] Total time: 1:15:57 (1.5192 s / it)
2025-01-19 13:34:09,484 [INFO] Averaged stats: lr: 0.0000  loss: 0.4192
2025-01-19 13:34:09,491 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [10]  [0/1]  eta: 0:00:00    time: 0.8610  data: 0.5723  max mem: 18430
Eval: data epoch: [10]  [0/1]  eta: 0:00:00    time: 0.9841  data: 0.6957  max mem: 18596
Eval: data epoch: [10] Total time: 0:00:01 (1.0118 s / it)
Eval: data epoch: [10] Total time: 0:00:01 (1.1988 s / it)
2025-01-19 13:34:10,714 [INFO] Saving checkpoint at epoch 10 to outputs_stage1_only/202501182338/checkpoint_10.pth.
2025-01-19 13:34:13,208 [INFO] Training Phase
2025-01-19 13:34:13,215 [INFO] Start training epoch 11, 3000 iters per inner epoch.
Train: data epoch: [11]  [   0/3000]  eta: 1:24:02  lr: 0.000024  loss: 0.5412  time: 1.6807  data: 0.0001  max mem: 18430Train: data epoch: [11]  [   0/3000]  eta: 1:24:07  lr: 0.000024  loss: 0.3360  time: 1.6826  data: 0.0000  max mem: 18596

Train: data epoch: [11]  [   5/3000]  eta: 1:19:14  lr: 0.000024  loss: 0.4183  time: 1.5875  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [   5/3000]  eta: 1:19:13  lr: 0.000024  loss: 0.7845  time: 1.5872  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [  10/3000]  eta: 1:18:10  lr: 0.000024  loss: 0.3124  time: 1.5689  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [  10/3000]  eta: 1:18:10  lr: 0.000024  loss: 0.2811  time: 1.5686  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [  15/3000]  eta: 1:17:29  lr: 0.000024  loss: 0.3601  time: 1.5575  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [  15/3000]  eta: 1:17:28  lr: 0.000024  loss: 0.1912  time: 1.5572  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [  20/3000]  eta: 1:16:39  lr: 0.000024  loss: 0.3631  time: 1.5364  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [  20/3000]  eta: 1:16:38  lr: 0.000024  loss: 0.1920  time: 1.5363  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [  25/3000]  eta: 1:16:15  lr: 0.000024  loss: 0.5314  time: 1.5230  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [  25/3000]  eta: 1:16:14  lr: 0.000024  loss: 0.2862  time: 1.5228  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [  30/3000]  eta: 1:16:03  lr: 0.000024  loss: 0.3588  time: 1.5189  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [  30/3000]  eta: 1:16:02  lr: 0.000024  loss: 0.4571  time: 1.5186  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [  35/3000]  eta: 1:15:45  lr: 0.000024  loss: 0.2445  time: 1.5137  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [  35/3000]  eta: 1:15:44  lr: 0.000024  loss: 0.3132  time: 1.5134  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [  40/3000]  eta: 1:15:07  lr: 0.000024  loss: 0.6194  time: 1.5011  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [  40/3000]  eta: 1:15:06  lr: 0.000024  loss: 0.6366  time: 1.5007  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [  45/3000]  eta: 1:14:38  lr: 0.000024  loss: 0.9744  time: 1.4868  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [  45/3000]  eta: 1:14:37  lr: 0.000024  loss: 0.3237  time: 1.4864  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [  50/3000]  eta: 1:14:17  lr: 0.000024  loss: 0.3749  time: 1.4716  data: 0.0000  max mem: 18430Train: data epoch: [11]  [  50/3000]  eta: 1:14:18  lr: 0.000024  loss: 0.2862  time: 1.4720  data: 0.0000  max mem: 18596

Train: data epoch: [11]  [  55/3000]  eta: 1:14:24  lr: 0.000024  loss: 0.7081  time: 1.4854  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [  55/3000]  eta: 1:14:24  lr: 0.000024  loss: 0.4991  time: 1.4851  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [  60/3000]  eta: 1:14:14  lr: 0.000024  loss: 0.1499  time: 1.4993  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [  60/3000]  eta: 1:14:13  lr: 0.000024  loss: 0.7017  time: 1.4990  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [  65/3000]  eta: 1:14:00  lr: 0.000024  loss: 0.5410  time: 1.5071  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [  65/3000]  eta: 1:13:59  lr: 0.000024  loss: 0.1586  time: 1.5067  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [  70/3000]  eta: 1:14:03  lr: 0.000024  loss: 0.3104  time: 1.5303  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [  70/3000]  eta: 1:14:02  lr: 0.000024  loss: 0.3702  time: 1.5301  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [  75/3000]  eta: 1:14:01  lr: 0.000024  loss: 0.1768  time: 1.5256  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [  75/3000]  eta: 1:14:01  lr: 0.000024  loss: 0.3014  time: 1.5253  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [  80/3000]  eta: 1:13:47  lr: 0.000024  loss: 0.7410  time: 1.5205  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [  80/3000]  eta: 1:13:47  lr: 0.000024  loss: 0.1758  time: 1.5202  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [  85/3000]  eta: 1:13:37  lr: 0.000024  loss: 0.4757  time: 1.5237  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [  85/3000]  eta: 1:13:36  lr: 0.000024  loss: 0.3122  time: 1.5233  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [  90/3000]  eta: 1:13:36  lr: 0.000024  loss: 0.3643  time: 1.5209  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [  90/3000]  eta: 1:13:35  lr: 0.000024  loss: 0.2261  time: 1.5204  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [  95/3000]  eta: 1:13:20  lr: 0.000024  loss: 0.7632  time: 1.5010  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [  95/3000]  eta: 1:13:19  lr: 0.000024  loss: 0.2081  time: 1.5006  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 100/3000]  eta: 1:13:03  lr: 0.000024  loss: 0.4073  time: 1.4924  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 100/3000]  eta: 1:13:02  lr: 0.000024  loss: 0.6156  time: 1.4919  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 105/3000]  eta: 1:12:40  lr: 0.000024  loss: 0.2709  time: 1.4652  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 105/3000]  eta: 1:12:38  lr: 0.000024  loss: 0.2535  time: 1.4648  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 110/3000]  eta: 1:12:37  lr: 0.000024  loss: 0.4826  time: 1.4630  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 110/3000]  eta: 1:12:36  lr: 0.000024  loss: 0.7391  time: 1.4625  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 115/3000]  eta: 1:12:32  lr: 0.000024  loss: 0.1703  time: 1.4793  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 115/3000]  eta: 1:12:31  lr: 0.000024  loss: 0.3384  time: 1.4788  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 120/3000]  eta: 1:12:23  lr: 0.000024  loss: 0.2959  time: 1.4904  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 120/3000]  eta: 1:12:22  lr: 0.000024  loss: 0.4927  time: 1.4899  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 125/3000]  eta: 1:12:17  lr: 0.000024  loss: 0.3725  time: 1.5222  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 125/3000]  eta: 1:12:16  lr: 0.000024  loss: 0.4434  time: 1.5218  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 130/3000]  eta: 1:12:05  lr: 0.000024  loss: 0.4096  time: 1.5046  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 130/3000]  eta: 1:12:04  lr: 0.000024  loss: 0.2246  time: 1.5043  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 135/3000]  eta: 1:12:02  lr: 0.000024  loss: 0.9271  time: 1.5079  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 135/3000]  eta: 1:12:01  lr: 0.000024  loss: 0.2951  time: 1.5076  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 140/3000]  eta: 1:11:51  lr: 0.000024  loss: 0.1990  time: 1.5042  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 140/3000]  eta: 1:11:50  lr: 0.000024  loss: 0.3860  time: 1.5038  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 145/3000]  eta: 1:11:48  lr: 0.000024  loss: 0.3313  time: 1.5120  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 145/3000]  eta: 1:11:47  lr: 0.000024  loss: 0.5717  time: 1.5115  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 150/3000]  eta: 1:11:42  lr: 0.000024  loss: 0.2757  time: 1.5245  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 150/3000]  eta: 1:11:41  lr: 0.000024  loss: 0.6062  time: 1.5239  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 155/3000]  eta: 1:11:34  lr: 0.000024  loss: 0.3538  time: 1.5163  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 155/3000]  eta: 1:11:33  lr: 0.000024  loss: 0.5975  time: 1.5158  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 160/3000]  eta: 1:11:19  lr: 0.000024  loss: 0.6006  time: 1.5013  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 160/3000]  eta: 1:11:18  lr: 0.000024  loss: 0.3130  time: 1.5009  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 165/3000]  eta: 1:11:17  lr: 0.000024  loss: 0.3192  time: 1.5069  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 165/3000]  eta: 1:11:16  lr: 0.000024  loss: 0.3085  time: 1.5065  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 170/3000]  eta: 1:11:08  lr: 0.000024  loss: 0.3393  time: 1.4981  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 170/3000]  eta: 1:11:07  lr: 0.000024  loss: 0.0658  time: 1.4980  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 175/3000]  eta: 1:11:01  lr: 0.000024  loss: 0.3348  time: 1.4987  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 175/3000]  eta: 1:11:00  lr: 0.000024  loss: 0.3355  time: 1.4984  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 180/3000]  eta: 1:10:51  lr: 0.000024  loss: 0.3724  time: 1.5130  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 180/3000]  eta: 1:10:50  lr: 0.000024  loss: 0.1683  time: 1.5127  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 185/3000]  eta: 1:10:48  lr: 0.000024  loss: 0.2619  time: 1.5123  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 185/3000]  eta: 1:10:47  lr: 0.000024  loss: 0.4824  time: 1.5121  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 190/3000]  eta: 1:10:42  lr: 0.000024  loss: 0.5145  time: 1.5222  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 190/3000]  eta: 1:10:41  lr: 0.000024  loss: 0.1119  time: 1.5212  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 195/3000]  eta: 1:10:33  lr: 0.000024  loss: 0.2447  time: 1.5181  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 195/3000]  eta: 1:10:32  lr: 0.000024  loss: 0.3806  time: 1.5173  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 200/3000]  eta: 1:10:29  lr: 0.000024  loss: 0.1855  time: 1.5380  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 200/3000]  eta: 1:10:28  lr: 0.000024  loss: 0.3606  time: 1.5371  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 205/3000]  eta: 1:10:17  lr: 0.000024  loss: 0.2876  time: 1.5077  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 205/3000]  eta: 1:10:16  lr: 0.000024  loss: 0.2087  time: 1.5066  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 210/3000]  eta: 1:10:10  lr: 0.000024  loss: 0.2518  time: 1.5027  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 210/3000]  eta: 1:10:08  lr: 0.000024  loss: 0.4597  time: 1.5023  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 215/3000]  eta: 1:10:03  lr: 0.000024  loss: 0.3908  time: 1.5108  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 215/3000]  eta: 1:10:02  lr: 0.000024  loss: 0.2845  time: 1.5101  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 220/3000]  eta: 1:10:01  lr: 0.000024  loss: 0.4134  time: 1.5184  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 220/3000]  eta: 1:10:00  lr: 0.000024  loss: 0.6567  time: 1.5178  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 225/3000]  eta: 1:09:58  lr: 0.000024  loss: 0.3222  time: 1.5520  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 225/3000]  eta: 1:09:56  lr: 0.000024  loss: 0.5239  time: 1.5514  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 230/3000]  eta: 1:09:52  lr: 0.000024  loss: 0.1773  time: 1.5608  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 230/3000]  eta: 1:09:51  lr: 0.000024  loss: 0.4120  time: 1.5602  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 235/3000]  eta: 1:09:39  lr: 0.000024  loss: 0.2506  time: 1.5355  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 235/3000]  eta: 1:09:38  lr: 0.000024  loss: 0.5927  time: 1.5352  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 240/3000]  eta: 1:09:31  lr: 0.000024  loss: 0.7052  time: 1.5113  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 240/3000]  eta: 1:09:29  lr: 0.000024  loss: 0.2783  time: 1.5110  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 245/3000]  eta: 1:09:21  lr: 0.000024  loss: 1.2110  time: 1.4826  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 245/3000]  eta: 1:09:19  lr: 0.000024  loss: 0.9881  time: 1.4823  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 250/3000]  eta: 1:09:14  lr: 0.000024  loss: 0.1901  time: 1.4768  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 250/3000]  eta: 1:09:12  lr: 0.000024  loss: 0.3387  time: 1.4766  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 255/3000]  eta: 1:09:08  lr: 0.000024  loss: 0.2197  time: 1.5044  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 255/3000]  eta: 1:09:06  lr: 0.000024  loss: 0.3788  time: 1.5042  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 260/3000]  eta: 1:08:58  lr: 0.000024  loss: 0.2708  time: 1.5025  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 260/3000]  eta: 1:08:57  lr: 0.000024  loss: 1.3970  time: 1.5023  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 265/3000]  eta: 1:08:47  lr: 0.000024  loss: 0.1345  time: 1.4927  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 265/3000]  eta: 1:08:46  lr: 0.000024  loss: 0.5504  time: 1.4924  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 270/3000]  eta: 1:08:41  lr: 0.000024  loss: 0.4657  time: 1.5007  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 270/3000]  eta: 1:08:40  lr: 0.000024  loss: 0.4358  time: 1.5005  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 275/3000]  eta: 1:08:36  lr: 0.000024  loss: 0.2778  time: 1.5059  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 275/3000]  eta: 1:08:35  lr: 0.000024  loss: 0.3883  time: 1.5056  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 280/3000]  eta: 1:08:28  lr: 0.000024  loss: 0.2177  time: 1.5097  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 280/3000]  eta: 1:08:27  lr: 0.000024  loss: 0.1422  time: 1.5093  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 285/3000]  eta: 1:08:20  lr: 0.000024  loss: 0.1076  time: 1.5266  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 285/3000]  eta: 1:08:19  lr: 0.000024  loss: 0.7115  time: 1.5263  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 290/3000]  eta: 1:08:12  lr: 0.000024  loss: 0.3060  time: 1.5137  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 290/3000]  eta: 1:08:11  lr: 0.000024  loss: 0.4904  time: 1.5134  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 295/3000]  eta: 1:08:04  lr: 0.000024  loss: 0.1596  time: 1.5005  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 295/3000]  eta: 1:08:03  lr: 0.000024  loss: 0.4060  time: 1.5003  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 300/3000]  eta: 1:07:57  lr: 0.000024  loss: 0.2788  time: 1.5070  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 300/3000]  eta: 1:07:56  lr: 0.000024  loss: 0.4090  time: 1.5068  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 305/3000]  eta: 1:07:53  lr: 0.000024  loss: 0.2339  time: 1.5284  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 305/3000]  eta: 1:07:52  lr: 0.000024  loss: 0.4482  time: 1.5280  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 310/3000]  eta: 1:07:45  lr: 0.000024  loss: 0.3437  time: 1.5273  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 310/3000]  eta: 1:07:44  lr: 0.000024  loss: 0.0853  time: 1.5269  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 315/3000]  eta: 1:07:38  lr: 0.000024  loss: 0.4935  time: 1.5322  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 315/3000]  eta: 1:07:37  lr: 0.000024  loss: 0.3981  time: 1.5317  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 320/3000]  eta: 1:07:34  lr: 0.000024  loss: 0.5126  time: 1.5532  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 320/3000]  eta: 1:07:33  lr: 0.000024  loss: 0.5444  time: 1.5527  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 325/3000]  eta: 1:07:25  lr: 0.000024  loss: 0.4220  time: 1.5234  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 325/3000]  eta: 1:07:24  lr: 0.000024  loss: 0.6016  time: 1.5231  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 330/3000]  eta: 1:07:21  lr: 0.000024  loss: 0.9504  time: 1.5511  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 330/3000]  eta: 1:07:20  lr: 0.000024  loss: 0.4492  time: 1.5506  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 335/3000]  eta: 1:07:14  lr: 0.000024  loss: 0.3757  time: 1.5509  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 335/3000]  eta: 1:07:13  lr: 0.000024  loss: 0.3390  time: 1.5503  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 340/3000]  eta: 1:07:07  lr: 0.000024  loss: 0.1793  time: 1.5323  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 340/3000]  eta: 1:07:06  lr: 0.000024  loss: 0.3321  time: 1.5317  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 345/3000]  eta: 1:06:57  lr: 0.000024  loss: 0.1023  time: 1.5312  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 345/3000]  eta: 1:06:56  lr: 0.000024  loss: 0.2717  time: 1.5306  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 350/3000]  eta: 1:06:49  lr: 0.000024  loss: 0.2169  time: 1.5002  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 350/3000]  eta: 1:06:47  lr: 0.000024  loss: 0.5131  time: 1.4996  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 355/3000]  eta: 1:06:44  lr: 0.000024  loss: 0.7640  time: 1.5155  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 355/3000]  eta: 1:06:43  lr: 0.000024  loss: 0.2185  time: 1.5150  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 360/3000]  eta: 1:06:36  lr: 0.000024  loss: 0.4411  time: 1.5066  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 360/3000]  eta: 1:06:34  lr: 0.000024  loss: 0.3314  time: 1.5061  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 365/3000]  eta: 1:06:30  lr: 0.000024  loss: 0.3716  time: 1.5316  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 365/3000]  eta: 1:06:29  lr: 0.000024  loss: 0.1324  time: 1.5312  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 370/3000]  eta: 1:06:24  lr: 0.000024  loss: 0.3573  time: 1.5498  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 370/3000]  eta: 1:06:22  lr: 0.000024  loss: 0.2959  time: 1.5496  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 375/3000]  eta: 1:06:18  lr: 0.000024  loss: 0.7339  time: 1.5453  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 375/3000]  eta: 1:06:17  lr: 0.000024  loss: 0.6020  time: 1.5450  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 380/3000]  eta: 1:06:11  lr: 0.000024  loss: 0.4076  time: 1.5573  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 380/3000]  eta: 1:06:10  lr: 0.000024  loss: 0.3160  time: 1.5570  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 385/3000]  eta: 1:06:02  lr: 0.000024  loss: 0.3968  time: 1.5367  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 385/3000]  eta: 1:06:01  lr: 0.000024  loss: 0.7213  time: 1.5364  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 390/3000]  eta: 1:05:55  lr: 0.000024  loss: 0.6957  time: 1.5282  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 390/3000]  eta: 1:05:54  lr: 0.000024  loss: 0.3478  time: 1.5278  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 395/3000]  eta: 1:05:47  lr: 0.000024  loss: 0.2729  time: 1.5118  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 395/3000]  eta: 1:05:46  lr: 0.000024  loss: 0.6597  time: 1.5114  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 400/3000]  eta: 1:05:41  lr: 0.000024  loss: 0.0701  time: 1.5139  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 400/3000]  eta: 1:05:40  lr: 0.000024  loss: 0.8017  time: 1.5136  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 405/3000]  eta: 1:05:34  lr: 0.000024  loss: 0.9054  time: 1.5316  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 405/3000]  eta: 1:05:33  lr: 0.000024  loss: 0.4559  time: 1.5309  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 410/3000]  eta: 1:05:23  lr: 0.000024  loss: 0.3399  time: 1.5065  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 410/3000]  eta: 1:05:24  lr: 0.000024  loss: 0.0693  time: 1.5074  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 415/3000]  eta: 1:05:17  lr: 0.000024  loss: 0.7307  time: 1.5152  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 415/3000]  eta: 1:05:16  lr: 0.000024  loss: 0.5743  time: 1.5144  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 420/3000]  eta: 1:05:06  lr: 0.000024  loss: 0.7574  time: 1.4782  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 420/3000]  eta: 1:05:05  lr: 0.000024  loss: 0.1370  time: 1.4774  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 425/3000]  eta: 1:04:59  lr: 0.000024  loss: 0.3786  time: 1.4775  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 425/3000]  eta: 1:04:58  lr: 0.000024  loss: 0.7904  time: 1.4772  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 430/3000]  eta: 1:04:49  lr: 0.000024  loss: 0.1902  time: 1.4749  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 430/3000]  eta: 1:04:47  lr: 0.000024  loss: 0.6289  time: 1.4745  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 435/3000]  eta: 1:04:43  lr: 0.000024  loss: 0.3310  time: 1.4838  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 435/3000]  eta: 1:04:42  lr: 0.000024  loss: 0.3253  time: 1.4834  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 440/3000]  eta: 1:04:38  lr: 0.000024  loss: 0.4165  time: 1.5322  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 440/3000]  eta: 1:04:37  lr: 0.000024  loss: 0.3122  time: 1.5318  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 445/3000]  eta: 1:04:31  lr: 0.000024  loss: 0.4057  time: 1.5303  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 445/3000]  eta: 1:04:30  lr: 0.000024  loss: 0.5582  time: 1.5298  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 450/3000]  eta: 1:04:25  lr: 0.000024  loss: 0.5940  time: 1.5694  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 450/3000]  eta: 1:04:24  lr: 0.000024  loss: 0.1047  time: 1.5688  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 455/3000]  eta: 1:04:17  lr: 0.000024  loss: 0.4863  time: 1.5518  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 455/3000]  eta: 1:04:16  lr: 0.000024  loss: 0.3045  time: 1.5510  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 460/3000]  eta: 1:04:07  lr: 0.000024  loss: 0.1334  time: 1.5147  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 460/3000]  eta: 1:04:06  lr: 0.000024  loss: 0.4137  time: 1.5120  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 465/3000]  eta: 1:04:01  lr: 0.000024  loss: 0.3590  time: 1.5179  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 465/3000]  eta: 1:03:59  lr: 0.000024  loss: 0.1070  time: 1.5151  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 470/3000]  eta: 1:03:53  lr: 0.000024  loss: 0.5110  time: 1.5020  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 470/3000]  eta: 1:03:52  lr: 0.000024  loss: 0.2011  time: 1.4997  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 475/3000]  eta: 1:03:47  lr: 0.000024  loss: 0.3782  time: 1.5242  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 475/3000]  eta: 1:03:46  lr: 0.000024  loss: 0.1937  time: 1.5220  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 480/3000]  eta: 1:03:39  lr: 0.000024  loss: 0.1884  time: 1.5376  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 480/3000]  eta: 1:03:38  lr: 0.000024  loss: 0.5415  time: 1.5373  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 485/3000]  eta: 1:03:31  lr: 0.000024  loss: 0.0972  time: 1.5185  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 485/3000]  eta: 1:03:29  lr: 0.000024  loss: 0.6456  time: 1.5183  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 490/3000]  eta: 1:03:23  lr: 0.000024  loss: 0.3411  time: 1.5211  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 490/3000]  eta: 1:03:22  lr: 0.000024  loss: 0.5150  time: 1.5208  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 495/3000]  eta: 1:03:17  lr: 0.000024  loss: 0.5660  time: 1.5182  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 495/3000]  eta: 1:03:16  lr: 0.000024  loss: 0.4284  time: 1.5180  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 500/3000]  eta: 1:03:11  lr: 0.000024  loss: 0.4114  time: 1.5327  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 500/3000]  eta: 1:03:10  lr: 0.000024  loss: 0.6701  time: 1.5325  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 505/3000]  eta: 1:03:04  lr: 0.000024  loss: 0.3830  time: 1.5535  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 505/3000]  eta: 1:03:03  lr: 0.000024  loss: 0.2065  time: 1.5533  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 510/3000]  eta: 1:02:56  lr: 0.000024  loss: 0.2667  time: 1.5498  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 510/3000]  eta: 1:02:55  lr: 0.000024  loss: 0.8714  time: 1.5496  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 515/3000]  eta: 1:02:50  lr: 0.000024  loss: 0.6242  time: 1.5448  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 515/3000]  eta: 1:02:48  lr: 0.000024  loss: 0.8868  time: 1.5445  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 520/3000]  eta: 1:02:42  lr: 0.000024  loss: 0.3145  time: 1.5319  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 520/3000]  eta: 1:02:41  lr: 0.000024  loss: 0.5045  time: 1.5316  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 525/3000]  eta: 1:02:35  lr: 0.000024  loss: 0.0941  time: 1.5263  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 525/3000]  eta: 1:02:33  lr: 0.000024  loss: 1.2037  time: 1.5260  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 530/3000]  eta: 1:02:28  lr: 0.000024  loss: 0.4640  time: 1.5360  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 530/3000]  eta: 1:02:26  lr: 0.000024  loss: 0.2789  time: 1.5357  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 535/3000]  eta: 1:02:19  lr: 0.000024  loss: 0.3788  time: 1.5157  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 535/3000]  eta: 1:02:18  lr: 0.000024  loss: 0.3809  time: 1.5155  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 540/3000]  eta: 1:02:13  lr: 0.000024  loss: 0.3516  time: 1.5317  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 540/3000]  eta: 1:02:12  lr: 0.000024  loss: 0.1424  time: 1.5314  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 545/3000]  eta: 1:02:06  lr: 0.000024  loss: 0.3391  time: 1.5355  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 545/3000]  eta: 1:02:05  lr: 0.000024  loss: 0.5908  time: 1.5353  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 550/3000]  eta: 1:01:59  lr: 0.000024  loss: 0.4098  time: 1.5336  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 550/3000]  eta: 1:01:58  lr: 0.000024  loss: 0.3641  time: 1.5332  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 555/3000]  eta: 1:01:51  lr: 0.000024  loss: 0.7163  time: 1.5388  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 555/3000]  eta: 1:01:50  lr: 0.000024  loss: 0.2416  time: 1.5384  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 560/3000]  eta: 1:01:42  lr: 0.000024  loss: 0.3410  time: 1.5153  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 560/3000]  eta: 1:01:41  lr: 0.000024  loss: 0.4298  time: 1.5149  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 565/3000]  eta: 1:01:34  lr: 0.000024  loss: 0.5318  time: 1.5011  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 565/3000]  eta: 1:01:33  lr: 0.000024  loss: 0.1808  time: 1.5007  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 570/3000]  eta: 1:01:27  lr: 0.000024  loss: 0.6560  time: 1.5069  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 570/3000]  eta: 1:01:26  lr: 0.000024  loss: 0.6655  time: 1.5066  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 575/3000]  eta: 1:01:21  lr: 0.000024  loss: 0.4468  time: 1.5258  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 575/3000]  eta: 1:01:20  lr: 0.000024  loss: 0.2028  time: 1.5255  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 580/3000]  eta: 1:01:13  lr: 0.000024  loss: 0.4310  time: 1.5342  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 580/3000]  eta: 1:01:12  lr: 0.000024  loss: 0.6991  time: 1.5340  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 585/3000]  eta: 1:01:06  lr: 0.000024  loss: 0.2459  time: 1.5387  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 585/3000]  eta: 1:01:04  lr: 0.000024  loss: 0.8542  time: 1.5384  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 590/3000]  eta: 1:00:59  lr: 0.000024  loss: 0.1100  time: 1.5452  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 590/3000]  eta: 1:00:58  lr: 0.000024  loss: 1.0349  time: 1.5449  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 595/3000]  eta: 1:00:49  lr: 0.000024  loss: 0.2754  time: 1.5071  data: 0.0000  max mem: 18430Train: data epoch: [11]  [ 595/3000]  eta: 1:00:50  lr: 0.000024  loss: 0.3228  time: 1.5074  data: 0.0000  max mem: 18596

Train: data epoch: [11]  [ 600/3000]  eta: 1:00:42  lr: 0.000024  loss: 0.2418  time: 1.5048  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 600/3000]  eta: 1:00:41  lr: 0.000024  loss: 0.1030  time: 1.5044  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 605/3000]  eta: 1:00:34  lr: 0.000024  loss: 0.7230  time: 1.5065  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 605/3000]  eta: 1:00:33  lr: 0.000024  loss: 0.2235  time: 1.5062  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 610/3000]  eta: 1:00:26  lr: 0.000024  loss: 0.2250  time: 1.4866  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 610/3000]  eta: 1:00:25  lr: 0.000024  loss: 0.3586  time: 1.4863  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 615/3000]  eta: 1:00:19  lr: 0.000024  loss: 0.2605  time: 1.5091  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 615/3000]  eta: 1:00:18  lr: 0.000024  loss: 0.1745  time: 1.5089  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 620/3000]  eta: 1:00:12  lr: 0.000024  loss: 0.0979  time: 1.5230  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 620/3000]  eta: 1:00:11  lr: 0.000024  loss: 0.5194  time: 1.5227  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 625/3000]  eta: 1:00:05  lr: 0.000024  loss: 0.4529  time: 1.5301  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 625/3000]  eta: 1:00:04  lr: 0.000024  loss: 0.5817  time: 1.5296  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 630/3000]  eta: 0:59:56  lr: 0.000024  loss: 0.5842  time: 1.5154  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 630/3000]  eta: 0:59:55  lr: 0.000024  loss: 0.3678  time: 1.5148  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 635/3000]  eta: 0:59:48  lr: 0.000024  loss: 0.8399  time: 1.5165  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 635/3000]  eta: 0:59:47  lr: 0.000024  loss: 0.2624  time: 1.5160  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 640/3000]  eta: 0:59:41  lr: 0.000024  loss: 0.2972  time: 1.5026  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 640/3000]  eta: 0:59:39  lr: 0.000024  loss: 0.2002  time: 1.5021  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 645/3000]  eta: 0:59:34  lr: 0.000024  loss: 0.9121  time: 1.5160  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 645/3000]  eta: 0:59:33  lr: 0.000024  loss: 0.7253  time: 1.5155  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 650/3000]  eta: 0:59:28  lr: 0.000024  loss: 0.6886  time: 1.5519  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 650/3000]  eta: 0:59:27  lr: 0.000024  loss: 0.1955  time: 1.5515  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 655/3000]  eta: 0:59:21  lr: 0.000024  loss: 0.5620  time: 1.5607  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 655/3000]  eta: 0:59:20  lr: 0.000024  loss: 0.6038  time: 1.5600  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 660/3000]  eta: 0:59:13  lr: 0.000024  loss: 0.4949  time: 1.5616  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 660/3000]  eta: 0:59:12  lr: 0.000024  loss: 0.3064  time: 1.5608  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 665/3000]  eta: 0:59:04  lr: 0.000024  loss: 0.2859  time: 1.5200  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 665/3000]  eta: 0:59:03  lr: 0.000024  loss: 0.0627  time: 1.5193  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 670/3000]  eta: 0:58:56  lr: 0.000024  loss: 0.5006  time: 1.4874  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 670/3000]  eta: 0:58:54  lr: 0.000024  loss: 1.0580  time: 1.4866  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 675/3000]  eta: 0:58:49  lr: 0.000024  loss: 0.6970  time: 1.4984  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 675/3000]  eta: 0:58:48  lr: 0.000024  loss: 1.0903  time: 1.4977  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 680/3000]  eta: 0:58:41  lr: 0.000024  loss: 0.6035  time: 1.4885  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 680/3000]  eta: 0:58:40  lr: 0.000024  loss: 1.1398  time: 1.4880  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 685/3000]  eta: 0:58:33  lr: 0.000024  loss: 0.1319  time: 1.5074  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 685/3000]  eta: 0:58:32  lr: 0.000024  loss: 0.2511  time: 1.5070  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 690/3000]  eta: 0:58:26  lr: 0.000024  loss: 0.2438  time: 1.5279  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 690/3000]  eta: 0:58:25  lr: 0.000024  loss: 0.2878  time: 1.5276  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 695/3000]  eta: 0:58:18  lr: 0.000024  loss: 0.5912  time: 1.5051  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 695/3000]  eta: 0:58:17  lr: 0.000024  loss: 0.3136  time: 1.5049  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 700/3000]  eta: 0:58:10  lr: 0.000024  loss: 0.9390  time: 1.5144  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 700/3000]  eta: 0:58:09  lr: 0.000024  loss: 0.4361  time: 1.5142  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 705/3000]  eta: 0:58:03  lr: 0.000024  loss: 0.4083  time: 1.5182  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 705/3000]  eta: 0:58:02  lr: 0.000024  loss: 0.2063  time: 1.5179  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 710/3000]  eta: 0:57:55  lr: 0.000024  loss: 0.6207  time: 1.5040  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 710/3000]  eta: 0:57:54  lr: 0.000024  loss: 0.4733  time: 1.5037  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 715/3000]  eta: 0:57:47  lr: 0.000024  loss: 0.6175  time: 1.5138  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 715/3000]  eta: 0:57:46  lr: 0.000024  loss: 0.4252  time: 1.5135  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 720/3000]  eta: 0:57:40  lr: 0.000024  loss: 0.1623  time: 1.5259  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 720/3000]  eta: 0:57:39  lr: 0.000024  loss: 0.2203  time: 1.5256  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 725/3000]  eta: 0:57:32  lr: 0.000024  loss: 0.3505  time: 1.5104  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 725/3000]  eta: 0:57:31  lr: 0.000024  loss: 0.3029  time: 1.5101  data: 0.0000  max mem: 18430
Train: data epoch: [11]  [ 730/3000]  eta: 0:57:23  lr: 0.000024  loss: 0.4918  time: 1.5035  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 730/3000]  eta: 0:57:22  lr: 0.000024  loss: 1.0817  time: 1.5033  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 735/3000]  eta: 0:57:15  lr: 0.000024  loss: 0.0623  time: 1.4770  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 735/3000]  eta: 0:57:14  lr: 0.000024  loss: 0.4087  time: 1.4768  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 740/3000]  eta: 0:57:08  lr: 0.000024  loss: 0.0716  time: 1.4875  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 740/3000]  eta: 0:57:07  lr: 0.000024  loss: 0.4271  time: 1.4872  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 745/3000]  eta: 0:57:01  lr: 0.000024  loss: 0.8306  time: 1.5049  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 745/3000]  eta: 0:57:00  lr: 0.000024  loss: 0.3850  time: 1.5046  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 750/3000]  eta: 0:56:53  lr: 0.000024  loss: 0.4952  time: 1.5162  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 750/3000]  eta: 0:56:52  lr: 0.000024  loss: 0.2230  time: 1.5160  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 755/3000]  eta: 0:56:45  lr: 0.000024  loss: 0.5270  time: 1.5365  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 755/3000]  eta: 0:56:44  lr: 0.000024  loss: 0.2842  time: 1.5362  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 760/3000]  eta: 0:56:38  lr: 0.000024  loss: 0.7221  time: 1.5213  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 760/3000]  eta: 0:56:37  lr: 0.000024  loss: 0.1352  time: 1.5210  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 765/3000]  eta: 0:56:31  lr: 0.000024  loss: 0.3542  time: 1.5325  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 765/3000]  eta: 0:56:30  lr: 0.000024  loss: 0.7585  time: 1.5323  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 770/3000]  eta: 0:56:24  lr: 0.000024  loss: 0.3126  time: 1.5437  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 770/3000]  eta: 0:56:23  lr: 0.000024  loss: 0.1680  time: 1.5434  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 775/3000]  eta: 0:56:15  lr: 0.000024  loss: 0.2272  time: 1.5185  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 775/3000]  eta: 0:56:14  lr: 0.000024  loss: 0.1582  time: 1.5182  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 780/3000]  eta: 0:56:08  lr: 0.000024  loss: 0.8508  time: 1.5176  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 780/3000]  eta: 0:56:07  lr: 0.000024  loss: 0.2386  time: 1.5173  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 785/3000]  eta: 0:56:01  lr: 0.000024  loss: 0.5664  time: 1.5246  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 785/3000]  eta: 0:56:00  lr: 0.000024  loss: 0.0748  time: 1.5243  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 790/3000]  eta: 0:55:52  lr: 0.000024  loss: 0.2754  time: 1.4885  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 790/3000]  eta: 0:55:51  lr: 0.000024  loss: 0.5416  time: 1.4882  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 795/3000]  eta: 0:55:44  lr: 0.000024  loss: 0.9201  time: 1.5098  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 795/3000]  eta: 0:55:43  lr: 0.000024  loss: 0.2721  time: 1.5094  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 800/3000]  eta: 0:55:38  lr: 0.000024  loss: 0.2127  time: 1.5232  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 800/3000]  eta: 0:55:37  lr: 0.000024  loss: 0.2520  time: 1.5225  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 805/3000]  eta: 0:55:29  lr: 0.000024  loss: 0.3498  time: 1.4750  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 805/3000]  eta: 0:55:28  lr: 0.000024  loss: 0.0989  time: 1.4743  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 810/3000]  eta: 0:55:22  lr: 0.000024  loss: 0.6646  time: 1.5132  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 810/3000]  eta: 0:55:21  lr: 0.000024  loss: 0.1156  time: 1.5126  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 815/3000]  eta: 0:55:13  lr: 0.000024  loss: 0.5995  time: 1.4967  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 815/3000]  eta: 0:55:12  lr: 0.000024  loss: 0.2502  time: 1.4957  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 820/3000]  eta: 0:55:05  lr: 0.000024  loss: 0.2764  time: 1.4790  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 820/3000]  eta: 0:55:04  lr: 0.000024  loss: 0.4075  time: 1.4782  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 825/3000]  eta: 0:54:58  lr: 0.000024  loss: 1.1633  time: 1.5117  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 825/3000]  eta: 0:54:57  lr: 0.000024  loss: 0.2775  time: 1.5110  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 830/3000]  eta: 0:54:50  lr: 0.000024  loss: 0.6746  time: 1.4900  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 830/3000]  eta: 0:54:49  lr: 0.000024  loss: 0.3370  time: 1.4892  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 835/3000]  eta: 0:54:43  lr: 0.000024  loss: 0.3725  time: 1.5155  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 835/3000]  eta: 0:54:42  lr: 0.000024  loss: 0.1888  time: 1.5152  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 840/3000]  eta: 0:54:35  lr: 0.000024  loss: 0.4847  time: 1.5153  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 840/3000]  eta: 0:54:34  lr: 0.000024  loss: 0.3679  time: 1.5150  data: 0.0000  max mem: 18603
Failed to load /data/data/GigaSpeech/5/AUD0000001354_S0001797.wav. Load 0-th sample for now
Train: data epoch: [11]  [ 845/3000]  eta: 0:54:26  lr: 0.000024  loss: 0.2774  time: 1.4902  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 845/3000]  eta: 0:54:25  lr: 0.000024  loss: 0.6095  time: 1.4900  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 850/3000]  eta: 0:54:17  lr: 0.000024  loss: 0.6195  time: 1.4705  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 850/3000]  eta: 0:54:16  lr: 0.000024  loss: 0.5430  time: 1.4703  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 855/3000]  eta: 0:54:10  lr: 0.000024  loss: 0.2002  time: 1.4644  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 855/3000]  eta: 0:54:09  lr: 0.000024  loss: 0.5074  time: 1.4641  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 860/3000]  eta: 0:54:02  lr: 0.000024  loss: 0.4024  time: 1.4551  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 860/3000]  eta: 0:54:01  lr: 0.000024  loss: 0.4558  time: 1.4549  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 865/3000]  eta: 0:53:55  lr: 0.000024  loss: 0.3558  time: 1.4966  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 865/3000]  eta: 0:53:54  lr: 0.000024  loss: 0.5448  time: 1.4964  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 870/3000]  eta: 0:53:48  lr: 0.000024  loss: 0.1518  time: 1.5350  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 870/3000]  eta: 0:53:47  lr: 0.000024  loss: 0.8253  time: 1.5347  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 875/3000]  eta: 0:53:41  lr: 0.000024  loss: 0.9584  time: 1.5484  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 875/3000]  eta: 0:53:40  lr: 0.000024  loss: 0.3783  time: 1.5482  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 880/3000]  eta: 0:53:33  lr: 0.000024  loss: 0.2071  time: 1.5592  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 880/3000]  eta: 0:53:33  lr: 0.000024  loss: 0.8483  time: 1.5589  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 885/3000]  eta: 0:53:25  lr: 0.000024  loss: 0.5987  time: 1.5252  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 885/3000]  eta: 0:53:24  lr: 0.000024  loss: 0.4182  time: 1.5248  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 890/3000]  eta: 0:53:18  lr: 0.000024  loss: 0.2466  time: 1.5168  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 890/3000]  eta: 0:53:17  lr: 0.000024  loss: 0.2625  time: 1.5166  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 895/3000]  eta: 0:53:10  lr: 0.000024  loss: 1.0811  time: 1.5061  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 895/3000]  eta: 0:53:09  lr: 0.000024  loss: 0.4731  time: 1.5055  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 900/3000]  eta: 0:53:02  lr: 0.000024  loss: 0.5897  time: 1.5012  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 900/3000]  eta: 0:53:01  lr: 0.000024  loss: 0.7251  time: 1.5004  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 905/3000]  eta: 0:52:55  lr: 0.000024  loss: 0.3179  time: 1.5126  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 905/3000]  eta: 0:52:54  lr: 0.000024  loss: 0.5600  time: 1.5119  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 910/3000]  eta: 0:52:47  lr: 0.000024  loss: 0.2984  time: 1.5035  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 910/3000]  eta: 0:52:46  lr: 0.000024  loss: 0.6509  time: 1.5028  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 915/3000]  eta: 0:52:39  lr: 0.000024  loss: 0.2332  time: 1.4890  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 915/3000]  eta: 0:52:38  lr: 0.000024  loss: 0.6216  time: 1.4887  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 920/3000]  eta: 0:52:31  lr: 0.000024  loss: 0.6555  time: 1.4944  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 920/3000]  eta: 0:52:30  lr: 0.000024  loss: 0.2485  time: 1.4942  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 925/3000]  eta: 0:52:23  lr: 0.000024  loss: 0.1871  time: 1.4769  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 925/3000]  eta: 0:52:22  lr: 0.000024  loss: 0.1414  time: 1.4766  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 930/3000]  eta: 0:52:14  lr: 0.000024  loss: 0.5037  time: 1.4686  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 930/3000]  eta: 0:52:13  lr: 0.000024  loss: 0.2157  time: 1.4684  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 935/3000]  eta: 0:52:07  lr: 0.000024  loss: 0.2163  time: 1.4935  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 935/3000]  eta: 0:52:06  lr: 0.000024  loss: 0.2097  time: 1.4932  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 940/3000]  eta: 0:51:59  lr: 0.000024  loss: 0.5256  time: 1.4727  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 940/3000]  eta: 0:51:58  lr: 0.000024  loss: 0.6713  time: 1.4725  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 945/3000]  eta: 0:51:52  lr: 0.000024  loss: 0.5355  time: 1.5044  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 945/3000]  eta: 0:51:51  lr: 0.000024  loss: 0.7061  time: 1.5041  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 950/3000]  eta: 0:51:44  lr: 0.000024  loss: 0.3418  time: 1.5109  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 950/3000]  eta: 0:51:43  lr: 0.000024  loss: 0.3438  time: 1.5105  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 955/3000]  eta: 0:51:37  lr: 0.000024  loss: 0.4211  time: 1.5033  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 955/3000]  eta: 0:51:36  lr: 0.000024  loss: 0.1668  time: 1.5030  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 960/3000]  eta: 0:51:29  lr: 0.000024  loss: 0.4838  time: 1.5217  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 960/3000]  eta: 0:51:28  lr: 0.000024  loss: 0.2460  time: 1.5213  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 965/3000]  eta: 0:51:21  lr: 0.000024  loss: 0.3706  time: 1.5029  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 965/3000]  eta: 0:51:20  lr: 0.000024  loss: 1.0777  time: 1.5025  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 970/3000]  eta: 0:51:14  lr: 0.000024  loss: 0.0779  time: 1.5198  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 970/3000]  eta: 0:51:13  lr: 0.000024  loss: 0.7376  time: 1.5195  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 975/3000]  eta: 0:51:06  lr: 0.000024  loss: 0.7795  time: 1.5042  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 975/3000]  eta: 0:51:05  lr: 0.000024  loss: 0.1958  time: 1.5039  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 980/3000]  eta: 0:50:58  lr: 0.000024  loss: 0.8322  time: 1.4953  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 980/3000]  eta: 0:50:57  lr: 0.000024  loss: 0.5614  time: 1.4951  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 985/3000]  eta: 0:50:51  lr: 0.000024  loss: 0.4279  time: 1.5198  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 985/3000]  eta: 0:50:50  lr: 0.000024  loss: 0.4459  time: 1.5195  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 990/3000]  eta: 0:50:42  lr: 0.000024  loss: 0.2584  time: 1.4810  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 990/3000]  eta: 0:50:41  lr: 0.000024  loss: 0.4472  time: 1.4807  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [ 995/3000]  eta: 0:50:35  lr: 0.000024  loss: 0.3996  time: 1.4919  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [ 995/3000]  eta: 0:50:34  lr: 0.000024  loss: 0.0582  time: 1.4916  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1000/3000]  eta: 0:50:28  lr: 0.000024  loss: 0.0713  time: 1.5124  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1000/3000]  eta: 0:50:27  lr: 0.000024  loss: 0.1233  time: 1.5122  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1005/3000]  eta: 0:50:20  lr: 0.000024  loss: 0.2636  time: 1.4818  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1005/3000]  eta: 0:50:19  lr: 0.000024  loss: 0.2777  time: 1.4816  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1010/3000]  eta: 0:50:12  lr: 0.000024  loss: 0.8198  time: 1.5174  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1010/3000]  eta: 0:50:11  lr: 0.000024  loss: 1.0090  time: 1.5173  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1015/3000]  eta: 0:50:05  lr: 0.000024  loss: 0.5656  time: 1.5231  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1015/3000]  eta: 0:50:04  lr: 0.000024  loss: 0.6387  time: 1.5228  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1020/3000]  eta: 0:49:58  lr: 0.000024  loss: 0.3510  time: 1.5275  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1020/3000]  eta: 0:49:57  lr: 0.000024  loss: 0.7196  time: 1.5272  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1025/3000]  eta: 0:49:50  lr: 0.000024  loss: 0.9324  time: 1.5345  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1025/3000]  eta: 0:49:49  lr: 0.000024  loss: 0.3169  time: 1.5342  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1030/3000]  eta: 0:49:43  lr: 0.000024  loss: 0.5422  time: 1.5479  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1030/3000]  eta: 0:49:42  lr: 0.000024  loss: 0.2951  time: 1.5474  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1035/3000]  eta: 0:49:36  lr: 0.000024  loss: 0.5442  time: 1.5476  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1035/3000]  eta: 0:49:35  lr: 0.000024  loss: 0.2271  time: 1.5470  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1040/3000]  eta: 0:49:28  lr: 0.000024  loss: 0.1449  time: 1.5269  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1040/3000]  eta: 0:49:27  lr: 0.000024  loss: 0.4549  time: 1.5264  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1045/3000]  eta: 0:49:20  lr: 0.000024  loss: 0.2857  time: 1.5104  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1045/3000]  eta: 0:49:19  lr: 0.000024  loss: 0.3292  time: 1.5097  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1050/3000]  eta: 0:49:11  lr: 0.000024  loss: 0.3446  time: 1.4760  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1050/3000]  eta: 0:49:11  lr: 0.000024  loss: 0.4659  time: 1.4755  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1055/3000]  eta: 0:49:04  lr: 0.000024  loss: 0.3841  time: 1.4801  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1055/3000]  eta: 0:49:03  lr: 0.000024  loss: 0.4126  time: 1.4798  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1060/3000]  eta: 0:48:56  lr: 0.000024  loss: 0.6612  time: 1.4793  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1060/3000]  eta: 0:48:56  lr: 0.000024  loss: 0.3183  time: 1.4790  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1065/3000]  eta: 0:48:49  lr: 0.000024  loss: 0.2643  time: 1.5017  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1065/3000]  eta: 0:48:48  lr: 0.000024  loss: 0.5002  time: 1.5015  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1070/3000]  eta: 0:48:41  lr: 0.000024  loss: 0.3271  time: 1.5099  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1070/3000]  eta: 0:48:40  lr: 0.000024  loss: 0.5624  time: 1.5096  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1075/3000]  eta: 0:48:33  lr: 0.000024  loss: 0.2137  time: 1.4951  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1075/3000]  eta: 0:48:32  lr: 0.000024  loss: 0.3013  time: 1.4949  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1080/3000]  eta: 0:48:26  lr: 0.000024  loss: 0.9390  time: 1.5164  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1080/3000]  eta: 0:48:25  lr: 0.000024  loss: 0.2684  time: 1.5161  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1085/3000]  eta: 0:48:19  lr: 0.000024  loss: 0.4861  time: 1.5130  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1085/3000]  eta: 0:48:18  lr: 0.000024  loss: 0.2719  time: 1.5127  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1090/3000]  eta: 0:48:11  lr: 0.000024  loss: 0.3401  time: 1.5132  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1090/3000]  eta: 0:48:10  lr: 0.000024  loss: 0.6973  time: 1.5129  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1095/3000]  eta: 0:48:04  lr: 0.000024  loss: 0.3705  time: 1.5290  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1095/3000]  eta: 0:48:03  lr: 0.000024  loss: 0.1895  time: 1.5288  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1100/3000]  eta: 0:47:56  lr: 0.000024  loss: 0.3941  time: 1.5214  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1100/3000]  eta: 0:47:55  lr: 0.000024  loss: 0.5309  time: 1.5211  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1105/3000]  eta: 0:47:48  lr: 0.000024  loss: 0.3056  time: 1.5149  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1105/3000]  eta: 0:47:48  lr: 0.000024  loss: 0.2323  time: 1.5147  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1110/3000]  eta: 0:47:40  lr: 0.000024  loss: 0.3064  time: 1.5093  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1110/3000]  eta: 0:47:39  lr: 0.000024  loss: 0.2402  time: 1.5091  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1115/3000]  eta: 0:47:33  lr: 0.000024  loss: 0.2831  time: 1.5033  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1115/3000]  eta: 0:47:32  lr: 0.000024  loss: 0.4941  time: 1.5030  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1120/3000]  eta: 0:47:25  lr: 0.000024  loss: 0.1158  time: 1.4971  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1120/3000]  eta: 0:47:25  lr: 0.000024  loss: 0.3467  time: 1.4968  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1125/3000]  eta: 0:47:18  lr: 0.000024  loss: 0.4667  time: 1.5175  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1125/3000]  eta: 0:47:17  lr: 0.000024  loss: 0.4023  time: 1.5172  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1130/3000]  eta: 0:47:10  lr: 0.000024  loss: 0.2987  time: 1.5224  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1130/3000]  eta: 0:47:10  lr: 0.000024  loss: 0.4216  time: 1.5221  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1135/3000]  eta: 0:47:03  lr: 0.000024  loss: 0.4066  time: 1.5177  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1135/3000]  eta: 0:47:02  lr: 0.000024  loss: 0.6064  time: 1.5175  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1140/3000]  eta: 0:46:54  lr: 0.000024  loss: 0.3540  time: 1.5137  data: 0.0000  max mem: 18603Train: data epoch: [11]  [1140/3000]  eta: 0:46:55  lr: 0.000024  loss: 0.4302  time: 1.5141  data: 0.0000  max mem: 18596

Train: data epoch: [11]  [1145/3000]  eta: 0:46:48  lr: 0.000024  loss: 0.3159  time: 1.5037  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1145/3000]  eta: 0:46:47  lr: 0.000024  loss: 0.2704  time: 1.5034  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1150/3000]  eta: 0:46:40  lr: 0.000024  loss: 0.6251  time: 1.5237  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1150/3000]  eta: 0:46:40  lr: 0.000024  loss: 0.2549  time: 1.5234  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1155/3000]  eta: 0:46:33  lr: 0.000024  loss: 0.1111  time: 1.5377  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1155/3000]  eta: 0:46:32  lr: 0.000024  loss: 0.4926  time: 1.5374  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1160/3000]  eta: 0:46:26  lr: 0.000024  loss: 0.1455  time: 1.5487  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1160/3000]  eta: 0:46:25  lr: 0.000024  loss: 0.1381  time: 1.5485  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1165/3000]  eta: 0:46:19  lr: 0.000024  loss: 0.2837  time: 1.5693  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1165/3000]  eta: 0:46:18  lr: 0.000024  loss: 0.0838  time: 1.5690  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1170/3000]  eta: 0:46:12  lr: 0.000024  loss: 0.1035  time: 1.5679  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1170/3000]  eta: 0:46:11  lr: 0.000024  loss: 0.2424  time: 1.5674  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1175/3000]  eta: 0:46:04  lr: 0.000024  loss: 0.4045  time: 1.5402  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1175/3000]  eta: 0:46:03  lr: 0.000024  loss: 0.5422  time: 1.5398  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1180/3000]  eta: 0:45:56  lr: 0.000024  loss: 0.3204  time: 1.5183  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1180/3000]  eta: 0:45:55  lr: 0.000024  loss: 0.3851  time: 1.5178  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1185/3000]  eta: 0:45:48  lr: 0.000024  loss: 0.2608  time: 1.4933  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1185/3000]  eta: 0:45:47  lr: 0.000024  loss: 0.3131  time: 1.4929  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1190/3000]  eta: 0:45:41  lr: 0.000024  loss: 0.2789  time: 1.4969  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1190/3000]  eta: 0:45:40  lr: 0.000024  loss: 0.2280  time: 1.4967  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1195/3000]  eta: 0:45:33  lr: 0.000024  loss: 0.3749  time: 1.5024  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1195/3000]  eta: 0:45:32  lr: 0.000024  loss: 0.2979  time: 1.5022  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1200/3000]  eta: 0:45:25  lr: 0.000024  loss: 0.2426  time: 1.5147  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1200/3000]  eta: 0:45:25  lr: 0.000024  loss: 0.2744  time: 1.5144  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1205/3000]  eta: 0:45:18  lr: 0.000024  loss: 0.4492  time: 1.5172  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1205/3000]  eta: 0:45:17  lr: 0.000024  loss: 0.9744  time: 1.5170  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1210/3000]  eta: 0:45:10  lr: 0.000024  loss: 0.2685  time: 1.4889  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1210/3000]  eta: 0:45:09  lr: 0.000024  loss: 0.6613  time: 1.4887  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1215/3000]  eta: 0:45:02  lr: 0.000024  loss: 0.5339  time: 1.4763  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1215/3000]  eta: 0:45:01  lr: 0.000024  loss: 0.2414  time: 1.4757  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1220/3000]  eta: 0:44:54  lr: 0.000024  loss: 0.8478  time: 1.4736  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1220/3000]  eta: 0:44:53  lr: 0.000024  loss: 0.5332  time: 1.4731  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1225/3000]  eta: 0:44:47  lr: 0.000024  loss: 0.5212  time: 1.4766  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1225/3000]  eta: 0:44:46  lr: 0.000024  loss: 0.5702  time: 1.4761  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1230/3000]  eta: 0:44:40  lr: 0.000024  loss: 0.5915  time: 1.5204  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1230/3000]  eta: 0:44:39  lr: 0.000024  loss: 0.2145  time: 1.5197  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1235/3000]  eta: 0:44:31  lr: 0.000024  loss: 0.4518  time: 1.5161  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1235/3000]  eta: 0:44:31  lr: 0.000024  loss: 0.1841  time: 1.5158  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1240/3000]  eta: 0:44:24  lr: 0.000024  loss: 0.2966  time: 1.5384  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1240/3000]  eta: 0:44:24  lr: 0.000024  loss: 0.3510  time: 1.5381  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1245/3000]  eta: 0:44:17  lr: 0.000024  loss: 1.1367  time: 1.5210  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1245/3000]  eta: 0:44:16  lr: 0.000024  loss: 0.1970  time: 1.5206  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1250/3000]  eta: 0:44:09  lr: 0.000024  loss: 0.5018  time: 1.4907  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1250/3000]  eta: 0:44:08  lr: 0.000024  loss: 0.4728  time: 1.4905  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1255/3000]  eta: 0:44:01  lr: 0.000024  loss: 0.8238  time: 1.5177  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1255/3000]  eta: 0:44:01  lr: 0.000024  loss: 0.2396  time: 1.5172  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1260/3000]  eta: 0:43:54  lr: 0.000024  loss: 0.3187  time: 1.5142  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1260/3000]  eta: 0:43:53  lr: 0.000024  loss: 0.2451  time: 1.5137  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1265/3000]  eta: 0:43:47  lr: 0.000024  loss: 0.3177  time: 1.5241  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1265/3000]  eta: 0:43:46  lr: 0.000024  loss: 0.2937  time: 1.5237  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1270/3000]  eta: 0:43:39  lr: 0.000024  loss: 0.2053  time: 1.5315  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1270/3000]  eta: 0:43:38  lr: 0.000024  loss: 0.4262  time: 1.5309  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1275/3000]  eta: 0:43:31  lr: 0.000024  loss: 0.4392  time: 1.5249  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1275/3000]  eta: 0:43:31  lr: 0.000024  loss: 0.5952  time: 1.5246  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1280/3000]  eta: 0:43:24  lr: 0.000024  loss: 0.5936  time: 1.5060  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1280/3000]  eta: 0:43:23  lr: 0.000024  loss: 0.1916  time: 1.5054  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1285/3000]  eta: 0:43:16  lr: 0.000024  loss: 0.7306  time: 1.5112  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1285/3000]  eta: 0:43:15  lr: 0.000024  loss: 1.0496  time: 1.5107  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1290/3000]  eta: 0:43:09  lr: 0.000024  loss: 0.9231  time: 1.5164  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1290/3000]  eta: 0:43:08  lr: 0.000024  loss: 0.5788  time: 1.5159  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1295/3000]  eta: 0:43:01  lr: 0.000024  loss: 0.5279  time: 1.5006  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1295/3000]  eta: 0:43:00  lr: 0.000024  loss: 0.1411  time: 1.5002  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1300/3000]  eta: 0:42:53  lr: 0.000024  loss: 0.0607  time: 1.5140  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1300/3000]  eta: 0:42:53  lr: 0.000024  loss: 0.2899  time: 1.5138  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1305/3000]  eta: 0:42:46  lr: 0.000024  loss: 1.3068  time: 1.5057  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1305/3000]  eta: 0:42:45  lr: 0.000024  loss: 0.2369  time: 1.5055  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1310/3000]  eta: 0:42:38  lr: 0.000024  loss: 0.8398  time: 1.4931  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1310/3000]  eta: 0:42:37  lr: 0.000024  loss: 0.1653  time: 1.4926  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1315/3000]  eta: 0:42:30  lr: 0.000024  loss: 0.4380  time: 1.5131  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1315/3000]  eta: 0:42:30  lr: 0.000024  loss: 0.2540  time: 1.5127  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1320/3000]  eta: 0:42:23  lr: 0.000024  loss: 0.7850  time: 1.5104  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1320/3000]  eta: 0:42:22  lr: 0.000024  loss: 0.8032  time: 1.5099  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1325/3000]  eta: 0:42:16  lr: 0.000024  loss: 0.7705  time: 1.5257  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1325/3000]  eta: 0:42:15  lr: 0.000024  loss: 0.2562  time: 1.5253  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1330/3000]  eta: 0:42:08  lr: 0.000024  loss: 0.7787  time: 1.5403  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1330/3000]  eta: 0:42:08  lr: 0.000024  loss: 0.1705  time: 1.5400  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1335/3000]  eta: 0:42:01  lr: 0.000024  loss: 0.8413  time: 1.5299  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1335/3000]  eta: 0:42:00  lr: 0.000024  loss: 0.3296  time: 1.5297  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1340/3000]  eta: 0:41:53  lr: 0.000024  loss: 0.4775  time: 1.5445  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1340/3000]  eta: 0:41:53  lr: 0.000024  loss: 0.2526  time: 1.5443  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1345/3000]  eta: 0:41:46  lr: 0.000024  loss: 0.3269  time: 1.5387  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1345/3000]  eta: 0:41:45  lr: 0.000024  loss: 0.3452  time: 1.5385  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1350/3000]  eta: 0:41:39  lr: 0.000024  loss: 0.4531  time: 1.5369  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1350/3000]  eta: 0:41:38  lr: 0.000024  loss: 0.1414  time: 1.5366  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1355/3000]  eta: 0:41:31  lr: 0.000024  loss: 0.4952  time: 1.5522  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1355/3000]  eta: 0:41:30  lr: 0.000024  loss: 0.7430  time: 1.5520  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1360/3000]  eta: 0:41:24  lr: 0.000024  loss: 0.3701  time: 1.5569  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1360/3000]  eta: 0:41:24  lr: 0.000024  loss: 0.2504  time: 1.5566  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1365/3000]  eta: 0:41:16  lr: 0.000024  loss: 0.0646  time: 1.5413  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1365/3000]  eta: 0:41:16  lr: 0.000024  loss: 0.3811  time: 1.5411  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1370/3000]  eta: 0:41:09  lr: 0.000024  loss: 0.6558  time: 1.5420  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1370/3000]  eta: 0:41:08  lr: 0.000024  loss: 0.9926  time: 1.5417  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1375/3000]  eta: 0:41:02  lr: 0.000024  loss: 0.6710  time: 1.5446  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1375/3000]  eta: 0:41:01  lr: 0.000024  loss: 0.4848  time: 1.5441  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1380/3000]  eta: 0:40:54  lr: 0.000024  loss: 0.1745  time: 1.5188  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1380/3000]  eta: 0:40:53  lr: 0.000024  loss: 0.2209  time: 1.5185  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1385/3000]  eta: 0:40:47  lr: 0.000024  loss: 0.4803  time: 1.5535  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1385/3000]  eta: 0:40:46  lr: 0.000024  loss: 0.6748  time: 1.5531  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1390/3000]  eta: 0:40:39  lr: 0.000024  loss: 0.4071  time: 1.5326  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1390/3000]  eta: 0:40:38  lr: 0.000024  loss: 0.1404  time: 1.5323  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1395/3000]  eta: 0:40:32  lr: 0.000024  loss: 0.1076  time: 1.5291  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1395/3000]  eta: 0:40:31  lr: 0.000024  loss: 0.4025  time: 1.5288  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1400/3000]  eta: 0:40:24  lr: 0.000024  loss: 0.6083  time: 1.5352  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1400/3000]  eta: 0:40:23  lr: 0.000024  loss: 0.9761  time: 1.5349  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1405/3000]  eta: 0:40:16  lr: 0.000024  loss: 0.5883  time: 1.5084  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1405/3000]  eta: 0:40:16  lr: 0.000024  loss: 0.3513  time: 1.5080  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1410/3000]  eta: 0:40:09  lr: 0.000024  loss: 0.2349  time: 1.5231  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1410/3000]  eta: 0:40:08  lr: 0.000024  loss: 0.3844  time: 1.5228  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1415/3000]  eta: 0:40:01  lr: 0.000024  loss: 0.1394  time: 1.4971  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1415/3000]  eta: 0:40:00  lr: 0.000024  loss: 0.8930  time: 1.4966  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1420/3000]  eta: 0:39:53  lr: 0.000024  loss: 0.7036  time: 1.4829  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1420/3000]  eta: 0:39:52  lr: 0.000024  loss: 0.4373  time: 1.4824  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1425/3000]  eta: 0:39:46  lr: 0.000024  loss: 0.6563  time: 1.4982  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1425/3000]  eta: 0:39:45  lr: 0.000024  loss: 0.0808  time: 1.4978  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1430/3000]  eta: 0:39:38  lr: 0.000024  loss: 0.5012  time: 1.5033  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1430/3000]  eta: 0:39:38  lr: 0.000024  loss: 0.3670  time: 1.5028  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1435/3000]  eta: 0:39:31  lr: 0.000024  loss: 0.5306  time: 1.5193  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1435/3000]  eta: 0:39:30  lr: 0.000024  loss: 0.1941  time: 1.5190  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1440/3000]  eta: 0:39:23  lr: 0.000024  loss: 0.1933  time: 1.5343  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1440/3000]  eta: 0:39:23  lr: 0.000024  loss: 0.5834  time: 1.5341  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1445/3000]  eta: 0:39:16  lr: 0.000024  loss: 0.6072  time: 1.5380  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1445/3000]  eta: 0:39:15  lr: 0.000024  loss: 0.4577  time: 1.5375  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1450/3000]  eta: 0:39:09  lr: 0.000024  loss: 1.1320  time: 1.5385  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1450/3000]  eta: 0:39:08  lr: 0.000024  loss: 0.4821  time: 1.5380  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1455/3000]  eta: 0:39:01  lr: 0.000024  loss: 0.3205  time: 1.5565  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1455/3000]  eta: 0:39:01  lr: 0.000024  loss: 0.2015  time: 1.5560  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1460/3000]  eta: 0:38:54  lr: 0.000024  loss: 0.3399  time: 1.5425  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1460/3000]  eta: 0:38:53  lr: 0.000024  loss: 0.3445  time: 1.5419  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1465/3000]  eta: 0:38:46  lr: 0.000024  loss: 0.4310  time: 1.5243  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1465/3000]  eta: 0:38:45  lr: 0.000024  loss: 0.3311  time: 1.5238  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1470/3000]  eta: 0:38:39  lr: 0.000024  loss: 0.6093  time: 1.5354  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1470/3000]  eta: 0:38:38  lr: 0.000024  loss: 0.2741  time: 1.5351  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1475/3000]  eta: 0:38:31  lr: 0.000024  loss: 0.2747  time: 1.5361  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1475/3000]  eta: 0:38:31  lr: 0.000024  loss: 0.5580  time: 1.5353  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1480/3000]  eta: 0:38:23  lr: 0.000024  loss: 0.6680  time: 1.5161  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1480/3000]  eta: 0:38:23  lr: 0.000024  loss: 0.6021  time: 1.5151  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1485/3000]  eta: 0:38:16  lr: 0.000024  loss: 0.2233  time: 1.5296  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1485/3000]  eta: 0:38:15  lr: 0.000024  loss: 0.2561  time: 1.5288  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1490/3000]  eta: 0:38:09  lr: 0.000024  loss: 0.4480  time: 1.5270  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1490/3000]  eta: 0:38:08  lr: 0.000024  loss: 0.2623  time: 1.5261  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1495/3000]  eta: 0:38:01  lr: 0.000024  loss: 0.6818  time: 1.5046  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1495/3000]  eta: 0:38:00  lr: 0.000024  loss: 0.4406  time: 1.5043  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1500/3000]  eta: 0:37:53  lr: 0.000024  loss: 0.4115  time: 1.5287  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1500/3000]  eta: 0:37:53  lr: 0.000024  loss: 0.3907  time: 1.5284  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1505/3000]  eta: 0:37:45  lr: 0.000024  loss: 0.9140  time: 1.5137  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1505/3000]  eta: 0:37:45  lr: 0.000024  loss: 0.6114  time: 1.5133  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1510/3000]  eta: 0:37:38  lr: 0.000024  loss: 0.1375  time: 1.5002  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1510/3000]  eta: 0:37:37  lr: 0.000024  loss: 0.5296  time: 1.4999  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1515/3000]  eta: 0:37:30  lr: 0.000024  loss: 0.3703  time: 1.5065  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1515/3000]  eta: 0:37:30  lr: 0.000024  loss: 0.4181  time: 1.5061  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1520/3000]  eta: 0:37:23  lr: 0.000024  loss: 0.7314  time: 1.5191  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1520/3000]  eta: 0:37:22  lr: 0.000024  loss: 0.4995  time: 1.5188  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1525/3000]  eta: 0:37:15  lr: 0.000024  loss: 0.1279  time: 1.5291  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1525/3000]  eta: 0:37:15  lr: 0.000024  loss: 0.5939  time: 1.5288  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1530/3000]  eta: 0:37:08  lr: 0.000024  loss: 0.1069  time: 1.5349  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1530/3000]  eta: 0:37:07  lr: 0.000024  loss: 0.4271  time: 1.5346  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1535/3000]  eta: 0:37:00  lr: 0.000024  loss: 0.1130  time: 1.5160  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1535/3000]  eta: 0:36:59  lr: 0.000024  loss: 0.1573  time: 1.5156  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1540/3000]  eta: 0:36:53  lr: 0.000024  loss: 0.4783  time: 1.5127  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1540/3000]  eta: 0:36:52  lr: 0.000024  loss: 0.4823  time: 1.5123  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1545/3000]  eta: 0:36:45  lr: 0.000024  loss: 0.1885  time: 1.5068  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1545/3000]  eta: 0:36:44  lr: 0.000024  loss: 0.4092  time: 1.5064  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1550/3000]  eta: 0:36:37  lr: 0.000024  loss: 0.2216  time: 1.4871  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1550/3000]  eta: 0:36:37  lr: 0.000024  loss: 0.3523  time: 1.4867  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1555/3000]  eta: 0:36:29  lr: 0.000024  loss: 0.3088  time: 1.4948  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1555/3000]  eta: 0:36:29  lr: 0.000024  loss: 0.3518  time: 1.4946  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1560/3000]  eta: 0:36:22  lr: 0.000024  loss: 0.3787  time: 1.4854  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1560/3000]  eta: 0:36:21  lr: 0.000024  loss: 0.4067  time: 1.4851  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1565/3000]  eta: 0:36:14  lr: 0.000024  loss: 0.7642  time: 1.4808  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1565/3000]  eta: 0:36:13  lr: 0.000024  loss: 0.4166  time: 1.4799  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1570/3000]  eta: 0:36:06  lr: 0.000024  loss: 0.5437  time: 1.4823  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1570/3000]  eta: 0:36:06  lr: 0.000024  loss: 0.2746  time: 1.4816  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1575/3000]  eta: 0:35:58  lr: 0.000024  loss: 1.1301  time: 1.4818  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1575/3000]  eta: 0:35:58  lr: 0.000024  loss: 0.2231  time: 1.4808  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1580/3000]  eta: 0:35:51  lr: 0.000024  loss: 0.3595  time: 1.4867  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1580/3000]  eta: 0:35:50  lr: 0.000024  loss: 0.1862  time: 1.4858  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1585/3000]  eta: 0:35:43  lr: 0.000024  loss: 0.1902  time: 1.4853  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1585/3000]  eta: 0:35:43  lr: 0.000024  loss: 0.2898  time: 1.4849  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1590/3000]  eta: 0:35:35  lr: 0.000024  loss: 0.3791  time: 1.4799  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1590/3000]  eta: 0:35:35  lr: 0.000024  loss: 0.4862  time: 1.4795  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1595/3000]  eta: 0:35:27  lr: 0.000024  loss: 0.3349  time: 1.4758  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1595/3000]  eta: 0:35:27  lr: 0.000024  loss: 0.5657  time: 1.4756  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1600/3000]  eta: 0:35:20  lr: 0.000024  loss: 0.2646  time: 1.4723  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1600/3000]  eta: 0:35:19  lr: 0.000024  loss: 0.3056  time: 1.4720  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1605/3000]  eta: 0:35:12  lr: 0.000024  loss: 0.4512  time: 1.4632  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1605/3000]  eta: 0:35:11  lr: 0.000024  loss: 0.4065  time: 1.4630  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1610/3000]  eta: 0:35:04  lr: 0.000024  loss: 0.2874  time: 1.4736  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1610/3000]  eta: 0:35:04  lr: 0.000024  loss: 0.2226  time: 1.4734  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1615/3000]  eta: 0:34:57  lr: 0.000024  loss: 0.6617  time: 1.4820  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1615/3000]  eta: 0:34:56  lr: 0.000024  loss: 0.4489  time: 1.4817  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1620/3000]  eta: 0:34:49  lr: 0.000024  loss: 0.1020  time: 1.4722  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1620/3000]  eta: 0:34:48  lr: 0.000024  loss: 0.5812  time: 1.4719  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1625/3000]  eta: 0:34:41  lr: 0.000024  loss: 0.1289  time: 1.4840  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1625/3000]  eta: 0:34:41  lr: 0.000024  loss: 0.4139  time: 1.4837  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1630/3000]  eta: 0:34:33  lr: 0.000024  loss: 0.2622  time: 1.4836  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1630/3000]  eta: 0:34:33  lr: 0.000024  loss: 0.5948  time: 1.4833  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1635/3000]  eta: 0:34:26  lr: 0.000024  loss: 0.2702  time: 1.4982  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1635/3000]  eta: 0:34:25  lr: 0.000024  loss: 0.7075  time: 1.4978  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1640/3000]  eta: 0:34:19  lr: 0.000024  loss: 0.9909  time: 1.5173  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1640/3000]  eta: 0:34:18  lr: 0.000024  loss: 1.1034  time: 1.5169  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1645/3000]  eta: 0:34:11  lr: 0.000024  loss: 0.2623  time: 1.5227  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1645/3000]  eta: 0:34:10  lr: 0.000024  loss: 0.7395  time: 1.5223  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1650/3000]  eta: 0:34:04  lr: 0.000024  loss: 0.8569  time: 1.5452  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1650/3000]  eta: 0:34:03  lr: 0.000024  loss: 0.4556  time: 1.5447  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1655/3000]  eta: 0:33:56  lr: 0.000024  loss: 0.2816  time: 1.5336  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1655/3000]  eta: 0:33:55  lr: 0.000024  loss: 0.2379  time: 1.5332  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1660/3000]  eta: 0:33:49  lr: 0.000024  loss: 0.7098  time: 1.5336  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1660/3000]  eta: 0:33:48  lr: 0.000024  loss: 0.0676  time: 1.5332  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1665/3000]  eta: 0:33:41  lr: 0.000024  loss: 0.3972  time: 1.5349  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1665/3000]  eta: 0:33:41  lr: 0.000024  loss: 0.1051  time: 1.5346  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1670/3000]  eta: 0:33:34  lr: 0.000024  loss: 0.4672  time: 1.5432  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1670/3000]  eta: 0:33:33  lr: 0.000024  loss: 0.2859  time: 1.5430  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1675/3000]  eta: 0:33:26  lr: 0.000024  loss: 0.8160  time: 1.5413  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1675/3000]  eta: 0:33:26  lr: 0.000024  loss: 0.5361  time: 1.5410  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1680/3000]  eta: 0:33:19  lr: 0.000024  loss: 0.3060  time: 1.5445  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1680/3000]  eta: 0:33:18  lr: 0.000024  loss: 0.3310  time: 1.5433  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1685/3000]  eta: 0:33:11  lr: 0.000024  loss: 0.7175  time: 1.5604  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1685/3000]  eta: 0:33:11  lr: 0.000024  loss: 0.5363  time: 1.5591  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1690/3000]  eta: 0:33:04  lr: 0.000024  loss: 0.2424  time: 1.5425  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1690/3000]  eta: 0:33:03  lr: 0.000024  loss: 0.3949  time: 1.5412  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1695/3000]  eta: 0:32:56  lr: 0.000024  loss: 0.2971  time: 1.5532  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1695/3000]  eta: 0:32:56  lr: 0.000024  loss: 0.4274  time: 1.5520  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1700/3000]  eta: 0:32:49  lr: 0.000024  loss: 1.0252  time: 1.5432  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1700/3000]  eta: 0:32:48  lr: 0.000024  loss: 0.2437  time: 1.5428  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1705/3000]  eta: 0:32:41  lr: 0.000024  loss: 0.3854  time: 1.5086  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1705/3000]  eta: 0:32:41  lr: 0.000024  loss: 0.3120  time: 1.5084  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1710/3000]  eta: 0:32:33  lr: 0.000024  loss: 0.2905  time: 1.4849  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1710/3000]  eta: 0:32:33  lr: 0.000024  loss: 0.5333  time: 1.4847  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1715/3000]  eta: 0:32:25  lr: 0.000024  loss: 0.3299  time: 1.4638  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1715/3000]  eta: 0:32:25  lr: 0.000024  loss: 0.4824  time: 1.4634  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1720/3000]  eta: 0:32:18  lr: 0.000024  loss: 0.2428  time: 1.4662  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1720/3000]  eta: 0:32:17  lr: 0.000024  loss: 0.5870  time: 1.4659  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1725/3000]  eta: 0:32:10  lr: 0.000024  loss: 0.2576  time: 1.4770  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1725/3000]  eta: 0:32:10  lr: 0.000024  loss: 0.1428  time: 1.4766  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1730/3000]  eta: 0:32:02  lr: 0.000024  loss: 0.4922  time: 1.4794  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1730/3000]  eta: 0:32:02  lr: 0.000024  loss: 0.4547  time: 1.4790  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1735/3000]  eta: 0:31:55  lr: 0.000024  loss: 0.8638  time: 1.4885  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1735/3000]  eta: 0:31:54  lr: 0.000024  loss: 0.3610  time: 1.4883  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1740/3000]  eta: 0:31:47  lr: 0.000024  loss: 0.5713  time: 1.4972  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1740/3000]  eta: 0:31:47  lr: 0.000024  loss: 0.1803  time: 1.4968  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1745/3000]  eta: 0:31:40  lr: 0.000024  loss: 0.3758  time: 1.4964  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1745/3000]  eta: 0:31:39  lr: 0.000024  loss: 0.1258  time: 1.4962  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1750/3000]  eta: 0:31:32  lr: 0.000024  loss: 0.2935  time: 1.4967  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1750/3000]  eta: 0:31:31  lr: 0.000024  loss: 0.5964  time: 1.4964  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1755/3000]  eta: 0:31:25  lr: 0.000024  loss: 0.2539  time: 1.5180  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1755/3000]  eta: 0:31:24  lr: 0.000024  loss: 0.4219  time: 1.5176  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1760/3000]  eta: 0:31:17  lr: 0.000023  loss: 0.3425  time: 1.5215  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1760/3000]  eta: 0:31:17  lr: 0.000023  loss: 0.6458  time: 1.5212  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1765/3000]  eta: 0:31:10  lr: 0.000023  loss: 0.1890  time: 1.5312  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1765/3000]  eta: 0:31:09  lr: 0.000023  loss: 0.2235  time: 1.5308  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1770/3000]  eta: 0:31:02  lr: 0.000023  loss: 0.3093  time: 1.5306  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1770/3000]  eta: 0:31:01  lr: 0.000023  loss: 0.4694  time: 1.5302  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1775/3000]  eta: 0:30:54  lr: 0.000023  loss: 0.1591  time: 1.5175  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1775/3000]  eta: 0:30:54  lr: 0.000023  loss: 0.2929  time: 1.5171  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1780/3000]  eta: 0:30:47  lr: 0.000023  loss: 0.4674  time: 1.5093  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1780/3000]  eta: 0:30:46  lr: 0.000023  loss: 0.1207  time: 1.5090  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1785/3000]  eta: 0:30:39  lr: 0.000023  loss: 0.6079  time: 1.5077  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1785/3000]  eta: 0:30:39  lr: 0.000023  loss: 0.1901  time: 1.5074  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1790/3000]  eta: 0:30:32  lr: 0.000023  loss: 0.1524  time: 1.5263  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1790/3000]  eta: 0:30:31  lr: 0.000023  loss: 0.2271  time: 1.5260  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1795/3000]  eta: 0:30:24  lr: 0.000023  loss: 0.1683  time: 1.5073  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1795/3000]  eta: 0:30:23  lr: 0.000023  loss: 0.0837  time: 1.5071  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1800/3000]  eta: 0:30:16  lr: 0.000023  loss: 0.1760  time: 1.4963  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1800/3000]  eta: 0:30:16  lr: 0.000023  loss: 0.4011  time: 1.4957  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1805/3000]  eta: 0:30:09  lr: 0.000023  loss: 0.8662  time: 1.5129  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1805/3000]  eta: 0:30:08  lr: 0.000023  loss: 0.1951  time: 1.5122  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1810/3000]  eta: 0:30:01  lr: 0.000023  loss: 0.3999  time: 1.4990  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1810/3000]  eta: 0:30:01  lr: 0.000023  loss: 0.2584  time: 1.4983  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1815/3000]  eta: 0:29:54  lr: 0.000023  loss: 0.3916  time: 1.5179  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1815/3000]  eta: 0:29:53  lr: 0.000023  loss: 0.2290  time: 1.5172  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1820/3000]  eta: 0:29:46  lr: 0.000023  loss: 0.1838  time: 1.5309  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1820/3000]  eta: 0:29:46  lr: 0.000023  loss: 0.5062  time: 1.5307  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1825/3000]  eta: 0:29:39  lr: 0.000023  loss: 0.6090  time: 1.5161  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1825/3000]  eta: 0:29:38  lr: 0.000023  loss: 0.2569  time: 1.5159  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1830/3000]  eta: 0:29:31  lr: 0.000023  loss: 0.8056  time: 1.5044  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1830/3000]  eta: 0:29:30  lr: 0.000023  loss: 0.2198  time: 1.5042  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1835/3000]  eta: 0:29:23  lr: 0.000023  loss: 0.8805  time: 1.5140  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1835/3000]  eta: 0:29:23  lr: 0.000023  loss: 0.1987  time: 1.5138  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1840/3000]  eta: 0:29:16  lr: 0.000023  loss: 0.2195  time: 1.4993  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1840/3000]  eta: 0:29:15  lr: 0.000023  loss: 0.1863  time: 1.4990  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1845/3000]  eta: 0:29:08  lr: 0.000023  loss: 0.5228  time: 1.4914  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1845/3000]  eta: 0:29:08  lr: 0.000023  loss: 0.2345  time: 1.4912  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1850/3000]  eta: 0:29:01  lr: 0.000023  loss: 0.3327  time: 1.5239  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1850/3000]  eta: 0:29:00  lr: 0.000023  loss: 0.0987  time: 1.5236  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1855/3000]  eta: 0:28:53  lr: 0.000023  loss: 0.2608  time: 1.5332  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1855/3000]  eta: 0:28:53  lr: 0.000023  loss: 0.4421  time: 1.5330  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1860/3000]  eta: 0:28:46  lr: 0.000023  loss: 0.6425  time: 1.5328  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1860/3000]  eta: 0:28:45  lr: 0.000023  loss: 0.2396  time: 1.5323  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1865/3000]  eta: 0:28:38  lr: 0.000023  loss: 0.2394  time: 1.5531  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1865/3000]  eta: 0:28:38  lr: 0.000023  loss: 0.1623  time: 1.5523  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1870/3000]  eta: 0:28:31  lr: 0.000023  loss: 0.5327  time: 1.5626  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1870/3000]  eta: 0:28:31  lr: 0.000023  loss: 0.2937  time: 1.5619  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1875/3000]  eta: 0:28:24  lr: 0.000023  loss: 0.2627  time: 1.5551  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1875/3000]  eta: 0:28:23  lr: 0.000023  loss: 0.2175  time: 1.5542  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1880/3000]  eta: 0:28:16  lr: 0.000023  loss: 0.1471  time: 1.5593  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1880/3000]  eta: 0:28:16  lr: 0.000023  loss: 0.5203  time: 1.5587  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1885/3000]  eta: 0:28:08  lr: 0.000023  loss: 0.3212  time: 1.5427  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1885/3000]  eta: 0:28:08  lr: 0.000023  loss: 0.0880  time: 1.5423  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1890/3000]  eta: 0:28:01  lr: 0.000023  loss: 0.9227  time: 1.5196  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1890/3000]  eta: 0:28:00  lr: 0.000023  loss: 0.1959  time: 1.5191  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1895/3000]  eta: 0:27:53  lr: 0.000023  loss: 0.2499  time: 1.4949  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1895/3000]  eta: 0:27:53  lr: 0.000023  loss: 0.8544  time: 1.4946  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1900/3000]  eta: 0:27:45  lr: 0.000023  loss: 0.4224  time: 1.4884  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1900/3000]  eta: 0:27:45  lr: 0.000023  loss: 0.6455  time: 1.4880  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1905/3000]  eta: 0:27:38  lr: 0.000023  loss: 0.1878  time: 1.4897  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1905/3000]  eta: 0:27:37  lr: 0.000023  loss: 0.5246  time: 1.4895  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1910/3000]  eta: 0:27:30  lr: 0.000023  loss: 0.2006  time: 1.4726  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1910/3000]  eta: 0:27:30  lr: 0.000023  loss: 0.0775  time: 1.4724  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1915/3000]  eta: 0:27:22  lr: 0.000023  loss: 0.2896  time: 1.4875  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1915/3000]  eta: 0:27:22  lr: 0.000023  loss: 0.5876  time: 1.4872  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1920/3000]  eta: 0:27:15  lr: 0.000023  loss: 0.1111  time: 1.4925  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1920/3000]  eta: 0:27:14  lr: 0.000023  loss: 0.4071  time: 1.4923  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1925/3000]  eta: 0:27:07  lr: 0.000023  loss: 0.4346  time: 1.4897  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1925/3000]  eta: 0:27:07  lr: 0.000023  loss: 0.4418  time: 1.4903  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1930/3000]  eta: 0:27:00  lr: 0.000023  loss: 0.7947  time: 1.5222  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1930/3000]  eta: 0:26:59  lr: 0.000023  loss: 0.3549  time: 1.5219  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1935/3000]  eta: 0:26:52  lr: 0.000023  loss: 0.5409  time: 1.5314  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1935/3000]  eta: 0:26:52  lr: 0.000023  loss: 0.4126  time: 1.5312  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1940/3000]  eta: 0:26:45  lr: 0.000023  loss: 0.4918  time: 1.5193  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1940/3000]  eta: 0:26:44  lr: 0.000023  loss: 0.3935  time: 1.5190  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1945/3000]  eta: 0:26:37  lr: 0.000023  loss: 0.4342  time: 1.5185  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1945/3000]  eta: 0:26:37  lr: 0.000023  loss: 0.2373  time: 1.5185  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1950/3000]  eta: 0:26:29  lr: 0.000023  loss: 0.3155  time: 1.4912  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1950/3000]  eta: 0:26:29  lr: 0.000023  loss: 0.7253  time: 1.4908  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1955/3000]  eta: 0:26:22  lr: 0.000023  loss: 1.3900  time: 1.4724  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1955/3000]  eta: 0:26:21  lr: 0.000023  loss: 0.1559  time: 1.4721  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1960/3000]  eta: 0:26:14  lr: 0.000023  loss: 0.9774  time: 1.4980  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1960/3000]  eta: 0:26:14  lr: 0.000023  loss: 0.2719  time: 1.4976  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1965/3000]  eta: 0:26:07  lr: 0.000023  loss: 0.1199  time: 1.5052  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1965/3000]  eta: 0:26:06  lr: 0.000023  loss: 0.4126  time: 1.5049  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1970/3000]  eta: 0:25:59  lr: 0.000023  loss: 0.2750  time: 1.5347  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1970/3000]  eta: 0:25:59  lr: 0.000023  loss: 0.3046  time: 1.5353  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1975/3000]  eta: 0:25:52  lr: 0.000023  loss: 0.5155  time: 1.5517  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1975/3000]  eta: 0:25:51  lr: 0.000023  loss: 0.5127  time: 1.5514  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1980/3000]  eta: 0:25:44  lr: 0.000023  loss: 0.2342  time: 1.5356  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1980/3000]  eta: 0:25:44  lr: 0.000023  loss: 0.2097  time: 1.5353  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1985/3000]  eta: 0:25:36  lr: 0.000023  loss: 0.7019  time: 1.5242  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1985/3000]  eta: 0:25:36  lr: 0.000023  loss: 0.5416  time: 1.5238  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1990/3000]  eta: 0:25:29  lr: 0.000023  loss: 0.4475  time: 1.5240  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1990/3000]  eta: 0:25:29  lr: 0.000023  loss: 0.4942  time: 1.5240  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [1995/3000]  eta: 0:25:22  lr: 0.000023  loss: 0.5249  time: 1.5317  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [1995/3000]  eta: 0:25:21  lr: 0.000023  loss: 0.4537  time: 1.5314  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2000/3000]  eta: 0:25:14  lr: 0.000023  loss: 0.7367  time: 1.5407  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2000/3000]  eta: 0:25:14  lr: 0.000023  loss: 0.1242  time: 1.5405  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2005/3000]  eta: 0:25:07  lr: 0.000023  loss: 0.6388  time: 1.5567  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2005/3000]  eta: 0:25:06  lr: 0.000023  loss: 0.2686  time: 1.5564  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2010/3000]  eta: 0:24:59  lr: 0.000023  loss: 0.2155  time: 1.5380  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2010/3000]  eta: 0:24:59  lr: 0.000023  loss: 0.3597  time: 1.5377  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2015/3000]  eta: 0:24:51  lr: 0.000023  loss: 0.1155  time: 1.5226  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2015/3000]  eta: 0:24:51  lr: 0.000023  loss: 0.5222  time: 1.5224  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2020/3000]  eta: 0:24:44  lr: 0.000023  loss: 0.1626  time: 1.5007  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2020/3000]  eta: 0:24:43  lr: 0.000023  loss: 0.3329  time: 1.5004  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2025/3000]  eta: 0:24:36  lr: 0.000023  loss: 0.1769  time: 1.4775  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2025/3000]  eta: 0:24:36  lr: 0.000023  loss: 0.1279  time: 1.4772  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2030/3000]  eta: 0:24:28  lr: 0.000023  loss: 0.0727  time: 1.4742  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2030/3000]  eta: 0:24:28  lr: 0.000023  loss: 0.4585  time: 1.4738  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2035/3000]  eta: 0:24:21  lr: 0.000023  loss: 0.1209  time: 1.4704  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2035/3000]  eta: 0:24:20  lr: 0.000023  loss: 0.5578  time: 1.4700  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2040/3000]  eta: 0:24:13  lr: 0.000023  loss: 0.8985  time: 1.4940  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2040/3000]  eta: 0:24:13  lr: 0.000023  loss: 0.6457  time: 1.4936  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2045/3000]  eta: 0:24:06  lr: 0.000023  loss: 0.3652  time: 1.5136  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2045/3000]  eta: 0:24:05  lr: 0.000023  loss: 0.4391  time: 1.5133  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2050/3000]  eta: 0:23:58  lr: 0.000023  loss: 0.3248  time: 1.5264  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2050/3000]  eta: 0:23:58  lr: 0.000023  loss: 0.3002  time: 1.5261  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2055/3000]  eta: 0:23:50  lr: 0.000023  loss: 0.1061  time: 1.5240  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2055/3000]  eta: 0:23:50  lr: 0.000023  loss: 1.3268  time: 1.5237  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2060/3000]  eta: 0:23:43  lr: 0.000023  loss: 0.6551  time: 1.5164  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2060/3000]  eta: 0:23:43  lr: 0.000023  loss: 0.3185  time: 1.5160  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2065/3000]  eta: 0:23:35  lr: 0.000023  loss: 0.3032  time: 1.5208  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2065/3000]  eta: 0:23:35  lr: 0.000023  loss: 0.4323  time: 1.5204  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2070/3000]  eta: 0:23:28  lr: 0.000023  loss: 0.2233  time: 1.5210  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2070/3000]  eta: 0:23:28  lr: 0.000023  loss: 0.4215  time: 1.5207  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2075/3000]  eta: 0:23:20  lr: 0.000023  loss: 0.8154  time: 1.5271  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2075/3000]  eta: 0:23:20  lr: 0.000023  loss: 0.2779  time: 1.5268  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2080/3000]  eta: 0:23:13  lr: 0.000023  loss: 0.6141  time: 1.5325  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2080/3000]  eta: 0:23:12  lr: 0.000023  loss: 0.1866  time: 1.5323  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2085/3000]  eta: 0:23:05  lr: 0.000023  loss: 0.5471  time: 1.5131  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2085/3000]  eta: 0:23:05  lr: 0.000023  loss: 1.3089  time: 1.5129  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2090/3000]  eta: 0:22:58  lr: 0.000023  loss: 0.3011  time: 1.5102  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2090/3000]  eta: 0:22:57  lr: 0.000023  loss: 0.4989  time: 1.5101  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2095/3000]  eta: 0:22:50  lr: 0.000023  loss: 0.5813  time: 1.5296  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2095/3000]  eta: 0:22:50  lr: 0.000023  loss: 0.4823  time: 1.5295  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2100/3000]  eta: 0:22:43  lr: 0.000023  loss: 0.2872  time: 1.5238  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2100/3000]  eta: 0:22:42  lr: 0.000023  loss: 0.7282  time: 1.5236  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2105/3000]  eta: 0:22:35  lr: 0.000023  loss: 0.5811  time: 1.5444  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2105/3000]  eta: 0:22:35  lr: 0.000023  loss: 0.5178  time: 1.5436  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2110/3000]  eta: 0:22:28  lr: 0.000023  loss: 0.1004  time: 1.5466  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2110/3000]  eta: 0:22:27  lr: 0.000023  loss: 0.2188  time: 1.5457  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2115/3000]  eta: 0:22:20  lr: 0.000023  loss: 0.7483  time: 1.5329  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2115/3000]  eta: 0:22:20  lr: 0.000023  loss: 0.7181  time: 1.5320  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2120/3000]  eta: 0:22:13  lr: 0.000023  loss: 0.4583  time: 1.5421  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2120/3000]  eta: 0:22:12  lr: 0.000023  loss: 0.3338  time: 1.5412  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2125/3000]  eta: 0:22:05  lr: 0.000023  loss: 0.1409  time: 1.5334  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2125/3000]  eta: 0:22:05  lr: 0.000023  loss: 0.5364  time: 1.5331  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2130/3000]  eta: 0:21:57  lr: 0.000023  loss: 0.1758  time: 1.5101  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2130/3000]  eta: 0:21:57  lr: 0.000023  loss: 0.4011  time: 1.5099  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2135/3000]  eta: 0:21:50  lr: 0.000023  loss: 0.3543  time: 1.5257  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2135/3000]  eta: 0:21:49  lr: 0.000023  loss: 0.1690  time: 1.5254  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2140/3000]  eta: 0:21:42  lr: 0.000023  loss: 0.3659  time: 1.5047  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2140/3000]  eta: 0:21:42  lr: 0.000023  loss: 0.3645  time: 1.5045  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2145/3000]  eta: 0:21:35  lr: 0.000023  loss: 0.5757  time: 1.4954  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2145/3000]  eta: 0:21:34  lr: 0.000023  loss: 0.3569  time: 1.4951  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2150/3000]  eta: 0:21:27  lr: 0.000023  loss: 0.0958  time: 1.5041  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2150/3000]  eta: 0:21:27  lr: 0.000023  loss: 0.0759  time: 1.5037  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2155/3000]  eta: 0:21:19  lr: 0.000023  loss: 0.1590  time: 1.4958  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2155/3000]  eta: 0:21:19  lr: 0.000023  loss: 0.7433  time: 1.4955  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2160/3000]  eta: 0:21:12  lr: 0.000023  loss: 0.4230  time: 1.4990  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2160/3000]  eta: 0:21:11  lr: 0.000023  loss: 0.5337  time: 1.4987  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2165/3000]  eta: 0:21:04  lr: 0.000023  loss: 0.3389  time: 1.5247  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2165/3000]  eta: 0:21:04  lr: 0.000023  loss: 0.1801  time: 1.5244  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2170/3000]  eta: 0:20:57  lr: 0.000023  loss: 0.3868  time: 1.5375  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2170/3000]  eta: 0:20:56  lr: 0.000023  loss: 0.2413  time: 1.5372  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2175/3000]  eta: 0:20:49  lr: 0.000023  loss: 0.2611  time: 1.5421  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2175/3000]  eta: 0:20:49  lr: 0.000023  loss: 0.2987  time: 1.5417  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2180/3000]  eta: 0:20:42  lr: 0.000023  loss: 0.5179  time: 1.5606  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2180/3000]  eta: 0:20:41  lr: 0.000023  loss: 0.1656  time: 1.5602  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2185/3000]  eta: 0:20:34  lr: 0.000023  loss: 0.3394  time: 1.5358  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2185/3000]  eta: 0:20:34  lr: 0.000023  loss: 0.3402  time: 1.5354  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2190/3000]  eta: 0:20:26  lr: 0.000023  loss: 0.6884  time: 1.5280  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2190/3000]  eta: 0:20:27  lr: 0.000023  loss: 0.0931  time: 1.5284  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2195/3000]  eta: 0:20:19  lr: 0.000023  loss: 0.3222  time: 1.4848  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2195/3000]  eta: 0:20:18  lr: 0.000023  loss: 0.4299  time: 1.4845  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2200/3000]  eta: 0:20:11  lr: 0.000023  loss: 0.2948  time: 1.4797  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2200/3000]  eta: 0:20:11  lr: 0.000023  loss: 0.1938  time: 1.4802  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2205/3000]  eta: 0:20:04  lr: 0.000023  loss: 0.4892  time: 1.4905  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2205/3000]  eta: 0:20:03  lr: 0.000023  loss: 0.4205  time: 1.4901  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2210/3000]  eta: 0:19:56  lr: 0.000023  loss: 0.2830  time: 1.4923  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2210/3000]  eta: 0:19:56  lr: 0.000023  loss: 0.1858  time: 1.4919  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2215/3000]  eta: 0:19:49  lr: 0.000023  loss: 0.1660  time: 1.5254  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2215/3000]  eta: 0:19:48  lr: 0.000023  loss: 0.1720  time: 1.5248  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2220/3000]  eta: 0:19:41  lr: 0.000023  loss: 0.1605  time: 1.5010  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2220/3000]  eta: 0:19:41  lr: 0.000023  loss: 0.3365  time: 1.5007  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2225/3000]  eta: 0:19:33  lr: 0.000023  loss: 0.1157  time: 1.5032  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2225/3000]  eta: 0:19:33  lr: 0.000023  loss: 0.2968  time: 1.5029  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2230/3000]  eta: 0:19:26  lr: 0.000023  loss: 0.3099  time: 1.5108  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2230/3000]  eta: 0:19:25  lr: 0.000023  loss: 0.4488  time: 1.5105  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2235/3000]  eta: 0:19:18  lr: 0.000023  loss: 0.0514  time: 1.5108  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2235/3000]  eta: 0:19:18  lr: 0.000023  loss: 0.3343  time: 1.5106  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2240/3000]  eta: 0:19:11  lr: 0.000023  loss: 0.1602  time: 1.5513  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2240/3000]  eta: 0:19:10  lr: 0.000023  loss: 0.1396  time: 1.5510  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2245/3000]  eta: 0:19:03  lr: 0.000023  loss: 0.2599  time: 1.5545  data: 0.0000  max mem: 18603Train: data epoch: [11]  [2245/3000]  eta: 0:19:03  lr: 0.000023  loss: 0.2315  time: 1.5550  data: 0.0000  max mem: 18596

Train: data epoch: [11]  [2250/3000]  eta: 0:18:56  lr: 0.000023  loss: 0.3024  time: 1.5543  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2250/3000]  eta: 0:18:55  lr: 0.000023  loss: 0.2017  time: 1.5539  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2255/3000]  eta: 0:18:48  lr: 0.000023  loss: 0.3401  time: 1.5284  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2255/3000]  eta: 0:18:48  lr: 0.000023  loss: 0.4898  time: 1.5278  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2260/3000]  eta: 0:18:40  lr: 0.000023  loss: 0.2367  time: 1.4991  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2260/3000]  eta: 0:18:40  lr: 0.000023  loss: 0.5444  time: 1.4998  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2265/3000]  eta: 0:18:33  lr: 0.000023  loss: 0.1178  time: 1.4780  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2265/3000]  eta: 0:18:32  lr: 0.000023  loss: 0.6261  time: 1.4777  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2270/3000]  eta: 0:18:25  lr: 0.000023  loss: 0.3568  time: 1.4835  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2270/3000]  eta: 0:18:25  lr: 0.000023  loss: 0.4382  time: 1.4830  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2275/3000]  eta: 0:18:18  lr: 0.000023  loss: 0.6618  time: 1.4925  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2275/3000]  eta: 0:18:17  lr: 0.000023  loss: 0.1636  time: 1.4922  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2280/3000]  eta: 0:18:10  lr: 0.000023  loss: 0.6198  time: 1.5093  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2280/3000]  eta: 0:18:10  lr: 0.000023  loss: 0.2060  time: 1.5092  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2285/3000]  eta: 0:18:03  lr: 0.000023  loss: 0.6415  time: 1.5237  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2285/3000]  eta: 0:18:02  lr: 0.000023  loss: 0.4790  time: 1.5233  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2290/3000]  eta: 0:17:55  lr: 0.000023  loss: 0.3073  time: 1.5127  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2290/3000]  eta: 0:17:55  lr: 0.000023  loss: 0.1281  time: 1.5124  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2295/3000]  eta: 0:17:47  lr: 0.000023  loss: 0.2615  time: 1.5202  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2295/3000]  eta: 0:17:47  lr: 0.000023  loss: 0.4257  time: 1.5199  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2300/3000]  eta: 0:17:40  lr: 0.000023  loss: 1.0294  time: 1.5112  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2300/3000]  eta: 0:17:39  lr: 0.000023  loss: 0.2791  time: 1.5107  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2305/3000]  eta: 0:17:32  lr: 0.000023  loss: 0.3990  time: 1.5169  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2305/3000]  eta: 0:17:32  lr: 0.000023  loss: 0.3416  time: 1.5166  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2310/3000]  eta: 0:17:25  lr: 0.000023  loss: 0.3906  time: 1.5190  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2310/3000]  eta: 0:17:24  lr: 0.000023  loss: 0.6020  time: 1.5186  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2315/3000]  eta: 0:17:17  lr: 0.000023  loss: 0.1522  time: 1.5049  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2315/3000]  eta: 0:17:17  lr: 0.000023  loss: 0.7375  time: 1.5045  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2320/3000]  eta: 0:17:09  lr: 0.000023  loss: 0.3903  time: 1.5160  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2320/3000]  eta: 0:17:09  lr: 0.000023  loss: 0.2141  time: 1.5157  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2325/3000]  eta: 0:17:02  lr: 0.000023  loss: 0.2816  time: 1.4890  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2325/3000]  eta: 0:17:02  lr: 0.000023  loss: 0.2885  time: 1.4887  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2330/3000]  eta: 0:16:54  lr: 0.000023  loss: 0.1623  time: 1.5007  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2330/3000]  eta: 0:16:54  lr: 0.000023  loss: 0.2832  time: 1.5003  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2335/3000]  eta: 0:16:47  lr: 0.000023  loss: 0.5006  time: 1.5228  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2335/3000]  eta: 0:16:46  lr: 0.000023  loss: 0.4885  time: 1.5225  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2340/3000]  eta: 0:16:39  lr: 0.000023  loss: 0.6882  time: 1.5029  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2340/3000]  eta: 0:16:39  lr: 0.000023  loss: 0.3857  time: 1.5025  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2345/3000]  eta: 0:16:32  lr: 0.000023  loss: 0.3585  time: 1.5293  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2345/3000]  eta: 0:16:31  lr: 0.000023  loss: 0.2798  time: 1.5289  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2350/3000]  eta: 0:16:24  lr: 0.000023  loss: 0.0476  time: 1.5092  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2350/3000]  eta: 0:16:24  lr: 0.000023  loss: 0.6712  time: 1.5089  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2355/3000]  eta: 0:16:16  lr: 0.000023  loss: 0.3319  time: 1.5098  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2355/3000]  eta: 0:16:16  lr: 0.000023  loss: 0.5430  time: 1.5096  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2360/3000]  eta: 0:16:09  lr: 0.000023  loss: 1.2766  time: 1.5171  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2360/3000]  eta: 0:16:09  lr: 0.000023  loss: 0.4443  time: 1.5169  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2365/3000]  eta: 0:16:01  lr: 0.000023  loss: 0.1244  time: 1.5155  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2365/3000]  eta: 0:16:01  lr: 0.000023  loss: 0.2734  time: 1.5153  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2370/3000]  eta: 0:15:54  lr: 0.000023  loss: 0.5237  time: 1.5301  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2370/3000]  eta: 0:15:54  lr: 0.000023  loss: 0.7368  time: 1.5299  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2375/3000]  eta: 0:15:46  lr: 0.000023  loss: 0.3040  time: 1.5337  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2375/3000]  eta: 0:15:46  lr: 0.000023  loss: 0.6343  time: 1.5334  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2380/3000]  eta: 0:15:39  lr: 0.000023  loss: 0.4488  time: 1.5289  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2380/3000]  eta: 0:15:38  lr: 0.000023  loss: 0.1482  time: 1.5285  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2385/3000]  eta: 0:15:31  lr: 0.000023  loss: 0.2497  time: 1.5246  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2385/3000]  eta: 0:15:31  lr: 0.000023  loss: 0.1533  time: 1.5243  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2390/3000]  eta: 0:15:23  lr: 0.000023  loss: 0.2506  time: 1.5062  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2390/3000]  eta: 0:15:23  lr: 0.000023  loss: 0.7775  time: 1.5059  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2395/3000]  eta: 0:15:16  lr: 0.000023  loss: 0.3423  time: 1.4766  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2395/3000]  eta: 0:15:16  lr: 0.000023  loss: 0.3526  time: 1.4764  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2400/3000]  eta: 0:15:08  lr: 0.000023  loss: 0.3866  time: 1.4912  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2400/3000]  eta: 0:15:08  lr: 0.000023  loss: 0.4500  time: 1.4910  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2405/3000]  eta: 0:15:01  lr: 0.000023  loss: 0.4663  time: 1.4894  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2405/3000]  eta: 0:15:00  lr: 0.000023  loss: 0.4221  time: 1.4892  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2410/3000]  eta: 0:14:53  lr: 0.000023  loss: 0.4936  time: 1.4893  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2410/3000]  eta: 0:14:53  lr: 0.000023  loss: 0.3855  time: 1.4891  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2415/3000]  eta: 0:14:45  lr: 0.000023  loss: 0.1908  time: 1.5104  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2415/3000]  eta: 0:14:45  lr: 0.000023  loss: 0.3484  time: 1.5100  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2420/3000]  eta: 0:14:38  lr: 0.000023  loss: 0.3353  time: 1.4988  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2420/3000]  eta: 0:14:38  lr: 0.000023  loss: 0.2526  time: 1.4985  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2425/3000]  eta: 0:14:30  lr: 0.000023  loss: 0.3905  time: 1.4868  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2425/3000]  eta: 0:14:30  lr: 0.000023  loss: 0.3243  time: 1.4860  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2430/3000]  eta: 0:14:23  lr: 0.000023  loss: 0.5202  time: 1.4821  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2430/3000]  eta: 0:14:22  lr: 0.000023  loss: 0.4286  time: 1.4812  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2435/3000]  eta: 0:14:15  lr: 0.000023  loss: 0.1483  time: 1.4975  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2435/3000]  eta: 0:14:15  lr: 0.000023  loss: 0.3326  time: 1.4967  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2440/3000]  eta: 0:14:07  lr: 0.000023  loss: 0.2141  time: 1.4826  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2440/3000]  eta: 0:14:07  lr: 0.000023  loss: 0.9442  time: 1.4818  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2445/3000]  eta: 0:14:00  lr: 0.000023  loss: 1.1005  time: 1.4990  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2445/3000]  eta: 0:14:00  lr: 0.000023  loss: 0.1717  time: 1.4986  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2450/3000]  eta: 0:13:52  lr: 0.000023  loss: 0.1523  time: 1.4999  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2450/3000]  eta: 0:13:52  lr: 0.000023  loss: 0.4951  time: 1.4995  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2455/3000]  eta: 0:13:45  lr: 0.000023  loss: 0.2540  time: 1.4916  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2455/3000]  eta: 0:13:44  lr: 0.000023  loss: 0.4531  time: 1.4913  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2460/3000]  eta: 0:13:37  lr: 0.000023  loss: 0.2226  time: 1.4905  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2460/3000]  eta: 0:13:37  lr: 0.000023  loss: 0.6303  time: 1.4901  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2465/3000]  eta: 0:13:29  lr: 0.000023  loss: 0.2448  time: 1.4610  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2465/3000]  eta: 0:13:29  lr: 0.000023  loss: 0.0980  time: 1.4608  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2470/3000]  eta: 0:13:22  lr: 0.000023  loss: 0.2617  time: 1.4647  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2470/3000]  eta: 0:13:22  lr: 0.000023  loss: 0.4159  time: 1.4645  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2475/3000]  eta: 0:13:14  lr: 0.000023  loss: 0.3047  time: 1.4531  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2475/3000]  eta: 0:13:14  lr: 0.000023  loss: 0.6020  time: 1.4537  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2480/3000]  eta: 0:13:06  lr: 0.000023  loss: 0.6717  time: 1.4486  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2480/3000]  eta: 0:13:06  lr: 0.000023  loss: 0.4150  time: 1.4484  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2485/3000]  eta: 0:12:59  lr: 0.000023  loss: 0.4858  time: 1.4751  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2485/3000]  eta: 0:12:59  lr: 0.000023  loss: 0.6714  time: 1.4748  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2490/3000]  eta: 0:12:51  lr: 0.000023  loss: 0.1981  time: 1.4899  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2490/3000]  eta: 0:12:51  lr: 0.000023  loss: 0.5095  time: 1.4895  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2495/3000]  eta: 0:12:44  lr: 0.000023  loss: 0.3740  time: 1.4941  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2495/3000]  eta: 0:12:44  lr: 0.000023  loss: 1.4346  time: 1.4940  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2500/3000]  eta: 0:12:36  lr: 0.000023  loss: 0.2669  time: 1.5299  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2500/3000]  eta: 0:12:36  lr: 0.000023  loss: 0.3535  time: 1.5294  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2505/3000]  eta: 0:12:29  lr: 0.000023  loss: 0.6393  time: 1.5377  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2505/3000]  eta: 0:12:29  lr: 0.000023  loss: 0.2288  time: 1.5372  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2510/3000]  eta: 0:12:21  lr: 0.000023  loss: 0.3576  time: 1.5444  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2510/3000]  eta: 0:12:21  lr: 0.000023  loss: 0.3531  time: 1.5440  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2515/3000]  eta: 0:12:14  lr: 0.000023  loss: 0.6832  time: 1.5426  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2515/3000]  eta: 0:12:13  lr: 0.000023  loss: 0.2201  time: 1.5423  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2520/3000]  eta: 0:12:06  lr: 0.000023  loss: 0.4883  time: 1.5485  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2520/3000]  eta: 0:12:06  lr: 0.000023  loss: 0.8340  time: 1.5482  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2525/3000]  eta: 0:11:59  lr: 0.000023  loss: 0.2713  time: 1.5538  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2525/3000]  eta: 0:11:58  lr: 0.000023  loss: 0.2189  time: 1.5535  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2530/3000]  eta: 0:11:51  lr: 0.000023  loss: 0.7048  time: 1.5312  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2530/3000]  eta: 0:11:51  lr: 0.000023  loss: 0.5556  time: 1.5309  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2535/3000]  eta: 0:11:43  lr: 0.000023  loss: 0.1498  time: 1.5297  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2535/3000]  eta: 0:11:43  lr: 0.000023  loss: 1.0970  time: 1.5294  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2540/3000]  eta: 0:11:36  lr: 0.000023  loss: 1.0192  time: 1.5175  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2540/3000]  eta: 0:11:36  lr: 0.000023  loss: 0.2199  time: 1.5172  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2545/3000]  eta: 0:11:28  lr: 0.000023  loss: 0.2819  time: 1.5168  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2545/3000]  eta: 0:11:28  lr: 0.000023  loss: 0.4706  time: 1.5165  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2550/3000]  eta: 0:11:21  lr: 0.000023  loss: 1.1557  time: 1.5347  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2550/3000]  eta: 0:11:21  lr: 0.000023  loss: 0.4071  time: 1.5343  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2555/3000]  eta: 0:11:13  lr: 0.000023  loss: 0.1816  time: 1.5372  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2555/3000]  eta: 0:11:13  lr: 0.000023  loss: 0.5944  time: 1.5368  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2560/3000]  eta: 0:11:06  lr: 0.000023  loss: 0.3327  time: 1.5468  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2560/3000]  eta: 0:11:06  lr: 0.000023  loss: 0.3137  time: 1.5462  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2565/3000]  eta: 0:10:58  lr: 0.000023  loss: 0.2867  time: 1.5129  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2565/3000]  eta: 0:10:58  lr: 0.000023  loss: 0.2424  time: 1.5121  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2570/3000]  eta: 0:10:51  lr: 0.000023  loss: 0.5438  time: 1.5208  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2570/3000]  eta: 0:10:50  lr: 0.000023  loss: 0.3893  time: 1.5199  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2575/3000]  eta: 0:10:43  lr: 0.000023  loss: 0.7874  time: 1.5338  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2575/3000]  eta: 0:10:43  lr: 0.000023  loss: 0.8460  time: 1.5331  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2580/3000]  eta: 0:10:35  lr: 0.000023  loss: 0.6344  time: 1.5341  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2580/3000]  eta: 0:10:35  lr: 0.000023  loss: 0.1799  time: 1.5335  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2585/3000]  eta: 0:10:28  lr: 0.000023  loss: 0.1078  time: 1.5547  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2585/3000]  eta: 0:10:28  lr: 0.000023  loss: 0.3905  time: 1.5543  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2590/3000]  eta: 0:10:20  lr: 0.000023  loss: 0.5600  time: 1.5526  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2590/3000]  eta: 0:10:20  lr: 0.000023  loss: 0.4512  time: 1.5522  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2595/3000]  eta: 0:10:13  lr: 0.000023  loss: 0.4690  time: 1.5293  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2595/3000]  eta: 0:10:13  lr: 0.000023  loss: 0.1381  time: 1.5290  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2600/3000]  eta: 0:10:05  lr: 0.000023  loss: 0.4406  time: 1.5178  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2600/3000]  eta: 0:10:05  lr: 0.000023  loss: 0.8981  time: 1.5175  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2605/3000]  eta: 0:09:58  lr: 0.000023  loss: 0.3176  time: 1.5140  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2605/3000]  eta: 0:09:57  lr: 0.000023  loss: 0.1320  time: 1.5136  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2610/3000]  eta: 0:09:50  lr: 0.000023  loss: 0.1557  time: 1.5058  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2610/3000]  eta: 0:09:50  lr: 0.000023  loss: 0.2169  time: 1.5052  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2615/3000]  eta: 0:09:42  lr: 0.000023  loss: 0.3867  time: 1.5081  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2615/3000]  eta: 0:09:42  lr: 0.000023  loss: 0.3408  time: 1.5077  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2620/3000]  eta: 0:09:35  lr: 0.000023  loss: 0.1876  time: 1.5142  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2620/3000]  eta: 0:09:35  lr: 0.000023  loss: 0.5241  time: 1.5136  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2625/3000]  eta: 0:09:27  lr: 0.000023  loss: 0.2674  time: 1.5167  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2625/3000]  eta: 0:09:27  lr: 0.000023  loss: 0.5700  time: 1.5162  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2630/3000]  eta: 0:09:20  lr: 0.000023  loss: 0.3306  time: 1.5168  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2630/3000]  eta: 0:09:20  lr: 0.000023  loss: 0.3531  time: 1.5166  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2635/3000]  eta: 0:09:12  lr: 0.000023  loss: 0.3265  time: 1.5341  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2635/3000]  eta: 0:09:12  lr: 0.000023  loss: 0.3432  time: 1.5337  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2640/3000]  eta: 0:09:05  lr: 0.000023  loss: 0.4248  time: 1.5332  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2640/3000]  eta: 0:09:05  lr: 0.000023  loss: 0.4373  time: 1.5329  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2645/3000]  eta: 0:08:57  lr: 0.000023  loss: 0.4243  time: 1.5420  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2645/3000]  eta: 0:08:57  lr: 0.000023  loss: 0.5208  time: 1.5417  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2650/3000]  eta: 0:08:50  lr: 0.000023  loss: 0.1910  time: 1.5417  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2650/3000]  eta: 0:08:49  lr: 0.000023  loss: 0.3411  time: 1.5415  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2655/3000]  eta: 0:08:42  lr: 0.000023  loss: 0.5252  time: 1.5399  data: 0.0000  max mem: 18596Train: data epoch: [11]  [2655/3000]  eta: 0:08:42  lr: 0.000023  loss: 0.2359  time: 1.5395  data: 0.0000  max mem: 18603

Train: data epoch: [11]  [2660/3000]  eta: 0:08:34  lr: 0.000023  loss: 0.3722  time: 1.5374  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2660/3000]  eta: 0:08:34  lr: 0.000023  loss: 0.0931  time: 1.5372  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2665/3000]  eta: 0:08:27  lr: 0.000023  loss: 0.3501  time: 1.5275  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2665/3000]  eta: 0:08:27  lr: 0.000023  loss: 0.3080  time: 1.5272  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2670/3000]  eta: 0:08:19  lr: 0.000023  loss: 0.2571  time: 1.5012  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2670/3000]  eta: 0:08:19  lr: 0.000023  loss: 0.1792  time: 1.5010  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2675/3000]  eta: 0:08:12  lr: 0.000023  loss: 1.0474  time: 1.5018  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2675/3000]  eta: 0:08:12  lr: 0.000023  loss: 0.5914  time: 1.5015  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2680/3000]  eta: 0:08:04  lr: 0.000023  loss: 0.1761  time: 1.4905  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2680/3000]  eta: 0:08:04  lr: 0.000023  loss: 0.0971  time: 1.4901  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2685/3000]  eta: 0:07:57  lr: 0.000023  loss: 0.4927  time: 1.4939  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2685/3000]  eta: 0:07:56  lr: 0.000023  loss: 0.5417  time: 1.4935  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2690/3000]  eta: 0:07:49  lr: 0.000023  loss: 0.4007  time: 1.5066  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2690/3000]  eta: 0:07:49  lr: 0.000023  loss: 0.1547  time: 1.5063  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2695/3000]  eta: 0:07:41  lr: 0.000023  loss: 0.1889  time: 1.5054  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2695/3000]  eta: 0:07:41  lr: 0.000023  loss: 0.3328  time: 1.5051  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2700/3000]  eta: 0:07:34  lr: 0.000023  loss: 0.1796  time: 1.5111  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2700/3000]  eta: 0:07:34  lr: 0.000023  loss: 0.3758  time: 1.5109  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2705/3000]  eta: 0:07:26  lr: 0.000023  loss: 0.3219  time: 1.5217  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2705/3000]  eta: 0:07:26  lr: 0.000023  loss: 0.1472  time: 1.5212  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2710/3000]  eta: 0:07:19  lr: 0.000023  loss: 0.0472  time: 1.5384  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2710/3000]  eta: 0:07:19  lr: 0.000023  loss: 0.3036  time: 1.5379  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2715/3000]  eta: 0:07:11  lr: 0.000023  loss: 0.4142  time: 1.5423  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2715/3000]  eta: 0:07:11  lr: 0.000023  loss: 0.2752  time: 1.5417  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2720/3000]  eta: 0:07:04  lr: 0.000023  loss: 0.2679  time: 1.5469  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2720/3000]  eta: 0:07:03  lr: 0.000023  loss: 0.6322  time: 1.5464  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2725/3000]  eta: 0:06:56  lr: 0.000023  loss: 0.3774  time: 1.5353  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2725/3000]  eta: 0:06:56  lr: 0.000023  loss: 0.2307  time: 1.5351  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2730/3000]  eta: 0:06:48  lr: 0.000023  loss: 0.4966  time: 1.5405  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2730/3000]  eta: 0:06:48  lr: 0.000023  loss: 0.2462  time: 1.5401  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2735/3000]  eta: 0:06:41  lr: 0.000023  loss: 0.2085  time: 1.5409  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2735/3000]  eta: 0:06:41  lr: 0.000023  loss: 0.3310  time: 1.5406  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2740/3000]  eta: 0:06:33  lr: 0.000023  loss: 0.2723  time: 1.5324  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2740/3000]  eta: 0:06:33  lr: 0.000023  loss: 0.5875  time: 1.5321  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2745/3000]  eta: 0:06:26  lr: 0.000023  loss: 0.9086  time: 1.5203  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2745/3000]  eta: 0:06:26  lr: 0.000023  loss: 0.2126  time: 1.5200  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2750/3000]  eta: 0:06:18  lr: 0.000023  loss: 0.1657  time: 1.5053  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2750/3000]  eta: 0:06:18  lr: 0.000023  loss: 0.3456  time: 1.5052  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2755/3000]  eta: 0:06:11  lr: 0.000023  loss: 0.4734  time: 1.4794  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2755/3000]  eta: 0:06:10  lr: 0.000023  loss: 0.3963  time: 1.4793  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2760/3000]  eta: 0:06:03  lr: 0.000023  loss: 0.3208  time: 1.4944  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2760/3000]  eta: 0:06:03  lr: 0.000023  loss: 0.4346  time: 1.4940  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2765/3000]  eta: 0:05:55  lr: 0.000023  loss: 0.3217  time: 1.4901  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2765/3000]  eta: 0:05:55  lr: 0.000023  loss: 0.7183  time: 1.4898  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2770/3000]  eta: 0:05:48  lr: 0.000023  loss: 0.3639  time: 1.4740  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2770/3000]  eta: 0:05:48  lr: 0.000023  loss: 0.7199  time: 1.4735  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2775/3000]  eta: 0:05:40  lr: 0.000023  loss: 0.4185  time: 1.4668  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2775/3000]  eta: 0:05:40  lr: 0.000023  loss: 0.4123  time: 1.4664  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2780/3000]  eta: 0:05:33  lr: 0.000023  loss: 0.5537  time: 1.4796  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2780/3000]  eta: 0:05:33  lr: 0.000023  loss: 0.6324  time: 1.4793  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2785/3000]  eta: 0:05:25  lr: 0.000023  loss: 0.1626  time: 1.4937  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2785/3000]  eta: 0:05:25  lr: 0.000023  loss: 0.2262  time: 1.4933  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2790/3000]  eta: 0:05:17  lr: 0.000023  loss: 0.4653  time: 1.4972  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2790/3000]  eta: 0:05:17  lr: 0.000023  loss: 0.2548  time: 1.4970  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2795/3000]  eta: 0:05:10  lr: 0.000023  loss: 0.4006  time: 1.5284  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2795/3000]  eta: 0:05:10  lr: 0.000023  loss: 0.9794  time: 1.5281  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2800/3000]  eta: 0:05:02  lr: 0.000023  loss: 0.3631  time: 1.5004  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2800/3000]  eta: 0:05:02  lr: 0.000023  loss: 0.3110  time: 1.5001  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2805/3000]  eta: 0:04:55  lr: 0.000023  loss: 0.4697  time: 1.5126  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2805/3000]  eta: 0:04:55  lr: 0.000023  loss: 0.1767  time: 1.5123  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2810/3000]  eta: 0:04:47  lr: 0.000023  loss: 0.4849  time: 1.5306  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2810/3000]  eta: 0:04:47  lr: 0.000023  loss: 0.5130  time: 1.5302  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2815/3000]  eta: 0:04:40  lr: 0.000023  loss: 0.8696  time: 1.5202  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2815/3000]  eta: 0:04:40  lr: 0.000023  loss: 0.8638  time: 1.5200  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2820/3000]  eta: 0:04:32  lr: 0.000023  loss: 0.3219  time: 1.5286  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2820/3000]  eta: 0:04:32  lr: 0.000023  loss: 0.1270  time: 1.5283  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2825/3000]  eta: 0:04:25  lr: 0.000023  loss: 0.3667  time: 1.5151  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2825/3000]  eta: 0:04:24  lr: 0.000023  loss: 0.0527  time: 1.5147  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2830/3000]  eta: 0:04:17  lr: 0.000023  loss: 0.0264  time: 1.5216  data: 0.0000  max mem: 18603Train: data epoch: [11]  [2830/3000]  eta: 0:04:17  lr: 0.000023  loss: 0.4450  time: 1.5219  data: 0.0000  max mem: 18596

Train: data epoch: [11]  [2835/3000]  eta: 0:04:09  lr: 0.000023  loss: 0.7109  time: 1.5431  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2835/3000]  eta: 0:04:09  lr: 0.000023  loss: 0.5040  time: 1.5428  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2840/3000]  eta: 0:04:02  lr: 0.000023  loss: 0.2861  time: 1.5583  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2840/3000]  eta: 0:04:02  lr: 0.000023  loss: 0.5248  time: 1.5580  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2845/3000]  eta: 0:03:54  lr: 0.000023  loss: 0.2023  time: 1.5542  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2845/3000]  eta: 0:03:54  lr: 0.000023  loss: 0.5742  time: 1.5539  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2850/3000]  eta: 0:03:47  lr: 0.000023  loss: 0.5983  time: 1.5627  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2850/3000]  eta: 0:03:47  lr: 0.000023  loss: 0.3038  time: 1.5625  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2855/3000]  eta: 0:03:39  lr: 0.000023  loss: 0.0524  time: 1.5608  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2855/3000]  eta: 0:03:39  lr: 0.000023  loss: 0.2703  time: 1.5605  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2860/3000]  eta: 0:03:32  lr: 0.000023  loss: 0.4067  time: 1.5254  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2860/3000]  eta: 0:03:32  lr: 0.000023  loss: 0.3664  time: 1.5249  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2865/3000]  eta: 0:03:24  lr: 0.000023  loss: 0.4184  time: 1.5272  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2865/3000]  eta: 0:03:24  lr: 0.000023  loss: 0.4786  time: 1.5267  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2870/3000]  eta: 0:03:16  lr: 0.000023  loss: 1.2789  time: 1.5103  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2870/3000]  eta: 0:03:16  lr: 0.000023  loss: 0.5605  time: 1.5098  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2875/3000]  eta: 0:03:09  lr: 0.000023  loss: 0.0462  time: 1.4922  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2875/3000]  eta: 0:03:09  lr: 0.000023  loss: 0.4303  time: 1.4917  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2880/3000]  eta: 0:03:01  lr: 0.000023  loss: 0.1517  time: 1.5155  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2880/3000]  eta: 0:03:01  lr: 0.000023  loss: 0.9305  time: 1.5152  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2885/3000]  eta: 0:02:54  lr: 0.000023  loss: 0.5746  time: 1.4977  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2885/3000]  eta: 0:02:54  lr: 0.000023  loss: 0.0809  time: 1.4974  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2890/3000]  eta: 0:02:46  lr: 0.000023  loss: 0.3713  time: 1.4897  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2890/3000]  eta: 0:02:46  lr: 0.000023  loss: 1.0417  time: 1.4894  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2895/3000]  eta: 0:02:39  lr: 0.000023  loss: 0.1510  time: 1.4955  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2895/3000]  eta: 0:02:38  lr: 0.000023  loss: 0.2523  time: 1.4941  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2900/3000]  eta: 0:02:31  lr: 0.000023  loss: 0.7111  time: 1.4909  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2900/3000]  eta: 0:02:31  lr: 0.000023  loss: 0.5576  time: 1.4895  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2905/3000]  eta: 0:02:23  lr: 0.000023  loss: 0.7058  time: 1.5259  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2905/3000]  eta: 0:02:23  lr: 0.000023  loss: 0.5802  time: 1.5245  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2910/3000]  eta: 0:02:16  lr: 0.000023  loss: 0.4749  time: 1.5282  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2910/3000]  eta: 0:02:16  lr: 0.000023  loss: 0.6324  time: 1.5267  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2915/3000]  eta: 0:02:08  lr: 0.000023  loss: 0.5710  time: 1.5252  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2915/3000]  eta: 0:02:08  lr: 0.000023  loss: 0.2986  time: 1.5249  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2920/3000]  eta: 0:02:01  lr: 0.000023  loss: 0.5081  time: 1.5236  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2920/3000]  eta: 0:02:01  lr: 0.000023  loss: 0.1038  time: 1.5231  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2925/3000]  eta: 0:01:53  lr: 0.000023  loss: 0.4564  time: 1.5207  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2925/3000]  eta: 0:01:53  lr: 0.000023  loss: 0.1450  time: 1.5203  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2930/3000]  eta: 0:01:46  lr: 0.000023  loss: 0.3308  time: 1.5052  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2930/3000]  eta: 0:01:45  lr: 0.000023  loss: 0.4536  time: 1.5049  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2935/3000]  eta: 0:01:38  lr: 0.000023  loss: 0.1915  time: 1.4979  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2935/3000]  eta: 0:01:38  lr: 0.000023  loss: 0.4713  time: 1.4975  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2940/3000]  eta: 0:01:30  lr: 0.000023  loss: 0.3058  time: 1.5226  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2940/3000]  eta: 0:01:30  lr: 0.000023  loss: 0.4034  time: 1.5225  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2945/3000]  eta: 0:01:23  lr: 0.000023  loss: 0.9523  time: 1.5215  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2945/3000]  eta: 0:01:23  lr: 0.000023  loss: 0.3572  time: 1.5213  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2950/3000]  eta: 0:01:15  lr: 0.000023  loss: 0.2198  time: 1.5429  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2950/3000]  eta: 0:01:15  lr: 0.000023  loss: 0.5985  time: 1.5426  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2955/3000]  eta: 0:01:08  lr: 0.000023  loss: 1.0305  time: 1.5420  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2955/3000]  eta: 0:01:08  lr: 0.000023  loss: 0.5504  time: 1.5418  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2960/3000]  eta: 0:01:00  lr: 0.000023  loss: 0.4012  time: 1.5268  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2960/3000]  eta: 0:01:00  lr: 0.000023  loss: 0.4477  time: 1.5266  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2965/3000]  eta: 0:00:53  lr: 0.000023  loss: 0.0909  time: 1.5074  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2965/3000]  eta: 0:00:53  lr: 0.000023  loss: 0.0758  time: 1.5071  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2970/3000]  eta: 0:00:45  lr: 0.000023  loss: 0.3635  time: 1.5163  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2970/3000]  eta: 0:00:45  lr: 0.000023  loss: 0.0910  time: 1.5161  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2975/3000]  eta: 0:00:37  lr: 0.000023  loss: 0.5769  time: 1.5155  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2975/3000]  eta: 0:00:37  lr: 0.000023  loss: 0.4068  time: 1.5152  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2980/3000]  eta: 0:00:30  lr: 0.000023  loss: 0.8822  time: 1.5026  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2980/3000]  eta: 0:00:30  lr: 0.000023  loss: 0.5771  time: 1.5022  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2985/3000]  eta: 0:00:22  lr: 0.000023  loss: 0.4390  time: 1.5078  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2985/3000]  eta: 0:00:22  lr: 0.000023  loss: 0.7075  time: 1.5076  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2990/3000]  eta: 0:00:15  lr: 0.000023  loss: 0.3733  time: 1.5156  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2990/3000]  eta: 0:00:15  lr: 0.000023  loss: 0.1929  time: 1.5159  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2995/3000]  eta: 0:00:07  lr: 0.000023  loss: 0.5126  time: 1.5229  data: 0.0000  max mem: 18596
Train: data epoch: [11]  [2995/3000]  eta: 0:00:07  lr: 0.000023  loss: 0.5835  time: 1.5227  data: 0.0000  max mem: 18603
Train: data epoch: [11]  [2999/3000]  eta: 0:00:01  lr: 0.000023  loss: 0.5083  time: 1.5270  data: 0.0000  max mem: 18596
Train: data epoch: [11] Total time: 1:15:44 (1.5149 s / it)
Train: data epoch: [11]  [2999/3000]  eta: 0:00:01  lr: 0.000023  loss: 0.2948  time: 1.5267  data: 0.0000  max mem: 18603
Train: data epoch: [11] Total time: 1:15:44 (1.5149 s / it)
2025-01-19 14:49:58,042 [INFO] Averaged stats: lr: 0.0000  loss: 0.4169
2025-01-19 14:49:58,049 [INFO] Validating Phase
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Eval: data epoch: [11]  [0/1]  eta: 0:00:00    time: 0.8201  data: 0.5297  max mem: 18603
Eval: data epoch: [11] Total time: 0:00:00 (0.9776 s / it)
Eval: data epoch: [11]  [0/1]  eta: 0:00:00    time: 0.9831  data: 0.6945  max mem: 18596
Eval: data epoch: [11] Total time: 0:00:01 (1.1651 s / it)
2025-01-19 14:49:59,240 [INFO] Saving checkpoint at epoch 11 to outputs_stage1_only/202501182338/checkpoint_11.pth.
2025-01-19 14:50:01,675 [INFO] Training Phase
2025-01-19 14:50:01,683 [INFO] Start training epoch 12, 3000 iters per inner epoch.
Train: data epoch: [12]  [   0/3000]  eta: 1:09:09  lr: 0.000023  loss: 0.5237  time: 1.3833  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [   0/3000]  eta: 1:09:11  lr: 0.000023  loss: 0.2041  time: 1.3839  data: 0.0000  max mem: 18603
Train: data epoch: [12]  [   5/3000]  eta: 1:16:29  lr: 0.000023  loss: 0.4498  time: 1.5324  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [   5/3000]  eta: 1:16:29  lr: 0.000023  loss: 0.4663  time: 1.5325  data: 0.0000  max mem: 18603
Train: data epoch: [12]  [  10/3000]  eta: 1:16:28  lr: 0.000023  loss: 0.6111  time: 1.5345  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [  10/3000]  eta: 1:16:26  lr: 0.000023  loss: 0.2885  time: 1.5340  data: 0.0000  max mem: 18603
Train: data epoch: [12]  [  15/3000]  eta: 1:15:55  lr: 0.000023  loss: 0.2308  time: 1.5260  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [  15/3000]  eta: 1:15:53  lr: 0.000023  loss: 0.3803  time: 1.5256  data: 0.0000  max mem: 18603
Train: data epoch: [12]  [  20/3000]  eta: 1:15:57  lr: 0.000023  loss: 0.7433  time: 1.5365  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [  20/3000]  eta: 1:15:55  lr: 0.000023  loss: 0.1900  time: 1.5360  data: 0.0000  max mem: 18603
Train: data epoch: [12]  [  25/3000]  eta: 1:15:44  lr: 0.000023  loss: 0.5737  time: 1.5262  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [  25/3000]  eta: 1:15:43  lr: 0.000023  loss: 0.4428  time: 1.5255  data: 0.0000  max mem: 18603
Train: data epoch: [12]  [  30/3000]  eta: 1:15:46  lr: 0.000023  loss: 0.5305  time: 1.5287  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [  30/3000]  eta: 1:15:45  lr: 0.000023  loss: 0.5113  time: 1.5284  data: 0.0000  max mem: 18603
Train: data epoch: [12]  [  35/3000]  eta: 1:16:03  lr: 0.000023  loss: 0.2291  time: 1.5494  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [  35/3000]  eta: 1:16:01  lr: 0.000023  loss: 0.2582  time: 1.5489  data: 0.0000  max mem: 18603
Train: data epoch: [12]  [  40/3000]  eta: 1:16:00  lr: 0.000023  loss: 0.2384  time: 1.5530  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [  40/3000]  eta: 1:15:59  lr: 0.000023  loss: 0.5259  time: 1.5525  data: 0.0000  max mem: 18603
Train: data epoch: [12]  [  45/3000]  eta: 1:15:58  lr: 0.000023  loss: 0.7326  time: 1.5624  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [  45/3000]  eta: 1:15:57  lr: 0.000023  loss: 0.7771  time: 1.5619  data: 0.0000  max mem: 18603
Train: data epoch: [12]  [  50/3000]  eta: 1:15:53  lr: 0.000023  loss: 0.8122  time: 1.5634  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [  50/3000]  eta: 1:15:52  lr: 0.000023  loss: 0.2416  time: 1.5629  data: 0.0000  max mem: 18603
Train: data epoch: [12]  [  55/3000]  eta: 1:15:18  lr: 0.000023  loss: 0.3365  time: 1.5257  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [  55/3000]  eta: 1:15:16  lr: 0.000023  loss: 0.6222  time: 1.5252  data: 0.0000  max mem: 18603
Train: data epoch: [12]  [  60/3000]  eta: 1:15:06  lr: 0.000023  loss: 0.1918  time: 1.5169  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [  60/3000]  eta: 1:15:05  lr: 0.000023  loss: 0.2721  time: 1.5167  data: 0.0000  max mem: 18603
Train: data epoch: [12]  [  65/3000]  eta: 1:15:06  lr: 0.000023  loss: 0.5768  time: 1.5187  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [  65/3000]  eta: 1:15:05  lr: 0.000023  loss: 0.6806  time: 1.5184  data: 0.0000  max mem: 18603
Train: data epoch: [12]  [  70/3000]  eta: 1:14:42  lr: 0.000023  loss: 0.8782  time: 1.4952  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [  70/3000]  eta: 1:14:41  lr: 0.000023  loss: 0.2037  time: 1.4949  data: 0.0000  max mem: 18603
Train: data epoch: [12]  [  75/3000]  eta: 1:14:33  lr: 0.000023  loss: 0.2741  time: 1.5154  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [  75/3000]  eta: 1:14:31  lr: 0.000023  loss: 0.2779  time: 1.5152  data: 0.0000  max mem: 18603
Train: data epoch: [12]  [  80/3000]  eta: 1:14:28  lr: 0.000023  loss: 0.2421  time: 1.5227  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [  80/3000]  eta: 1:14:27  lr: 0.000023  loss: 0.6799  time: 1.5224  data: 0.0000  max mem: 18603
Train: data epoch: [12]  [  85/3000]  eta: 1:14:16  lr: 0.000023  loss: 0.2101  time: 1.5071  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [  85/3000]  eta: 1:14:15  lr: 0.000023  loss: 0.5710  time: 1.5069  data: 0.0000  max mem: 18603
Train: data epoch: [12]  [  90/3000]  eta: 1:14:05  lr: 0.000023  loss: 0.2838  time: 1.5201  data: 0.0000  max mem: 18596
Train: data epoch: [12]  [  90/3000]  eta: 1:14:04  lr: 0.000023  loss: 0.5739  time: 1.5199  data: 0.0000  max mem: 18603
W0119 14:52:23.676004 999961 site-packages/torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGINT death signal, shutting down workers
W0119 14:52:23.677349 999961 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1000010 closing signal SIGINT
W0119 14:52:23.677674 999961 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1000011 closing signal SIGINT
Traceback (most recent call last):
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 105, in <module>
    main()
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 101, in main
    runner.train()
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py", line 302, in train
    train_stats = self.train_epoch(cur_epoch)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py", line 130, in train_epoch
    self.scaler.scale(loss).backward()
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 105, in <module>
[rank0]:     main()
[rank0]:   File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 101, in main
[rank0]:     runner.train()
[rank0]:   File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py", line 302, in train
[rank0]:     train_stats = self.train_epoch(cur_epoch)
[rank0]:   File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py", line 130, in train_epoch
[rank0]:     self.scaler.scale(loss).backward()
[rank0]:   File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/_tensor.py", line 581, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: KeyboardInterrupt
[rank1]: Traceback (most recent call last):
[rank1]:   File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 105, in <module>
[rank1]:     main()
[rank1]:   File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/train.py", line 101, in main
[rank1]:     runner.train()
[rank1]:   File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py", line 302, in train
[rank1]:     train_stats = self.train_epoch(cur_epoch)
[rank1]:   File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/utils/runner.py", line 130, in train_epoch
[rank1]:     self.scaler.scale(loss).backward()
[rank1]:   File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/_tensor.py", line 581, in backward
[rank1]:     torch.autograd.backward(
[rank1]:   File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]: KeyboardInterrupt
[1;34mwandb[0m: üöÄ View run [33mllama-3.2-1B-1percent-stage1[0m at: [34mhttps://wandb.ai/kihoon090/audio_lm/runs/qklhlv91[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250118_233842-qklhlv91/logs[0m
Traceback (most recent call last):
  File "/data/anaconda3/envs/salmonn_env/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 260, in launch_agent
    result = agent.run()
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 696, in run
    result = self._invoke_run(role)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 855, in _invoke_run
    time.sleep(monitor_interval)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 999961 got signal: 2
Force batch size as 1
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:14<00:14, 14.73s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:19<00:00,  8.70s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:19<00:00,  9.60s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
Loading training prompts done!
Traceback (most recent call last):
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_efficiency_salmonn.py", line 230, in <module>
    main(args)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_efficiency_salmonn.py", line 176, in main
    salmonn_preprocessor = load_preprocessor(cfg)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_efficiency_salmonn.py", line 35, in load_preprocessor
    salmonn_preprocessor.to(cfg.config.run.device)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1340, in to
    return self._apply(convert)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 927, in _apply
    param_applied = fn(param)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1326, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 18.38 MiB is free. Including non-PyTorch memory, this process has 7.09 GiB memory in use. Process 1049882 has 24.63 GiB memory in use. Of the allocated memory 6.72 GiB is allocated by PyTorch, and 65.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Force batch size as 1
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:03<00:03,  3.22s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  1.91s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.11s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
Loading training prompts done!
  0%|          | 0/110 [00:00<?, ?it/s]  1%|          | 1/110 [00:01<02:10,  1.19s/it]  2%|‚ñè         | 2/110 [00:01<01:34,  1.14it/s]  3%|‚ñé         | 3/110 [00:02<01:24,  1.27it/s]  4%|‚ñé         | 4/110 [00:03<01:17,  1.36it/s]  5%|‚ñç         | 5/110 [00:03<01:14,  1.41it/s]  5%|‚ñå         | 6/110 [00:04<01:12,  1.44it/s]  6%|‚ñã         | 7/110 [00:05<01:10,  1.46it/s]  7%|‚ñã         | 8/110 [00:05<01:09,  1.46it/s]  8%|‚ñä         | 9/110 [00:06<01:08,  1.48it/s]  9%|‚ñâ         | 10/110 [00:07<01:08,  1.45it/s] 10%|‚ñà         | 11/110 [00:07<01:07,  1.46it/s] 11%|‚ñà         | 12/110 [00:08<01:06,  1.47it/s] 12%|‚ñà‚ñè        | 13/110 [00:09<01:05,  1.47it/s] 13%|‚ñà‚ñé        | 14/110 [00:09<01:05,  1.47it/s] 14%|‚ñà‚ñé        | 15/110 [00:10<01:04,  1.47it/s] 15%|‚ñà‚ñç        | 16/110 [00:11<01:03,  1.47it/s] 15%|‚ñà‚ñå        | 17/110 [00:11<01:03,  1.47it/s] 16%|‚ñà‚ñã        | 18/110 [00:12<01:02,  1.47it/s] 17%|‚ñà‚ñã        | 19/110 [00:13<01:02,  1.46it/s] 18%|‚ñà‚ñä        | 20/110 [00:14<01:01,  1.46it/s] 19%|‚ñà‚ñâ        | 21/110 [00:14<01:00,  1.47it/s] 20%|‚ñà‚ñà        | 22/110 [00:15<00:59,  1.48it/s] 21%|‚ñà‚ñà        | 23/110 [00:16<00:58,  1.49it/s] 22%|‚ñà‚ñà‚ñè       | 24/110 [00:16<00:57,  1.49it/s] 23%|‚ñà‚ñà‚ñé       | 25/110 [00:17<00:55,  1.53it/s] 24%|‚ñà‚ñà‚ñé       | 26/110 [00:18<00:55,  1.51it/s] 25%|‚ñà‚ñà‚ñç       | 27/110 [00:18<00:54,  1.51it/s] 25%|‚ñà‚ñà‚ñå       | 28/110 [00:19<00:54,  1.49it/s] 26%|‚ñà‚ñà‚ñã       | 29/110 [00:20<00:55,  1.46it/s] 27%|‚ñà‚ñà‚ñã       | 30/110 [00:20<00:55,  1.44it/s] 28%|‚ñà‚ñà‚ñä       | 31/110 [00:21<00:55,  1.43it/s] 29%|‚ñà‚ñà‚ñâ       | 32/110 [00:22<00:53,  1.45it/s] 30%|‚ñà‚ñà‚ñà       | 33/110 [00:22<00:52,  1.48it/s] 31%|‚ñà‚ñà‚ñà       | 34/110 [00:23<00:51,  1.49it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 35/110 [00:24<00:50,  1.49it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 36/110 [00:24<00:49,  1.49it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 37/110 [00:25<00:49,  1.48it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 38/110 [00:26<00:49,  1.47it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 39/110 [00:26<00:48,  1.47it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 40/110 [00:27<00:47,  1.46it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 41/110 [00:28<00:47,  1.45it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 42/110 [00:28<00:47,  1.44it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 43/110 [00:29<00:45,  1.46it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 44/110 [00:30<00:45,  1.47it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 45/110 [00:30<00:44,  1.48it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 46/110 [00:31<00:43,  1.48it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 47/110 [00:32<00:42,  1.48it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 48/110 [00:33<00:41,  1.48it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 49/110 [00:33<00:41,  1.46it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 50/110 [00:34<00:41,  1.46it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 51/110 [00:35<00:40,  1.46it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 52/110 [00:35<00:39,  1.46it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 53/110 [00:36<00:38,  1.48it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 54/110 [00:37<00:38,  1.47it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 55/110 [00:37<00:37,  1.47it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 56/110 [00:38<00:36,  1.48it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 57/110 [00:39<00:36,  1.47it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 58/110 [00:39<00:35,  1.47it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 59/110 [00:40<00:34,  1.48it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 60/110 [00:41<00:34,  1.46it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 61/110 [00:41<00:32,  1.49it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 62/110 [00:42<00:31,  1.50it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 63/110 [00:43<00:30,  1.52it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 64/110 [00:43<00:31,  1.46it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 65/110 [00:44<00:30,  1.46it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 66/110 [00:45<00:29,  1.48it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 67/110 [00:45<00:29,  1.48it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 68/110 [00:46<00:28,  1.48it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 69/110 [00:47<00:27,  1.49it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 70/110 [00:47<00:26,  1.49it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 71/110 [00:48<00:26,  1.48it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 72/110 [00:49<00:25,  1.48it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 73/110 [00:49<00:25,  1.47it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 74/110 [00:50<00:24,  1.48it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 75/110 [00:51<00:23,  1.47it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 76/110 [00:51<00:23,  1.47it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 77/110 [00:52<00:22,  1.48it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 78/110 [00:53<00:22,  1.45it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 79/110 [00:54<00:21,  1.47it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 80/110 [00:54<00:20,  1.48it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 81/110 [00:55<00:19,  1.47it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 82/110 [00:56<00:19,  1.45it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 83/110 [00:56<00:18,  1.47it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 84/110 [00:57<00:17,  1.47it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 85/110 [00:58<00:16,  1.48it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 86/110 [00:58<00:16,  1.49it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 87/110 [00:59<00:15,  1.49it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 88/110 [01:00<00:14,  1.50it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 89/110 [01:00<00:14,  1.49it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 90/110 [01:01<00:13,  1.49it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 91/110 [01:02<00:12,  1.48it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 92/110 [01:02<00:12,  1.49it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 93/110 [01:03<00:11,  1.49it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 94/110 [01:04<00:10,  1.48it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 95/110 [01:04<00:10,  1.49it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 96/110 [01:05<00:09,  1.51it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 97/110 [01:06<00:08,  1.50it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 98/110 [01:06<00:08,  1.50it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 99/110 [01:07<00:07,  1.48it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 100/110 [01:08<00:06,  1.43it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 101/110 [01:08<00:06,  1.46it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 102/110 [01:09<00:05,  1.48it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 103/110 [01:10<00:04,  1.48it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 104/110 [01:10<00:04,  1.48it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 105/110 [01:11<00:03,  1.49it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 106/110 [01:12<00:02,  1.50it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 107/110 [01:12<00:02,  1.49it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 108/110 [01:13<00:01,  1.49it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 109/110 [01:14<00:00,  1.50it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 110/110 [01:14<00:00,  1.50it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 110/110 [01:14<00:00,  1.47it/s]
Average memory used during inference: 9.1761 GB
Average inference time: 0.4762 seconds
Average TTFT: 0.4381 seconds
Average TPOT: 0.0380 seconds
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:03<00:03,  3.07s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  1.91s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.08s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
trainable params: 2293760 || all params: 3215046656 || trainable%: 0.0713445323015555
Loading training prompts done!
  0%|          | 0/368 [00:00<?, ?it/s]/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  0%|          | 1/368 [00:06<42:46,  6.99s/it]  1%|          | 2/368 [00:10<30:55,  5.07s/it]  1%|          | 3/368 [00:14<27:35,  4.54s/it]  1%|          | 4/368 [00:18<26:59,  4.45s/it]  1%|‚ñè         | 5/368 [00:23<26:27,  4.37s/it]  2%|‚ñè         | 6/368 [00:27<27:18,  4.53s/it]  2%|‚ñè         | 7/368 [00:31<25:15,  4.20s/it]  2%|‚ñè         | 8/368 [00:34<23:36,  3.93s/it]  2%|‚ñè         | 9/368 [00:38<22:11,  3.71s/it]  3%|‚ñé         | 10/368 [00:41<21:29,  3.60s/it]  3%|‚ñé         | 11/368 [00:44<20:17,  3.41s/it]  3%|‚ñé         | 12/368 [00:48<20:44,  3.49s/it]  4%|‚ñé         | 13/368 [00:52<21:56,  3.71s/it]  4%|‚ñç         | 14/368 [00:56<21:49,  3.70s/it]  4%|‚ñç         | 15/368 [00:59<20:49,  3.54s/it]  4%|‚ñç         | 16/368 [01:02<21:09,  3.61s/it]  5%|‚ñç         | 17/368 [01:06<21:12,  3.62s/it]  5%|‚ñç         | 18/368 [01:09<19:31,  3.35s/it]  5%|‚ñå         | 19/368 [01:12<19:05,  3.28s/it]  5%|‚ñå         | 20/368 [01:15<18:07,  3.12s/it]  6%|‚ñå         | 21/368 [01:18<17:44,  3.07s/it]  6%|‚ñå         | 22/368 [01:21<18:44,  3.25s/it]  6%|‚ñã         | 23/368 [01:25<18:46,  3.26s/it]  7%|‚ñã         | 24/368 [01:28<18:38,  3.25s/it]  7%|‚ñã         | 25/368 [01:33<21:25,  3.75s/it]  7%|‚ñã         | 26/368 [01:36<20:15,  3.56s/it]  7%|‚ñã         | 27/368 [01:39<20:22,  3.58s/it]  8%|‚ñä         | 28/368 [01:44<21:42,  3.83s/it]  8%|‚ñä         | 29/368 [01:48<22:31,  3.99s/it]  8%|‚ñä         | 30/368 [01:55<26:43,  4.74s/it]  8%|‚ñä         | 31/368 [02:00<26:58,  4.80s/it]  9%|‚ñä         | 32/368 [02:05<28:08,  5.03s/it]  9%|‚ñâ         | 33/368 [02:10<26:50,  4.81s/it]  9%|‚ñâ         | 34/368 [02:13<25:15,  4.54s/it] 10%|‚ñâ         | 35/368 [02:16<22:20,  4.03s/it] 10%|‚ñâ         | 36/368 [02:19<20:51,  3.77s/it] 10%|‚ñà         | 37/368 [02:22<18:42,  3.39s/it] 10%|‚ñà         | 37/368 [02:23<21:24,  3.88s/it]
Traceback (most recent call last):
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 184, in <module>
    main(args)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/evaluate_salmonn.py", line 119, in main
    speech_embeds, speech_atts = salmonn_preprocessor.encode_speech(spectrogram, raw_wav=raw_wav, audio_padding_mask=audio_padding_mask)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/models/salmonn.py", line 279, in encode_speech
    audio_embeds, _ = self.beats.extract_features(raw_wav, padding_mask=audio_padding_mask, feature_only=True)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/models/beats/BEATs.py", line 141, in extract_features
    fbank = self.preprocess(source, fbank_mean=fbank_mean, fbank_std=fbank_std).to(torch.float32)
  File "/data/yh/level4-cv-finalproject-hackathon-cv-18-lv3/models/beats/BEATs.py", line 127, in preprocess
    fbank = ta_kaldi.fbank(waveform, num_mel_bins=128, sample_frequency=16000, frame_length=25, frame_shift=10)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torchaudio/compliance/kaldi.py", line 621, in fbank
    mel_energies, _ = get_mel_banks(
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torchaudio/compliance/kaldi.py", line 492, in get_mel_banks
    center_freqs = inverse_mel_scale(center_mel)  # size (num_bins)
  File "/data/anaconda3/envs/salmonn_env/lib/python3.9/site-packages/torchaudio/compliance/kaldi.py", line 323, in inverse_mel_scale
    return 700.0 * ((mel_freq / 1127.0).exp() - 1.0)
KeyboardInterrupt
